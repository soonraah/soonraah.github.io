<!doctype html><html lang=ja dir=auto><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-NSEGH2YT17"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NSEGH2YT17")</script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる | Froglog</title>
<meta name=keywords content="dlt,ELT,data pipeline,BigQuery"><meta name=description content="このポストについて 前回の記事 dlt 入門 - ELT の Extract と Load を担う data load tool では dlt の概要を説明した。
この記事ではそれを踏まえ、dlt を使って CSV ファイルを BigQuery に load するという一連の開発作業をやってみる。
現実の CSV はそのまま使えなかったりするので、BigQuery に入れるまでに泥臭い前処理のような作業があることが多い。
そのへんもまとめて dlt でやってみるとこんな感じ、というのが示せるとよい。
やりたいこと 個人で管理しているお金の情報を個人用の BigQuery に置きたい、という要件を想定。
データ概要 具体的には MoneyForward のデータを load していく。
個人では API を利用できないので、web UI から export できる CSV のデータで収入・支出詳細と資産推移月次を対象とする。
CSV の export 方法は以下を参照。
入出金履歴はダウンロードできますか – マネーフォワード MEサポートサイト データの内容は次のようになっている。
収入・支出詳細_2023-11-01_2023-11-30.csv &#34;計算対象&#34;,&#34;日付&#34;,&#34;内容&#34;,&#34;金額（円）&#34;,&#34;保有金融機関&#34;,&#34;大項目&#34;,&#34;中項目&#34;,&#34;メモ&#34;,&#34;振替&#34;,&#34;ID&#34; &#34;1&#34;,&#34;2023/11/30&#34;,&#34;AMAZON.CO.JP&#34;,&#34;-2830&#34;,&#34;楽天カード&#34;,&#34;食費&#34;,&#34;食料品&#34;,&#34;&#34;,&#34;0&#34;,&#34;EPv92ZjQcOxgWQx_cLbhD1&#34; &#34;1&#34;,&#34;2023/11/24&#34;,&#34;東京ガス&#34;,&#34;-4321&#34;,&#34;楽天カード&#34;,&#34;水道・光熱費&#34;,&#34;ガス・灯油代&#34;,&#34;&#34;,&#34;0&#34;,&#34;r6wuQPfrIRS6aFpNYZE5Eh&#34; &#34;1&#34;,&#34;2023/11/24&#34;,&#34;給与 カ) フロッグログ&#34;,&#34;700000&#34;,&#34;みずほ銀行&#34;,&#34;収入&#34;,&#34;給与&#34;,&#34;&#34;,&#34;0&#34;,&#34;doettKpYyNp0Tml9KQQXm1&#34; ヘッダーがあり、各列に名前が付いている。
encoding が CP932 であることに注意。"><meta name=author content="soonraah"><link rel=canonical href=https://soonraah.github.io/posts/load-csv-data-into-bq-by-dlt/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://soonraah.github.io/favicon2.ico><link rel=icon type=image/png sizes=16x16 href=https://soonraah.github.io/image/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://soonraah.github.io/image/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://soonraah.github.io/static/image/favicon/apple-touch-icon.png><link rel=mask-icon href=https://soonraah.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-73329599-2","auto"),ga("send","pageview"))</script><meta property="og:title" content="現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる"><meta property="og:description" content="このポストについて 前回の記事 dlt 入門 - ELT の Extract と Load を担う data load tool では dlt の概要を説明した。
この記事ではそれを踏まえ、dlt を使って CSV ファイルを BigQuery に load するという一連の開発作業をやってみる。
現実の CSV はそのまま使えなかったりするので、BigQuery に入れるまでに泥臭い前処理のような作業があることが多い。
そのへんもまとめて dlt でやってみるとこんな感じ、というのが示せるとよい。
やりたいこと 個人で管理しているお金の情報を個人用の BigQuery に置きたい、という要件を想定。
データ概要 具体的には MoneyForward のデータを load していく。
個人では API を利用できないので、web UI から export できる CSV のデータで収入・支出詳細と資産推移月次を対象とする。
CSV の export 方法は以下を参照。
入出金履歴はダウンロードできますか – マネーフォワード MEサポートサイト データの内容は次のようになっている。
収入・支出詳細_2023-11-01_2023-11-30.csv &#34;計算対象&#34;,&#34;日付&#34;,&#34;内容&#34;,&#34;金額（円）&#34;,&#34;保有金融機関&#34;,&#34;大項目&#34;,&#34;中項目&#34;,&#34;メモ&#34;,&#34;振替&#34;,&#34;ID&#34; &#34;1&#34;,&#34;2023/11/30&#34;,&#34;AMAZON.CO.JP&#34;,&#34;-2830&#34;,&#34;楽天カード&#34;,&#34;食費&#34;,&#34;食料品&#34;,&#34;&#34;,&#34;0&#34;,&#34;EPv92ZjQcOxgWQx_cLbhD1&#34; &#34;1&#34;,&#34;2023/11/24&#34;,&#34;東京ガス&#34;,&#34;-4321&#34;,&#34;楽天カード&#34;,&#34;水道・光熱費&#34;,&#34;ガス・灯油代&#34;,&#34;&#34;,&#34;0&#34;,&#34;r6wuQPfrIRS6aFpNYZE5Eh&#34; &#34;1&#34;,&#34;2023/11/24&#34;,&#34;給与 カ) フロッグログ&#34;,&#34;700000&#34;,&#34;みずほ銀行&#34;,&#34;収入&#34;,&#34;給与&#34;,&#34;&#34;,&#34;0&#34;,&#34;doettKpYyNp0Tml9KQQXm1&#34; ヘッダーがあり、各列に名前が付いている。
encoding が CP932 であることに注意。"><meta property="og:type" content="article"><meta property="og:url" content="https://soonraah.github.io/posts/load-csv-data-into-bq-by-dlt/"><meta property="og:image" content="https://soonraah.github.io/image/photo/huu-thong-z3vRZHah6tQ-unsplash.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-12-20T09:30:00+09:00"><meta property="article:modified_time" content="2023-12-20T09:30:00+09:00"><meta property="og:site_name" content="Froglog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://soonraah.github.io/image/photo/huu-thong-z3vRZHah6tQ-unsplash.jpg"><meta name=twitter:title content="現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる"><meta name=twitter:description content="このポストについて 前回の記事 dlt 入門 - ELT の Extract と Load を担う data load tool では dlt の概要を説明した。
この記事ではそれを踏まえ、dlt を使って CSV ファイルを BigQuery に load するという一連の開発作業をやってみる。
現実の CSV はそのまま使えなかったりするので、BigQuery に入れるまでに泥臭い前処理のような作業があることが多い。
そのへんもまとめて dlt でやってみるとこんな感じ、というのが示せるとよい。
やりたいこと 個人で管理しているお金の情報を個人用の BigQuery に置きたい、という要件を想定。
データ概要 具体的には MoneyForward のデータを load していく。
個人では API を利用できないので、web UI から export できる CSV のデータで収入・支出詳細と資産推移月次を対象とする。
CSV の export 方法は以下を参照。
入出金履歴はダウンロードできますか – マネーフォワード MEサポートサイト データの内容は次のようになっている。
収入・支出詳細_2023-11-01_2023-11-30.csv &#34;計算対象&#34;,&#34;日付&#34;,&#34;内容&#34;,&#34;金額（円）&#34;,&#34;保有金融機関&#34;,&#34;大項目&#34;,&#34;中項目&#34;,&#34;メモ&#34;,&#34;振替&#34;,&#34;ID&#34; &#34;1&#34;,&#34;2023/11/30&#34;,&#34;AMAZON.CO.JP&#34;,&#34;-2830&#34;,&#34;楽天カード&#34;,&#34;食費&#34;,&#34;食料品&#34;,&#34;&#34;,&#34;0&#34;,&#34;EPv92ZjQcOxgWQx_cLbhD1&#34; &#34;1&#34;,&#34;2023/11/24&#34;,&#34;東京ガス&#34;,&#34;-4321&#34;,&#34;楽天カード&#34;,&#34;水道・光熱費&#34;,&#34;ガス・灯油代&#34;,&#34;&#34;,&#34;0&#34;,&#34;r6wuQPfrIRS6aFpNYZE5Eh&#34; &#34;1&#34;,&#34;2023/11/24&#34;,&#34;給与 カ) フロッグログ&#34;,&#34;700000&#34;,&#34;みずほ銀行&#34;,&#34;収入&#34;,&#34;給与&#34;,&#34;&#34;,&#34;0&#34;,&#34;doettKpYyNp0Tml9KQQXm1&#34; ヘッダーがあり、各列に名前が付いている。
encoding が CP932 であることに注意。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://soonraah.github.io/posts/"},{"@type":"ListItem","position":3,"name":"現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる","item":"https://soonraah.github.io/posts/load-csv-data-into-bq-by-dlt/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる","name":"現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる","description":"このポストについて 前回の記事 dlt 入門 - ELT の Extract と Load を担う data load tool では dlt の概要を説明した。\nこの記事ではそれを踏まえ、dlt を使って CSV ファイルを BigQuery に load するという一連の開発作業をやってみる。\n現実の CSV はそのまま使えなかったりするので、BigQuery に入れるまでに泥臭い前処理のような作業があることが多い。\nそのへんもまとめて dlt でやってみるとこんな感じ、というのが示せるとよい。\nやりたいこと 個人で管理しているお金の情報を個人用の BigQuery に置きたい、という要件を想定。\nデータ概要 具体的には MoneyForward のデータを load していく。\n個人では API を利用できないので、web UI から export できる CSV のデータで収入・支出詳細と資産推移月次を対象とする。\nCSV の export 方法は以下を参照。\n入出金履歴はダウンロードできますか – マネーフォワード MEサポートサイト データの内容は次のようになっている。\n収入・支出詳細_2023-11-01_2023-11-30.csv \u0026#34;計算対象\u0026#34;,\u0026#34;日付\u0026#34;,\u0026#34;内容\u0026#34;,\u0026#34;金額（円）\u0026#34;,\u0026#34;保有金融機関\u0026#34;,\u0026#34;大項目\u0026#34;,\u0026#34;中項目\u0026#34;,\u0026#34;メモ\u0026#34;,\u0026#34;振替\u0026#34;,\u0026#34;ID\u0026#34; \u0026#34;1\u0026#34;,\u0026#34;2023/11/30\u0026#34;,\u0026#34;AMAZON.CO.JP\u0026#34;,\u0026#34;-2830\u0026#34;,\u0026#34;楽天カード\u0026#34;,\u0026#34;食費\u0026#34;,\u0026#34;食料品\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;EPv92ZjQcOxgWQx_cLbhD1\u0026#34; \u0026#34;1\u0026#34;,\u0026#34;2023/11/24\u0026#34;,\u0026#34;東京ガス\u0026#34;,\u0026#34;-4321\u0026#34;,\u0026#34;楽天カード\u0026#34;,\u0026#34;水道・光熱費\u0026#34;,\u0026#34;ガス・灯油代\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;r6wuQPfrIRS6aFpNYZE5Eh\u0026#34; \u0026#34;1\u0026#34;,\u0026#34;2023/11/24\u0026#34;,\u0026#34;給与 カ) フロッグログ\u0026#34;,\u0026#34;700000\u0026#34;,\u0026#34;みずほ銀行\u0026#34;,\u0026#34;収入\u0026#34;,\u0026#34;給与\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;doettKpYyNp0Tml9KQQXm1\u0026#34; ヘッダーがあり、各列に名前が付いている。\nencoding が CP932 であることに注意。","keywords":["dlt","ELT","data pipeline","BigQuery"],"articleBody":"このポストについて 前回の記事 dlt 入門 - ELT の Extract と Load を担う data load tool では dlt の概要を説明した。\nこの記事ではそれを踏まえ、dlt を使って CSV ファイルを BigQuery に load するという一連の開発作業をやってみる。\n現実の CSV はそのまま使えなかったりするので、BigQuery に入れるまでに泥臭い前処理のような作業があることが多い。\nそのへんもまとめて dlt でやってみるとこんな感じ、というのが示せるとよい。\nやりたいこと 個人で管理しているお金の情報を個人用の BigQuery に置きたい、という要件を想定。\nデータ概要 具体的には MoneyForward のデータを load していく。\n個人では API を利用できないので、web UI から export できる CSV のデータで収入・支出詳細と資産推移月次を対象とする。\nCSV の export 方法は以下を参照。\n入出金履歴はダウンロードできますか – マネーフォワード MEサポートサイト データの内容は次のようになっている。\n収入・支出詳細_2023-11-01_2023-11-30.csv \"計算対象\",\"日付\",\"内容\",\"金額（円）\",\"保有金融機関\",\"大項目\",\"中項目\",\"メモ\",\"振替\",\"ID\" \"1\",\"2023/11/30\",\"AMAZON.CO.JP\",\"-2830\",\"楽天カード\",\"食費\",\"食料品\",\"\",\"0\",\"EPv92ZjQcOxgWQx_cLbhD1\" \"1\",\"2023/11/24\",\"東京ガス\",\"-4321\",\"楽天カード\",\"水道・光熱費\",\"ガス・灯油代\",\"\",\"0\",\"r6wuQPfrIRS6aFpNYZE5Eh\" \"1\",\"2023/11/24\",\"給与 カ) フロッグログ\",\"700000\",\"みずほ銀行\",\"収入\",\"給与\",\"\",\"0\",\"doettKpYyNp0Tml9KQQXm1\" ヘッダーがあり、各列に名前が付いている。\nencoding が CP932 であることに注意。\nID の列があるので、行の識別に使えそう。\n資産推移月次.csv \"日付\",\"合計（円）\",\"預金・現金・暗号資産（円）\",\"株式(現物)（円）\",\"投資信託（円）\",\"債券（円）\",\"年金（円）\",\"ポイント（円）\" \"2023/12/17\",\"11505000\",\"5000000\",\"300000\",\"5000000\",\"200000\",\"1000000\",\"5000\" \"2023/12/16\",\"11505000\",\"5000000\",\"300000\",\"5000000\",\"200000\",\"1000000\",\"5000\" \"2023/12/15\",\"11505000\",\"5000000\",\"300000\",\"5000000\",\"200000\",\"1000000\",\"5000\" encoding などについては同じ。\n当月についてはすべての 日付 がある一方、それ以前については 2023/11/30, 2023/10/31, … のように月末だけが含まれている。\n日付 で行の識別ができる。\nこれらはまず最初に GCS bucket に手動で置くものとする。\n開発手順 CSV そのままだと schema まわりで問題が起こりそうなので、そのあたり try \u0026 error で解決したい。\nというのをいきなり BigQuery 上でやると手間もかかるし汚くなるので、最初は試験的にローカル環境の DuckDB に load するようにする。\nDuckDB でうまくいったら BigQuery へと移行する。\n全体の流れとしては次のようになる。\ndlt のインストール DuckDB 用の pipeline project を作成 pipeline の実装 configuration pipeline の実行 Streamlit app による結果の確認 schema の調整 不要レコードのフィルタリング incremental loading への対応 BigQuery への移行 BigQuery への load の確認 開発作業 1. dlt のインストール destination としては DuckDB, BigQuery を使うので、以下のようにして一緒にインストールする。\n(Poetry の例)\npoetry add 'dlt[duckdb,bigquery]' インストールされたバージョンを確認。\n$ dlt --version dlt 0.3.25 また、gsfs, pandas, streamlit, google-cloud-bigquery-storage も必要になるのでインストールしておく。\n2. pipeline project を作成 次のコマンドで pipeline project を用意する。\n$ dlt init filesystem duckdb これは verified source として Filesystem、destination として DuckDB を指定して pipeline project を作るという意味。\nFilesystem はローカルのファイルシステムや S3, GCS のようなクラウドストレージからファイルを読むことが可能。\nこのコマンドが成功すると次のようなディレクトリ構造が作られる。\n. ├── .dlt │ ├── .sources │ ├── config.toml │ └── secrets.toml ├── .gitignore ├── filesystem │ ├── README.md │ ├── __init__.py │ ├── helpers.py │ ├── readers.py │ └── settings.py └── filesystem_pipeline.py filesystem/ 以下には Filesystem を使うための関数定義や README などが生成されている。\nfilesystem_pipeline.py には pipeline の実装例がある。\nこれを元に修正していってもいいが、今回は新しく money_forward_pipeline.py を用意して実装していく。\n3. pipeline の実装 money_forward_pipeline.py は次のように実装した。\nimport dlt from filesystem import filesystem, read_csv, readers class MFFileSpec: def __init__(self, file_name: str, table_name: str): self.file_name = file_name self.table_name = table_name def run_pipeline() -\u003e None: pipeline = dlt.pipeline( pipeline_name=\"money_forward\", destination=\"duckdb\", dataset_name=\"raw_money_forward\", full_refresh=True, ) load_info = pipeline.run(money_forward()) print(load_info) @dlt.source def money_forward() -\u003e dlt.extract.source.DltSource: file_specs = [ MFFileSpec(\"収入・支出詳細_*.csv\", \"income_expence_details\"), MFFileSpec(\"資産推移月次.csv\", \"monthly_assets\"), ] for file_spec in file_specs: records = filesystem( file_glob=f\"money_forward/{file_spec.file_name}\", ) | read_csv(encoding=\"cp932\") yield records.with_name(file_spec.table_name) if __name__ == \"__main__\": run_pipeline() pipeline を作り、run() するときに source として money_forward() を与えている。\n試験的に何度か table を作り直すので full_refresh=True を指定している。\nちなみに source は resource をグルーピングしたものであり、resource ごとに table が作られると思っていい。\nmoney_forward() の中では収入・支出詳細と資産推移月次のそれぞれに対して resource が作られる。\nfilesystem() でファイルをリストアップし、それを read_csv() に渡してデータを読んでいる。\nここで GCS の bucket や認証情報が指定されていないことに気づくかもしれない。\n関数 filesystem() の定義は次のようになっている。\n@dlt.resource( primary_key=\"file_url\", spec=FilesystemConfigurationResource, standalone=True ) def filesystem( bucket_url: str = dlt.secrets.value, credentials: Union[FileSystemCredentials, AbstractFileSystem] = dlt.secrets.value, file_glob: Optional[str] = \"*\", files_per_page: int = DEFAULT_CHUNK_SIZE, extract_content: bool = False, ) -\u003e Iterator[List[FileItem]]: ... bucket_url, credentials のデフォルト値は dlt.sercrets.value になっている。\nこれは設定ファイルや環境変数などから取得された設定値を使うことを意味する。\n4. configuration 認証情報などの設定は dlt の設定ファイルや環境変数で与えることができる。\nここでは設定ファイルを使うことにする。\nbucket や認証情報を次のように記載した。\n.dlt/config.toml [sources.money_forward_pipeline] bucket_url = \"gs://\" .dlt/secrets.toml [sources.credentials] client_email = \"\" private_key = \"\" project_id = \"\" .dlt/secrets.toml の情報から GcpServiceAccountCredentials の object が自動で生成され、それが filesystem() に渡されるようになる。\n認証情報の各値を得るには GCP で service account を作り、key を用意してやる必要がある。\nservice account には必要な権限を付与しておく。\n当然だが .dlt/secrets.toml は GitHub 等に push してはいけない。\nちなみに設定周りの便利な機能として、足りない設定があったときに設定できる場所の一覧を教えてくれるというものがある。\n例えば bucket_url を書き忘れて pipeline を実行した場合、次のようなエラーメッセージが出力される。\nFollowing fields are missing: ['bucket_url'] in configuration with spec FilesystemConfigurationResource for field \"bucket_url\" config providers and keys were tried in following order: In Environment Variables key MONEY_FORWARD__SOURCES__MONEY_FORWARD_PIPELINE__FILESYSTEM__BUCKET_URL was not found. In Environment Variables key MONEY_FORWARD__SOURCES__MONEY_FORWARD_PIPELINE__BUCKET_URL was not found. In Environment Variables key MONEY_FORWARD__SOURCES__BUCKET_URL was not found. In Environment Variables key MONEY_FORWARD__BUCKET_URL was not found. In secrets.toml key money_forward.sources.money_forward_pipeline.filesystem.bucket_url was not found. In secrets.toml key money_forward.sources.money_forward_pipeline.bucket_url was not found. In secrets.toml key money_forward.sources.bucket_url was not found. In secrets.toml key money_forward.bucket_url was not found. In config.toml key money_forward.sources.money_forward_pipeline.filesystem.bucket_url was not found. In config.toml key money_forward.sources.money_forward_pipeline.bucket_url was not found. In config.toml key money_forward.sources.bucket_url was not found. In config.toml key money_forward.bucket_url was not found. In Environment Variables key SOURCES__MONEY_FORWARD_PIPELINE__FILESYSTEM__BUCKET_URL was not found. In Environment Variables key SOURCES__MONEY_FORWARD_PIPELINE__BUCKET_URL was not found. In Environment Variables key SOURCES__BUCKET_URL was not found. In Environment Variables key BUCKET_URL was not found. In secrets.toml key sources.money_forward_pipeline.filesystem.bucket_url was not found. In secrets.toml key sources.money_forward_pipeline.bucket_url was not found. In secrets.toml key sources.bucket_url was not found. In secrets.toml key bucket_url was not found. In config.toml key sources.money_forward_pipeline.filesystem.bucket_url was not found. In config.toml key sources.money_forward_pipeline.bucket_url was not found. In config.toml key sources.bucket_url was not found. In config.toml key bucket_url was not found. bucket_url が設定される可能性のある環境変数や設定ファイル中のパスが優先度順で一覧となって表示されている。\nこれにより何か設定が間違っているときにどこに書き足せばいいかがわかるようになっている。\nちなみにパスの違いはスコープの違いを表していて、\nIn secrets.toml key sources.money_forward_pipeline.filesystem.bucket_url was not found. In secrets.toml key sources.money_forward_pipeline.bucket_url was not found. の2つを比べると前者は pipeline money_forward_pipeline の filesystem という resource のための bucket_url だが、後者はそれより広く filesystem 以外でも参照できるものとなっている。\n設定まわりの挙動について、詳しくは Configuration | dlt Docs を参照のこと。\n5. pipeline の実行 ここまでで pipeline を実行する準備が整ったので実行してみる。\n$ python money_forward_pipeline.py 成功していれば次のようなログが出力される。\nPipeline money_forward completed in 2.96 seconds 1 load package(s) were loaded to destination duckdb and into dataset raw_money_forward_20231218112802 The duckdb destination used duckdb:////Users/sonoyou/Dev/python/dlt_sample/money_forward.duckdb location to store data Load package 1702898884.890725 is LOADED and contains no failed jobs 6. Streamlit app による結果の確認 意図したとおりにデータが load されたのか結果を確認する方法はいくつかあるが、ここでは Streamlit を使ってみる。\n$ dlt pipeline money_forward show このコマンドを実行すると web ブラウザが立ち上がり、Streamlit app が表示される。\n左のサイドバーで “Explore data” を選択すると DuckDB と接続し、pipeline により作られた table の schema やデータを見ることができる。\nさらにクエリを実行することも可能。便利。\nStreamlit app の画面: Exprole data\n“Load info” を選択すると load 時の統計情報などを見ることができる。\nStreamlit app の画面: Load info\nさて、前者の画面で table income_expence_detals (収入・支出詳細) の schema を見ると明らかにおかしいことがわかる。\nこちらを修正していく必要がある。\n日本語 column 名の修正 _dlt の prefix がついた column は dlt の管理用なのでいったん無視するとして、それ以外だと x, id のみしかない。\n元の CSV のヘッダーに記載されていた column 名は日本語表記になっており、1つだけ ID という英字表記の column があった。\nどうやら日本語表記の column 名に問題がありそうだ。\ncolumn 名を指定する方法はいくつかあるが、read_csv() の引数 names を使うことにした。\n関数 money_forward() を次のように修正した。\n@dlt.source def money_forward() -\u003e dlt.extract.source.DltSource: file_specs = [ MFFileSpec( file_name=\"収入・支出詳細_*.csv\", table_name=\"income_expence_details\", columns=[\"is_calc_target\", \"date\", \"details\", \"amount_yen\", \"financial_institution\", \"main_group\", \"mid_group\", \"memo\", \"transfer\", \"id\"], ), MFFileSpec( file_name=\"資産推移月次.csv\", table_name=\"monthly_assets\", columns=[\"date\", \"total_yen\", \"deposit_cash_crypto_yen\",\"stock_yen\",\"investment_trust_yen\",\"bond_yen\",\"pension_yen\",\"point_yen\"], ), ] for file_spec in file_specs: records = filesystem( file_glob=f\"money_forward/{file_spec.file_name}\", ) | read_csv(encoding=\"cp932\", names=file_spec.columns, header=0) yield records.with_name(file_spec.table_name) class MFFileSpec に field columns を追加し、read_csv() に names として渡すようにした。\ncolumns には CSV のヘッダー情報を英訳したものをハードコードしている。\nこちらの read_csv() では内部的に pandas が使われており、pandas.read_csv() の引数を取ることができる。\nnames で明示的に column 名を指定した形になる。\n再度 pipeline を実行し、できあがった table income_expence_details の schema を確認してみる。\nname data_type nullable 0 is_calc_target bigint true 1 date text true 2 details text true 3 amount_yen bigint true 4 financial_institution text true 5 main_group text true 6 mid_group text true 7 transfer bigint true 8 id text true 9 _dlt_load_id text false 10 _dlt_id text false 11 memo text true (なぜか memo が末尾になっているのが気になるが)\n大丈夫そうに見えるけど、もう少し手直しする。\n7. schema の調整 上記の column date に注目したい。\nこれは日付を表す column だが、型が text になっている。\ndate 型として扱えるようにしたい。\nここでは add_map() により resource に対して変換をかませる対応を行った。\nmoney_forward_pipeline.py に次の変換用関数を追加する。\ndef convert_date(d: dict) -\u003e dict: if \"date\" in d: d[\"date\"] = datetime.datetime.strptime(d[\"date\"], \"%Y/%m/%d\").date() return d 元の値としては \"2023/11/30\" のような文字列が入っているため、これを Python の datetime.date に変換する。\n関数 money_forward() の中で resource を yield しているところに ad_map() でこれを適用。\nfor file_spec in file_specs: records = filesystem( file_glob=f\"money_forward/{file_spec.file_name}\", ) | read_csv(encoding=\"cp932\", names=file_spec.column_names, header=0) yield records.add_map(convert_date).with_name(file_spec.table_name) 再度 pipeline を実行すると column date の型が date になることが確認できた。\nただし、本来はこのような schema 調整は import_schema_path, export_schema_path による YAML の編集で対応する方が好ましい。\n今回はそれがうまくいかなかったので add_map() による変換を使うやり方にした。\n詳しくは Adjust a schema | dlt Docs を参照。\nちなみに同じ要領で add_map() により個人情報をマスキングしたりもできる。\n8. 不要レコードのフィルタリング ここで資産推移月次の CSV ファイルについて思い出してみる。\n当月についてはすべての 日付 がある一方、それ以前については 2023/11/30, 2023/10/31, … のように月末だけが含まれている。\n月次である以上、月ごとのデータのみが含まれていることが好ましい。\nつまり当月の 2023/12/17, 2023/12/17 のような途中のレコードは不要であり、2023/11/30, 2023/10/31 のような月末のレコードのみを残したい。\nそれには add_filter() を使う。\n次のようなフィルタリング用の関数を用意する。\ndate の値の日付が月末だった場合のみ True を返す。\ndef is_last_day_of_month(d: dict) -\u003e bool: if \"date\" in d: date = d[\"date\"] return date.day == calendar.monthrange(date.year, date.month)[1] return True やはりこれを add_filter() により resource に加えるわけだが、今回は table monthly_assets (資産推移月次) にのみ適用したい。\n次のようにした。\nfor file_spec in file_specs: records = filesystem( file_glob=f\"money_forward/{file_spec.file_name}\", ) | read_csv(encoding=\"cp932\", names=file_spec.column_names, header=0) records = records.add_map(convert_date) if file_spec.table_name == \"monthly_assets\": records = records.add_filter(is_last_day_of_month) yield records.with_name(file_spec.table_name) これで pipeline を再実行したところ、table monthly_assets に月末以外のレコードが含まれないようになった。\n9. incremental loading への対応 DWH への load を運用するにあたり、load を incremental に行えるかについても考えないといけない。\n例えば 収入・支出詳細_2023-11-01_2023-11-30.csv にレコードの追加や変更があった場合、同じファイルを再度 load したい。\n資産推移月次.csv については export 時に毎回全期間書き出しなのでこちらも同じファイルを load することになる。\nこのとき、単に新しくレコードが追加されるだけだと、例えば同じ買い物が2回計上されてしまうなどの問題が生じる。\ndlt では incremental loading をサポートしており、primary_key (または merge_key) により load 後のレコードの重複を避けることができる。\n前述のとおり収入・支出詳細は ID、資産推移月次は 日付 でレコードを一意にできる。\n@dlt.source def money_forward() -\u003e dlt.extract.source.DltSource: file_specs = [ MFFileSpec( file_name=\"収入・支出詳細_*.csv\", table_name=\"income_expence_details\", columns=[\"is_calc_target\", \"date\", \"details\", \"amount_yen\", \"financial_institution\", \"main_group\", \"mid_group\", \"memo\", \"transfer\", \"id\"], primary_keys=[\"id\"], ), MFFileSpec( file_name=\"資産推移月次.csv\", table_name=\"monthly_assets\", columns=[\"date\", \"total_yen\", \"deposit_cash_crypto_yen\",\"stock_yen\",\"investment_trust_yen\",\"bond_yen\",\"pension_yen\",\"point_yen\"], primary_keys=[\"date\"], ), ] for file_spec in file_specs: records = filesystem( file_glob=f\"money_forward/{file_spec.file_name}\", ) | read_csv(encoding=\"cp932\", names=file_spec.columns, header=0) records = records.add_map(convert_date) if file_spec.table_name == \"monthly_assets\": records = records.add_filter(is_last_day_of_month) @dlt.transformer(name=file_spec.table_name, primary_key=file_spec.primary_keys, write_disposition=\"merge\") def dummy(items): return items yield records | dummy class MFFileSpec に primary_keys を追加した。\n生成される resource に primary_keys を指定するために @dlt.transformer として関数 dummy() を定義した。\ndummy() は transform の処理自体は何もしないが、primary_key および write_disposition を指定している。\nwrite_disposition は incremental loading の挙動を決めるパラメータであり、\"replace\", \"append\", \"merge\" の3つが指定できる。\nここでは primary_keys を使って追加・変更されたレコードを一意にしたかったので \"merge\" を指定した。\n詳しくは Pipeline Tutorial | dlt Docs を参照。\nDuckDB, BigQuery ともにすべての write_disposition をサポートしているが、destination によってはサポートされないものもあるので注意。\n(dummy() なんか使わずにもっときれいにできる方法があるかもしれないが…)\n10. BigQuery への移行 ローカルの DuckDB でやりたいことができるようになってきたのでいよいよ BigQuery へと移行する。\n2点変更すればよい。\nまずは pipeline の destination 指定を \"duckdb\" から \"bigquery\" に変更する。\npipeline = dlt.pipeline( pipeline_name=\"money_forward\", destination=\"bigquery\", dataset_name=\"raw_money_forward\", full_refresh=True, ) 加えて .dlt/secrets.toml に destination 用の設定を追加する。\n[destination.bigquery] location = \"asia-northeast1\" [destination.bigquery.credentials] client_email = \"\" private_key = \"\" project_id = \"\" 以上で destination 変更が完了。\npipeline を再実行すると成功した。\n11. BigQuery への load の確認 BigQuery にデータが load されているかを確認する。\nもちろん前述の Streamlit app から見る方法でも確認できるが、Cloud Shell 上から bq コマンドで確認する。\nまずは dataset を確認。\n$ bq ls datasetId ------------------------------------------ raw_money_forward_20231219112438 raw_money_forward_20231219112438_staging dlt.pipeline() で指定した dataset 名に timestamp っぽい文字列がついた名前で dataset が作成されている。\nこの timestamp は邪魔だと思ったが、除外する方法が分からなかった。\nload ごとに dataset を分けるべきという思想なのだろうか…？\n[追記]\ndlt.pipeline() の引数で full_refresh=True になっていたのが dataset 名に timestamp がついてしまう原因だった。\nfull_refresh=True は名前のとおり完全に作り直す挙動になっており、try \u0026 error で何度も作り直すためにつけていた。\nTrue のときに dataset 名が一意になるよう、timestamp をつけるという挙動は理解できる。\nこれを False にすると timestamp のない raw_money_forward という名前で dataset が作成されることを確認した。\n末尾に _staging がついているものは incremental loading のための一時的なデータ置き場のようなもの。\ndadtaset raw_money_forward_20231219112438 の table 一覧を見てみる。\n$ bq query --nouse_legacy_sql 'select table_name from raw_money_forward_20231219112438.INFORMATION_SCHEMA.TABLES' +------------------------+ | table_name | +------------------------+ | _dlt_loads | | _dlt_version | | monthly_assets | | _dlt_pipeline_state | | income_expence_details | +------------------------+ monthly_assets, income_expence_details の2つの table が作成されていることを確認した。\nデータもちゃんと入っている模様。\n_dlt の prefix を持つ table には dlt 関連の管理情報が含まれている。\n意図して見ることは少ないと思うが、何か問題が起こったときには参照することになるだろう。\nというわけで BiqQuery への load まで成功した。\nいろいろ編集したが、最終的な money_forwared_pipeline.py のコードを貼っておく。\nimport calendar import datetime import typing import dlt from filesystem import filesystem, read_csv, readers class MFFileSpec: def __init__(self, file_name: str, table_name: str, columns: typing.List[str], primary_keys: typing.List[str]): self.file_name = file_name self.table_name = table_name self.columns = columns self.primary_keys = primary_keys def run_pipeline() -\u003e None: pipeline = dlt.pipeline( pipeline_name=\"money_forward\", destination=\"bigquery\", dataset_name=\"raw_money_forward\", full_refresh=False, ) load_info = pipeline.run(money_forward()) print(load_info) @dlt.source def money_forward() -\u003e dlt.extract.source.DltSource: file_specs = [ MFFileSpec( file_name=\"収入・支出詳細_*.csv\", table_name=\"income_expence_details\", columns=[\"is_calc_target\", \"date\", \"details\", \"amount_yen\", \"financial_institution\", \"main_group\", \"mid_group\", \"memo\", \"transfer\", \"id\"], primary_keys=[\"id\"], ), MFFileSpec( file_name=\"資産推移月次.csv\", table_name=\"monthly_assets\", columns=[\"date\", \"total_yen\", \"deposit_cash_crypto_yen\",\"stock_yen\",\"investment_trust_yen\",\"bond_yen\",\"pension_yen\",\"point_yen\"], primary_keys=[\"date\"], ), ] for file_spec in file_specs: records = filesystem( file_glob=f\"money_forward/{file_spec.file_name}\", ) | read_csv(encoding=\"cp932\", names=file_spec.columns, header=0) records = records.add_map(convert_date) if file_spec.table_name == \"monthly_assets\": records = records.add_filter(is_last_day_of_month) @dlt.transformer(name=file_spec.table_name, primary_key=file_spec.primary_keys, write_disposition=\"merge\") def dummy(items): return items yield records | dummy def convert_date(d: dict) -\u003e dict: if \"date\" in d: d[\"date\"] = datetime.datetime.strptime(d[\"date\"], \"%Y/%m/%d\").date() return d def is_last_day_of_month(d: dict) -\u003e bool: if \"date\" in d: date = d[\"date\"] return date.day == calendar.monthrange(date.year, date.month)[1] return True if __name__ == \"__main__\": run_pipeline() まとめ ドキュメントを見ながら一通りのことができるものを実装することができた。\nコードに関しては慣れればもう少しきれいに書くことができそうな気がする。\n最後に destination を DuckDB から BiqQuery に変更する作業はとても簡単で体験が良かった。\n現在業務で DWH の移行を考えているが、こういう機能があると移行がとても楽だし、DuckDB のようなローカルで気楽に検証できる環境に切り替えられるのもすごくいい。\nDDD みを感じる。\nもちろん GCS 上の CSV ファイルを BiqQuery に読み込む方法は公式で提供されているので、dlt は必須ではない。\nしかし上記のように容易に destination が変えられたり verified source が提供されていたりというところや、前処理のようなごちゃごちゃしたこと pipeline の定義と一緒に Python でどうとでも書けるところにメリットがある。\nまた今回は触れていないが schema evolution についての配慮もある。\n仕事で使ってみてもいいと思った。\nただし dlt が動くマシンにデータを載せることになるので、基本的にはあまり大きなデータの移動には向かない。\nライトなユースケースがマッチするだろう。\n","wordCount":"1666","inLanguage":"ja","image":"https://soonraah.github.io/image/photo/huu-thong-z3vRZHah6tQ-unsplash.jpg","datePublished":"2023-12-20T09:30:00+09:00","dateModified":"2023-12-20T09:30:00+09:00","author":{"@type":"Person","name":"soonraah"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://soonraah.github.io/posts/load-csv-data-into-bq-by-dlt/"},"publisher":{"@type":"Organization","name":"Froglog","logo":{"@type":"ImageObject","url":"https://soonraah.github.io/favicon2.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://soonraah.github.io accesskey=h title="Home (Alt + H)"><img src=https://soonraah.github.io/image/brand/favicon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://soonraah.github.io/about/ title=About><span>About</span></a></li><li><a href=https://soonraah.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://soonraah.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://soonraah.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://soonraah.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://soonraah.github.io>ホーム</a>&nbsp;»&nbsp;<a href=https://soonraah.github.io/posts/>Posts</a></div><h1 class=post-title>現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる</h1><div class=post-meta><span title='2023-12-20 09:30:00 +0900 JST'>12月 20, 2023</span>&nbsp;·&nbsp;soonraah</div></header><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/photo/huu-thong-z3vRZHah6tQ-unsplash.jpg alt=cake><p><a href=https://unsplash.com/ja/%E5%86%99%E7%9C%9F/%E7%99%BD%E3%81%84%E5%B8%83%E5%9C%B0%E3%81%AB%E8%B5%A4%E3%81%A8%E7%99%BD%E3%81%AE%E4%B8%B8%E3%81%84%E6%9E%9C%E5%AE%9F-z3vRZHah6tQ>Photo by Huu Thong on Unsplash</a></p></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目次</span></summary><div class=inner><ul><li><a href=#%e3%81%93%e3%81%ae%e3%83%9d%e3%82%b9%e3%83%88%e3%81%ab%e3%81%a4%e3%81%84%e3%81%a6 aria-label=このポストについて>このポストについて</a></li><li><a href=#%e3%82%84%e3%82%8a%e3%81%9f%e3%81%84%e3%81%93%e3%81%a8 aria-label=やりたいこと>やりたいこと</a><ul><li><a href=#%e3%83%87%e3%83%bc%e3%82%bf%e6%a6%82%e8%a6%81 aria-label=データ概要>データ概要</a></li><li><a href=#%e9%96%8b%e7%99%ba%e6%89%8b%e9%a0%86 aria-label=開発手順>開発手順</a></li></ul></li><li><a href=#%e9%96%8b%e7%99%ba%e4%bd%9c%e6%a5%ad aria-label=開発作業>開発作業</a><ul><li><a href=#1-dlt-%e3%81%ae%e3%82%a4%e3%83%b3%e3%82%b9%e3%83%88%e3%83%bc%e3%83%ab aria-label="1. dlt のインストール">1. dlt のインストール</a></li><li><a href=#2-pipeline-project-%e3%82%92%e4%bd%9c%e6%88%90 aria-label="2. pipeline project を作成">2. pipeline project を作成</a></li><li><a href=#3-pipeline-%e3%81%ae%e5%ae%9f%e8%a3%85 aria-label="3. pipeline の実装">3. pipeline の実装</a></li><li><a href=#4-configuration aria-label="4. configuration">4. configuration</a></li><li><a href=#5-pipeline-%e3%81%ae%e5%ae%9f%e8%a1%8c aria-label="5. pipeline の実行">5. pipeline の実行</a></li><li><a href=#6-streamlit-app-%e3%81%ab%e3%82%88%e3%82%8b%e7%b5%90%e6%9e%9c%e3%81%ae%e7%a2%ba%e8%aa%8d aria-label="6. Streamlit app による結果の確認">6. Streamlit app による結果の確認</a></li><li><a href=#%e6%97%a5%e6%9c%ac%e8%aa%9e-column-%e5%90%8d%e3%81%ae%e4%bf%ae%e6%ad%a3 aria-label="日本語 column 名の修正">日本語 column 名の修正</a></li><li><a href=#7-schema-%e3%81%ae%e8%aa%bf%e6%95%b4 aria-label="7. schema の調整">7. schema の調整</a></li><li><a href=#8-%e4%b8%8d%e8%a6%81%e3%83%ac%e3%82%b3%e3%83%bc%e3%83%89%e3%81%ae%e3%83%95%e3%82%a3%e3%83%ab%e3%82%bf%e3%83%aa%e3%83%b3%e3%82%b0 aria-label="8. 不要レコードのフィルタリング">8. 不要レコードのフィルタリング</a></li><li><a href=#9-incremental-loading-%e3%81%b8%e3%81%ae%e5%af%be%e5%bf%9c aria-label="9. incremental loading への対応">9. incremental loading への対応</a></li><li><a href=#10-bigquery-%e3%81%b8%e3%81%ae%e7%a7%bb%e8%a1%8c aria-label="10. BigQuery への移行">10. BigQuery への移行</a></li><li><a href=#11-bigquery-%e3%81%b8%e3%81%ae-load-%e3%81%ae%e7%a2%ba%e8%aa%8d aria-label="11. BigQuery への load の確認">11. BigQuery への load の確認</a></li></ul></li><li><a href=#%e3%81%be%e3%81%a8%e3%82%81 aria-label=まとめ>まとめ</a></li></ul></div></details></div><div class=post-content><h2 id=このポストについて>このポストについて<a hidden class=anchor aria-hidden=true href=#このポストについて>#</a></h2><p>前回の記事 <a href=../what-is-dlt/>dlt 入門 - ELT の Extract と Load を担う data load tool</a> では dlt の概要を説明した。<br>この記事ではそれを踏まえ、dlt を使って CSV ファイルを BigQuery に load するという一連の開発作業をやってみる。</p><p>現実の CSV はそのまま使えなかったりするので、BigQuery に入れるまでに泥臭い前処理のような作業があることが多い。<br>そのへんもまとめて dlt でやってみるとこんな感じ、というのが示せるとよい。</p><h2 id=やりたいこと>やりたいこと<a hidden class=anchor aria-hidden=true href=#やりたいこと>#</a></h2><p>個人で管理しているお金の情報を個人用の BigQuery に置きたい、という要件を想定。</p><h3 id=データ概要>データ概要<a hidden class=anchor aria-hidden=true href=#データ概要>#</a></h3><p>具体的には MoneyForward のデータを load していく。<br>個人では API を利用できないので、web UI から export できる CSV のデータで収入・支出詳細と資産推移月次を対象とする。<br>CSV の export 方法は以下を参照。</p><ul><li><a href=https://support.me.moneyforward.com/hc/ja/articles/900004382483-%E5%85%A5%E5%87%BA%E9%87%91%E5%B1%A5%E6%AD%B4%E3%81%AF%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%81%8B>入出金履歴はダウンロードできますか – マネーフォワード MEサポートサイト</a></li></ul><p>データの内容は次のようになっている。</p><ul><li><code>収入・支出詳細_2023-11-01_2023-11-30.csv</code></li></ul><pre tabindex=0><code>&#34;計算対象&#34;,&#34;日付&#34;,&#34;内容&#34;,&#34;金額（円）&#34;,&#34;保有金融機関&#34;,&#34;大項目&#34;,&#34;中項目&#34;,&#34;メモ&#34;,&#34;振替&#34;,&#34;ID&#34;
&#34;1&#34;,&#34;2023/11/30&#34;,&#34;AMAZON.CO.JP&#34;,&#34;-2830&#34;,&#34;楽天カード&#34;,&#34;食費&#34;,&#34;食料品&#34;,&#34;&#34;,&#34;0&#34;,&#34;EPv92ZjQcOxgWQx_cLbhD1&#34;
&#34;1&#34;,&#34;2023/11/24&#34;,&#34;東京ガス&#34;,&#34;-4321&#34;,&#34;楽天カード&#34;,&#34;水道・光熱費&#34;,&#34;ガス・灯油代&#34;,&#34;&#34;,&#34;0&#34;,&#34;r6wuQPfrIRS6aFpNYZE5Eh&#34;
&#34;1&#34;,&#34;2023/11/24&#34;,&#34;給与 カ) フロッグログ&#34;,&#34;700000&#34;,&#34;みずほ銀行&#34;,&#34;収入&#34;,&#34;給与&#34;,&#34;&#34;,&#34;0&#34;,&#34;doettKpYyNp0Tml9KQQXm1&#34;
</code></pre><p>ヘッダーがあり、各列に名前が付いている。<br>encoding が CP932 であることに注意。<br><code>ID</code> の列があるので、行の識別に使えそう。</p><ul><li><code>資産推移月次.csv</code></li></ul><pre tabindex=0><code>&#34;日付&#34;,&#34;合計（円）&#34;,&#34;預金・現金・暗号資産（円）&#34;,&#34;株式(現物)（円）&#34;,&#34;投資信託（円）&#34;,&#34;債券（円）&#34;,&#34;年金（円）&#34;,&#34;ポイント（円）&#34;
&#34;2023/12/17&#34;,&#34;11505000&#34;,&#34;5000000&#34;,&#34;300000&#34;,&#34;5000000&#34;,&#34;200000&#34;,&#34;1000000&#34;,&#34;5000&#34;
&#34;2023/12/16&#34;,&#34;11505000&#34;,&#34;5000000&#34;,&#34;300000&#34;,&#34;5000000&#34;,&#34;200000&#34;,&#34;1000000&#34;,&#34;5000&#34;
&#34;2023/12/15&#34;,&#34;11505000&#34;,&#34;5000000&#34;,&#34;300000&#34;,&#34;5000000&#34;,&#34;200000&#34;,&#34;1000000&#34;,&#34;5000&#34;
</code></pre><p>encoding などについては同じ。<br>当月についてはすべての <code>日付</code> がある一方、それ以前については <code>2023/11/30</code>, <code>2023/10/31</code>, &mldr; のように月末だけが含まれている。<br><code>日付</code> で行の識別ができる。</p><p>これらはまず最初に GCS bucket に手動で置くものとする。</p><h3 id=開発手順>開発手順<a hidden class=anchor aria-hidden=true href=#開発手順>#</a></h3><p>CSV そのままだと schema まわりで問題が起こりそうなので、そのあたり try & error で解決したい。<br>というのをいきなり BigQuery 上でやると手間もかかるし汚くなるので、最初は試験的にローカル環境の DuckDB に load するようにする。<br>DuckDB でうまくいったら BigQuery へと移行する。</p><p>全体の流れとしては次のようになる。</p><ol><li>dlt のインストール</li><li>DuckDB 用の pipeline project を作成</li><li>pipeline の実装</li><li>configuration</li><li>pipeline の実行</li><li>Streamlit app による結果の確認</li><li>schema の調整</li><li>不要レコードのフィルタリング</li><li>incremental loading への対応</li><li>BigQuery への移行</li><li>BigQuery への load の確認</li></ol><h2 id=開発作業>開発作業<a hidden class=anchor aria-hidden=true href=#開発作業>#</a></h2><h3 id=1-dlt-のインストール>1. dlt のインストール<a hidden class=anchor aria-hidden=true href=#1-dlt-のインストール>#</a></h3><p>destination としては DuckDB, BigQuery を使うので、以下のようにして一緒にインストールする。<br>(Poetry の例)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>poetry add <span style=color:#e6db74>&#39;dlt[duckdb,bigquery]&#39;</span>
</span></span></code></pre></div><p>インストールされたバージョンを確認。</p><pre tabindex=0><code>$ dlt --version                  
dlt 0.3.25
</code></pre><p>また、gsfs, pandas, streamlit, google-cloud-bigquery-storage も必要になるのでインストールしておく。</p><h3 id=2-pipeline-project-を作成>2. pipeline project を作成<a hidden class=anchor aria-hidden=true href=#2-pipeline-project-を作成>#</a></h3><p>次のコマンドで pipeline project を用意する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ dlt init filesystem duckdb
</span></span></code></pre></div><p>これは <a href=https://dlthub.com/docs/dlt-ecosystem/verified-sources/>verified source</a> として Filesystem、<a href=https://dlthub.com/docs/dlt-ecosystem/destinations/>destination</a> として DuckDB を指定して pipeline project を作るという意味。<br>Filesystem はローカルのファイルシステムや S3, GCS のようなクラウドストレージからファイルを読むことが可能。</p><p>このコマンドが成功すると次のようなディレクトリ構造が作られる。</p><pre tabindex=0><code>.
├── .dlt
│   ├── .sources
│   ├── config.toml
│   └── secrets.toml
├── .gitignore
├── filesystem
│   ├── README.md
│   ├── __init__.py
│   ├── helpers.py
│   ├── readers.py
│   └── settings.py
└── filesystem_pipeline.py
</code></pre><p><code>filesystem/</code> 以下には Filesystem を使うための関数定義や README などが生成されている。<br><code>filesystem_pipeline.py</code> には pipeline の実装例がある。<br>これを元に修正していってもいいが、今回は新しく <code>money_forward_pipeline.py</code> を用意して実装していく。</p><h3 id=3-pipeline-の実装>3. pipeline の実装<a hidden class=anchor aria-hidden=true href=#3-pipeline-の実装>#</a></h3><p><code>money_forward_pipeline.py</code> は次のように実装した。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> dlt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> filesystem <span style=color:#f92672>import</span> filesystem, read_csv, readers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MFFileSpec</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, file_name: str, table_name: str):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>file_name <span style=color:#f92672>=</span> file_name
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>table_name <span style=color:#f92672>=</span> table_name
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_pipeline</span>() <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>    pipeline <span style=color:#f92672>=</span> dlt<span style=color:#f92672>.</span>pipeline(
</span></span><span style=display:flex><span>        pipeline_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;money_forward&#34;</span>,
</span></span><span style=display:flex><span>        destination<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;duckdb&#34;</span>,
</span></span><span style=display:flex><span>        dataset_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;raw_money_forward&#34;</span>,
</span></span><span style=display:flex><span>        full_refresh<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    load_info <span style=color:#f92672>=</span> pipeline<span style=color:#f92672>.</span>run(money_forward())
</span></span><span style=display:flex><span>    print(load_info)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@dlt.source</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>money_forward</span>() <span style=color:#f92672>-&gt;</span> dlt<span style=color:#f92672>.</span>extract<span style=color:#f92672>.</span>source<span style=color:#f92672>.</span>DltSource:
</span></span><span style=display:flex><span>    file_specs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        MFFileSpec(<span style=color:#e6db74>&#34;収入・支出詳細_*.csv&#34;</span>, <span style=color:#e6db74>&#34;income_expence_details&#34;</span>),
</span></span><span style=display:flex><span>        MFFileSpec(<span style=color:#e6db74>&#34;資産推移月次.csv&#34;</span>, <span style=color:#e6db74>&#34;monthly_assets&#34;</span>),
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> file_spec <span style=color:#f92672>in</span> file_specs:
</span></span><span style=display:flex><span>        records <span style=color:#f92672>=</span> filesystem(
</span></span><span style=display:flex><span>            file_glob<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;money_forward/</span><span style=color:#e6db74>{</span>file_spec<span style=color:#f92672>.</span>file_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>,
</span></span><span style=display:flex><span>        ) <span style=color:#f92672>|</span> read_csv(encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cp932&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>yield</span> records<span style=color:#f92672>.</span>with_name(file_spec<span style=color:#f92672>.</span>table_name)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    run_pipeline()
</span></span></code></pre></div><p>pipeline を作り、<code>run()</code> するときに source として <code>money_forward()</code> を与えている。<br>試験的に何度か table を作り直すので <code>full_refresh=True</code> を指定している。<br>ちなみに source は resource をグルーピングしたものであり、resource ごとに table が作られると思っていい。</p><p><code>money_forward()</code> の中では収入・支出詳細と資産推移月次のそれぞれに対して resource が作られる。<br><code>filesystem()</code> でファイルをリストアップし、それを <code>read_csv()</code> に渡してデータを読んでいる。</p><p>ここで GCS の bucket や認証情報が指定されていないことに気づくかもしれない。<br>関数 <code>filesystem()</code> の定義は次のようになっている。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#a6e22e>@dlt.resource</span>(
</span></span><span style=display:flex><span>    primary_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;file_url&#34;</span>, spec<span style=color:#f92672>=</span>FilesystemConfigurationResource, standalone<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>filesystem</span>(
</span></span><span style=display:flex><span>    bucket_url: str <span style=color:#f92672>=</span> dlt<span style=color:#f92672>.</span>secrets<span style=color:#f92672>.</span>value,
</span></span><span style=display:flex><span>    credentials: Union[FileSystemCredentials, AbstractFileSystem] <span style=color:#f92672>=</span> dlt<span style=color:#f92672>.</span>secrets<span style=color:#f92672>.</span>value,
</span></span><span style=display:flex><span>    file_glob: Optional[str] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;*&#34;</span>,
</span></span><span style=display:flex><span>    files_per_page: int <span style=color:#f92672>=</span> DEFAULT_CHUNK_SIZE,
</span></span><span style=display:flex><span>    extract_content: bool <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>) <span style=color:#f92672>-&gt;</span> Iterator[List[FileItem]]:
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span></code></pre></div><p><code>bucket_url</code>, <code>credentials</code> のデフォルト値は <code>dlt.sercrets.value</code> になっている。<br>これは設定ファイルや環境変数などから取得された設定値を使うことを意味する。</p><h3 id=4-configuration>4. configuration<a hidden class=anchor aria-hidden=true href=#4-configuration>#</a></h3><p>認証情報などの設定は dlt の設定ファイルや環境変数で与えることができる。<br>ここでは設定ファイルを使うことにする。<br>bucket や認証情報を次のように記載した。</p><ul><li><code>.dlt/config.toml</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>[<span style=color:#a6e22e>sources</span>.<span style=color:#a6e22e>money_forward_pipeline</span>]
</span></span><span style=display:flex><span><span style=color:#a6e22e>bucket_url</span> = <span style=color:#e6db74>&#34;gs://&lt;BUCKET_NAME&gt;&#34;</span>
</span></span></code></pre></div><ul><li><code>.dlt/secrets.toml</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>[<span style=color:#a6e22e>sources</span>.<span style=color:#a6e22e>credentials</span>]
</span></span><span style=display:flex><span><span style=color:#a6e22e>client_email</span> = <span style=color:#e6db74>&#34;&lt;CLIENT_EMAIL&gt;&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>private_key</span> = <span style=color:#e6db74>&#34;&lt;PRIVATE_KEY&gt;&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>project_id</span> = <span style=color:#e6db74>&#34;&lt;PROJECT_ID&gt;&#34;</span>
</span></span></code></pre></div><p><code>.dlt/secrets.toml</code> の情報から <code>GcpServiceAccountCredentials</code> の object が自動で生成され、それが <code>filesystem()</code> に渡されるようになる。<br>認証情報の各値を得るには GCP で service account を作り、key を用意してやる必要がある。<br>service account には必要な権限を付与しておく。<br>当然だが <code>.dlt/secrets.toml</code> は GitHub 等に push してはいけない。</p><p>ちなみに設定周りの便利な機能として、足りない設定があったときに設定できる場所の一覧を教えてくれるというものがある。<br>例えば <code>bucket_url</code> を書き忘れて pipeline を実行した場合、次のようなエラーメッセージが出力される。</p><pre tabindex=0><code>Following fields are missing: [&#39;bucket_url&#39;] in configuration with spec FilesystemConfigurationResource
        for field &#34;bucket_url&#34; config providers and keys were tried in following order:
                In Environment Variables key MONEY_FORWARD__SOURCES__MONEY_FORWARD_PIPELINE__FILESYSTEM__BUCKET_URL was not found.
                In Environment Variables key MONEY_FORWARD__SOURCES__MONEY_FORWARD_PIPELINE__BUCKET_URL was not found.
                In Environment Variables key MONEY_FORWARD__SOURCES__BUCKET_URL was not found.
                In Environment Variables key MONEY_FORWARD__BUCKET_URL was not found.
                In secrets.toml key money_forward.sources.money_forward_pipeline.filesystem.bucket_url was not found.
                In secrets.toml key money_forward.sources.money_forward_pipeline.bucket_url was not found.
                In secrets.toml key money_forward.sources.bucket_url was not found.
                In secrets.toml key money_forward.bucket_url was not found.
                In config.toml key money_forward.sources.money_forward_pipeline.filesystem.bucket_url was not found.
                In config.toml key money_forward.sources.money_forward_pipeline.bucket_url was not found.
                In config.toml key money_forward.sources.bucket_url was not found.
                In config.toml key money_forward.bucket_url was not found.
                In Environment Variables key SOURCES__MONEY_FORWARD_PIPELINE__FILESYSTEM__BUCKET_URL was not found.
                In Environment Variables key SOURCES__MONEY_FORWARD_PIPELINE__BUCKET_URL was not found.
                In Environment Variables key SOURCES__BUCKET_URL was not found.
                In Environment Variables key BUCKET_URL was not found.
                In secrets.toml key sources.money_forward_pipeline.filesystem.bucket_url was not found.
                In secrets.toml key sources.money_forward_pipeline.bucket_url was not found.
                In secrets.toml key sources.bucket_url was not found.
                In secrets.toml key bucket_url was not found.
                In config.toml key sources.money_forward_pipeline.filesystem.bucket_url was not found.
                In config.toml key sources.money_forward_pipeline.bucket_url was not found.
                In config.toml key sources.bucket_url was not found.
                In config.toml key bucket_url was not found.
</code></pre><p><code>bucket_url</code> が設定される可能性のある環境変数や設定ファイル中のパスが優先度順で一覧となって表示されている。<br>これにより何か設定が間違っているときにどこに書き足せばいいかがわかるようになっている。<br>ちなみにパスの違いはスコープの違いを表していて、</p><pre tabindex=0><code>                In secrets.toml key sources.money_forward_pipeline.filesystem.bucket_url was not found.
                In secrets.toml key sources.money_forward_pipeline.bucket_url was not found.
</code></pre><p>の2つを比べると前者は pipeline <code>money_forward_pipeline</code> の <code>filesystem</code> という resource のための <code>bucket_url</code> だが、後者はそれより広く <code>filesystem</code> 以外でも参照できるものとなっている。</p><p>設定まわりの挙動について、詳しくは <a href=https://dlthub.com/docs/general-usage/credentials>Configuration | dlt Docs</a> を参照のこと。</p><h3 id=5-pipeline-の実行>5. pipeline の実行<a hidden class=anchor aria-hidden=true href=#5-pipeline-の実行>#</a></h3><p>ここまでで pipeline を実行する準備が整ったので実行してみる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ python money_forward_pipeline.py
</span></span></code></pre></div><p>成功していれば次のようなログが出力される。</p><pre tabindex=0><code>Pipeline money_forward completed in 2.96 seconds
1 load package(s) were loaded to destination duckdb and into dataset raw_money_forward_20231218112802
The duckdb destination used duckdb:////Users/sonoyou/Dev/python/dlt_sample/money_forward.duckdb location to store data
Load package 1702898884.890725 is LOADED and contains no failed jobs
</code></pre><h3 id=6-streamlit-app-による結果の確認>6. Streamlit app による結果の確認<a hidden class=anchor aria-hidden=true href=#6-streamlit-app-による結果の確認>#</a></h3><p>意図したとおりにデータが load されたのか結果を確認する方法はいくつかあるが、ここでは Streamlit を使ってみる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ dlt pipeline money_forward show
</span></span></code></pre></div><p>このコマンドを実行すると web ブラウザが立ち上がり、Streamlit app が表示される。<br>左のサイドバーで &ldquo;Explore data&rdquo; を選択すると DuckDB と接続し、pipeline により作られた table の schema やデータを見ることができる。<br>さらにクエリを実行することも可能。便利。</p><figure class=align-center><img loading=lazy src=/image/dlt/dlt_streamlit_explore_data.png#center alt="Streamlit app の画面: Exprole data"><figcaption><p>Streamlit app の画面: Exprole data</p></figcaption></figure><p>&ldquo;Load info&rdquo; を選択すると load 時の統計情報などを見ることができる。</p><figure class=align-center><img loading=lazy src=/image/dlt/dlt_streamlit_load_info.png#center alt="Streamlit app の画面: Load info"><figcaption><p>Streamlit app の画面: Load info</p></figcaption></figure><p>さて、前者の画面で table <code>income_expence_detals</code> (収入・支出詳細) の schema を見ると明らかにおかしいことがわかる。<br>こちらを修正していく必要がある。</p><h3 id=日本語-column-名の修正>日本語 column 名の修正<a hidden class=anchor aria-hidden=true href=#日本語-column-名の修正>#</a></h3><p><code>_dlt</code> の prefix がついた column は dlt の管理用なのでいったん無視するとして、それ以外だと <code>x</code>, <code>id</code> のみしかない。<br>元の CSV のヘッダーに記載されていた column 名は日本語表記になっており、1つだけ <code>ID</code> という英字表記の column があった。<br>どうやら日本語表記の column 名に問題がありそうだ。</p><p>column 名を指定する方法はいくつかあるが、<code>read_csv()</code> の引数 <code>names</code> を使うことにした。<br>関数 <code>money_forward()</code> を次のように修正した。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#a6e22e>@dlt.source</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>money_forward</span>() <span style=color:#f92672>-&gt;</span> dlt<span style=color:#f92672>.</span>extract<span style=color:#f92672>.</span>source<span style=color:#f92672>.</span>DltSource:
</span></span><span style=display:flex><span>    file_specs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        MFFileSpec(
</span></span><span style=display:flex><span>            file_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;収入・支出詳細_*.csv&#34;</span>,
</span></span><span style=display:flex><span>            table_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;income_expence_details&#34;</span>,
</span></span><span style=display:flex><span>            columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;is_calc_target&#34;</span>, <span style=color:#e6db74>&#34;date&#34;</span>, <span style=color:#e6db74>&#34;details&#34;</span>, <span style=color:#e6db74>&#34;amount_yen&#34;</span>, <span style=color:#e6db74>&#34;financial_institution&#34;</span>, <span style=color:#e6db74>&#34;main_group&#34;</span>, <span style=color:#e6db74>&#34;mid_group&#34;</span>, <span style=color:#e6db74>&#34;memo&#34;</span>, <span style=color:#e6db74>&#34;transfer&#34;</span>, <span style=color:#e6db74>&#34;id&#34;</span>],
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>        MFFileSpec(
</span></span><span style=display:flex><span>            file_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;資産推移月次.csv&#34;</span>,
</span></span><span style=display:flex><span>            table_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;monthly_assets&#34;</span>,
</span></span><span style=display:flex><span>            columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;date&#34;</span>, <span style=color:#e6db74>&#34;total_yen&#34;</span>, <span style=color:#e6db74>&#34;deposit_cash_crypto_yen&#34;</span>,<span style=color:#e6db74>&#34;stock_yen&#34;</span>,<span style=color:#e6db74>&#34;investment_trust_yen&#34;</span>,<span style=color:#e6db74>&#34;bond_yen&#34;</span>,<span style=color:#e6db74>&#34;pension_yen&#34;</span>,<span style=color:#e6db74>&#34;point_yen&#34;</span>],
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> file_spec <span style=color:#f92672>in</span> file_specs:
</span></span><span style=display:flex><span>        records <span style=color:#f92672>=</span> filesystem(
</span></span><span style=display:flex><span>            file_glob<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;money_forward/</span><span style=color:#e6db74>{</span>file_spec<span style=color:#f92672>.</span>file_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>,
</span></span><span style=display:flex><span>        ) <span style=color:#f92672>|</span> read_csv(encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cp932&#34;</span>, names<span style=color:#f92672>=</span>file_spec<span style=color:#f92672>.</span>columns, header<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>yield</span> records<span style=color:#f92672>.</span>with_name(file_spec<span style=color:#f92672>.</span>table_name)
</span></span></code></pre></div><p>class <code>MFFileSpec</code> に field <code>columns</code> を追加し、<code>read_csv()</code> に <code>names</code> として渡すようにした。<br><code>columns</code> には CSV のヘッダー情報を英訳したものをハードコードしている。<br>こちらの <code>read_csv()</code> では内部的に pandas が使われており、<a href=https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#><code>pandas.read_csv()</code></a> の引数を取ることができる。<br><code>names</code> で明示的に column 名を指定した形になる。</p><p>再度 pipeline を実行し、できあがった table <code>income_expence_details</code> の schema を確認してみる。</p><table><thead><tr><th style=text-align:right></th><th>name</th><th>data_type</th><th>nullable</th></tr></thead><tbody><tr><td style=text-align:right>0</td><td>is_calc_target</td><td>bigint</td><td>true</td></tr><tr><td style=text-align:right>1</td><td>date</td><td>text</td><td>true</td></tr><tr><td style=text-align:right>2</td><td>details</td><td>text</td><td>true</td></tr><tr><td style=text-align:right>3</td><td>amount_yen</td><td>bigint</td><td>true</td></tr><tr><td style=text-align:right>4</td><td>financial_institution</td><td>text</td><td>true</td></tr><tr><td style=text-align:right>5</td><td>main_group</td><td>text</td><td>true</td></tr><tr><td style=text-align:right>6</td><td>mid_group</td><td>text</td><td>true</td></tr><tr><td style=text-align:right>7</td><td>transfer</td><td>bigint</td><td>true</td></tr><tr><td style=text-align:right>8</td><td>id</td><td>text</td><td>true</td></tr><tr><td style=text-align:right>9</td><td>_dlt_load_id</td><td>text</td><td>false</td></tr><tr><td style=text-align:right>10</td><td>_dlt_id</td><td>text</td><td>false</td></tr><tr><td style=text-align:right>11</td><td>memo</td><td>text</td><td>true</td></tr></tbody></table><p>(なぜか <code>memo</code> が末尾になっているのが気になるが)<br>大丈夫そうに見えるけど、もう少し手直しする。</p><h3 id=7-schema-の調整>7. schema の調整<a hidden class=anchor aria-hidden=true href=#7-schema-の調整>#</a></h3><p>上記の column <code>date</code> に注目したい。<br>これは日付を表す column だが、型が <code>text</code> になっている。<br><code>date</code> 型として扱えるようにしたい。</p><p>ここでは <code>add_map()</code> により resource に対して変換をかませる対応を行った。<br><code>money_forward_pipeline.py</code> に次の変換用関数を追加する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>convert_date</span>(d: dict) <span style=color:#f92672>-&gt;</span> dict:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;date&#34;</span> <span style=color:#f92672>in</span> d:
</span></span><span style=display:flex><span>        d[<span style=color:#e6db74>&#34;date&#34;</span>] <span style=color:#f92672>=</span> datetime<span style=color:#f92672>.</span>datetime<span style=color:#f92672>.</span>strptime(d[<span style=color:#e6db74>&#34;date&#34;</span>], <span style=color:#e6db74>&#34;%Y/%m/</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>&#34;</span>)<span style=color:#f92672>.</span>date()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> d
</span></span></code></pre></div><p>元の値としては <code>"2023/11/30"</code> のような文字列が入っているため、これを Python の <code>datetime.date</code> に変換する。<br>関数 <code>money_forward()</code> の中で resource を <code>yield</code> しているところに <code>ad_map()</code> でこれを適用。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>    <span style=color:#66d9ef>for</span> file_spec <span style=color:#f92672>in</span> file_specs:
</span></span><span style=display:flex><span>        records <span style=color:#f92672>=</span> filesystem(
</span></span><span style=display:flex><span>            file_glob<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;money_forward/</span><span style=color:#e6db74>{</span>file_spec<span style=color:#f92672>.</span>file_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>,
</span></span><span style=display:flex><span>        ) <span style=color:#f92672>|</span> read_csv(encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cp932&#34;</span>, names<span style=color:#f92672>=</span>file_spec<span style=color:#f92672>.</span>column_names, header<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>yield</span> records<span style=color:#f92672>.</span>add_map(convert_date)<span style=color:#f92672>.</span>with_name(file_spec<span style=color:#f92672>.</span>table_name)
</span></span></code></pre></div><p>再度 pipeline を実行すると column <code>date</code> の型が <code>date</code> になることが確認できた。</p><p>ただし、本来はこのような schema 調整は <code>import_schema_path</code>, <code>export_schema_path</code> による YAML の編集で対応する方が好ましい。<br>今回はそれがうまくいかなかったので <code>add_map()</code> による変換を使うやり方にした。<br>詳しくは <a href=https://dlthub.com/docs/walkthroughs/adjust-a-schema>Adjust a schema | dlt Docs</a> を参照。</p><p>ちなみに同じ要領で <code>add_map()</code> により個人情報をマスキングしたりもできる。</p><h3 id=8-不要レコードのフィルタリング>8. 不要レコードのフィルタリング<a hidden class=anchor aria-hidden=true href=#8-不要レコードのフィルタリング>#</a></h3><p>ここで資産推移月次の CSV ファイルについて思い出してみる。</p><blockquote><p>当月についてはすべての <code>日付</code> がある一方、それ以前については <code>2023/11/30</code>, <code>2023/10/31</code>, &mldr; のように月末だけが含まれている。</p></blockquote><p>月次である以上、月ごとのデータのみが含まれていることが好ましい。<br>つまり当月の <code>2023/12/17</code>, <code>2023/12/17</code> のような途中のレコードは不要であり、<code>2023/11/30</code>, <code>2023/10/31</code> のような月末のレコードのみを残したい。<br>それには <code>add_filter()</code> を使う。</p><p>次のようなフィルタリング用の関数を用意する。<br><code>date</code> の値の日付が月末だった場合のみ <code>True</code> を返す。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>is_last_day_of_month</span>(d: dict) <span style=color:#f92672>-&gt;</span> bool:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;date&#34;</span> <span style=color:#f92672>in</span> d:
</span></span><span style=display:flex><span>        date <span style=color:#f92672>=</span> d[<span style=color:#e6db74>&#34;date&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> date<span style=color:#f92672>.</span>day <span style=color:#f92672>==</span> calendar<span style=color:#f92672>.</span>monthrange(date<span style=color:#f92672>.</span>year, date<span style=color:#f92672>.</span>month)[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>True</span>
</span></span></code></pre></div><p>やはりこれを <code>add_filter()</code> により resource に加えるわけだが、今回は table <code>monthly_assets</code> (資産推移月次) にのみ適用したい。<br>次のようにした。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>    <span style=color:#66d9ef>for</span> file_spec <span style=color:#f92672>in</span> file_specs:
</span></span><span style=display:flex><span>        records <span style=color:#f92672>=</span> filesystem(
</span></span><span style=display:flex><span>            file_glob<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;money_forward/</span><span style=color:#e6db74>{</span>file_spec<span style=color:#f92672>.</span>file_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>,
</span></span><span style=display:flex><span>        ) <span style=color:#f92672>|</span> read_csv(encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cp932&#34;</span>, names<span style=color:#f92672>=</span>file_spec<span style=color:#f92672>.</span>column_names, header<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        records <span style=color:#f92672>=</span> records<span style=color:#f92672>.</span>add_map(convert_date)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> file_spec<span style=color:#f92672>.</span>table_name <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;monthly_assets&#34;</span>:
</span></span><span style=display:flex><span>            records <span style=color:#f92672>=</span> records<span style=color:#f92672>.</span>add_filter(is_last_day_of_month)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>yield</span> records<span style=color:#f92672>.</span>with_name(file_spec<span style=color:#f92672>.</span>table_name)
</span></span></code></pre></div><p>これで pipeline を再実行したところ、table <code>monthly_assets</code> に月末以外のレコードが含まれないようになった。</p><h3 id=9-incremental-loading-への対応>9. incremental loading への対応<a hidden class=anchor aria-hidden=true href=#9-incremental-loading-への対応>#</a></h3><p>DWH への load を運用するにあたり、load を incremental に行えるかについても考えないといけない。<br>例えば <code>収入・支出詳細_2023-11-01_2023-11-30.csv</code> にレコードの追加や変更があった場合、同じファイルを再度 load したい。<br><code>資産推移月次.csv</code> については export 時に毎回全期間書き出しなのでこちらも同じファイルを load することになる。<br>このとき、単に新しくレコードが追加されるだけだと、例えば同じ買い物が2回計上されてしまうなどの問題が生じる。</p><p>dlt では incremental loading をサポートしており、<code>primary_key</code> (または <code>merge_key</code>) により load 後のレコードの重複を避けることができる。<br>前述のとおり収入・支出詳細は <code>ID</code>、資産推移月次は <code>日付</code> でレコードを一意にできる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#a6e22e>@dlt.source</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>money_forward</span>() <span style=color:#f92672>-&gt;</span> dlt<span style=color:#f92672>.</span>extract<span style=color:#f92672>.</span>source<span style=color:#f92672>.</span>DltSource:
</span></span><span style=display:flex><span>    file_specs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        MFFileSpec(
</span></span><span style=display:flex><span>            file_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;収入・支出詳細_*.csv&#34;</span>,
</span></span><span style=display:flex><span>            table_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;income_expence_details&#34;</span>,
</span></span><span style=display:flex><span>            columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;is_calc_target&#34;</span>, <span style=color:#e6db74>&#34;date&#34;</span>, <span style=color:#e6db74>&#34;details&#34;</span>, <span style=color:#e6db74>&#34;amount_yen&#34;</span>, <span style=color:#e6db74>&#34;financial_institution&#34;</span>, <span style=color:#e6db74>&#34;main_group&#34;</span>, <span style=color:#e6db74>&#34;mid_group&#34;</span>, <span style=color:#e6db74>&#34;memo&#34;</span>, <span style=color:#e6db74>&#34;transfer&#34;</span>, <span style=color:#e6db74>&#34;id&#34;</span>],
</span></span><span style=display:flex><span>            primary_keys<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;id&#34;</span>],
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>        MFFileSpec(
</span></span><span style=display:flex><span>            file_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;資産推移月次.csv&#34;</span>,
</span></span><span style=display:flex><span>            table_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;monthly_assets&#34;</span>,
</span></span><span style=display:flex><span>            columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;date&#34;</span>, <span style=color:#e6db74>&#34;total_yen&#34;</span>, <span style=color:#e6db74>&#34;deposit_cash_crypto_yen&#34;</span>,<span style=color:#e6db74>&#34;stock_yen&#34;</span>,<span style=color:#e6db74>&#34;investment_trust_yen&#34;</span>,<span style=color:#e6db74>&#34;bond_yen&#34;</span>,<span style=color:#e6db74>&#34;pension_yen&#34;</span>,<span style=color:#e6db74>&#34;point_yen&#34;</span>],
</span></span><span style=display:flex><span>            primary_keys<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;date&#34;</span>],
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> file_spec <span style=color:#f92672>in</span> file_specs:
</span></span><span style=display:flex><span>        records <span style=color:#f92672>=</span> filesystem(
</span></span><span style=display:flex><span>            file_glob<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;money_forward/</span><span style=color:#e6db74>{</span>file_spec<span style=color:#f92672>.</span>file_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>,
</span></span><span style=display:flex><span>        ) <span style=color:#f92672>|</span> read_csv(encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cp932&#34;</span>, names<span style=color:#f92672>=</span>file_spec<span style=color:#f92672>.</span>columns, header<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        records <span style=color:#f92672>=</span> records<span style=color:#f92672>.</span>add_map(convert_date)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> file_spec<span style=color:#f92672>.</span>table_name <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;monthly_assets&#34;</span>:
</span></span><span style=display:flex><span>            records <span style=color:#f92672>=</span> records<span style=color:#f92672>.</span>add_filter(is_last_day_of_month)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>@dlt.transformer</span>(name<span style=color:#f92672>=</span>file_spec<span style=color:#f92672>.</span>table_name, primary_key<span style=color:#f92672>=</span>file_spec<span style=color:#f92672>.</span>primary_keys, write_disposition<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;merge&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>dummy</span>(items):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> items
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>yield</span> records <span style=color:#f92672>|</span> dummy
</span></span></code></pre></div><p>class <code>MFFileSpec</code> に <code>primary_keys</code> を追加した。<br>生成される resource に <code>primary_keys</code> を指定するために <code>@dlt.transformer</code> として関数 <code>dummy()</code> を定義した。<br><code>dummy()</code> は transform の処理自体は何もしないが、<code>primary_key</code> および <code>write_disposition</code> を指定している。</p><p><code>write_disposition</code> は incremental loading の挙動を決めるパラメータであり、<code>"replace"</code>, <code>"append"</code>, <code>"merge"</code> の3つが指定できる。<br>ここでは <code>primary_keys</code> を使って追加・変更されたレコードを一意にしたかったので <code>"merge"</code> を指定した。<br>詳しくは <a href=https://dlthub.com/docs/build-a-pipeline-tutorial#declarative-loading>Pipeline Tutorial | dlt Docs</a> を参照。<br>DuckDB, BigQuery ともにすべての <code>write_disposition</code> をサポートしているが、destination によってはサポートされないものもあるので注意。</p><p>(<code>dummy()</code> なんか使わずにもっときれいにできる方法があるかもしれないが…)</p><h3 id=10-bigquery-への移行>10. BigQuery への移行<a hidden class=anchor aria-hidden=true href=#10-bigquery-への移行>#</a></h3><p>ローカルの DuckDB でやりたいことができるようになってきたのでいよいよ BigQuery へと移行する。<br>2点変更すればよい。<br>まずは pipeline の destination 指定を <code>"duckdb"</code> から <code>"bigquery"</code> に変更する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>    pipeline <span style=color:#f92672>=</span> dlt<span style=color:#f92672>.</span>pipeline(
</span></span><span style=display:flex><span>        pipeline_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;money_forward&#34;</span>,
</span></span><span style=display:flex><span>        destination<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;bigquery&#34;</span>,
</span></span><span style=display:flex><span>        dataset_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;raw_money_forward&#34;</span>,
</span></span><span style=display:flex><span>        full_refresh<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>加えて <code>.dlt/secrets.toml</code> に destination 用の設定を追加する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>[<span style=color:#a6e22e>destination</span>.<span style=color:#a6e22e>bigquery</span>]
</span></span><span style=display:flex><span><span style=color:#a6e22e>location</span> = <span style=color:#e6db74>&#34;asia-northeast1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#a6e22e>destination</span>.<span style=color:#a6e22e>bigquery</span>.<span style=color:#a6e22e>credentials</span>]
</span></span><span style=display:flex><span><span style=color:#a6e22e>client_email</span> = <span style=color:#e6db74>&#34;&lt;CLIENT_EMAIL&gt;&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>private_key</span> = <span style=color:#e6db74>&#34;&lt;PRIVATE_KEY&gt;&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>project_id</span> = <span style=color:#e6db74>&#34;&lt;PROJECT_ID&gt;&#34;</span>
</span></span></code></pre></div><p>以上で destination 変更が完了。<br>pipeline を再実行すると成功した。</p><h3 id=11-bigquery-への-load-の確認>11. BigQuery への load の確認<a hidden class=anchor aria-hidden=true href=#11-bigquery-への-load-の確認>#</a></h3><p>BigQuery にデータが load されているかを確認する。<br>もちろん前述の Streamlit app から見る方法でも確認できるが、Cloud Shell 上から bq コマンドで確認する。<br>まずは dataset を確認。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ bq ls
</span></span><span style=display:flex><span>                 datasetId                  
</span></span><span style=display:flex><span> ------------------------------------------ 
</span></span><span style=display:flex><span>  raw_money_forward_20231219112438          
</span></span><span style=display:flex><span>  raw_money_forward_20231219112438_staging 
</span></span></code></pre></div><p><code>dlt.pipeline()</code> で指定した dataset 名に timestamp っぽい文字列がついた名前で dataset が作成されている。<br>この timestamp は邪魔だと思ったが、除外する方法が分からなかった。<br>load ごとに dataset を分けるべきという思想なのだろうか…？</p><hr><p>[追記]<br><code>dlt.pipeline()</code> の引数で <code>full_refresh=True</code> になっていたのが dataset 名に timestamp がついてしまう原因だった。<br><code>full_refresh=True</code> は名前のとおり完全に作り直す挙動になっており、try & error で何度も作り直すためにつけていた。<br><code>True</code> のときに dataset 名が一意になるよう、timestamp をつけるという挙動は理解できる。<br>これを <code>False</code> にすると timestamp のない <code>raw_money_forward</code> という名前で dataset が作成されることを確認した。</p><hr><p>末尾に <code>_staging</code> がついているものは incremental loading のための一時的なデータ置き場のようなもの。<br>dadtaset <code>raw_money_forward_20231219112438</code> の table 一覧を見てみる。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ bq query --nouse_legacy_sql <span style=color:#e6db74>&#39;select table_name from raw_money_forward_20231219112438.INFORMATION_SCHEMA.TABLES&#39;</span>
</span></span><span style=display:flex><span>+------------------------+
</span></span><span style=display:flex><span>|       table_name       |
</span></span><span style=display:flex><span>+------------------------+
</span></span><span style=display:flex><span>| _dlt_loads             |
</span></span><span style=display:flex><span>| _dlt_version           |
</span></span><span style=display:flex><span>| monthly_assets         |
</span></span><span style=display:flex><span>| _dlt_pipeline_state    |
</span></span><span style=display:flex><span>| income_expence_details |
</span></span><span style=display:flex><span>+------------------------+
</span></span></code></pre></div><p><code>monthly_assets</code>, <code>income_expence_details</code> の2つの table が作成されていることを確認した。<br>データもちゃんと入っている模様。</p><p><code>_dlt</code> の prefix を持つ table には dlt 関連の管理情報が含まれている。<br>意図して見ることは少ないと思うが、何か問題が起こったときには参照することになるだろう。</p><p>というわけで BiqQuery への load まで成功した。<br>いろいろ編集したが、最終的な <code>money_forwared_pipeline.py</code> のコードを貼っておく。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> calendar
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> datetime
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> typing
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> dlt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> filesystem <span style=color:#f92672>import</span> filesystem, read_csv, readers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MFFileSpec</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, file_name: str, table_name: str, columns: typing<span style=color:#f92672>.</span>List[str], primary_keys: typing<span style=color:#f92672>.</span>List[str]):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>file_name <span style=color:#f92672>=</span> file_name
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>table_name <span style=color:#f92672>=</span> table_name
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>columns <span style=color:#f92672>=</span> columns
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>primary_keys <span style=color:#f92672>=</span> primary_keys
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_pipeline</span>() <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>    pipeline <span style=color:#f92672>=</span> dlt<span style=color:#f92672>.</span>pipeline(
</span></span><span style=display:flex><span>        pipeline_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;money_forward&#34;</span>,
</span></span><span style=display:flex><span>        destination<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;bigquery&#34;</span>,
</span></span><span style=display:flex><span>        dataset_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;raw_money_forward&#34;</span>,
</span></span><span style=display:flex><span>        full_refresh<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    load_info <span style=color:#f92672>=</span> pipeline<span style=color:#f92672>.</span>run(money_forward())
</span></span><span style=display:flex><span>    print(load_info)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@dlt.source</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>money_forward</span>() <span style=color:#f92672>-&gt;</span> dlt<span style=color:#f92672>.</span>extract<span style=color:#f92672>.</span>source<span style=color:#f92672>.</span>DltSource:
</span></span><span style=display:flex><span>    file_specs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        MFFileSpec(
</span></span><span style=display:flex><span>            file_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;収入・支出詳細_*.csv&#34;</span>,
</span></span><span style=display:flex><span>            table_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;income_expence_details&#34;</span>,
</span></span><span style=display:flex><span>            columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;is_calc_target&#34;</span>, <span style=color:#e6db74>&#34;date&#34;</span>, <span style=color:#e6db74>&#34;details&#34;</span>, <span style=color:#e6db74>&#34;amount_yen&#34;</span>, <span style=color:#e6db74>&#34;financial_institution&#34;</span>, <span style=color:#e6db74>&#34;main_group&#34;</span>, <span style=color:#e6db74>&#34;mid_group&#34;</span>, <span style=color:#e6db74>&#34;memo&#34;</span>, <span style=color:#e6db74>&#34;transfer&#34;</span>, <span style=color:#e6db74>&#34;id&#34;</span>],
</span></span><span style=display:flex><span>            primary_keys<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;id&#34;</span>],
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>        MFFileSpec(
</span></span><span style=display:flex><span>            file_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;資産推移月次.csv&#34;</span>,
</span></span><span style=display:flex><span>            table_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;monthly_assets&#34;</span>,
</span></span><span style=display:flex><span>            columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;date&#34;</span>, <span style=color:#e6db74>&#34;total_yen&#34;</span>, <span style=color:#e6db74>&#34;deposit_cash_crypto_yen&#34;</span>,<span style=color:#e6db74>&#34;stock_yen&#34;</span>,<span style=color:#e6db74>&#34;investment_trust_yen&#34;</span>,<span style=color:#e6db74>&#34;bond_yen&#34;</span>,<span style=color:#e6db74>&#34;pension_yen&#34;</span>,<span style=color:#e6db74>&#34;point_yen&#34;</span>],
</span></span><span style=display:flex><span>            primary_keys<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;date&#34;</span>],
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> file_spec <span style=color:#f92672>in</span> file_specs:
</span></span><span style=display:flex><span>        records <span style=color:#f92672>=</span> filesystem(
</span></span><span style=display:flex><span>            file_glob<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;money_forward/</span><span style=color:#e6db74>{</span>file_spec<span style=color:#f92672>.</span>file_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>,
</span></span><span style=display:flex><span>        ) <span style=color:#f92672>|</span> read_csv(encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cp932&#34;</span>, names<span style=color:#f92672>=</span>file_spec<span style=color:#f92672>.</span>columns, header<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        records <span style=color:#f92672>=</span> records<span style=color:#f92672>.</span>add_map(convert_date)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> file_spec<span style=color:#f92672>.</span>table_name <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;monthly_assets&#34;</span>:
</span></span><span style=display:flex><span>            records <span style=color:#f92672>=</span> records<span style=color:#f92672>.</span>add_filter(is_last_day_of_month)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>@dlt.transformer</span>(name<span style=color:#f92672>=</span>file_spec<span style=color:#f92672>.</span>table_name, primary_key<span style=color:#f92672>=</span>file_spec<span style=color:#f92672>.</span>primary_keys, write_disposition<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;merge&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>dummy</span>(items):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> items
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>yield</span> records <span style=color:#f92672>|</span> dummy
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>convert_date</span>(d: dict) <span style=color:#f92672>-&gt;</span> dict:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;date&#34;</span> <span style=color:#f92672>in</span> d:
</span></span><span style=display:flex><span>        d[<span style=color:#e6db74>&#34;date&#34;</span>] <span style=color:#f92672>=</span> datetime<span style=color:#f92672>.</span>datetime<span style=color:#f92672>.</span>strptime(d[<span style=color:#e6db74>&#34;date&#34;</span>], <span style=color:#e6db74>&#34;%Y/%m/</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>&#34;</span>)<span style=color:#f92672>.</span>date()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> d
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>is_last_day_of_month</span>(d: dict) <span style=color:#f92672>-&gt;</span> bool:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;date&#34;</span> <span style=color:#f92672>in</span> d:
</span></span><span style=display:flex><span>        date <span style=color:#f92672>=</span> d[<span style=color:#e6db74>&#34;date&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> date<span style=color:#f92672>.</span>day <span style=color:#f92672>==</span> calendar<span style=color:#f92672>.</span>monthrange(date<span style=color:#f92672>.</span>year, date<span style=color:#f92672>.</span>month)[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    run_pipeline()
</span></span></code></pre></div><h2 id=まとめ>まとめ<a hidden class=anchor aria-hidden=true href=#まとめ>#</a></h2><p>ドキュメントを見ながら一通りのことができるものを実装することができた。<br>コードに関しては慣れればもう少しきれいに書くことができそうな気がする。</p><p>最後に destination を DuckDB から BiqQuery に変更する作業はとても簡単で体験が良かった。<br>現在業務で DWH の移行を考えているが、こういう機能があると移行がとても楽だし、DuckDB のようなローカルで気楽に検証できる環境に切り替えられるのもすごくいい。<br>DDD みを感じる。</p><p>もちろん GCS 上の CSV ファイルを BiqQuery に読み込む方法は<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv?hl=ja">公式</a>で提供されているので、dlt は必須ではない。<br>しかし上記のように容易に destination が変えられたり verified source が提供されていたりというところや、前処理のようなごちゃごちゃしたこと pipeline の定義と一緒に Python でどうとでも書けるところにメリットがある。<br>また今回は触れていないが schema evolution についての配慮もある。</p><p>仕事で使ってみてもいいと思った。<br>ただし dlt が動くマシンにデータを載せることになるので、基本的にはあまり大きなデータの移動には向かない。<br>ライトなユースケースがマッチするだろう。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://soonraah.github.io/tags/dlt/>dlt</a></li><li><a href=https://soonraah.github.io/tags/elt/>ELT</a></li><li><a href=https://soonraah.github.io/tags/data-pipeline/>data pipeline</a></li><li><a href=https://soonraah.github.io/tags/bigquery/>BigQuery</a></li></ul><nav class=paginav><a class=next href=https://soonraah.github.io/posts/what-is-dlt/><span class=title>次へ »</span><br><span>dlt 入門 - ELT の Extract と Load を担う data load tool</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる on x" href="https://x.com/intent/tweet/?text=%e7%8f%be%e5%ae%9f%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e3%82%92%20BigQuery%20%e3%81%ab%20load%20%e3%81%99%e3%82%8b%e4%bb%95%e7%b5%84%e3%81%bf%e3%82%92%e4%bd%9c%e3%82%8b%e3%81%a8%e3%81%84%e3%81%86%e6%b3%a5%e8%87%ad%e3%81%84%e4%bd%9c%e6%a5%ad%e3%82%92%20dlt%20%e3%81%a7%e3%82%84%e3%81%a3%e3%81%a6%e3%81%bf%e3%82%8b&amp;url=https%3a%2f%2fsoonraah.github.io%2fposts%2fload-csv-data-into-bq-by-dlt%2f&amp;hashtags=dlt%2cELT%2cdatapipeline%2cBigQuery"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsoonraah.github.io%2fposts%2fload-csv-data-into-bq-by-dlt%2f&amp;title=%e7%8f%be%e5%ae%9f%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e3%82%92%20BigQuery%20%e3%81%ab%20load%20%e3%81%99%e3%82%8b%e4%bb%95%e7%b5%84%e3%81%bf%e3%82%92%e4%bd%9c%e3%82%8b%e3%81%a8%e3%81%84%e3%81%86%e6%b3%a5%e8%87%ad%e3%81%84%e4%bd%9c%e6%a5%ad%e3%82%92%20dlt%20%e3%81%a7%e3%82%84%e3%81%a3%e3%81%a6%e3%81%bf%e3%82%8b&amp;summary=%e7%8f%be%e5%ae%9f%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e3%82%92%20BigQuery%20%e3%81%ab%20load%20%e3%81%99%e3%82%8b%e4%bb%95%e7%b5%84%e3%81%bf%e3%82%92%e4%bd%9c%e3%82%8b%e3%81%a8%e3%81%84%e3%81%86%e6%b3%a5%e8%87%ad%e3%81%84%e4%bd%9c%e6%a5%ad%e3%82%92%20dlt%20%e3%81%a7%e3%82%84%e3%81%a3%e3%81%a6%e3%81%bf%e3%82%8b&amp;source=https%3a%2f%2fsoonraah.github.io%2fposts%2fload-csv-data-into-bq-by-dlt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsoonraah.github.io%2fposts%2fload-csv-data-into-bq-by-dlt%2f&title=%e7%8f%be%e5%ae%9f%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e3%82%92%20BigQuery%20%e3%81%ab%20load%20%e3%81%99%e3%82%8b%e4%bb%95%e7%b5%84%e3%81%bf%e3%82%92%e4%bd%9c%e3%82%8b%e3%81%a8%e3%81%84%e3%81%86%e6%b3%a5%e8%87%ad%e3%81%84%e4%bd%9c%e6%a5%ad%e3%82%92%20dlt%20%e3%81%a7%e3%82%84%e3%81%a3%e3%81%a6%e3%81%bf%e3%82%8b"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsoonraah.github.io%2fposts%2fload-csv-data-into-bq-by-dlt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる on whatsapp" href="https://api.whatsapp.com/send?text=%e7%8f%be%e5%ae%9f%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e3%82%92%20BigQuery%20%e3%81%ab%20load%20%e3%81%99%e3%82%8b%e4%bb%95%e7%b5%84%e3%81%bf%e3%82%92%e4%bd%9c%e3%82%8b%e3%81%a8%e3%81%84%e3%81%86%e6%b3%a5%e8%87%ad%e3%81%84%e4%bd%9c%e6%a5%ad%e3%82%92%20dlt%20%e3%81%a7%e3%82%84%e3%81%a3%e3%81%a6%e3%81%bf%e3%82%8b%20-%20https%3a%2f%2fsoonraah.github.io%2fposts%2fload-csv-data-into-bq-by-dlt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる on telegram" href="https://telegram.me/share/url?text=%e7%8f%be%e5%ae%9f%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e3%82%92%20BigQuery%20%e3%81%ab%20load%20%e3%81%99%e3%82%8b%e4%bb%95%e7%b5%84%e3%81%bf%e3%82%92%e4%bd%9c%e3%82%8b%e3%81%a8%e3%81%84%e3%81%86%e6%b3%a5%e8%87%ad%e3%81%84%e4%bd%9c%e6%a5%ad%e3%82%92%20dlt%20%e3%81%a7%e3%82%84%e3%81%a3%e3%81%a6%e3%81%bf%e3%82%8b&amp;url=https%3a%2f%2fsoonraah.github.io%2fposts%2fload-csv-data-into-bq-by-dlt%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e7%8f%be%e5%ae%9f%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e3%82%92%20BigQuery%20%e3%81%ab%20load%20%e3%81%99%e3%82%8b%e4%bb%95%e7%b5%84%e3%81%bf%e3%82%92%e4%bd%9c%e3%82%8b%e3%81%a8%e3%81%84%e3%81%86%e6%b3%a5%e8%87%ad%e3%81%84%e4%bd%9c%e6%a5%ad%e3%82%92%20dlt%20%e3%81%a7%e3%82%84%e3%81%a3%e3%81%a6%e3%81%bf%e3%82%8b&u=https%3a%2f%2fsoonraah.github.io%2fposts%2fload-csv-data-into-bq-by-dlt%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://soonraah.github.io>Froglog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="コピー";function s(){t.innerHTML="コピーされました!",setTimeout(()=>{t.innerHTML="コピー"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>