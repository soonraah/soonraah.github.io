<!doctype html><html lang=ja dir=auto><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-NSEGH2YT17"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NSEGH2YT17")</script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Apache Flink の DataStream API 利用時の CSV ファイル読み込み | Froglog</title>
<meta name=keywords content="Apache Flink,stream processing"><meta name=description content="ストリーム処理における CSV ファイルの読み込み Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。
しかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。
star schema における dimension table 的な情報をストリームに結合したい場合 等が考えられる。
このポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。
Flink は現時点の stable である v1.11 を想定。
CSV ファイルを読む方法 DataStream API ベースの実装で CSV ファイルを読むには StreamExecutionEnvironment のメソッドである readFile() を使う。
overload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。
まず1つめは FileInputFormat<OUT> inputFormat であり、こちらは data stream の生成に用いる入力フォーマットを指定する。
おそらく最も一般的なのが TextInputFormat だと思われる。
もちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。
PojoCsvInputFormat RowCsvInputFormat TupleCsvInputFormat なんとなく名前でわかると思うが、それぞれ readFile() の結果として返される DataStreamSource が内包する型が異なる。"><meta name=author content="soonraah"><link rel=canonical href=https://soonraah.github.io/posts/read-csv-by-flink-datastream-api/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://soonraah.github.io/favicon2.ico><link rel=icon type=image/png sizes=16x16 href=https://soonraah.github.io/image/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://soonraah.github.io/image/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://soonraah.github.io/static/image/favicon/apple-touch-icon.png><link rel=mask-icon href=https://soonraah.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-73329599-2","auto"),ga("send","pageview"))</script><meta property="og:title" content="Apache Flink の DataStream API 利用時の CSV ファイル読み込み"><meta property="og:description" content="ストリーム処理における CSV ファイルの読み込み Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。
しかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。
star schema における dimension table 的な情報をストリームに結合したい場合 等が考えられる。
このポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。
Flink は現時点の stable である v1.11 を想定。
CSV ファイルを読む方法 DataStream API ベースの実装で CSV ファイルを読むには StreamExecutionEnvironment のメソッドである readFile() を使う。
overload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。
まず1つめは FileInputFormat<OUT> inputFormat であり、こちらは data stream の生成に用いる入力フォーマットを指定する。
おそらく最も一般的なのが TextInputFormat だと思われる。
もちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。
PojoCsvInputFormat RowCsvInputFormat TupleCsvInputFormat なんとなく名前でわかると思うが、それぞれ readFile() の結果として返される DataStreamSource が内包する型が異なる。"><meta property="og:type" content="article"><meta property="og:url" content="https://soonraah.github.io/posts/read-csv-by-flink-datastream-api/"><meta property="og:image" content="https://soonraah.github.io/image/photo/mika-baumeister-Wpnoqo2plFA-unsplash.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-12-01T00:30:00+09:00"><meta property="article:modified_time" content="2020-12-01T00:30:00+09:00"><meta property="og:site_name" content="Froglog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://soonraah.github.io/image/photo/mika-baumeister-Wpnoqo2plFA-unsplash.jpg"><meta name=twitter:title content="Apache Flink の DataStream API 利用時の CSV ファイル読み込み"><meta name=twitter:description content="ストリーム処理における CSV ファイルの読み込み Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。
しかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。
star schema における dimension table 的な情報をストリームに結合したい場合 等が考えられる。
このポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。
Flink は現時点の stable である v1.11 を想定。
CSV ファイルを読む方法 DataStream API ベースの実装で CSV ファイルを読むには StreamExecutionEnvironment のメソッドである readFile() を使う。
overload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。
まず1つめは FileInputFormat<OUT> inputFormat であり、こちらは data stream の生成に用いる入力フォーマットを指定する。
おそらく最も一般的なのが TextInputFormat だと思われる。
もちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。
PojoCsvInputFormat RowCsvInputFormat TupleCsvInputFormat なんとなく名前でわかると思うが、それぞれ readFile() の結果として返される DataStreamSource が内包する型が異なる。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://soonraah.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Apache Flink の DataStream API 利用時の CSV ファイル読み込み","item":"https://soonraah.github.io/posts/read-csv-by-flink-datastream-api/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Apache Flink の DataStream API 利用時の CSV ファイル読み込み","name":"Apache Flink の DataStream API 利用時の CSV ファイル読み込み","description":"ストリーム処理における CSV ファイルの読み込み Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。\nしかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。\nstar schema における dimension table 的な情報をストリームに結合したい場合 等が考えられる。\nこのポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。\nFlink は現時点の stable である v1.11 を想定。\nCSV ファイルを読む方法 DataStream API ベースの実装で CSV ファイルを読むには StreamExecutionEnvironment のメソッドである readFile() を使う。\noverload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。\nまず1つめは FileInputFormat\u0026lt;OUT\u0026gt; inputFormat であり、こちらは data stream の生成に用いる入力フォーマットを指定する。\nおそらく最も一般的なのが TextInputFormat だと思われる。\nもちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。\nPojoCsvInputFormat RowCsvInputFormat TupleCsvInputFormat なんとなく名前でわかると思うが、それぞれ readFile() の結果として返される DataStreamSource が内包する型が異なる。","keywords":["Apache Flink","stream processing"],"articleBody":"ストリーム処理における CSV ファイルの読み込み Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。\nしかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。\nstar schema における dimension table 的な情報をストリームに結合したい場合 等が考えられる。\nこのポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。\nFlink は現時点の stable である v1.11 を想定。\nCSV ファイルを読む方法 DataStream API ベースの実装で CSV ファイルを読むには StreamExecutionEnvironment のメソッドである readFile() を使う。\noverload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。\nまず1つめは FileInputFormat inputFormat であり、こちらは data stream の生成に用いる入力フォーマットを指定する。\nおそらく最も一般的なのが TextInputFormat だと思われる。\nもちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。\nPojoCsvInputFormat RowCsvInputFormat TupleCsvInputFormat なんとなく名前でわかると思うが、それぞれ readFile() の結果として返される DataStreamSource が内包する型が異なる。\nこれについては後述の実験にて確認する。\n次に FileProcessingMode watchType も見ておきたい。\nこの引数ではデータソースの監視についてのモードを指定する。\nモードは2つある。\nFileProcessingMode.PROCESS_CONTINUOUSLY 対象のファイルが更新され、その更新に追随する必要がある場合に利用 指定のインターバルでファイルの更新をチェック 更新があった場合はファイル全体を読む FileProcessingMode.PROCESS_ONCE 対象のファイルの更新がない、更新について考えない場合に利用 最初に一度だけファイルを読む おそらく多くの場合は前者が必要になるのではないだろうか。\n利用にあたっては更新があった場合にファイル全体が読まれるということに注意が必要だ。\n例えばファイル末尾にレコードを1件追加するような更新であったとしても、全レコードが再度ストリームに流されるということである。\n詳しくは ドキュメント を参照。\nこれはファイル全体で1つの atomic な単位だとみなされているものと思われる。\nレコード単位で処理していくストリーム処理にファイルというバルクな単位のデータを流そうとしているのでこうなってしまう。\nそう考えるとやはり static なファイルのデータは dimension table として情報を付加するような、ストリームの本川に合流する支川のような使い方が想定されているのだろう。\nちなみに CsvReader というものもあるが、こちらは DataSet API、つまりバッチ処理向けのようなので今回は扱わない。\n実験 実際にコードを書いて readFile() で CSV を読んでみる。\nここでは PojoCsvInputFormat と TupleCsvInputFormat を切り替えられるようにした。\nコード package com.example.entry import org.apache.flink.api.common.typeinfo.BasicTypeInfo import org.apache.flink.api.java.io.{PojoCsvInputFormat, TupleCsvInputFormat} import org.apache.flink.api.java.tuple.Tuple3 import org.apache.flink.api.java.typeutils.{PojoField, PojoTypeInfo, TupleTypeInfo} import org.apache.flink.core.fs.Path import org.apache.flink.streaming.api.functions.source.FileProcessingMode import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, createTypeInformation} import scala.collection.JavaConverters._ import scala.concurrent.duration._ /** * The experiment to read CSV file by Flink. * It reads CSV file as POJOs or tuples and just prints on console. */ object ReadCsvFileExperimentRunner { /** POJO */ case class Company(name: String, ticker: String, numEmployees: Int) /** Tuple */ type CompanyTuple = Tuple3[String, String, Int] def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val companiesFilePath = \"data/companies.csv\" val interval = 10.seconds args.headOption match { case None | Some(\"pojo\") =\u003e val inputFormat = createPojoCsvInputFormat(companiesFilePath) env .readFile(inputFormat, companiesFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, interval.toMillis) .map(_.toString) .print() case Some(\"tuple\") =\u003e val inputFormat = createTupleCsvInputFormat(companiesFilePath) env .readFile(inputFormat, companiesFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, interval.toMillis) .map(_.toString) .print() case _ =\u003e throw new RuntimeException(s\"Unsupported input format: ${args(0)}\") } env.execute() } private def createPojoCsvInputFormat(csvFilePath: String): PojoCsvInputFormat[Company] = { val clazz = classOf[Company] val pojoFields = Seq( new PojoField(clazz.getDeclaredField(\"name\"), BasicTypeInfo.STRING_TYPE_INFO), new PojoField(clazz.getDeclaredField(\"ticker\"), BasicTypeInfo.STRING_TYPE_INFO), new PojoField(clazz.getDeclaredField(\"numEmployees\"), BasicTypeInfo.INT_TYPE_INFO) ).asJava val pojoTypeInfo = new PojoTypeInfo[Company](clazz, pojoFields) val fieldNames = Array(\"name\", \"ticker\", \"numEmployees\") val inputFormat = new PojoCsvInputFormat[Company](new Path(csvFilePath), pojoTypeInfo, fieldNames) inputFormat.setSkipFirstLineAsHeader(true) inputFormat } private def createTupleCsvInputFormat(csvFilePath: String): TupleCsvInputFormat[CompanyTuple] = { val types = Seq( BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.INT_TYPE_INFO ) val tupleTypeInfo = new TupleTypeInfo[CompanyTuple](classOf[CompanyTuple], types: _*) val inputFormat = new TupleCsvInputFormat[CompanyTuple](new Path(csvFilePath), tupleTypeInfo) inputFormat.setSkipFirstLineAsHeader(true) inputFormat } } ポイントは POJO 版も tuple 版も型情報を作ってやる必要があるということだ。\nそれぞれ PojoTypeInfo, TupleTypeInfo を用意してやる必要があり、これがやや癖があって面倒。\nあるフィールドを数値として読むことは可能だが、日付の parse のようなことはできないようである。\nというのを考えると TextInputFormat で読んで自分で parse するのと比べてあまりうれしくないような…\nデータ 実験用のデータとして会社情報を示す簡単な CSV ファイルを適当に作って data/companies.csv に配置。\nname,ticker,num_employees Alphabet Inc,GOOG,98771 Apple Inc,AAPL,147000 Facebook Inc,FB,49942 Amazon.com Inc,AMZN,798000 実行 まずは POJO 版を実行してみた。\nプログラムが起動するとすぐに以下が出力された。\n[info] running com.example.entry.ReadCsvFileExperimentRunner pojo 2\u003e Company(Alphabet Inc,GOOG,98771) 1\u003e Company(Facebook Inc,FB,49942) 2\u003e Company(Apple Inc,AAPL,147000) 1\u003e Company(Amazon.com Inc,AMZN,798000) Company インスタンスとして CSV ファイルの内容を取得できている。\nプログラムは止まっていないが CSV ファイルの内容を一通り吐き出したところで出力は止まった。\nここで CSV ファイルに次の1行を追加してみる。\nMicrosoft Corporation,MSFT,163000 すると出力は\n[info] running com.example.entry.ReadCsvFileExperimentRunner pojo 2\u003e Company(Alphabet Inc,GOOG,98771) 1\u003e Company(Facebook Inc,FB,49942) 2\u003e Company(Apple Inc,AAPL,147000) 1\u003e Company(Amazon.com Inc,AMZN,798000) 2\u003e Company(Alphabet Inc,GOOG,98771) 1\u003e Company(Amazon.com Inc,AMZN,798000) 2\u003e Company(Apple Inc,AAPL,147000) 1\u003e Company(Microsoft Corporation,MSFT,163000) 2\u003e Company(Facebook Inc,FB,49942) となり、最初に出力された4行に加えて新たに5行追加された。\nCSV ファイルには1行追加しただけだが、既存の行も含む CSV ファイル全体が再度出力された。\nドキュメントに記載されているとおりの仕様となっている。\ntuple 版で実行すると出力は次のようになった。\n[info] running com.example.entry.ReadCsvFileExperimentRunner tuple 1\u003e (Facebook Inc,FB,49942) 1\u003e (Amazon.com Inc,AMZN,798000) 2\u003e (Alphabet Inc,GOOG,98771) 2\u003e (Apple Inc,AAPL,147000) tuple として読めているようだ。\nwatchType が同じなので CSV ファイルの更新についての挙動は同様だった。\nちなみに実行可能なプロジェクトは GitHub に置いている。\nsbt 'runMain com.example.entry.ReadCsvFileExperimentRunner pojo' または sbt 'runMain com.example.entry.ReadCsvFileExperimentRunner tuple' で実行できる。\n(Ctrl + C で終了)\nまとめ TextInputFormat で読んで自分で parse するのと比べ、*CsvInputFormat を使う方法はコーディングとしてはあまりメリットが感じられなかった。\nまた、ストリーム処理においてやはりファイルというデータソースは傍流なんだなという感じ。\nちなみに Table API で CSV を読むこともおそらく可能。\n気が向いたら書く。\n","wordCount":"498","inLanguage":"ja","image":"https://soonraah.github.io/image/photo/mika-baumeister-Wpnoqo2plFA-unsplash.jpg","datePublished":"2020-12-01T00:30:00+09:00","dateModified":"2020-12-01T00:30:00+09:00","author":{"@type":"Person","name":"soonraah"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://soonraah.github.io/posts/read-csv-by-flink-datastream-api/"},"publisher":{"@type":"Organization","name":"Froglog","logo":{"@type":"ImageObject","url":"https://soonraah.github.io/favicon2.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://soonraah.github.io accesskey=h title="Home (Alt + H)"><img src=https://soonraah.github.io/image/brand/favicon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://soonraah.github.io/about/ title=About><span>About</span></a></li><li><a href=https://soonraah.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://soonraah.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://soonraah.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://soonraah.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://soonraah.github.io>ホーム</a>&nbsp;»&nbsp;<a href=https://soonraah.github.io/posts/>Posts</a></div><h1 class=post-title>Apache Flink の DataStream API 利用時の CSV ファイル読み込み</h1><div class=post-meta><span title='2020-12-01 00:30:00 +0900 JST'>12月 1, 2020</span>&nbsp;·&nbsp;soonraah</div></header><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/photo/mika-baumeister-Wpnoqo2plFA-unsplash.jpg alt=CSV><p><a href=https://unsplash.com/photos/Wpnoqo2plFA>Photo by Mika Baumeister on Unsplash</a></p></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目次</span></summary><div class=inner><ul><li><a href=#%e3%82%b9%e3%83%88%e3%83%aa%e3%83%bc%e3%83%a0%e5%87%a6%e7%90%86%e3%81%ab%e3%81%8a%e3%81%91%e3%82%8b-csv-%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e3%81%ae%e8%aa%ad%e3%81%bf%e8%be%bc%e3%81%bf aria-label="ストリーム処理における CSV ファイルの読み込み">ストリーム処理における CSV ファイルの読み込み</a></li><li><a href=#csv-%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e3%82%92%e8%aa%ad%e3%82%80%e6%96%b9%e6%b3%95 aria-label="CSV ファイルを読む方法">CSV ファイルを読む方法</a></li><li><a href=#%e5%ae%9f%e9%a8%93 aria-label=実験>実験</a><ul><li><a href=#%e3%82%b3%e3%83%bc%e3%83%89 aria-label=コード>コード</a></li><li><a href=#%e3%83%87%e3%83%bc%e3%82%bf aria-label=データ>データ</a></li><li><a href=#%e5%ae%9f%e8%a1%8c aria-label=実行>実行</a></li></ul></li><li><a href=#%e3%81%be%e3%81%a8%e3%82%81 aria-label=まとめ>まとめ</a></li></ul></div></details></div><div class=post-content><h2 id=ストリーム処理における-csv-ファイルの読み込み>ストリーム処理における CSV ファイルの読み込み<a hidden class=anchor aria-hidden=true href=#ストリーム処理における-csv-ファイルの読み込み>#</a></h2><p>Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。<br>しかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。<br><a href=https://soonraah.github.io/posts/flink-join-by-broadcast-state-pattern/>star schema における dimension table 的な情報をストリームに結合したい場合</a> 等が考えられる。</p><p>このポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。<br>Flink は現時点の stable である v1.11 を想定。</p><h2 id=csv-ファイルを読む方法>CSV ファイルを読む方法<a hidden class=anchor aria-hidden=true href=#csv-ファイルを読む方法>#</a></h2><p>DataStream API ベースの実装で CSV ファイルを読むには <code>StreamExecutionEnvironment</code> のメソッドである <a href=https://ci.apache.org/projects/flink/flink-docs-release-1.11/api/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.html#readFile-org.apache.flink.api.common.io.FileInputFormat-java.lang.String-org.apache.flink.streaming.api.functions.source.FileProcessingMode-long-><code>readFile()</code></a> を使う。<br>overload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。</p><p>まず1つめは <code>FileInputFormat&lt;OUT> inputFormat</code> であり、こちらは data stream の生成に用いる入力フォーマットを指定する。<br>おそらく最も一般的なのが <code>TextInputFormat</code> だと思われる。<br>もちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。</p><ul><li><code>PojoCsvInputFormat</code></li><li><code>RowCsvInputFormat</code></li><li><code>TupleCsvInputFormat</code></li></ul><p>なんとなく名前でわかると思うが、それぞれ <code>readFile()</code> の結果として返される <code>DataStreamSource</code> が内包する型が異なる。<br>これについては後述の実験にて確認する。</p><p>次に <code>FileProcessingMode watchType</code> も見ておきたい。<br>この引数ではデータソースの監視についてのモードを指定する。<br>モードは2つある。</p><ul><li><code>FileProcessingMode.PROCESS_CONTINUOUSLY</code><ul><li>対象のファイルが更新され、その更新に追随する必要がある場合に利用</li><li>指定のインターバルでファイルの更新をチェック</li><li><em>更新があった場合はファイル全体を読む</em></li></ul></li><li><code>FileProcessingMode.PROCESS_ONCE</code><ul><li>対象のファイルの更新がない、更新について考えない場合に利用</li><li>最初に一度だけファイルを読む</li></ul></li></ul><p>おそらく多くの場合は前者が必要になるのではないだろうか。<br>利用にあたっては更新があった場合にファイル全体が読まれるということに注意が必要だ。<br>例えばファイル末尾にレコードを1件追加するような更新であったとしても、全レコードが再度ストリームに流されるということである。<br>詳しくは <a href=https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html#data-sources>ドキュメント</a> を参照。</p><p>これはファイル全体で1つの atomic な単位だとみなされているものと思われる。<br>レコード単位で処理していくストリーム処理にファイルというバルクな単位のデータを流そうとしているのでこうなってしまう。<br>そう考えるとやはり static なファイルのデータは dimension table として情報を付加するような、ストリームの本川に合流する支川のような使い方が想定されているのだろう。</p><p>ちなみに <a href=https://ci.apache.org/projects/flink/flink-docs-release-1.11/api/java/org/apache/flink/api/java/io/CsvReader.html><code>CsvReader</code></a> というものもあるが、こちらは DataSet API、つまりバッチ処理向けのようなので今回は扱わない。</p><h2 id=実験>実験<a hidden class=anchor aria-hidden=true href=#実験>#</a></h2><p>実際にコードを書いて <code>readFile()</code> で CSV を読んでみる。<br>ここでは <code>PojoCsvInputFormat</code> と <code>TupleCsvInputFormat</code> を切り替えられるようにした。</p><h3 id=コード>コード<a hidden class=anchor aria-hidden=true href=#コード>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>package</span> com.example.entry
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> org.apache.flink.api.common.typeinfo.BasicTypeInfo
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> org.apache.flink.api.java.io.<span style=color:#f92672>{</span><span style=color:#a6e22e>PojoCsvInputFormat</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>TupleCsvInputFormat</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> org.apache.flink.api.java.tuple.Tuple3
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> org.apache.flink.api.java.typeutils.<span style=color:#f92672>{</span><span style=color:#a6e22e>PojoField</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>PojoTypeInfo</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>TupleTypeInfo</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> org.apache.flink.core.fs.Path
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> org.apache.flink.streaming.api.functions.source.FileProcessingMode
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> org.apache.flink.streaming.api.scala.<span style=color:#f92672>{</span><span style=color:#a6e22e>StreamExecutionEnvironment</span><span style=color:#f92672>,</span> createTypeInformation<span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> scala.collection.JavaConverters._
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> scala.concurrent.duration._
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>/**
</span></span></span><span style=display:flex><span><span style=color:#75715e> * The experiment to read CSV file by Flink.
</span></span></span><span style=display:flex><span><span style=color:#75715e> * It reads CSV file as POJOs or tuples and just prints on console.
</span></span></span><span style=display:flex><span><span style=color:#75715e> */</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>object</span> <span style=color:#a6e22e>ReadCsvFileExperimentRunner</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>/** POJO */</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>case</span> <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Company</span><span style=color:#f92672>(</span>name<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>String</span><span style=color:#f92672>,</span> ticker<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>String</span><span style=color:#f92672>,</span> numEmployees<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>Int</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>/** Tuple */</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>type</span> <span style=color:#66d9ef>CompanyTuple</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>Tuple3</span><span style=color:#f92672>[</span><span style=color:#66d9ef>String</span>, <span style=color:#66d9ef>String</span>, <span style=color:#66d9ef>Int</span><span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> main<span style=color:#f92672>(</span>args<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>Array</span><span style=color:#f92672>[</span><span style=color:#66d9ef>String</span><span style=color:#f92672>])</span><span style=color:#66d9ef>:</span> <span style=color:#66d9ef>Unit</span> <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>val</span> env <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>StreamExecutionEnvironment</span><span style=color:#f92672>.</span>getExecutionEnvironment
</span></span><span style=display:flex><span>    env<span style=color:#f92672>.</span>setParallelism<span style=color:#f92672>(</span><span style=color:#ae81ff>2</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>val</span> companiesFilePath <span style=color:#66d9ef>=</span> <span style=color:#e6db74>&#34;data/companies.csv&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>val</span> interval <span style=color:#66d9ef>=</span> <span style=color:#ae81ff>10.</span>seconds
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    args<span style=color:#f92672>.</span>headOption <span style=color:#66d9ef>match</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>case</span> <span style=color:#a6e22e>None</span> <span style=color:#f92672>|</span> <span style=color:#a6e22e>Some</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;pojo&#34;</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>=&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>val</span> inputFormat <span style=color:#66d9ef>=</span> createPojoCsvInputFormat<span style=color:#f92672>(</span>companiesFilePath<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        env
</span></span><span style=display:flex><span>          <span style=color:#f92672>.</span>readFile<span style=color:#f92672>(</span>inputFormat<span style=color:#f92672>,</span> companiesFilePath<span style=color:#f92672>,</span> <span style=color:#a6e22e>FileProcessingMode</span><span style=color:#f92672>.</span><span style=color:#a6e22e>PROCESS_CONTINUOUSLY</span><span style=color:#f92672>,</span> interval<span style=color:#f92672>.</span>toMillis<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>.</span>map<span style=color:#f92672>(</span><span style=color:#66d9ef>_</span><span style=color:#f92672>.</span>toString<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>.</span>print<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>case</span> <span style=color:#a6e22e>Some</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;tuple&#34;</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>=&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>val</span> inputFormat <span style=color:#66d9ef>=</span> createTupleCsvInputFormat<span style=color:#f92672>(</span>companiesFilePath<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        env
</span></span><span style=display:flex><span>          <span style=color:#f92672>.</span>readFile<span style=color:#f92672>(</span>inputFormat<span style=color:#f92672>,</span> companiesFilePath<span style=color:#f92672>,</span> <span style=color:#a6e22e>FileProcessingMode</span><span style=color:#f92672>.</span><span style=color:#a6e22e>PROCESS_CONTINUOUSLY</span><span style=color:#f92672>,</span> interval<span style=color:#f92672>.</span>toMillis<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>.</span>map<span style=color:#f92672>(</span><span style=color:#66d9ef>_</span><span style=color:#f92672>.</span>toString<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>.</span>print<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>case</span> <span style=color:#66d9ef>_</span> <span style=color:#66d9ef>=&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>throw</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>RuntimeException</span><span style=color:#f92672>(</span><span style=color:#e6db74>s&#34;Unsupported input format: </span><span style=color:#e6db74>${</span>args<span style=color:#f92672>(</span><span style=color:#ae81ff>0</span><span style=color:#f92672>)</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    env<span style=color:#f92672>.</span>execute<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>def</span> createPojoCsvInputFormat<span style=color:#f92672>(</span>csvFilePath<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>String</span><span style=color:#f92672>)</span><span style=color:#66d9ef>:</span> <span style=color:#66d9ef>PojoCsvInputFormat</span><span style=color:#f92672>[</span><span style=color:#66d9ef>Company</span><span style=color:#f92672>]</span> <span style=color:#66d9ef>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>val</span> clazz <span style=color:#66d9ef>=</span> classOf<span style=color:#f92672>[</span><span style=color:#66d9ef>Company</span><span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>val</span> pojoFields <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>Seq</span><span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>PojoField</span><span style=color:#f92672>(</span>clazz<span style=color:#f92672>.</span>getDeclaredField<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;name&#34;</span><span style=color:#f92672>),</span> <span style=color:#a6e22e>BasicTypeInfo</span><span style=color:#f92672>.</span><span style=color:#a6e22e>STRING_TYPE_INFO</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>PojoField</span><span style=color:#f92672>(</span>clazz<span style=color:#f92672>.</span>getDeclaredField<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;ticker&#34;</span><span style=color:#f92672>),</span> <span style=color:#a6e22e>BasicTypeInfo</span><span style=color:#f92672>.</span><span style=color:#a6e22e>STRING_TYPE_INFO</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>PojoField</span><span style=color:#f92672>(</span>clazz<span style=color:#f92672>.</span>getDeclaredField<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;numEmployees&#34;</span><span style=color:#f92672>),</span> <span style=color:#a6e22e>BasicTypeInfo</span><span style=color:#f92672>.</span><span style=color:#a6e22e>INT_TYPE_INFO</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>).</span>asJava
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>val</span> pojoTypeInfo <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>PojoTypeInfo</span><span style=color:#f92672>[</span><span style=color:#66d9ef>Company</span><span style=color:#f92672>](</span>clazz<span style=color:#f92672>,</span> pojoFields<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>val</span> fieldNames <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>Array</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;name&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;ticker&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;numEmployees&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>val</span> inputFormat <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>PojoCsvInputFormat</span><span style=color:#f92672>[</span><span style=color:#66d9ef>Company</span><span style=color:#f92672>](</span><span style=color:#66d9ef>new</span> <span style=color:#a6e22e>Path</span><span style=color:#f92672>(</span>csvFilePath<span style=color:#f92672>),</span> pojoTypeInfo<span style=color:#f92672>,</span> fieldNames<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    inputFormat<span style=color:#f92672>.</span>setSkipFirstLineAsHeader<span style=color:#f92672>(</span><span style=color:#66d9ef>true</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    inputFormat
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>def</span> createTupleCsvInputFormat<span style=color:#f92672>(</span>csvFilePath<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>String</span><span style=color:#f92672>)</span><span style=color:#66d9ef>:</span> <span style=color:#66d9ef>TupleCsvInputFormat</span><span style=color:#f92672>[</span><span style=color:#66d9ef>CompanyTuple</span><span style=color:#f92672>]</span> <span style=color:#66d9ef>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>val</span> types <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>Seq</span><span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>BasicTypeInfo</span><span style=color:#f92672>.</span><span style=color:#a6e22e>STRING_TYPE_INFO</span><span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>BasicTypeInfo</span><span style=color:#f92672>.</span><span style=color:#a6e22e>STRING_TYPE_INFO</span><span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>BasicTypeInfo</span><span style=color:#f92672>.</span><span style=color:#a6e22e>INT_TYPE_INFO</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>val</span> tupleTypeInfo <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>TupleTypeInfo</span><span style=color:#f92672>[</span><span style=color:#66d9ef>CompanyTuple</span><span style=color:#f92672>](</span>classOf<span style=color:#f92672>[</span><span style=color:#66d9ef>CompanyTuple</span><span style=color:#f92672>],</span> types<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>_</span><span style=color:#66d9ef>*</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>val</span> inputFormat <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>TupleCsvInputFormat</span><span style=color:#f92672>[</span><span style=color:#66d9ef>CompanyTuple</span><span style=color:#f92672>](</span><span style=color:#66d9ef>new</span> <span style=color:#a6e22e>Path</span><span style=color:#f92672>(</span>csvFilePath<span style=color:#f92672>),</span> tupleTypeInfo<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    inputFormat<span style=color:#f92672>.</span>setSkipFirstLineAsHeader<span style=color:#f92672>(</span><span style=color:#66d9ef>true</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    inputFormat
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>ポイントは POJO 版も tuple 版も型情報を作ってやる必要があるということだ。<br>それぞれ <code>PojoTypeInfo</code>, <code>TupleTypeInfo</code> を用意してやる必要があり、これがやや癖があって面倒。<br>あるフィールドを数値として読むことは可能だが、日付の parse のようなことはできないようである。<br>というのを考えると <code>TextInputFormat</code> で読んで自分で parse するのと比べてあまりうれしくないような…</p><h3 id=データ>データ<a hidden class=anchor aria-hidden=true href=#データ>#</a></h3><p>実験用のデータとして会社情報を示す簡単な CSV ファイルを適当に作って <code>data/companies.csv</code> に配置。</p><pre tabindex=0><code>name,ticker,num_employees
Alphabet Inc,GOOG,98771
Apple Inc,AAPL,147000
Facebook Inc,FB,49942
Amazon.com Inc,AMZN,798000
</code></pre><h3 id=実行>実行<a hidden class=anchor aria-hidden=true href=#実行>#</a></h3><p>まずは POJO 版を実行してみた。<br>プログラムが起動するとすぐに以下が出力された。</p><pre tabindex=0><code>[info] running com.example.entry.ReadCsvFileExperimentRunner pojo
2&gt; Company(Alphabet Inc,GOOG,98771)
1&gt; Company(Facebook Inc,FB,49942)
2&gt; Company(Apple Inc,AAPL,147000)
1&gt; Company(Amazon.com Inc,AMZN,798000)
</code></pre><p><code>Company</code> インスタンスとして CSV ファイルの内容を取得できている。<br>プログラムは止まっていないが CSV ファイルの内容を一通り吐き出したところで出力は止まった。<br>ここで CSV ファイルに次の1行を追加してみる。</p><pre tabindex=0><code>Microsoft Corporation,MSFT,163000
</code></pre><p>すると出力は</p><pre tabindex=0><code>[info] running com.example.entry.ReadCsvFileExperimentRunner pojo
2&gt; Company(Alphabet Inc,GOOG,98771)
1&gt; Company(Facebook Inc,FB,49942)
2&gt; Company(Apple Inc,AAPL,147000)
1&gt; Company(Amazon.com Inc,AMZN,798000)
2&gt; Company(Alphabet Inc,GOOG,98771)
1&gt; Company(Amazon.com Inc,AMZN,798000)
2&gt; Company(Apple Inc,AAPL,147000)
1&gt; Company(Microsoft Corporation,MSFT,163000)
2&gt; Company(Facebook Inc,FB,49942)
</code></pre><p>となり、最初に出力された4行に加えて新たに5行追加された。<br>CSV ファイルには1行追加しただけだが、既存の行も含む CSV ファイル全体が再度出力された。<br>ドキュメントに記載されているとおりの仕様となっている。</p><p>tuple 版で実行すると出力は次のようになった。</p><pre tabindex=0><code>[info] running com.example.entry.ReadCsvFileExperimentRunner tuple
1&gt; (Facebook Inc,FB,49942)
1&gt; (Amazon.com Inc,AMZN,798000)
2&gt; (Alphabet Inc,GOOG,98771)
2&gt; (Apple Inc,AAPL,147000)
</code></pre><p>tuple として読めているようだ。<br><code>watchType</code> が同じなので CSV ファイルの更新についての挙動は同様だった。</p><p>ちなみに実行可能なプロジェクトは <a href=https://github.com/soonraah/flink-experiment>GitHub</a> に置いている。<br><code>sbt 'runMain com.example.entry.ReadCsvFileExperimentRunner pojo' </code>または <code>sbt 'runMain com.example.entry.ReadCsvFileExperimentRunner tuple' </code>で実行できる。<br>(<code>Ctrl + C</code> で終了)</p><h2 id=まとめ>まとめ<a hidden class=anchor aria-hidden=true href=#まとめ>#</a></h2><p><code>TextInputFormat</code> で読んで自分で parse するのと比べ、<code>*CsvInputFormat</code> を使う方法はコーディングとしてはあまりメリットが感じられなかった。<br>また、ストリーム処理においてやはりファイルというデータソースは傍流なんだなという感じ。</p><p>ちなみに Table API で CSV を読むこともおそらく可能。<br>気が向いたら書く。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://soonraah.github.io/tags/apache-flink/>Apache Flink</a></li><li><a href=https://soonraah.github.io/tags/stream-processing/>stream processing</a></li></ul><nav class=paginav><a class=prev href=https://soonraah.github.io/posts/what-is-a-data-lake/><span class=title>« 前へ</span><br><span>いまさらながらのデータレイク</span>
</a><a class=next href=https://soonraah.github.io/posts/ml-accuracy-profit-ethic-issue/><span class=title>次へ »</span><br><span>機械学習の精度と利益と倫理とイシューと</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Apache Flink の DataStream API 利用時の CSV ファイル読み込み on x" href="https://x.com/intent/tweet/?text=Apache%20Flink%20%e3%81%ae%20DataStream%20API%20%e5%88%a9%e7%94%a8%e6%99%82%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e8%aa%ad%e3%81%bf%e8%be%bc%e3%81%bf&amp;url=https%3a%2f%2fsoonraah.github.io%2fposts%2fread-csv-by-flink-datastream-api%2f&amp;hashtags=ApacheFlink%2cstreamprocessing"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Flink の DataStream API 利用時の CSV ファイル読み込み on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsoonraah.github.io%2fposts%2fread-csv-by-flink-datastream-api%2f&amp;title=Apache%20Flink%20%e3%81%ae%20DataStream%20API%20%e5%88%a9%e7%94%a8%e6%99%82%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e8%aa%ad%e3%81%bf%e8%be%bc%e3%81%bf&amp;summary=Apache%20Flink%20%e3%81%ae%20DataStream%20API%20%e5%88%a9%e7%94%a8%e6%99%82%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e8%aa%ad%e3%81%bf%e8%be%bc%e3%81%bf&amp;source=https%3a%2f%2fsoonraah.github.io%2fposts%2fread-csv-by-flink-datastream-api%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Flink の DataStream API 利用時の CSV ファイル読み込み on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsoonraah.github.io%2fposts%2fread-csv-by-flink-datastream-api%2f&title=Apache%20Flink%20%e3%81%ae%20DataStream%20API%20%e5%88%a9%e7%94%a8%e6%99%82%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e8%aa%ad%e3%81%bf%e8%be%bc%e3%81%bf"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Flink の DataStream API 利用時の CSV ファイル読み込み on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsoonraah.github.io%2fposts%2fread-csv-by-flink-datastream-api%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Flink の DataStream API 利用時の CSV ファイル読み込み on whatsapp" href="https://api.whatsapp.com/send?text=Apache%20Flink%20%e3%81%ae%20DataStream%20API%20%e5%88%a9%e7%94%a8%e6%99%82%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e8%aa%ad%e3%81%bf%e8%be%bc%e3%81%bf%20-%20https%3a%2f%2fsoonraah.github.io%2fposts%2fread-csv-by-flink-datastream-api%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Flink の DataStream API 利用時の CSV ファイル読み込み on telegram" href="https://telegram.me/share/url?text=Apache%20Flink%20%e3%81%ae%20DataStream%20API%20%e5%88%a9%e7%94%a8%e6%99%82%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e8%aa%ad%e3%81%bf%e8%be%bc%e3%81%bf&amp;url=https%3a%2f%2fsoonraah.github.io%2fposts%2fread-csv-by-flink-datastream-api%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Flink の DataStream API 利用時の CSV ファイル読み込み on ycombinator" href="https://news.ycombinator.com/submitlink?t=Apache%20Flink%20%e3%81%ae%20DataStream%20API%20%e5%88%a9%e7%94%a8%e6%99%82%e3%81%ae%20CSV%20%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e8%aa%ad%e3%81%bf%e8%be%bc%e3%81%bf&u=https%3a%2f%2fsoonraah.github.io%2fposts%2fread-csv-by-flink-datastream-api%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://soonraah.github.io>Froglog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="コピー";function s(){t.innerHTML="コピーされました!",setTimeout(()=>{t.innerHTML="コピー"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>