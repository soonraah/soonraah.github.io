<!doctype html><html lang=ja dir=auto><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-NSEGH2YT17"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NSEGH2YT17")</script><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>near real time で更新される Apache Iceberg の table のメンテナンス | Froglog</title><meta name=keywords content="Data Lake,Lakehouse,Apache Iceberg"><meta name=description content="前回のポストでは merge on read で Apache Iceberg の table を near real time で更新するということを行った。
このポストではそのメンテナンスについて触れて、かつそれを実行してみる。
merge on read の課題 merge on read で table を更新する場合、copy on write の場合と違い table 全体を洗い替えする必要はなく差分のみを追記することになる。
したがって更新にかかる時間は copy on write よりも短くなる。
一方で merge on read の名のとおり読み出し時に積み重なった差分とベースを merge して最新の snapshot とするため、読み出しの速度は copy on write より遅くなる。
長時間更新され差分がたくさん存在しているとなおさら遅い。
なので
更新頻度が低く、参照頻度が高いユースケース -> copy on write 更新頻度が高く、参照頻度が低いユースケース -> merge on write という使い分けがよいとされている。
前回ポストの例では一晩更新を続けた後の merge on read の table に対して簡単な select 文を実行したところ、6分程度かかってしまった。"><meta name=author content="soonraah"><link rel=canonical href=https://soonraah.github.io/posts/maintain-iceberg-table-updated-in-near-real-time/><link crossorigin=anonymous href=/assets/css/stylesheet.min.ec8da366ca2fb647537ccb7a8f6fa5b4e9cd3c7a0d3171dd2d3baad1e49c8bfc.css integrity="sha256-7I2jZsovtkdTfMt6j2+ltOnNPHoNMXHdLTuq0eSci/w=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://soonraah.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://soonraah.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://soonraah.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://soonraah.github.io/apple-touch-icon.png><link rel=mask-icon href=https://soonraah.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-73329599-2","auto"),ga("send","pageview"))</script><meta property="og:title" content="near real time で更新される Apache Iceberg の table のメンテナンス"><meta property="og:description" content="前回のポストでは merge on read で Apache Iceberg の table を near real time で更新するということを行った。
このポストではそのメンテナンスについて触れて、かつそれを実行してみる。
merge on read の課題 merge on read で table を更新する場合、copy on write の場合と違い table 全体を洗い替えする必要はなく差分のみを追記することになる。
したがって更新にかかる時間は copy on write よりも短くなる。
一方で merge on read の名のとおり読み出し時に積み重なった差分とベースを merge して最新の snapshot とするため、読み出しの速度は copy on write より遅くなる。
長時間更新され差分がたくさん存在しているとなおさら遅い。
なので
更新頻度が低く、参照頻度が高いユースケース -> copy on write 更新頻度が高く、参照頻度が低いユースケース -> merge on write という使い分けがよいとされている。
前回ポストの例では一晩更新を続けた後の merge on read の table に対して簡単な select 文を実行したところ、6分程度かかってしまった。"><meta property="og:type" content="article"><meta property="og:url" content="https://soonraah.github.io/posts/maintain-iceberg-table-updated-in-near-real-time/"><meta property="og:image" content="https://soonraah.github.io/image/photo/annie-spratt-Tno1Zd3T6yY-unsplash.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-28T09:00:00+09:00"><meta property="article:modified_time" content="2023-05-28T09:00:00+09:00"><meta property="og:site_name" content="Froglog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://soonraah.github.io/image/photo/annie-spratt-Tno1Zd3T6yY-unsplash.jpg"><meta name=twitter:title content="near real time で更新される Apache Iceberg の table のメンテナンス"><meta name=twitter:description content="前回のポストでは merge on read で Apache Iceberg の table を near real time で更新するということを行った。
このポストではそのメンテナンスについて触れて、かつそれを実行してみる。
merge on read の課題 merge on read で table を更新する場合、copy on write の場合と違い table 全体を洗い替えする必要はなく差分のみを追記することになる。
したがって更新にかかる時間は copy on write よりも短くなる。
一方で merge on read の名のとおり読み出し時に積み重なった差分とベースを merge して最新の snapshot とするため、読み出しの速度は copy on write より遅くなる。
長時間更新され差分がたくさん存在しているとなおさら遅い。
なので
更新頻度が低く、参照頻度が高いユースケース -> copy on write 更新頻度が高く、参照頻度が低いユースケース -> merge on write という使い分けがよいとされている。
前回ポストの例では一晩更新を続けた後の merge on read の table に対して簡単な select 文を実行したところ、6分程度かかってしまった。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://soonraah.github.io/posts/"},{"@type":"ListItem","position":3,"name":"near real time で更新される Apache Iceberg の table のメンテナンス","item":"https://soonraah.github.io/posts/maintain-iceberg-table-updated-in-near-real-time/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"near real time で更新される Apache Iceberg の table のメンテナンス","name":"near real time で更新される Apache Iceberg の table のメンテナンス","description":"前回のポストでは merge on read で Apache Iceberg の table を near real time で更新するということを行った。\nこのポストではそのメンテナンスについて触れて、かつそれを実行してみる。\nmerge on read の課題 merge on read で table を更新する場合、copy on write の場合と違い table 全体を洗い替えする必要はなく差分のみを追記することになる。\nしたがって更新にかかる時間は copy on write よりも短くなる。\n一方で merge on read の名のとおり読み出し時に積み重なった差分とベースを merge して最新の snapshot とするため、読み出しの速度は copy on write より遅くなる。\n長時間更新され差分がたくさん存在しているとなおさら遅い。\nなので\n更新頻度が低く、参照頻度が高いユースケース -\u0026gt; copy on write 更新頻度が高く、参照頻度が低いユースケース -\u0026gt; merge on write という使い分けがよいとされている。\n前回ポストの例では一晩更新を続けた後の merge on read の table に対して簡単な select 文を実行したところ、6分程度かかってしまった。","keywords":["Data Lake","Lakehouse","Apache Iceberg"],"articleBody":"前回のポストでは merge on read で Apache Iceberg の table を near real time で更新するということを行った。\nこのポストではそのメンテナンスについて触れて、かつそれを実行してみる。\nmerge on read の課題 merge on read で table を更新する場合、copy on write の場合と違い table 全体を洗い替えする必要はなく差分のみを追記することになる。\nしたがって更新にかかる時間は copy on write よりも短くなる。\n一方で merge on read の名のとおり読み出し時に積み重なった差分とベースを merge して最新の snapshot とするため、読み出しの速度は copy on write より遅くなる。\n長時間更新され差分がたくさん存在しているとなおさら遅い。\nなので\n更新頻度が低く、参照頻度が高いユースケース -\u003e copy on write 更新頻度が高く、参照頻度が低いユースケース -\u003e merge on write という使い分けがよいとされている。\n前回ポストの例では一晩更新を続けた後の merge on read の table に対して簡単な select 文を実行したところ、6分程度かかってしまった。\nレコード数はたかだか128件程度であることを考えるとかなり遅いと言える。\nこのままでは使い物にならない。\nしかし更新頻度が高く、参照もよく行われる場合はどうすればいいか？\nというところで compaction が必要になってくる。\nCompaction compaction は追加された差分ファイルをベースファイルと merge して新たなベースファイルを作るという処理である。\ncompaction 後の select クエリは compaction 以前の差分ファイルを読む必要がなくなるため、読み込みが速くなる。\nしたがって長期的に運用される merge on read の table では定期的に compaction が行われることが望ましい。\nIceberg の公式ドキュメントに compaction の記載があるが、Dremio の記事の方が図もあってわかりやすい。\nCompact data files | Apache Iceberg Compaction in Apache Iceberg: Fine-Tuning Your Iceberg Table’s Data Files | Dremio Iceberg で Spark から compaction を行う方法は2つある。\n1つは SparkActions.rewriteDataFiles() を使う方法、もう1つは SparkSQL 内で procedure rewrite_data_files を呼び出す方法だ。\n今回は主に SparkSQL ベースで実装しているということもあり後者にした。\nちなみに compaction は Iceberg 固有の機能ではなく、Hudi や Delta Lake などでも存在している。\nmerge on read をサポートする table format においては一般的なトピックだと思われる。\nその他のメンテナンス compaction 以外でも以下2点も対応する。\n古い snapshot の削除 長期間 table の更新を続けると snapshot が蓄積していく。\nデータが大きくなり続けるため、定期的に snapshot を削除していくことが推奨されている。\nprocedure expire_snapshots により指定の時刻より古い snapshot を削除することができる。\n古い metadata file の削除 データ参照の入り口である metadata file も同様に増え続ける。\nこれも定期的に削除するが、頻繁に更新が行われる table ではこれも削除した方がよい。\nmetadata file の削除は create table 時に table property として write.metadata.delete-after-commit.enabled および write.metadata.previous-versions-max を指定することで自動で行われる。\nこのように Iceberg の table 構造は物理的・論理的に多層的になっているので色々なレベルで配慮が必要という印象。\nメンテナンスの実装 以上を踏まえて、前回ポスト時の実装に対して変更を追加した。\nまず前回作った更新用の app 中の create table を変更。\ntblproperties として古い metadata file を削除するための設定を追加した。(末尾の2つ)\n// Create table spark .sql( \"\"\"create table | my_catalog.my_db.device_temperature |( | device_id int, | operation string, | temperature double, | ts timestamp |) |using iceberg |tblproperties ( | 'format-version' = '2', | 'write.delete.mode' = 'merge-on-read', | 'write.update.mode' = 'merge-on-read', | 'write.merge.mode' = 'merge-on-read', | 'write.metadata.delete-after-commit.enabled' = 'true', | 'write.metadata.previous-versions-max' = '100' |) |\"\"\".stripMargin ) 次に更新処理とは別プロセスとしてメンテナンスを実施する app を追加した。\n通常の更新処理の mini batch の中にメンテナンスを組み込んでもよかったのだが、そうすると mini batch が遅れる可能性がある。\n運用を考えてもメンテナンス用の処理は別で実行できるようになっていた方がいいだろう。\nIceberg は後述の1点を気をつければ並列書き込みが可能であるため、別プロセスで実施する方針とした。\nwhile (true) { println(\"Execute compaction\") retry(3) { () =\u003e spark .sql( \"\"\"call my_catalog.system.rewrite_data_files( | table =\u003e 'my_db.device_temperature', | strategy =\u003e 'binpack' |) |\"\"\".stripMargin ) .show(truncate = false) } val ts = Timestamp.from(Instant.now().minusSeconds(20 * 60)) println(s\"Expire snapshots older than ${ts.toString}\") retry(3) { () =\u003e spark .sql( s\"\"\"call my_catalog.system.expire_snapshots( | table =\u003e 'my_db.device_temperature', | older_than =\u003e timestamp '${ts.toString}', | retain_last =\u003e 20 |) |\"\"\".stripMargin ) .show(truncate = false) } Thread.sleep(10 * 60 * 1000) } 無限ループによりおよそ10分に1回、それぞれ procedure 呼び出しにより compaction と snapshot 削除が実行されるようになっている。\nここで retry() は自前の実装だが、名前のとおり失敗しても指定回数まで retry するというものになっている。\nなぜ retry が必要かというと、Iceberg は並列した書き込みができるが lock などは取らず、楽観的な実行となっている。(optimistic concurrency)\n一連の書き込み処理の準備が終わって最後に commit するときに、その table が他のプロセスにより更新されたことがわかると失敗となる。\n実際に retry なしの場合は次のようなエラーが出ることがあった。\n[error] java.lang.RuntimeException: Cannot commit rewrite because of a ValidationException or CommitFailedException. This usually means that this rewrite has conflicted with another concurrent Iceberg operation. To reduce the likelihood of conflicts, set partial-progress.enabled which will break up the rewrite into multiple smaller commits controlled by partial-progress.max-commits. Separate smaller rewrite commits can succeed independently while any commits that conflict with another Iceberg operation will be ignored. This mode will create additional snapshots in the table history, one for each commit. このメッセージでは partial-progress.enabled の設定が推奨されているが、今回の問題設定はそもそも細かい更新だったのでちょっと違うかなというところで設定していない。\nこれを設定したところで retry 的な配慮は結局必要になるというのもある。\nメンテナンスの実行結果 以上のように実装した2つの処理を並列実行した。\n30秒に1回、table を更新 約10分に1回、table をメンテナンス 2日以上これをまわしっぱなしにした。\nCompaction の効果 select 文を実行したところ、数秒で完了した。\ncompaction を実施する前は一晩更新を続けた後の select で6分かかっていたので、かなり速くなったと言える。\ncompaction により差分の merge のコストが小さくなったからだ。\nOLAP ならこれぐらいで十分だろう。\nSnapshot の削除 metadata file は JSON 形式になっており、次のような形で利用可能な snapshot の情報が記載されている。\n{ ..., \"snapshots\": [ { \"sequence-number\": 25, \"snapshot-id\": 8639168923204820422, \"parent-snapshot-id\": 5556833698284258695, \"timestamp-ms\": 1684722810993, \"summary\": { \"operation\": \"overwrite\", \"spark.app.id\": \"local-1684722133108\", \"added-data-files\": \"5\", \"added-position-delete-files\": \"4\", \"added-delete-files\": \"4\", \"added-records\": \"26\", \"added-files-size\": \"13020\", \"added-position-deletes\": \"27\", \"changed-partition-count\": \"1\", \"total-records\": \"141\", \"total-files-size\": \"52397\", \"total-data-files\": \"10\", \"total-delete-files\": \"25\", \"total-position-deletes\": \"51\", \"total-equality-deletes\": \"0\" }, \"manifest-list\": \"data/warehouse/my_db/device_temperature/metadata/snap-8639168923204820422-1-7ea46920-6ac3-444d-aed1-8dbc8d0c24fd.avro\", \"schema-id\": 0 }, ... ], ... } 最新の metadata file に含まれる snapshot の数をカウントしてみる。\n$ cat data/warehouse/my_db/device_temperature/metadata/v6133.metadata.json | jq '.snapshots | length' 59 table は30秒に1回更新されて新しい snapshot が作られている。\nそれと並行して10分に1回、その時点より20分以上前の snapshot を削除しているため、snapshot の数はおおよそ40〜60ぐらいになる。(実際は compaction で作られた snapshot も入ってくるためこれより少し多くなる)\nsnapshot の削除が効いていることがわかった。\n他の metadata file の shapshot 数も確認したが、だいたい上記の範囲におさまっていた。\nmetadata file の削除 metadata file の削除が効いているかも確認。\n$ ls -l data/warehouse/my_db/device_temperature/metadata/v*.metadata.json | wc -l 101 create table では 'write.metadata.previous-versions-max' = '100' を指定していた。\n現行 version 1 件 + 過去の version 100 件ということで期待どおりのファイル数にコントロールされていることがわかった。\n雑感 compaction などのメンテナンスが期待どおりに実行されていることが確認できた。\nmerge on read の table を長期的に運用する場合、こういったメンテナンスの処理や設定を導入することは必須となるだろう。\nIceberg で merge on read の table を高頻度で更新するにあたり、課題だなと思ったことを3点挙げておく。\nファイル形式 Apache Hudi や Delta Lake で merge on read の table を更新する場合、ベースファイルは Parquet 等の列指向、差分ファイルは Avro などの行指向がデフォルトになっている。\n直感的に少量の差分は行指向になっているのが効率がいいように思うが、一方で Iceberg でこういった形での更新ができるのかがわからなかった。\n複数のファイル形式がまざっていても manifest で吸収できるので読み込みはできるはず、しかし Spark からこういった書き込みができるという記述をドキュメントで見つけることができなかった。\n更新にかかる時間 以下は merge on read で table 更新を行う Spark app において、Spark web UI 上で表示される各 mini batch の実行時間を表すグラフである。\n約10分周期で mini batch の実行時間が増加していき、ストンと下がるということが確認される。\nこの下がっている部分は compaction のタイミングであり、逆に言うと compaction しないと書き込みの時間が増大するということになる。\n“merge on read” の名のとおり、本来は読み込み時に差分を merge するのであって書き込み時はそうならないはず。\nしかし差分が増えると書き込み時間が増えていくというのはこの仕様に合わない。\n(Iceberg のソースコードを読めばわかりそうだが、今回はそこまでやってない)\n求められる技術力 Iceberg の table は ACID transaction, time travel, schema/partition evolution など運用上便利な機能がサポートされている。\n一方でこれを運用していくには table format についての知見や Spark などの分散処理の知見を持つデータエンジニアのリソースが必要となってくる。\nBigQuery や Snowflake などの DWH を中心としたアーキテクチャと比べて人材をそろえるハードルが高いと考えられる。\nこれは Iceberg というよりは Lakehouse Architecture と DWH の差だろう。\n…などと課題を挙げてはみたが、総じて Iceberg はいいものだと感じた。\nうまく抽象化されたレイヤーを挟むことでデータレイクのいろいろな課題を解決しており、かつ動作も比較的わかりやすい。\nエンジニアリングが強い組織だと Iceberg を使って Lakehouse Architecture のも悪くなさそう。\n","wordCount":"733","inLanguage":"ja","image":"https://soonraah.github.io/image/photo/annie-spratt-Tno1Zd3T6yY-unsplash.jpg","datePublished":"2023-05-28T09:00:00+09:00","dateModified":"2023-05-28T09:00:00+09:00","author":{"@type":"Person","name":"soonraah"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://soonraah.github.io/posts/maintain-iceberg-table-updated-in-near-real-time/"},"publisher":{"@type":"Organization","name":"Froglog","logo":{"@type":"ImageObject","url":"https://soonraah.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://soonraah.github.io accesskey=h title="Home (Alt + H)"><img src=https://soonraah.github.io/image/brand/favicon.png alt=logo aria-label=logo height=35>Home</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://soonraah.github.io/about/ title=About><span>About</span></a></li><li><a href=https://soonraah.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://soonraah.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://soonraah.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://soonraah.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://soonraah.github.io>Home</a>&nbsp;»&nbsp;<a href=https://soonraah.github.io/posts/>Posts</a></div><h1 class=post-title>near real time で更新される Apache Iceberg の table のメンテナンス</h1><div class=post-meta><span title='2023-05-28 09:00:00 +0900 JST'>5月 28, 2023</span>&nbsp;·&nbsp;soonraah</div></header><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/photo/annie-spratt-Tno1Zd3T6yY-unsplash.jpg alt=iceberg><p><a href=https://unsplash.com/photos/Tno1Zd3T6yY>Photo by Annie Spratt on Unsplash</a></p></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目次</span></summary><div class=inner><ul><li><a href=#merge-on-read-%e3%81%ae%e8%aa%b2%e9%a1%8c aria-label="merge on read の課題">merge on read の課題</a></li><li><a href=#compaction aria-label=Compaction>Compaction</a></li><li><a href=#%e3%81%9d%e3%81%ae%e4%bb%96%e3%81%ae%e3%83%a1%e3%83%b3%e3%83%86%e3%83%8a%e3%83%b3%e3%82%b9 aria-label=その他のメンテナンス>その他のメンテナンス</a><ul><li><a href=#%e5%8f%a4%e3%81%84-snapshot-%e3%81%ae%e5%89%8a%e9%99%a4 aria-label="古い snapshot の削除">古い snapshot の削除</a></li><li><a href=#%e5%8f%a4%e3%81%84-metadata-file-%e3%81%ae%e5%89%8a%e9%99%a4 aria-label="古い metadata file の削除">古い metadata file の削除</a></li></ul></li><li><a href=#%e3%83%a1%e3%83%b3%e3%83%86%e3%83%8a%e3%83%b3%e3%82%b9%e3%81%ae%e5%ae%9f%e8%a3%85 aria-label=メンテナンスの実装>メンテナンスの実装</a></li><li><a href=#%e3%83%a1%e3%83%b3%e3%83%86%e3%83%8a%e3%83%b3%e3%82%b9%e3%81%ae%e5%ae%9f%e8%a1%8c%e7%b5%90%e6%9e%9c aria-label=メンテナンスの実行結果>メンテナンスの実行結果</a><ul><li><a href=#compaction-%e3%81%ae%e5%8a%b9%e6%9e%9c aria-label="Compaction の効果">Compaction の効果</a></li><li><a href=#snapshot-%e3%81%ae%e5%89%8a%e9%99%a4 aria-label="Snapshot の削除">Snapshot の削除</a></li><li><a href=#metadata-file-%e3%81%ae%e5%89%8a%e9%99%a4 aria-label="metadata file の削除">metadata file の削除</a></li></ul></li><li><a href=#%e9%9b%91%e6%84%9f aria-label=雑感>雑感</a><ul><li><a href=#%e3%83%95%e3%82%a1%e3%82%a4%e3%83%ab%e5%bd%a2%e5%bc%8f aria-label=ファイル形式>ファイル形式</a></li><li><a href=#%e6%9b%b4%e6%96%b0%e3%81%ab%e3%81%8b%e3%81%8b%e3%82%8b%e6%99%82%e9%96%93 aria-label=更新にかかる時間>更新にかかる時間</a></li><li><a href=#%e6%b1%82%e3%82%81%e3%82%89%e3%82%8c%e3%82%8b%e6%8a%80%e8%a1%93%e5%8a%9b aria-label=求められる技術力>求められる技術力</a></li></ul></li></ul></div></details></div><div class=post-content><p><a href=../update-iceberg-table-in-near-real-time>前回のポスト</a>では merge on read で Apache Iceberg の table を near real time で更新するということを行った。<br>このポストではそのメンテナンスについて触れて、かつそれを実行してみる。</p><h2 id=merge-on-read-の課題>merge on read の課題<a hidden class=anchor aria-hidden=true href=#merge-on-read-の課題>#</a></h2><p>merge on read で table を更新する場合、copy on write の場合と違い table 全体を洗い替えする必要はなく差分のみを追記することになる。<br>したがって更新にかかる時間は copy on write よりも短くなる。<br>一方で merge on read の名のとおり読み出し時に積み重なった差分とベースを merge して最新の snapshot とするため、読み出しの速度は copy on write より遅くなる。<br>長時間更新され差分がたくさん存在しているとなおさら遅い。<br>なので</p><ul><li>更新頻度が低く、参照頻度が高いユースケース -> copy on write</li><li>更新頻度が高く、参照頻度が低いユースケース -> merge on write</li></ul><p>という使い分けがよいとされている。</p><p>前回ポストの例では一晩更新を続けた後の merge on read の table に対して簡単な <code>select</code> 文を実行したところ、6分程度かかってしまった。<br>レコード数はたかだか128件程度であることを考えるとかなり遅いと言える。<br>このままでは使い物にならない。</p><p>しかし更新頻度が高く、参照もよく行われる場合はどうすればいいか？<br>というところで compaction が必要になってくる。</p><h2 id=compaction>Compaction<a hidden class=anchor aria-hidden=true href=#compaction>#</a></h2><p>compaction は追加された差分ファイルをベースファイルと merge して新たなベースファイルを作るという処理である。<br>compaction 後の <code>select</code> クエリは compaction 以前の差分ファイルを読む必要がなくなるため、読み込みが速くなる。<br>したがって長期的に運用される merge on read の table では定期的に compaction が行われることが望ましい。<br>Iceberg の公式ドキュメントに compaction の記載があるが、Dremio の記事の方が図もあってわかりやすい。</p><ul><li><a href=https://iceberg.apache.org/docs/latest/maintenance/#compact-data-files>Compact data files | Apache Iceberg</a></li><li><a href=https://www.dremio.com/blog/compaction-in-apache-iceberg-fine-tuning-your-iceberg-tables-data-files/>Compaction in Apache Iceberg: Fine-Tuning Your Iceberg Table’s Data Files | Dremio</a></li></ul><p>Iceberg で Spark から compaction を行う方法は2つある。<br>1つは <a href=https://github.com/apache/iceberg/blob/apache-iceberg-1.2.1/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/SparkActions.java#LL71C38-L71C54><code>SparkActions.rewriteDataFiles()</code></a> を使う方法、もう1つは SparkSQL 内で procedure <code>rewrite_data_files</code> を呼び出す方法だ。<br>今回は主に SparkSQL ベースで実装しているということもあり後者にした。</p><p>ちなみに compaction は Iceberg 固有の機能ではなく、<a href=https://hudi.apache.org/docs/compaction>Hudi</a> や <a href=https://docs.delta.io/latest/best-practices.html#-delta-compact-files>Delta Lake</a> などでも存在している。<br>merge on read をサポートする table format においては一般的なトピックだと思われる。</p><h2 id=その他のメンテナンス>その他のメンテナンス<a hidden class=anchor aria-hidden=true href=#その他のメンテナンス>#</a></h2><p>compaction 以外でも以下2点も対応する。</p><h3 id=古い-snapshot-の削除>古い snapshot の削除<a hidden class=anchor aria-hidden=true href=#古い-snapshot-の削除>#</a></h3><p>長期間 table の更新を続けると snapshot が蓄積していく。<br>データが大きくなり続けるため、定期的に snapshot を削除していくことが推奨されている。<br>procedure <code>expire_snapshots</code> により指定の時刻より古い snapshot を削除することができる。</p><h3 id=古い-metadata-file-の削除>古い metadata file の削除<a hidden class=anchor aria-hidden=true href=#古い-metadata-file-の削除>#</a></h3><p>データ参照の入り口である metadata file も同様に増え続ける。<br>これも定期的に削除するが、頻繁に更新が行われる table ではこれも削除した方がよい。<br>metadata file の削除は <code>create table</code> 時に table property として <code>write.metadata.delete-after-commit.enabled</code> および <code>write.metadata.previous-versions-max</code> を指定することで自動で行われる。</p><p>このように Iceberg の table 構造は物理的・論理的に多層的になっているので色々なレベルで配慮が必要という印象。</p><h2 id=メンテナンスの実装>メンテナンスの実装<a hidden class=anchor aria-hidden=true href=#メンテナンスの実装>#</a></h2><p>以上を踏まえて、前回ポスト時の実装に対して変更を追加した。</p><p>まず前回作った更新用の app 中の <code>create table</code> を変更。<br><code>tblproperties</code> として古い metadata file を削除するための設定を追加した。(末尾の2つ)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#75715e>// Create table
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>spark
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>sql<span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;create table
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |    my_catalog.my_db.device_temperature
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |(
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |    device_id int,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |    operation string,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |    temperature double,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |    ts timestamp
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |using iceberg
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |tblproperties (
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |    &#39;format-version&#39; = &#39;2&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |    &#39;write.delete.mode&#39; = &#39;merge-on-read&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |    &#39;write.update.mode&#39; = &#39;merge-on-read&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |    &#39;write.merge.mode&#39; = &#39;merge-on-read&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |    &#39;write.metadata.delete-after-commit.enabled&#39; = &#39;true&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |    &#39;write.metadata.previous-versions-max&#39; = &#39;100&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      |&#34;&#34;&#34;</span><span style=color:#f92672>.</span>stripMargin
</span></span><span style=display:flex><span>  <span style=color:#f92672>)</span>
</span></span></code></pre></div><p>次に更新処理とは別プロセスとしてメンテナンスを実施する app を追加した。<br>通常の更新処理の mini batch の中にメンテナンスを組み込んでもよかったのだが、そうすると mini batch が遅れる可能性がある。<br>運用を考えてもメンテナンス用の処理は別で実行できるようになっていた方がいいだろう。<br>Iceberg は後述の1点を気をつければ並列書き込みが可能であるため、別プロセスで実施する方針とした。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>while</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>true</span><span style=color:#f92672>)</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  println<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Execute compaction&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  retry<span style=color:#f92672>(</span><span style=color:#ae81ff>3</span><span style=color:#f92672>)</span> <span style=color:#f92672>{</span> <span style=color:#f92672>()</span> <span style=color:#66d9ef>=&gt;</span>
</span></span><span style=display:flex><span>    spark
</span></span><span style=display:flex><span>      <span style=color:#f92672>.</span>sql<span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;call my_catalog.system.rewrite_data_files(
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          |    table =&gt; &#39;my_db.device_temperature&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          |    strategy =&gt; &#39;binpack&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          |)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          |&#34;&#34;&#34;</span><span style=color:#f92672>.</span>stripMargin
</span></span><span style=display:flex><span>      <span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>.</span>show<span style=color:#f92672>(</span>truncate <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>false</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>val</span> ts <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>Timestamp</span><span style=color:#f92672>.</span>from<span style=color:#f92672>(</span><span style=color:#a6e22e>Instant</span><span style=color:#f92672>.</span>now<span style=color:#f92672>().</span>minusSeconds<span style=color:#f92672>(</span><span style=color:#ae81ff>20</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>60</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  println<span style=color:#f92672>(</span><span style=color:#e6db74>s&#34;Expire snapshots older than </span><span style=color:#e6db74>${</span>ts<span style=color:#f92672>.</span>toString<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  retry<span style=color:#f92672>(</span><span style=color:#ae81ff>3</span><span style=color:#f92672>)</span> <span style=color:#f92672>{</span> <span style=color:#f92672>()</span> <span style=color:#66d9ef>=&gt;</span>
</span></span><span style=display:flex><span>    spark
</span></span><span style=display:flex><span>      <span style=color:#f92672>.</span>sql<span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>s&#34;&#34;&#34;call my_catalog.system.expire_snapshots(
</span></span></span><span style=display:flex><span><span style=color:#e6db74>           |    table =&gt; &#39;my_db.device_temperature&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>           |    older_than =&gt; timestamp &#39;</span><span style=color:#e6db74>${</span>ts<span style=color:#f92672>.</span>toString<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>           |    retain_last =&gt; 20
</span></span></span><span style=display:flex><span><span style=color:#e6db74>           |)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>           |&#34;&#34;&#34;</span><span style=color:#f92672>.</span>stripMargin
</span></span><span style=display:flex><span>      <span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>.</span>show<span style=color:#f92672>(</span>truncate <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>false</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>Thread</span><span style=color:#f92672>.</span>sleep<span style=color:#f92672>(</span><span style=color:#ae81ff>10</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>60</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>1000</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>無限ループによりおよそ10分に1回、それぞれ procedure 呼び出しにより compaction と snapshot 削除が実行されるようになっている。<br>ここで <code>retry()</code> は自前の実装だが、名前のとおり失敗しても指定回数まで retry するというものになっている。</p><p>なぜ retry が必要かというと、Iceberg は並列した書き込みができるが lock などは取らず、楽観的な実行となっている。(optimistic concurrency)<br>一連の書き込み処理の準備が終わって最後に commit するときに、その table が他のプロセスにより更新されたことがわかると失敗となる。<br>実際に retry なしの場合は次のようなエラーが出ることがあった。</p><pre tabindex=0><code>[error] java.lang.RuntimeException: Cannot commit rewrite because of a ValidationException or CommitFailedException. This usually means that this rewrite has conflicted with another concurrent Iceberg operation. To reduce the likelihood of conflicts, set partial-progress.enabled which will break up the rewrite into multiple smaller commits controlled by partial-progress.max-commits. Separate smaller rewrite commits can succeed independently while any commits that conflict with another Iceberg operation will be ignored. This mode will create additional snapshots in the table history, one for each commit.
</code></pre><p>このメッセージでは <code>partial-progress.enabled</code> の設定が推奨されているが、今回の問題設定はそもそも細かい更新だったのでちょっと違うかなというところで設定していない。<br>これを設定したところで retry 的な配慮は結局必要になるというのもある。</p><h2 id=メンテナンスの実行結果>メンテナンスの実行結果<a hidden class=anchor aria-hidden=true href=#メンテナンスの実行結果>#</a></h2><p>以上のように実装した2つの処理を並列実行した。</p><ul><li>30秒に1回、table を更新</li><li>約10分に1回、table をメンテナンス</li></ul><p>2日以上これをまわしっぱなしにした。</p><h3 id=compaction-の効果>Compaction の効果<a hidden class=anchor aria-hidden=true href=#compaction-の効果>#</a></h3><p><code>select</code> 文を実行したところ、数秒で完了した。<br>compaction を実施する前は一晩更新を続けた後の <code>select</code> で6分かかっていたので、かなり速くなったと言える。<br>compaction により差分の merge のコストが小さくなったからだ。<br>OLAP ならこれぐらいで十分だろう。</p><h3 id=snapshot-の削除>Snapshot の削除<a hidden class=anchor aria-hidden=true href=#snapshot-の削除>#</a></h3><p>metadata file は JSON 形式になっており、次のような形で利用可能な snapshot の情報が記載されている。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#960050;background-color:#1e0010>...,</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;snapshots&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;sequence-number&#34;</span>: <span style=color:#ae81ff>25</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;snapshot-id&#34;</span>: <span style=color:#ae81ff>8639168923204820422</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;parent-snapshot-id&#34;</span>: <span style=color:#ae81ff>5556833698284258695</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;timestamp-ms&#34;</span>: <span style=color:#ae81ff>1684722810993</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;summary&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;operation&#34;</span>: <span style=color:#e6db74>&#34;overwrite&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;spark.app.id&#34;</span>: <span style=color:#e6db74>&#34;local-1684722133108&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;added-data-files&#34;</span>: <span style=color:#e6db74>&#34;5&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;added-position-delete-files&#34;</span>: <span style=color:#e6db74>&#34;4&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;added-delete-files&#34;</span>: <span style=color:#e6db74>&#34;4&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;added-records&#34;</span>: <span style=color:#e6db74>&#34;26&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;added-files-size&#34;</span>: <span style=color:#e6db74>&#34;13020&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;added-position-deletes&#34;</span>: <span style=color:#e6db74>&#34;27&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;changed-partition-count&#34;</span>: <span style=color:#e6db74>&#34;1&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;total-records&#34;</span>: <span style=color:#e6db74>&#34;141&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;total-files-size&#34;</span>: <span style=color:#e6db74>&#34;52397&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;total-data-files&#34;</span>: <span style=color:#e6db74>&#34;10&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;total-delete-files&#34;</span>: <span style=color:#e6db74>&#34;25&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;total-position-deletes&#34;</span>: <span style=color:#e6db74>&#34;51&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;total-equality-deletes&#34;</span>: <span style=color:#e6db74>&#34;0&#34;</span>
</span></span><span style=display:flex><span>      },
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;manifest-list&#34;</span>: <span style=color:#e6db74>&#34;data/warehouse/my_db/device_temperature/metadata/snap-8639168923204820422-1-7ea46920-6ac3-444d-aed1-8dbc8d0c24fd.avro&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;schema-id&#34;</span>: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#960050;background-color:#1e0010>...</span>
</span></span><span style=display:flex><span>  ],
</span></span><span style=display:flex><span>  <span style=color:#960050;background-color:#1e0010>...</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>最新の metadata file に含まれる snapshot の数をカウントしてみる。</p><pre tabindex=0><code>$ cat data/warehouse/my_db/device_temperature/metadata/v6133.metadata.json | jq &#39;.snapshots | length&#39;
59
</code></pre><p>table は30秒に1回更新されて新しい snapshot が作られている。<br>それと並行して10分に1回、その時点より20分以上前の snapshot を削除しているため、snapshot の数はおおよそ40〜60ぐらいになる。(実際は compaction で作られた snapshot も入ってくるためこれより少し多くなる)<br>snapshot の削除が効いていることがわかった。<br>他の metadata file の shapshot 数も確認したが、だいたい上記の範囲におさまっていた。</p><h3 id=metadata-file-の削除>metadata file の削除<a hidden class=anchor aria-hidden=true href=#metadata-file-の削除>#</a></h3><p>metadata file の削除が効いているかも確認。</p><pre tabindex=0><code>$ ls -l data/warehouse/my_db/device_temperature/metadata/v*.metadata.json | wc -l
     101
</code></pre><p><code>create table</code> では <code>'write.metadata.previous-versions-max' = '100'</code> を指定していた。<br>現行 version 1 件 + 過去の version 100 件ということで期待どおりのファイル数にコントロールされていることがわかった。</p><h2 id=雑感>雑感<a hidden class=anchor aria-hidden=true href=#雑感>#</a></h2><p>compaction などのメンテナンスが期待どおりに実行されていることが確認できた。<br>merge on read の table を長期的に運用する場合、こういったメンテナンスの処理や設定を導入することは必須となるだろう。</p><p>Iceberg で merge on read の table を高頻度で更新するにあたり、課題だなと思ったことを3点挙げておく。</p><h3 id=ファイル形式>ファイル形式<a hidden class=anchor aria-hidden=true href=#ファイル形式>#</a></h3><p>Apache Hudi や Delta Lake で merge on read の table を更新する場合、ベースファイルは Parquet 等の列指向、差分ファイルは Avro などの行指向がデフォルトになっている。<br>直感的に少量の差分は行指向になっているのが効率がいいように思うが、一方で Iceberg でこういった形での更新ができるのかがわからなかった。<br>複数のファイル形式がまざっていても manifest で吸収できるので読み込みはできるはず、しかし Spark からこういった書き込みができるという記述をドキュメントで見つけることができなかった。</p><h3 id=更新にかかる時間>更新にかかる時間<a hidden class=anchor aria-hidden=true href=#更新にかかる時間>#</a></h3><p>以下は merge on read で table 更新を行う Spark app において、Spark web UI 上で表示される各 mini batch の実行時間を表すグラフである。</p><figure class=center><img loading=lazy src=/image/iceberg/duration_to_update_mor_table.png></figure><p>約10分周期で mini batch の実行時間が増加していき、ストンと下がるということが確認される。<br>この下がっている部分は compaction のタイミングであり、逆に言うと compaction しないと書き込みの時間が増大するということになる。<br>&ldquo;merge on read&rdquo; の名のとおり、本来は読み込み時に差分を merge するのであって書き込み時はそうならないはず。<br>しかし差分が増えると書き込み時間が増えていくというのはこの仕様に合わない。<br>(Iceberg のソースコードを読めばわかりそうだが、今回はそこまでやってない)</p><h3 id=求められる技術力>求められる技術力<a hidden class=anchor aria-hidden=true href=#求められる技術力>#</a></h3><p>Iceberg の table は ACID transaction, time travel, schema/partition evolution など運用上便利な機能がサポートされている。<br>一方でこれを運用していくには table format についての知見や Spark などの分散処理の知見を持つデータエンジニアのリソースが必要となってくる。<br>BigQuery や Snowflake などの DWH を中心としたアーキテクチャと比べて人材をそろえるハードルが高いと考えられる。<br>これは Iceberg というよりは Lakehouse Architecture と DWH の差だろう。</p><p>…などと課題を挙げてはみたが、総じて Iceberg はいいものだと感じた。<br>うまく抽象化されたレイヤーを挟むことでデータレイクのいろいろな課題を解決しており、かつ動作も比較的わかりやすい。<br>エンジニアリングが強い組織だと Iceberg を使って Lakehouse Architecture のも悪くなさそう。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://soonraah.github.io/tags/data-lake/>Data Lake</a></li><li><a href=https://soonraah.github.io/tags/lakehouse/>Lakehouse</a></li><li><a href=https://soonraah.github.io/tags/apache-iceberg/>Apache Iceberg</a></li></ul><nav class=paginav><a class=next href=https://soonraah.github.io/posts/update-iceberg-table-in-near-real-time/><span class=title>次のページ »</span><br><span>Apache Iceberg の table を near real time で更新する</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share near real time で更新される Apache Iceberg の table のメンテナンス on twitter" href="https://twitter.com/intent/tweet/?text=near%20real%20time%20%e3%81%a7%e6%9b%b4%e6%96%b0%e3%81%95%e3%82%8c%e3%82%8b%20Apache%20Iceberg%20%e3%81%ae%20table%20%e3%81%ae%e3%83%a1%e3%83%b3%e3%83%86%e3%83%8a%e3%83%b3%e3%82%b9&amp;url=https%3a%2f%2fsoonraah.github.io%2fposts%2fmaintain-iceberg-table-updated-in-near-real-time%2f&amp;hashtags=DataLake%2cLakehouse%2cApacheIceberg"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share near real time で更新される Apache Iceberg の table のメンテナンス on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsoonraah.github.io%2fposts%2fmaintain-iceberg-table-updated-in-near-real-time%2f&amp;title=near%20real%20time%20%e3%81%a7%e6%9b%b4%e6%96%b0%e3%81%95%e3%82%8c%e3%82%8b%20Apache%20Iceberg%20%e3%81%ae%20table%20%e3%81%ae%e3%83%a1%e3%83%b3%e3%83%86%e3%83%8a%e3%83%b3%e3%82%b9&amp;summary=near%20real%20time%20%e3%81%a7%e6%9b%b4%e6%96%b0%e3%81%95%e3%82%8c%e3%82%8b%20Apache%20Iceberg%20%e3%81%ae%20table%20%e3%81%ae%e3%83%a1%e3%83%b3%e3%83%86%e3%83%8a%e3%83%b3%e3%82%b9&amp;source=https%3a%2f%2fsoonraah.github.io%2fposts%2fmaintain-iceberg-table-updated-in-near-real-time%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share near real time で更新される Apache Iceberg の table のメンテナンス on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsoonraah.github.io%2fposts%2fmaintain-iceberg-table-updated-in-near-real-time%2f&title=near%20real%20time%20%e3%81%a7%e6%9b%b4%e6%96%b0%e3%81%95%e3%82%8c%e3%82%8b%20Apache%20Iceberg%20%e3%81%ae%20table%20%e3%81%ae%e3%83%a1%e3%83%b3%e3%83%86%e3%83%8a%e3%83%b3%e3%82%b9"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share near real time で更新される Apache Iceberg の table のメンテナンス on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsoonraah.github.io%2fposts%2fmaintain-iceberg-table-updated-in-near-real-time%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share near real time で更新される Apache Iceberg の table のメンテナンス on whatsapp" href="https://api.whatsapp.com/send?text=near%20real%20time%20%e3%81%a7%e6%9b%b4%e6%96%b0%e3%81%95%e3%82%8c%e3%82%8b%20Apache%20Iceberg%20%e3%81%ae%20table%20%e3%81%ae%e3%83%a1%e3%83%b3%e3%83%86%e3%83%8a%e3%83%b3%e3%82%b9%20-%20https%3a%2f%2fsoonraah.github.io%2fposts%2fmaintain-iceberg-table-updated-in-near-real-time%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share near real time で更新される Apache Iceberg の table のメンテナンス on telegram" href="https://telegram.me/share/url?text=near%20real%20time%20%e3%81%a7%e6%9b%b4%e6%96%b0%e3%81%95%e3%82%8c%e3%82%8b%20Apache%20Iceberg%20%e3%81%ae%20table%20%e3%81%ae%e3%83%a1%e3%83%b3%e3%83%86%e3%83%8a%e3%83%b3%e3%82%b9&amp;url=https%3a%2f%2fsoonraah.github.io%2fposts%2fmaintain-iceberg-table-updated-in-near-real-time%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://soonraah.github.io>Froglog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>