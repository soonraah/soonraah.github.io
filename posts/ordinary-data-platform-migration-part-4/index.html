<!doctype html><html lang=ja dir=auto><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-NSEGH2YT17"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NSEGH2YT17")</script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編 | Froglog</title>
<meta name=keywords content="LangChain,LangGraph,LLM,data platform,ふつうのデータ基盤移行"><meta name=description content="このポストについて
データ基盤移行について書いていくシリーズです。
シリーズ一覧はこちらから。
前回 Part 3. アーキテクチャ編ではどういったシステム構成にしたかを書きました。
今回はその技術スタックへと移行するための苦労と効率化について書きます。
(次は CI/CD の話をすると書きましたが…スマンありゃウソだった)
スコープ
今回はやや小さいスコープの話です。
データ基盤における ETL (ELT) 処理の移行作業を対象としています。
移行作業における工数的な課題を AI ワークフローを作って効率化して軽減したという話になります。
ETL 以外の移行作業は今回はスコープ外となります。
課題
旧データ基盤から新データ基盤へと table およびそれを更新するための処理を移行するにあたり工数面での課題が2つあります。

技術スタックの移行
column 命名などの標準化

これらについて述べます。
技術スタックの移行
データ基盤の移行において、新旧の環境で技術スタックは次のようになっています。

旧データ基盤

ETL: Glue Job


新データ基盤

ELT: dbt-databricks



つまり Glue Job の Python コードを dbt model、つまり SQL に翻訳する必要があり、それなりに手間がかかります。
さらにこの Python コードは次のような問題もあり、移行のハードルを上げます。

UDF を実装して特殊な処理を行っているケースがある
Spark の API だけでなく Glue の API をふんだんに使っている (なるべく Spark に寄せればいいものを…)
(ここ数年の業務で見た中で一番というぐらいに) コード品質が低い

column 命名などの標準化
旧データ基盤は利用者への配慮があまりない状態で table の schema が作られており、利用者にとって使いにくいものとなっていました。
それを改善するため、新データ基盤では次のようなルールを導入しました。"><meta name=author content="soonraah"><link rel=canonical href=https://soonraah.github.io/posts/ordinary-data-platform-migration-part-4/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://soonraah.github.io/favicon2.ico><link rel=icon type=image/png sizes=16x16 href=https://soonraah.github.io/image/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://soonraah.github.io/image/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://soonraah.github.io/static/image/favicon/apple-touch-icon.png><link rel=mask-icon href=https://soonraah.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=ja href=https://soonraah.github.io/posts/ordinary-data-platform-migration-part-4/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://soonraah.github.io/posts/ordinary-data-platform-migration-part-4/"><meta property="og:site_name" content="Froglog"><meta property="og:title" content="ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編"><meta property="og:description" content="このポストについて データ基盤移行について書いていくシリーズです。
シリーズ一覧はこちらから。
前回 Part 3. アーキテクチャ編ではどういったシステム構成にしたかを書きました。
今回はその技術スタックへと移行するための苦労と効率化について書きます。
(次は CI/CD の話をすると書きましたが…スマンありゃウソだった)
スコープ 今回はやや小さいスコープの話です。
データ基盤における ETL (ELT) 処理の移行作業を対象としています。
移行作業における工数的な課題を AI ワークフローを作って効率化して軽減したという話になります。
ETL 以外の移行作業は今回はスコープ外となります。
課題 旧データ基盤から新データ基盤へと table およびそれを更新するための処理を移行するにあたり工数面での課題が2つあります。
技術スタックの移行 column 命名などの標準化 これらについて述べます。
技術スタックの移行 データ基盤の移行において、新旧の環境で技術スタックは次のようになっています。
旧データ基盤 ETL: Glue Job 新データ基盤 ELT: dbt-databricks つまり Glue Job の Python コードを dbt model、つまり SQL に翻訳する必要があり、それなりに手間がかかります。
さらにこの Python コードは次のような問題もあり、移行のハードルを上げます。
UDF を実装して特殊な処理を行っているケースがある Spark の API だけでなく Glue の API をふんだんに使っている (なるべく Spark に寄せればいいものを…) (ここ数年の業務で見た中で一番というぐらいに) コード品質が低い column 命名などの標準化 旧データ基盤は利用者への配慮があまりない状態で table の schema が作られており、利用者にとって使いにくいものとなっていました。
それを改善するため、新データ基盤では次のようなルールを導入しました。"><meta property="og:locale" content="ja"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-18T07:30:00+09:00"><meta property="article:modified_time" content="2025-06-18T07:30:00+09:00"><meta property="article:tag" content="LangChain"><meta property="article:tag" content="LangGraph"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Data Platform"><meta property="article:tag" content="ふつうのデータ基盤移行"><meta property="og:image" content="https://soonraah.github.io/image/ordinary-data-plagform-migration/workflow.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://soonraah.github.io/image/ordinary-data-plagform-migration/workflow.jpg"><meta name=twitter:title content="ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編"><meta name=twitter:description content="このポストについて
データ基盤移行について書いていくシリーズです。
シリーズ一覧はこちらから。
前回 Part 3. アーキテクチャ編ではどういったシステム構成にしたかを書きました。
今回はその技術スタックへと移行するための苦労と効率化について書きます。
(次は CI/CD の話をすると書きましたが…スマンありゃウソだった)
スコープ
今回はやや小さいスコープの話です。
データ基盤における ETL (ELT) 処理の移行作業を対象としています。
移行作業における工数的な課題を AI ワークフローを作って効率化して軽減したという話になります。
ETL 以外の移行作業は今回はスコープ外となります。
課題
旧データ基盤から新データ基盤へと table およびそれを更新するための処理を移行するにあたり工数面での課題が2つあります。

技術スタックの移行
column 命名などの標準化

これらについて述べます。
技術スタックの移行
データ基盤の移行において、新旧の環境で技術スタックは次のようになっています。

旧データ基盤

ETL: Glue Job


新データ基盤

ELT: dbt-databricks



つまり Glue Job の Python コードを dbt model、つまり SQL に翻訳する必要があり、それなりに手間がかかります。
さらにこの Python コードは次のような問題もあり、移行のハードルを上げます。

UDF を実装して特殊な処理を行っているケースがある
Spark の API だけでなく Glue の API をふんだんに使っている (なるべく Spark に寄せればいいものを…)
(ここ数年の業務で見た中で一番というぐらいに) コード品質が低い

column 命名などの標準化
旧データ基盤は利用者への配慮があまりない状態で table の schema が作られており、利用者にとって使いにくいものとなっていました。
それを改善するため、新データ基盤では次のようなルールを導入しました。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://soonraah.github.io/posts/"},{"@type":"ListItem","position":2,"name":"ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編","item":"https://soonraah.github.io/posts/ordinary-data-platform-migration-part-4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編","name":"ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編","description":"このポストについて データ基盤移行について書いていくシリーズです。\nシリーズ一覧はこちらから。\n前回 Part 3. アーキテクチャ編ではどういったシステム構成にしたかを書きました。\n今回はその技術スタックへと移行するための苦労と効率化について書きます。\n(次は CI/CD の話をすると書きましたが…スマンありゃウソだった)\nスコープ 今回はやや小さいスコープの話です。\nデータ基盤における ETL (ELT) 処理の移行作業を対象としています。\n移行作業における工数的な課題を AI ワークフローを作って効率化して軽減したという話になります。\nETL 以外の移行作業は今回はスコープ外となります。\n課題 旧データ基盤から新データ基盤へと table およびそれを更新するための処理を移行するにあたり工数面での課題が2つあります。\n技術スタックの移行 column 命名などの標準化 これらについて述べます。\n技術スタックの移行 データ基盤の移行において、新旧の環境で技術スタックは次のようになっています。\n旧データ基盤 ETL: Glue Job 新データ基盤 ELT: dbt-databricks つまり Glue Job の Python コードを dbt model、つまり SQL に翻訳する必要があり、それなりに手間がかかります。\nさらにこの Python コードは次のような問題もあり、移行のハードルを上げます。\nUDF を実装して特殊な処理を行っているケースがある Spark の API だけでなく Glue の API をふんだんに使っている (なるべく Spark に寄せればいいものを…) (ここ数年の業務で見た中で一番というぐらいに) コード品質が低い column 命名などの標準化 旧データ基盤は利用者への配慮があまりない状態で table の schema が作られており、利用者にとって使いにくいものとなっていました。\nそれを改善するため、新データ基盤では次のようなルールを導入しました。\n","keywords":["LangChain","LangGraph","LLM","data platform","ふつうのデータ基盤移行"],"articleBody":"このポストについて データ基盤移行について書いていくシリーズです。\nシリーズ一覧はこちらから。\n前回 Part 3. アーキテクチャ編ではどういったシステム構成にしたかを書きました。\n今回はその技術スタックへと移行するための苦労と効率化について書きます。\n(次は CI/CD の話をすると書きましたが…スマンありゃウソだった)\nスコープ 今回はやや小さいスコープの話です。\nデータ基盤における ETL (ELT) 処理の移行作業を対象としています。\n移行作業における工数的な課題を AI ワークフローを作って効率化して軽減したという話になります。\nETL 以外の移行作業は今回はスコープ外となります。\n課題 旧データ基盤から新データ基盤へと table およびそれを更新するための処理を移行するにあたり工数面での課題が2つあります。\n技術スタックの移行 column 命名などの標準化 これらについて述べます。\n技術スタックの移行 データ基盤の移行において、新旧の環境で技術スタックは次のようになっています。\n旧データ基盤 ETL: Glue Job 新データ基盤 ELT: dbt-databricks つまり Glue Job の Python コードを dbt model、つまり SQL に翻訳する必要があり、それなりに手間がかかります。\nさらにこの Python コードは次のような問題もあり、移行のハードルを上げます。\nUDF を実装して特殊な処理を行っているケースがある Spark の API だけでなく Glue の API をふんだんに使っている (なるべく Spark に寄せればいいものを…) (ここ数年の業務で見た中で一番というぐらいに) コード品質が低い column 命名などの標準化 旧データ基盤は利用者への配慮があまりない状態で table の schema が作られており、利用者にとって使いにくいものとなっていました。\nそれを改善するため、新データ基盤では次のようなルールを導入しました。\n時刻を表す column は timestamp 型にし、column 名を \u003c過去分詞形\u003e_at とする ex. created_at 複数の要素を表す column は array 型にし、column 名を \u003c複数形\u003e とする ex. features etc. これらルールの適用により利用者にとっての利便性が向上しますが、一方でモデリング時に手間と注意力を要します。\n移行対象の table が数件程度ならこのような対応も人手で行えますが、数十〜の table を対応するには工数がかかりすぎます。(それでも少ない方ですが)\nとはいえ従来的なプログラミングで簡単に変換できるようなものでもありません。\nそこで AI ワークフローを作ることにしました。\nAI ワークフローって？ Anthropic のブログ記事 Building Effective AI Agents \\ Anthropic において、エージェントと対比する形でワークフローについて述べられています。\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths. Agents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks. (エージェントの定義はいろいろあるようですが) 今回作ったのはここで言うエージェントほど自由ではなく、ワークフローの範疇となります。\n同記事で挙げられている Workflow: Evaluator-optimizer を含んだワークフローを構築します。\nソリューション ワークフロー LangGraph を使って次のようなワークフローを実装しました。\nflowchart TD start((Start)) retrieve_target_table[\"①移行対象の旧データ基盤 table の schema を取得\"] retrieve_migrated_table[\"②移行済みの旧データ基盤 table の schema を取得\"] retrieve_migrated_dbt_model[\"③移行済みの dbt model を取得\"] generate_dbt_model[\"④LLM による dbt model 生成\"] sql_evaluator[\"⑤LLM による生成された dbt model の評価\"] end_((End)) start --\u003e retrieve_target_table retrieve_target_table --\u003e generate_dbt_model start --\u003e retrieve_migrated_table retrieve_migrated_table --\u003e retrieve_migrated_dbt_model retrieve_migrated_dbt_model --\u003e generate_dbt_model generate_dbt_model --\u003e sql_evaluator sql_evaluator -- dbt model が適切 --\u003e end_ sql_evaluator -- dbt model が不適切 --\u003e generate_dbt_model AI ワークフローと言いつつ、LLM を使っているステップは④と⑤だけだったりします。\n各ステップを解説します。\n①移行対象の旧データ基盤 table の schema を取得 最初に分岐した左のラインでは移行したい table の情報を取得します。\ntable 情報はどこから取ってきてもいいのですが、OpenMetadata を導入していたためそちらから取ってきました。\nこの table の schema を入力として dbt model を生成します。\n本来は Glue Job の Python コードを入力として与えるのが正当なやり方だと思いますが、ここではそうしません。\n複数ファイルに散らばる Python コードを読ませるのが多少手間なのと、(ここ数年の業務で見た中で一番というぐらいに) コード品質が低いコードを LLM に読ませるのが忍びないからです。\nじゃあ何の情報から dbt model を作るの？というのが右のラインになります。\n②移行済みの旧データ基盤 table の schema を取得 右のラインではすでに移行が完了した table についての情報を取得します。\nまずは移行対象と同様に schema 情報を OpenMetadata から取得します。\n③移行済みの dbt model を取得 次に移行済みの dbt model を取得します。\nこれはローカルに clone された git repository からファイルとして取得する想定です。\nさて、この②と③により dbt model 生成の入出力の例が得られました。\nこれにより dbt model 生成のプロンプトで few-shot prompting ができるようになります。\nfew-shot prompting は簡単に言うと、LLM への指示に対する入出力の例を提示することにより、得たい出力を得やすくするという手法です。\n④LLM による dbt model 生成 ここでようやく LLM の出番です。\nプロンプトには\ndbt model を作れという指示 指示の背景 column 命名などの標準化ルール few-shot prompting の入出力例 入力となる移行対象 table の schema を与えます。\nそうすると何らかの dbt model (SQL) っぽいものが出力されました。\n⑤LLM による生成された dbt model の評価 ④で終わってもよかったのですが、生成物を LLM で評価するというフェーズも追加しました。\n④のプロンプトとその出力の dbt model を LLM に与え、「タスクの実行結果は適切だったと思うか？」と問いかけます。\n適切なら終了します。めでたしめでたし。\n不適切と判断された場合はその理由つけて④に差し戻し、評価結果に配慮させながら④をやり直します。\n無限ループにならないよう N 回繰り返すと失敗するような制御も必要です。\nこの LLM に評価させるというのが前述の Building Effective AI Agents \\ Anthropic における Workflow: Evaluator-optimizer となります。\nagent desing pattern においては self-reflection とも言われます。\n技術スタック ワークフローの実装には次のような技術スタックを使いました。\nLangGraph: ワークフローの構成 LangChain: ワークフローの node 内での prompt 生成、LLM 呼び出し、結果成形などの一連の処理 Amazon Bedrock: LLM は Bedrock 経由で Claude 3.7 Sonnet を利用 これらを使ったワークフロー実装はそこまで難しいものではなく、だいたい1週間ちょっとぐらいでコーディングしました。\nワークフローの導入効果 一部の table 移行で試したところ、ちゃんと測ったわけではありませんが体感で50~80%ぐらいは工数削減できたかなと思います。\n比較的ローコストで大きく工数を削減できたのでこの結果には満足しています。\n勉強したての AI ワークフローのユースケースとして実際に役に立つものを作れたのも良かったです。\nfew-shot prompting だのみなので、他の dbt model と異なる特殊な処理をしているケースには対応が難しく、そういった場合は人間様の出番になります。\nまあ作業を100%置き換えることを目指していたわけではないため、それは良しとしています。\n何回か試した中では評価で不適切とみなされることはありませんでした。\nこれは dbt model 生成とその評価に同じ LLM を使っているからかもしれません。\n人間も自分の仕事を自分で評価すると甘くなります。\nself-reflection ではなく、評価に別の LLM を使うという crossーreflection という agent design pattern にするといいかもしれません。\nQ\u0026A Q. ワークフロー作らんでも Cursor や Cline みたいなエージェントに命令したらええんちゃうん？ A.\nおそらくそのやり方でもできるでしょう。\nもっと言うと Glue Job の Python コードを含むプロジェクトのファイルをまるっとコンテキストとして持てるのであれば、精度も上がるかもしれません。\nただ残念ながら弊社は出遅れており、そういったソフトウェア開発エージェントがまだ利用できない状態です…\nワークフローを自前で作ることのメリットを強いて上げるのであれば、決まった処理の流れを曖昧な自然言語ではなくコードで定義できるというところになるでしょうか。\nQ. Glue Job は Spark やろ？せやったらそのままのコードで Databricks に移行できるんちゃうん？ A.\nApache Spark はプログラミングモデルは DataFrame 形式で非常にとっつきやすいのですが、アーキテクチャを理解していないとパフォーマンスの問題に対応できません。\nアーキテクチャを理解せずに Spark を使っている開発者も割と多いです。\n弊社もそういう状況で Spark を理解するエンジニアを安定的に雇用するのも困難なため、このデータ基盤移行によりコードとしての Spark は捨てて ELT については完全に SQL 化する運用にシフトしようとしています。\nまた、前述のとおり Spark のコードと言っても Glue Job の API が結構入り込んでいてポータビリティが低くなってしまっていました。\nさらに (ここ数年の業務で見た中で一番というぐらいに) コード品質が低く、これを残したくない気持ちがありました。\nQ. AI ワークフローはどないして勉強したらええのん？ A.\n今回の技術選定、ひいてはそもそものワークフロー作成のアイディアは以下の書籍に大きく影響を受けています。\n西見 公宏; 吉田 真吾; 大嶋 勇樹. LangChainとLangGraphによるRAG・AIエージェント［実践］入門 エンジニア選書. 株式会社技術評論社. 個人的にはとても面白い内容でした。\nAI エージェントやワークフローについて学ぶのにいい書籍だと思います。(が、この分野は古くなるのも早いので注意)\nこの書籍について書いたポストもご参照いただければと。\n次回予告 次こそ CI/CD の話をします！\n","wordCount":"502","inLanguage":"ja","image":"https://soonraah.github.io/image/ordinary-data-plagform-migration/workflow.jpg","datePublished":"2025-06-18T07:30:00+09:00","dateModified":"2025-06-18T07:30:00+09:00","author":{"@type":"Person","name":"soonraah"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://soonraah.github.io/posts/ordinary-data-platform-migration-part-4/"},"publisher":{"@type":"Organization","name":"Froglog","logo":{"@type":"ImageObject","url":"https://soonraah.github.io/favicon2.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://soonraah.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://soonraah.github.io/image/brand/favicon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://soonraah.github.io/about/ title=About><span>About</span></a></li><li><a href=https://soonraah.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://soonraah.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://soonraah.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://soonraah.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://soonraah.github.io/>ホーム</a>&nbsp;»&nbsp;<a href=https://soonraah.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編</h1><div class=post-meta><span title='2025-06-18 07:30:00 +0900 JST'>6月 18, 2025</span>&nbsp;·&nbsp;soonraah</div></header><figure class=entry-cover><img loading=eager src=https://soonraah.github.io/image/ordinary-data-plagform-migration/workflow.jpg alt=workflow><figcaption>Image by OpenAI DALL·E</figcaption></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目次</span></summary><div class=inner><ul><li><a href=#%e3%81%93%e3%81%ae%e3%83%9d%e3%82%b9%e3%83%88%e3%81%ab%e3%81%a4%e3%81%84%e3%81%a6 aria-label=このポストについて>このポストについて</a></li><li><a href=#%e3%82%b9%e3%82%b3%e3%83%bc%e3%83%97 aria-label=スコープ>スコープ</a></li><li><a href=#%e8%aa%b2%e9%a1%8c aria-label=課題>課題</a><ul><li><a href=#%e6%8a%80%e8%a1%93%e3%82%b9%e3%82%bf%e3%83%83%e3%82%af%e3%81%ae%e7%a7%bb%e8%a1%8c aria-label=技術スタックの移行>技術スタックの移行</a></li><li><a href=#column-%e5%91%bd%e5%90%8d%e3%81%aa%e3%81%a9%e3%81%ae%e6%a8%99%e6%ba%96%e5%8c%96 aria-label="column 命名などの標準化">column 命名などの標準化</a></li></ul></li><li><a href=#ai-%e3%83%af%e3%83%bc%e3%82%af%e3%83%95%e3%83%ad%e3%83%bc%e3%81%a3%e3%81%a6 aria-label="AI ワークフローって？">AI ワークフローって？</a></li><li><a href=#%e3%82%bd%e3%83%aa%e3%83%a5%e3%83%bc%e3%82%b7%e3%83%a7%e3%83%b3 aria-label=ソリューション>ソリューション</a><ul><li><a href=#%e3%83%af%e3%83%bc%e3%82%af%e3%83%95%e3%83%ad%e3%83%bc aria-label=ワークフロー>ワークフロー</a><ul><li><a href=#%e7%a7%bb%e8%a1%8c%e5%af%be%e8%b1%a1%e3%81%ae%e6%97%a7%e3%83%87%e3%83%bc%e3%82%bf%e5%9f%ba%e7%9b%a4-table-%e3%81%ae-schema-%e3%82%92%e5%8f%96%e5%be%97 aria-label="①移行対象の旧データ基盤 table の schema を取得">①移行対象の旧データ基盤 table の schema を取得</a></li><li><a href=#%e7%a7%bb%e8%a1%8c%e6%b8%88%e3%81%bf%e3%81%ae%e6%97%a7%e3%83%87%e3%83%bc%e3%82%bf%e5%9f%ba%e7%9b%a4-table-%e3%81%ae-schema-%e3%82%92%e5%8f%96%e5%be%97 aria-label="②移行済みの旧データ基盤 table の schema を取得">②移行済みの旧データ基盤 table の schema を取得</a></li><li><a href=#%e7%a7%bb%e8%a1%8c%e6%b8%88%e3%81%bf%e3%81%ae-dbt-model-%e3%82%92%e5%8f%96%e5%be%97 aria-label="③移行済みの dbt model を取得">③移行済みの dbt model を取得</a></li><li><a href=#llm-%e3%81%ab%e3%82%88%e3%82%8b-dbt-model-%e7%94%9f%e6%88%90 aria-label="④LLM による dbt model 生成">④LLM による dbt model 生成</a></li><li><a href=#llm-%e3%81%ab%e3%82%88%e3%82%8b%e7%94%9f%e6%88%90%e3%81%95%e3%82%8c%e3%81%9f-dbt-model-%e3%81%ae%e8%a9%95%e4%be%a1 aria-label="⑤LLM による生成された dbt model の評価">⑤LLM による生成された dbt model の評価</a></li></ul></li><li><a href=#%e6%8a%80%e8%a1%93%e3%82%b9%e3%82%bf%e3%83%83%e3%82%af aria-label=技術スタック>技術スタック</a></li></ul></li><li><a href=#%e3%83%af%e3%83%bc%e3%82%af%e3%83%95%e3%83%ad%e3%83%bc%e3%81%ae%e5%b0%8e%e5%85%a5%e5%8a%b9%e6%9e%9c aria-label=ワークフローの導入効果>ワークフローの導入効果</a></li><li><a href=#qa aria-label=Q&amp;A>Q&amp;A</a><ul><ul><li><a href=#q-%e3%83%af%e3%83%bc%e3%82%af%e3%83%95%e3%83%ad%e3%83%bc%e4%bd%9c%e3%82%89%e3%82%93%e3%81%a7%e3%82%82-cursor-%e3%82%84-cline-%e3%81%bf%e3%81%9f%e3%81%84%e3%81%aa%e3%82%a8%e3%83%bc%e3%82%b8%e3%82%a7%e3%83%b3%e3%83%88%e3%81%ab%e5%91%bd%e4%bb%a4%e3%81%97%e3%81%9f%e3%82%89%e3%81%88%e3%81%88%e3%82%93%e3%81%a1%e3%82%83%e3%81%86%e3%82%93 aria-label="Q. ワークフロー作らんでも Cursor や Cline みたいなエージェントに命令したらええんちゃうん？">Q. ワークフロー作らんでも Cursor や Cline みたいなエージェントに命令したらええんちゃうん？</a></li><li><a href=#q-glue-job-%e3%81%af-spark-%e3%82%84%e3%82%8d%e3%81%9b%e3%82%84%e3%81%a3%e3%81%9f%e3%82%89%e3%81%9d%e3%81%ae%e3%81%be%e3%81%be%e3%81%ae%e3%82%b3%e3%83%bc%e3%83%89%e3%81%a7-databricks-%e3%81%ab%e7%a7%bb%e8%a1%8c%e3%81%a7%e3%81%8d%e3%82%8b%e3%82%93%e3%81%a1%e3%82%83%e3%81%86%e3%82%93 aria-label="Q. Glue Job は Spark やろ？せやったらそのままのコードで Databricks に移行できるんちゃうん？">Q. Glue Job は Spark やろ？せやったらそのままのコードで Databricks に移行できるんちゃうん？</a></li><li><a href=#q-ai-%e3%83%af%e3%83%bc%e3%82%af%e3%83%95%e3%83%ad%e3%83%bc%e3%81%af%e3%81%a9%e3%81%aa%e3%81%84%e3%81%97%e3%81%a6%e5%8b%89%e5%bc%b7%e3%81%97%e3%81%9f%e3%82%89%e3%81%88%e3%81%88%e3%81%ae%e3%82%93 aria-label="Q. AI ワークフローはどないして勉強したらええのん？">Q. AI ワークフローはどないして勉強したらええのん？</a></li></ul></ul></li><li><a href=#%e6%ac%a1%e5%9b%9e%e4%ba%88%e5%91%8a aria-label=次回予告>次回予告</a></li></ul></div></details></div><div class=post-content><h2 id=このポストについて>このポストについて<a hidden class=anchor aria-hidden=true href=#このポストについて>#</a></h2><p>データ基盤移行について書いていくシリーズです。<br>シリーズ一覧は<a href=/tags/%E3%81%B5%E3%81%A4%E3%81%86%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E5%9F%BA%E7%9B%A4%E7%A7%BB%E8%A1%8C/>こちら</a>から。</p><p>前回 <a href=/posts/ordinary-data-platform-migration-part-3/>Part 3. アーキテクチャ編</a>ではどういったシステム構成にしたかを書きました。<br>今回はその技術スタックへと移行するための苦労と効率化について書きます。</p><p>(次は CI/CD の話をすると書きましたが…スマンありゃウソだった)</p><h2 id=スコープ>スコープ<a hidden class=anchor aria-hidden=true href=#スコープ>#</a></h2><p>今回はやや小さいスコープの話です。<br>データ基盤における ETL (ELT) 処理の移行作業を対象としています。<br>移行作業における工数的な課題を AI ワークフローを作って効率化して軽減したという話になります。<br>ETL 以外の移行作業は今回はスコープ外となります。</p><h2 id=課題>課題<a hidden class=anchor aria-hidden=true href=#課題>#</a></h2><p>旧データ基盤から新データ基盤へと table およびそれを更新するための処理を移行するにあたり工数面での課題が2つあります。</p><ul><li>技術スタックの移行</li><li>column 命名などの標準化</li></ul><p>これらについて述べます。</p><h3 id=技術スタックの移行>技術スタックの移行<a hidden class=anchor aria-hidden=true href=#技術スタックの移行>#</a></h3><p>データ基盤の移行において、新旧の環境で技術スタックは次のようになっています。</p><ul><li>旧データ基盤<ul><li>ETL: Glue Job</li></ul></li><li>新データ基盤<ul><li>ELT: dbt-databricks</li></ul></li></ul><p>つまり Glue Job の Python コードを dbt model、つまり SQL に翻訳する必要があり、それなりに手間がかかります。<br>さらにこの Python コードは次のような問題もあり、移行のハードルを上げます。</p><ul><li>UDF を実装して特殊な処理を行っているケースがある</li><li>Spark の API だけでなく Glue の API をふんだんに使っている (なるべく Spark に寄せればいいものを…)</li><li>(ここ数年の業務で見た中で一番というぐらいに) コード品質が低い</li></ul><h3 id=column-命名などの標準化>column 命名などの標準化<a hidden class=anchor aria-hidden=true href=#column-命名などの標準化>#</a></h3><p>旧データ基盤は利用者への配慮があまりない状態で table の schema が作られており、利用者にとって使いにくいものとなっていました。<br>それを改善するため、新データ基盤では次のようなルールを導入しました。</p><ul><li>時刻を表す column は timestamp 型にし、column 名を <code>&lt;過去分詞形>_at</code> とする<ul><li>ex. <code>created_at</code></li></ul></li><li>複数の要素を表す column は array 型にし、column 名を <code>&lt;複数形></code> とする<ul><li>ex. <code>features</code></li></ul></li><li>etc.</li></ul><p>これらルールの適用により利用者にとっての利便性が向上しますが、一方でモデリング時に手間と注意力を要します。</p><p>移行対象の table が数件程度ならこのような対応も人手で行えますが、数十〜の table を対応するには工数がかかりすぎます。(それでも少ない方ですが)<br>とはいえ従来的なプログラミングで簡単に変換できるようなものでもありません。<br>そこで AI ワークフローを作ることにしました。</p><h2 id=ai-ワークフローって>AI ワークフローって？<a hidden class=anchor aria-hidden=true href=#ai-ワークフローって>#</a></h2><p>Anthropic のブログ記事 <a href=https://www.anthropic.com/engineering/building-effective-agents>Building Effective AI Agents \ Anthropic</a> において、エージェントと対比する形でワークフローについて述べられています。</p><blockquote><ul><li>Workflows are systems where LLMs and tools are orchestrated through predefined code paths.</li><li>Agents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.</li></ul></blockquote><p>(エージェントの定義はいろいろあるようですが) 今回作ったのはここで言うエージェントほど自由ではなく、ワークフローの範疇となります。<br>同記事で挙げられている Workflow: Evaluator-optimizer を含んだワークフローを構築します。</p><h2 id=ソリューション>ソリューション<a hidden class=anchor aria-hidden=true href=#ソリューション>#</a></h2><h3 id=ワークフロー>ワークフロー<a hidden class=anchor aria-hidden=true href=#ワークフロー>#</a></h3><p><a href=https://langchain-ai.github.io/langgraph/concepts/why-langgraph/>LangGraph</a> を使って次のようなワークフローを実装しました。</p><pre class=mermaid>flowchart TD
    start((Start))
    retrieve_target_table["①移行対象の旧データ基盤 table の schema を取得"]
    retrieve_migrated_table["②移行済みの旧データ基盤 table の schema を取得"]
    retrieve_migrated_dbt_model["③移行済みの dbt model を取得"]
    generate_dbt_model["④LLM による dbt model 生成"]
    sql_evaluator["⑤LLM による生成された dbt model の評価"]
    end_((End))
    start --> retrieve_target_table
    retrieve_target_table --> generate_dbt_model
    start --> retrieve_migrated_table
    retrieve_migrated_table --> retrieve_migrated_dbt_model
    retrieve_migrated_dbt_model --> generate_dbt_model
    generate_dbt_model --> sql_evaluator
    sql_evaluator -- dbt model が適切 --> end_
    sql_evaluator -- dbt model が不適切 --> generate_dbt_model
</pre><p>AI ワークフローと言いつつ、LLM を使っているステップは④と⑤だけだったりします。<br>各ステップを解説します。</p><h4 id=移行対象の旧データ基盤-table-の-schema-を取得>①移行対象の旧データ基盤 table の schema を取得<a hidden class=anchor aria-hidden=true href=#移行対象の旧データ基盤-table-の-schema-を取得>#</a></h4><p>最初に分岐した左のラインでは移行したい table の情報を取得します。<br>table 情報はどこから取ってきてもいいのですが、OpenMetadata を導入していたためそちらから取ってきました。<br>この table の schema を入力として dbt model を生成します。</p><p>本来は Glue Job の Python コードを入力として与えるのが正当なやり方だと思いますが、ここではそうしません。<br>複数ファイルに散らばる Python コードを読ませるのが多少手間なのと、(ここ数年の業務で見た中で一番というぐらいに) コード品質が低いコードを LLM に読ませるのが忍びないからです。</p><p>じゃあ何の情報から dbt model を作るの？というのが右のラインになります。</p><h4 id=移行済みの旧データ基盤-table-の-schema-を取得>②移行済みの旧データ基盤 table の schema を取得<a hidden class=anchor aria-hidden=true href=#移行済みの旧データ基盤-table-の-schema-を取得>#</a></h4><p>右のラインではすでに移行が完了した table についての情報を取得します。<br>まずは移行対象と同様に schema 情報を OpenMetadata から取得します。</p><h4 id=移行済みの-dbt-model-を取得>③移行済みの dbt model を取得<a hidden class=anchor aria-hidden=true href=#移行済みの-dbt-model-を取得>#</a></h4><p>次に移行済みの dbt model を取得します。<br>これはローカルに clone された git repository からファイルとして取得する想定です。</p><p>さて、この②と③により dbt model 生成の入出力の例が得られました。<br>これにより dbt model 生成のプロンプトで <a href=https://www.promptingguide.ai/jp/techniques/fewshot>few-shot prompting</a> ができるようになります。<br>few-shot prompting は簡単に言うと、LLM への指示に対する入出力の例を提示することにより、得たい出力を得やすくするという手法です。</p><h4 id=llm-による-dbt-model-生成>④LLM による dbt model 生成<a hidden class=anchor aria-hidden=true href=#llm-による-dbt-model-生成>#</a></h4><p>ここでようやく LLM の出番です。<br>プロンプトには</p><ul><li>dbt model を作れという指示</li><li>指示の背景</li><li>column 命名などの標準化ルール</li><li>few-shot prompting の入出力例</li><li>入力となる移行対象 table の schema</li></ul><p>を与えます。<br>そうすると何らかの dbt model (SQL) っぽいものが出力されました。</p><h4 id=llm-による生成された-dbt-model-の評価>⑤LLM による生成された dbt model の評価<a hidden class=anchor aria-hidden=true href=#llm-による生成された-dbt-model-の評価>#</a></h4><p>④で終わってもよかったのですが、生成物を LLM で評価するというフェーズも追加しました。<br>④のプロンプトとその出力の dbt model を LLM に与え、「タスクの実行結果は適切だったと思うか？」と問いかけます。<br>適切なら終了します。めでたしめでたし。</p><p>不適切と判断された場合はその理由つけて④に差し戻し、評価結果に配慮させながら④をやり直します。<br>無限ループにならないよう N 回繰り返すと失敗するような制御も必要です。</p><p>この LLM に評価させるというのが前述の <a href=https://www.anthropic.com/engineering/building-effective-agents>Building Effective AI Agents \ Anthropic</a> における Workflow: Evaluator-optimizer となります。<br><a href=https://arxiv.org/abs/2405.10467>agent desing pattern</a> においては self-reflection とも言われます。</p><h3 id=技術スタック>技術スタック<a hidden class=anchor aria-hidden=true href=#技術スタック>#</a></h3><p>ワークフローの実装には次のような技術スタックを使いました。</p><ul><li><a href=https://langchain-ai.github.io/langgraph/concepts/why-langgraph/>LangGraph</a>: ワークフローの構成</li><li><a href=https://python.langchain.com/docs/introduction/>LangChain</a>: ワークフローの node 内での prompt 生成、LLM 呼び出し、結果成形などの一連の処理</li><li><a href=https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html>Amazon Bedrock</a>: LLM は Bedrock 経由で Claude 3.7 Sonnet を利用</li></ul><p>これらを使ったワークフロー実装はそこまで難しいものではなく、だいたい1週間ちょっとぐらいでコーディングしました。</p><h2 id=ワークフローの導入効果>ワークフローの導入効果<a hidden class=anchor aria-hidden=true href=#ワークフローの導入効果>#</a></h2><p>一部の table 移行で試したところ、ちゃんと測ったわけではありませんが体感で50~80%ぐらいは工数削減できたかなと思います。<br>比較的ローコストで大きく工数を削減できたのでこの結果には満足しています。<br>勉強したての AI ワークフローのユースケースとして実際に役に立つものを作れたのも良かったです。</p><p>few-shot prompting だのみなので、他の dbt model と異なる特殊な処理をしているケースには対応が難しく、そういった場合は人間様の出番になります。<br>まあ作業を100%置き換えることを目指していたわけではないため、それは良しとしています。</p><p>何回か試した中では評価で不適切とみなされることはありませんでした。<br>これは dbt model 生成とその評価に同じ LLM を使っているからかもしれません。<br>人間も自分の仕事を自分で評価すると甘くなります。<br>self-reflection ではなく、評価に別の LLM を使うという crossーreflection という agent design pattern にするといいかもしれません。</p><h2 id=qa>Q&amp;A<a hidden class=anchor aria-hidden=true href=#qa>#</a></h2><h4 id=q-ワークフロー作らんでも-cursor-や-cline-みたいなエージェントに命令したらええんちゃうん>Q. ワークフロー作らんでも Cursor や Cline みたいなエージェントに命令したらええんちゃうん？<a hidden class=anchor aria-hidden=true href=#q-ワークフロー作らんでも-cursor-や-cline-みたいなエージェントに命令したらええんちゃうん>#</a></h4><p>A.<br>おそらくそのやり方でもできるでしょう。<br>もっと言うと Glue Job の Python コードを含むプロジェクトのファイルをまるっとコンテキストとして持てるのであれば、精度も上がるかもしれません。<br>ただ残念ながら弊社は出遅れており、そういったソフトウェア開発エージェントがまだ利用できない状態です…</p><p>ワークフローを自前で作ることのメリットを強いて上げるのであれば、決まった処理の流れを曖昧な自然言語ではなくコードで定義できるというところになるでしょうか。</p><h4 id=q-glue-job-は-spark-やろせやったらそのままのコードで-databricks-に移行できるんちゃうん>Q. Glue Job は Spark やろ？せやったらそのままのコードで Databricks に移行できるんちゃうん？<a hidden class=anchor aria-hidden=true href=#q-glue-job-は-spark-やろせやったらそのままのコードで-databricks-に移行できるんちゃうん>#</a></h4><p>A.<br>Apache Spark はプログラミングモデルは DataFrame 形式で非常にとっつきやすいのですが、アーキテクチャを理解していないとパフォーマンスの問題に対応できません。<br>アーキテクチャを理解せずに Spark を使っている開発者も割と多いです。<br>弊社もそういう状況で Spark を理解するエンジニアを安定的に雇用するのも困難なため、このデータ基盤移行によりコードとしての Spark は捨てて ELT については完全に SQL 化する運用にシフトしようとしています。</p><p>また、前述のとおり Spark のコードと言っても Glue Job の API が結構入り込んでいてポータビリティが低くなってしまっていました。<br>さらに (ここ数年の業務で見た中で一番というぐらいに) コード品質が低く、これを残したくない気持ちがありました。</p><h4 id=q-ai-ワークフローはどないして勉強したらええのん>Q. AI ワークフローはどないして勉強したらええのん？<a hidden class=anchor aria-hidden=true href=#q-ai-ワークフローはどないして勉強したらええのん>#</a></h4><p>A.<br>今回の技術選定、ひいてはそもそものワークフロー作成のアイディアは以下の書籍に大きく影響を受けています。</p><ul><li><a href=https://gihyo.jp/book/2024/978-4-297-14530-9>西見 公宏; 吉田 真吾; 大嶋 勇樹. LangChainとLangGraphによるRAG・AIエージェント［実践］入門 エンジニア選書. 株式会社技術評論社.</a></li></ul><p>個人的にはとても面白い内容でした。<br>AI エージェントやワークフローについて学ぶのにいい書籍だと思います。(が、この分野は古くなるのも早いので注意)</p><p>この書籍について書いた<a href=/posts/reading-note-agent-book/>ポスト</a>もご参照いただければと。</p><h2 id=次回予告>次回予告<a hidden class=anchor aria-hidden=true href=#次回予告>#</a></h2><p>次こそ CI/CD の話をします！</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://soonraah.github.io/tags/langchain/>LangChain</a></li><li><a href=https://soonraah.github.io/tags/langgraph/>LangGraph</a></li><li><a href=https://soonraah.github.io/tags/llm/>LLM</a></li><li><a href=https://soonraah.github.io/tags/data-platform/>Data Platform</a></li><li><a href=https://soonraah.github.io/tags/%E3%81%B5%E3%81%A4%E3%81%86%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E5%9F%BA%E7%9B%A4%E7%A7%BB%E8%A1%8C/>ふつうのデータ基盤移行</a></li></ul><nav class=paginav><a class=next href=https://soonraah.github.io/posts/ordinary-data-platform-migration-part-3/><span class=title>次へ »</span><br><span>ふつうのデータ基盤移行 - Part 3. アーキテクチャ編</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編 on x" href="https://x.com/intent/tweet/?text=%e3%81%b5%e3%81%a4%e3%81%86%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e5%9f%ba%e7%9b%a4%e7%a7%bb%e8%a1%8c%20-%20Part%204.%20AI%20%e3%83%af%e3%83%bc%e3%82%af%e3%83%95%e3%83%ad%e3%83%bc%e3%81%a7%e7%a7%bb%e8%a1%8c%e4%bd%9c%e6%a5%ad%e5%8a%b9%e7%8e%87%e5%8c%96%e7%b7%a8&amp;url=https%3a%2f%2fsoonraah.github.io%2fposts%2fordinary-data-platform-migration-part-4%2f&amp;hashtags=LangChain%2cLangGraph%2cLLM%2cdataplatform%2c%e3%81%b5%e3%81%a4%e3%81%86%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e5%9f%ba%e7%9b%a4%e7%a7%bb%e8%a1%8c"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsoonraah.github.io%2fposts%2fordinary-data-platform-migration-part-4%2f&amp;title=%e3%81%b5%e3%81%a4%e3%81%86%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e5%9f%ba%e7%9b%a4%e7%a7%bb%e8%a1%8c%20-%20Part%204.%20AI%20%e3%83%af%e3%83%bc%e3%82%af%e3%83%95%e3%83%ad%e3%83%bc%e3%81%a7%e7%a7%bb%e8%a1%8c%e4%bd%9c%e6%a5%ad%e5%8a%b9%e7%8e%87%e5%8c%96%e7%b7%a8&amp;summary=%e3%81%b5%e3%81%a4%e3%81%86%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e5%9f%ba%e7%9b%a4%e7%a7%bb%e8%a1%8c%20-%20Part%204.%20AI%20%e3%83%af%e3%83%bc%e3%82%af%e3%83%95%e3%83%ad%e3%83%bc%e3%81%a7%e7%a7%bb%e8%a1%8c%e4%bd%9c%e6%a5%ad%e5%8a%b9%e7%8e%87%e5%8c%96%e7%b7%a8&amp;source=https%3a%2f%2fsoonraah.github.io%2fposts%2fordinary-data-platform-migration-part-4%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsoonraah.github.io%2fposts%2fordinary-data-platform-migration-part-4%2f&title=%e3%81%b5%e3%81%a4%e3%81%86%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e5%9f%ba%e7%9b%a4%e7%a7%bb%e8%a1%8c%20-%20Part%204.%20AI%20%e3%83%af%e3%83%bc%e3%82%af%e3%83%95%e3%83%ad%e3%83%bc%e3%81%a7%e7%a7%bb%e8%a1%8c%e4%bd%9c%e6%a5%ad%e5%8a%b9%e7%8e%87%e5%8c%96%e7%b7%a8"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsoonraah.github.io%2fposts%2fordinary-data-platform-migration-part-4%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編 on whatsapp" href="https://api.whatsapp.com/send?text=%e3%81%b5%e3%81%a4%e3%81%86%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e5%9f%ba%e7%9b%a4%e7%a7%bb%e8%a1%8c%20-%20Part%204.%20AI%20%e3%83%af%e3%83%bc%e3%82%af%e3%83%95%e3%83%ad%e3%83%bc%e3%81%a7%e7%a7%bb%e8%a1%8c%e4%bd%9c%e6%a5%ad%e5%8a%b9%e7%8e%87%e5%8c%96%e7%b7%a8%20-%20https%3a%2f%2fsoonraah.github.io%2fposts%2fordinary-data-platform-migration-part-4%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編 on telegram" href="https://telegram.me/share/url?text=%e3%81%b5%e3%81%a4%e3%81%86%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e5%9f%ba%e7%9b%a4%e7%a7%bb%e8%a1%8c%20-%20Part%204.%20AI%20%e3%83%af%e3%83%bc%e3%82%af%e3%83%95%e3%83%ad%e3%83%bc%e3%81%a7%e7%a7%bb%e8%a1%8c%e4%bd%9c%e6%a5%ad%e5%8a%b9%e7%8e%87%e5%8c%96%e7%b7%a8&amp;url=https%3a%2f%2fsoonraah.github.io%2fposts%2fordinary-data-platform-migration-part-4%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e3%81%b5%e3%81%a4%e3%81%86%e3%81%ae%e3%83%87%e3%83%bc%e3%82%bf%e5%9f%ba%e7%9b%a4%e7%a7%bb%e8%a1%8c%20-%20Part%204.%20AI%20%e3%83%af%e3%83%bc%e3%82%af%e3%83%95%e3%83%ad%e3%83%bc%e3%81%a7%e7%a7%bb%e8%a1%8c%e4%bd%9c%e6%a5%ad%e5%8a%b9%e7%8e%87%e5%8c%96%e7%b7%a8&u=https%3a%2f%2fsoonraah.github.io%2fposts%2fordinary-data-platform-migration-part-4%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://soonraah.github.io/>Froglog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="コピー";function s(){t.innerHTML="コピーされました!",setTimeout(()=>{t.innerHTML="コピー"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script type=module>
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true });
  </script></body></html>