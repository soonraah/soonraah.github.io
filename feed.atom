<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="jp">
  <title>Froglog</title>
  <subtitle></subtitle>
  <id>https://soonraah.github.io/</id>
  <author>
    <name>Froglog</name>
    <uri>https://soonraah.github.io/</uri>
  </author>
  <updated>2020-07-18T07:51:12Z</updated>
  <link rel="self" type="application/atom+xml" href="https://soonraah.github.io/feed.atom" hreflang="jp"/>
  <link rel="alternate" type="text/html" href="https://soonraah.github.io/" hreflang="jp"/>
  <entry>
    <title>Apache Spark 3.0.0 について調べた</title>
    <author>
      <name>soonraah</name>
      <uri>https://example.com/</uri>
    </author>
    <id>https://soonraah.github.io/study-spark-3-0-0/</id>
    <updated>2020-07-12T00:00:00Z</updated>
    <published>2020-07-12T00:00:00Z</published>
    <content type="html"><![CDATA[<div class="flex flex-wrap justify-center"><a class="rounded font-content-sans font-semibold text-raven-700 bg-raven-100 hover:bg-raven-200 py-2 px-4 m-2" href="https://soonraah.github.io/tags/apache-spark/">Apache spark (1)</a>
</div>

<h2 id="はじめに">はじめに</h2>
<p>Apache Spark 3.0.0 がリリースされました。</p>
<ul>
<li><a href="https://spark.apache.org/releases/spark-release-3-0-0.html">Spark Release 3.0.0</a></li>
</ul>
<p>release note を見て個人的に気になったところなど簡単に調べました。
書いてみると Databricks の記事へのリンクばっかになってしまった…</p>
<h2 id="全体感">全体感</h2>
<p>こちらの記事を読めば全体感は OK.</p>
<ul>
<li><a href="https://databricks.com/jp/blog/2020/06/18/introducing-apache-spark-3-0-now-available-in-databricks-runtime-7-0.html">Introducing Apache Spark 3.0</a></li>
</ul>
<p>公式の release note には</p>
<blockquote>
<p>Python is now the most widely used language on Spark.</p>
</blockquote>
<p>とあってそうなん？ってなったけど、こちらの記事だと</p>
<blockquote>
<p>Python is now the most widely used language on Spark and, consequently, was a key focus area of Spark 3.0 development. 68% of notebook commands on Databricks are in Python.</p>
</blockquote>
<p>と書いてありどうやら Databricks の notebook の話らしく、だったらまあそうかなという感じ。
プロダクトコードへの実装というよりは、アドホック分析や検証用途の話なんでしょう。</p>
<h2 id="project-hydrogen-accelerator-aware-scheduler">[Project Hydrogen] Accelerator-aware Scheduler</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-24615">SPARK-24615</a></li>
</ul>
<p>Spark 上で deep learning できるようにすることを目指す Project Hydrogen、その3つの大きな目標のうちの一つ。</p>
<ul>
<li><a href="https://www.slideshare.net/MatthewStubbs6/big-data-ldn-2018-project-hydrogen-unifying-ai-with-apache-spark/26">Big Data LDN 2018: PROJECT HYDROGEN: UNIFYING AI WITH APACHE SPARK</a></li>
</ul>
<p>YARN や Kubernetes では GPU や FPGA を扱えるようになっているので Spark でも扱えるようにしたいというモチベーション。
<a href="https://spark.apache.org/docs/3.0.0/running-on-yarn.html#resource-allocation-and-configuration-overview">Spark のドキュメント</a> によると</p>
<blockquote>
<p>For example, the user wants to request 2 GPUs for each executor. The user can just specify <code>spark.executor.resource.gpu.amount=2</code> and Spark will handle requesting <code>yarn.io/gpu</code> resource type from YARN.</p>
</blockquote>
<p>のようにして executor に GPU リソースを要求できるみたいです。</p>
<h2 id="adaptive-query-execution">Adaptive Query Execution</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-31412">SPARK-31412</a></li>
</ul>
<p>平たく言うと実行時に得られる統計情報を使って plan を最適化すると、静的に生成された plan より効率化できるよねという話。
<code>spark.sql.adaptive.enabled=true</code> にすることで有効になります。</p>
<p>処理の途中で中間生成物が materialize されるタイミングで、その時点の統計情報を使って残りの処理を最適化する、というのを繰り返します。</p>
<p>Spark 3.0.0 では以下3つの AQE が実装されました。</p>
<ul>
<li>Coalescing Post Shuffle Partitions</li>
<li>Converting sort-merge join to broadcast join</li>
<li>Optimizing Skew Join</li>
</ul>
<p>Spark 2 以前だとこのあたりは実行しつつチューニングするような運用になりがちでした。
特に skew の解消は salt を追加したりなど面倒だったりします。
これらが自動で最適化されるというのは運用上うれしいところ。
急なデータ傾向の変化に対しても自動で最適化して対応できるという面があります。</p>
<p>AQE に関してもやはり Databricks の解説記事がわかりやすいです。
図もいい感じ。</p>
<ul>
<li><a href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html">Adaptive Query Execution: Speeding Up Spark SQL at Runtime</a></li>
</ul>
<h2 id="dynamic-partition-pruning">Dynamic Partition Pruning</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-11150">SPARK-11150</a></li>
</ul>
<p>こちらも AQE 同様にクエリのパフォーマンスを改善する目的で導入されたもの。
改善幅は AQE より大きいようです。
やはり実行時の情報を使って partition pruning を行い、不要な partition の参照を減らすという方法。</p>
<p>主に <a href="https://ja.wikipedia.org/wiki/%E3%82%B9%E3%82%BF%E3%83%BC%E3%82%B9%E3%82%AD%E3%83%BC%E3%83%9E">star schema</a> における join 時のように、静的には partition pruning が行えない場合を想定しています。
比較的小さいことが多いと思われる dimension table 側を broadcast する broadcast join において、broadcast された情報を fact table の partition pruning に利用するというやり口。</p>
<ul>
<li><a href="https://www.slideshare.net/databricks/dynamic-partition-pruning-in-apache-spark">Dynamic Partition Pruning in Apache Spark</a></li>
</ul>
<h2 id="structured-streaming-ui">Structured Streaming UI</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-29543">SPARK-29543</a></li>
</ul>
<p>&quot;Structured Streaming&quot; というタブが UI に追加された件。
Spark のドキュメントに例があります。</p>
<blockquote>
<p><img src="https://spark.apache.org/docs/3.0.0/img/webui-structured-streaming-detail.png" alt="Structured Streaming Query Statistics"></p>
</blockquote>

<blockquote>
  <p> </p>
  <footer>&#8212; Apache Spark <cite title="Structured Streaming Tab"><a rel="noopener nofollow" href="https://spark.apache.org/docs/3.0.0/web-ui.html#structured-streaming-tab">Structured Streaming Tab</a></cite></footer>
</blockquote>

<p>Spark 2.4 系では Structured Streaming を動かしていてもせいぜい job や stage が増えていくという味気ないものしか見えませんでした。
Spark 3.0.0 で実際に動かしてみたけど欲しかったやつ！という感じ。
ストリーム処理では入力データ量の変化の可視化がマストだと思ってます。</p>
<h2 id="catalog-plugin-api">Catalog plugin API</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-31121">SPARK-31121</a></li>
<li><a href="https://docs.google.com/document/d/1zLFiA1VuaWeVxeTDXNg8bL6GP3BVoOZBkewFtEnjEoo/edit">SPIP: Spark API for Table Metadata</a></li>
</ul>
<p>これまでは CTAS (Create Table As Select) の操作はあったが、外部のデータソースに対して DDL 的な操作をする API が足りていませんでした。
CTAS も挙動に実装依存の曖昧さがありました。
そこで <code>create</code>, <code>alter</code>, <code>load</code>, <code>drop</code> 等のテーブル操作をできるようにしたという話。</p>
<p>ドキュメントの <a href="https://spark.apache.org/docs/3.0.0/sql-ref-syntax.html#ddl-statements">DDL Statements</a> のあたりを読め何ができるかわかります。
以前のバージョンでも一部のデータソースについてはできた模様 (ex. <a href="https://spark.apache.org/docs/2.4.6/sql-migration-guide-hive-compatibility.html#supported-hive-features">Hive</a>)。</p>
<p>今の自分の業務では Spark から DDL を扱うようなことはしないのでそれほど恩恵は感じられません。
notebook からアドホックな Spark のバッチを動かすというような使い方をしていればうれしいかもしれません。</p>
<h2 id="add-an-api-that-allows-a-user-to-define-and-observe-arbitrary-metrics-on-batch-and-streaming-queries">Add an API that allows a user to define and observe arbitrary metrics on batch and streaming queries</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-29345">SPARK-29345</a></li>
</ul>
<p>クエリの途中の段階で何らかの metrics を仕込んでおいて callback 的にその metrics にアクセスできる仕組み。
<a href="https://spark.apache.org/docs/3.0.0/api/scala/org/apache/spark/sql/Dataset.html#observe(name:String,expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset%5BT%5D">Dataset#observe() の API ドキュメント</a> を読むのが一番早いです。
この例ではストリーム処理を扱っているが、バッチ処理の例を自分で書いて試してみました。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Register listener
</span><span class="c1"></span><span class="n">spark</span>
  <span class="o">.</span><span class="n">listenerManager</span>
  <span class="o">.</span><span class="n">register</span><span class="o">(</span><span class="k">new</span> <span class="nc">QueryExecutionListener</span> <span class="o">{</span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">onSuccess</span><span class="o">(</span><span class="n">funcName</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">qe</span><span class="k">:</span> <span class="kt">QueryExecution</span><span class="o">,</span> <span class="n">durationNs</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">num</span> <span class="k">=</span> <span class="n">qe</span><span class="o">.</span><span class="n">observedMetrics</span>
        <span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="s">&#34;my_metrics&#34;</span><span class="o">)</span>
        <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getAs</span><span class="o">[</span><span class="kt">Long</span><span class="o">](</span><span class="s">&#34;num&#34;</span><span class="o">))</span>
        <span class="o">.</span><span class="n">getOrElse</span><span class="o">(-</span><span class="mf">100.0</span><span class="o">)</span>

      <span class="n">println</span><span class="o">(</span><span class="s">s&#34;num of data: </span><span class="si">$num</span><span class="s">&#34;</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">onFailure</span><span class="o">(</span><span class="n">funcName</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">qe</span><span class="k">:</span> <span class="kt">QueryExecution</span><span class="o">,</span> <span class="n">exception</span><span class="k">:</span> <span class="kt">Exception</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{}</span>
  <span class="o">})</span>

<span class="c1">// Make DataFrame
</span><span class="c1"></span><span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="nc">Seq</span>
  <span class="o">.</span><span class="n">range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1000</span><span class="o">)</span>
  <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="k">_</span><span class="o">,</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="s">&#34;c&#34;</span><span class="o">)(</span><span class="nc">Random</span><span class="o">.</span><span class="n">nextInt</span><span class="o">(</span><span class="mi">3</span><span class="o">)),</span> <span class="n">math</span><span class="o">.</span><span class="n">random</span><span class="o">()))</span>
  <span class="o">.</span><span class="n">toDF</span><span class="o">(</span><span class="s">&#34;id&#34;</span><span class="o">,</span> <span class="s">&#34;type&#34;</span><span class="o">,</span> <span class="s">&#34;value&#34;</span><span class="o">)</span>

<span class="c1">// Observe and process
</span><span class="c1"></span><span class="k">val</span> <span class="n">dfResult</span> <span class="k">=</span> <span class="n">df</span>
  <span class="o">.</span><span class="n">observe</span><span class="o">(</span><span class="s">&#34;my_metrics&#34;</span><span class="o">,</span> <span class="n">count</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;*&#34;</span><span class="o">).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;num&#34;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;type&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">agg</span><span class="o">(</span><span class="n">avg</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;value&#34;</span><span class="o">).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;avg_value&#34;</span><span class="o">))</span>

<span class="c1">// Run
</span><span class="c1"></span><span class="n">dfResult</span><span class="o">.</span><span class="n">show</span>
</code></pre></div><p>これを動かしたときの出力は次のようになりました。</p>
<pre><code>+----+------------------+
|type|         avg_value|
+----+------------------+
|   c|0.5129435063033314|
|   b|0.4693004460694317|
|   a|0.4912087482418599|
+----+------------------+

num of data: 1000
</code></pre><p><code>observe()</code> はその出力の DataFrame に対して schema やデータの中身を変更することはありません。
metrics を仕込むのみ。
logical plan を出力してみると <code>observe()</code> を入れることにより途中に <code>CollectMetrics</code> という plan が挿入されていました。
ソースを見ると accumulator を使っている模様。
なので <code>observe()</code> の集計のみで一度 job が動くわけではなく、クエリが2回走るという感じではありません。
全体の処理の中でひっそりと accumulator で脇で集計しておくといった趣でしょうか。</p>
<p>これは結構有用だと思います。
例えば何らかの集計をやるにしてもその最初や途中で、例えば入力データは何件あったか？みたいなことをログに出しておきたいことがあります。
というか accumulator で頑張ってそういうものを作ったことがある…
これがフレームワーク側でサポートされるのはうれしいです。</p>
<h2 id="まとめ">まとめ</h2>
<p>2つのダイナミックな最適化に期待大。
気が向いたら追加でまた調べるかもしれません。</p>
<table>
<thead>
<tr>
<th>aaa</th>
<th>bbb</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2</td>
</tr>
</tbody>
</table>]]></content>
  </entry>
</feed>
