<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="jp">
  <title>Froglog</title>
  <subtitle></subtitle>
  <id>https://soonraah.github.io/</id>
  <author>
    <name>Froglog</name>
    <uri>https://soonraah.github.io/</uri>
  </author>
  <updated>2020-08-06T13:55:38Z</updated>
  <link rel="self" type="application/atom+xml" href="https://soonraah.github.io/feed.atom" hreflang="jp"/>
  <link rel="alternate" type="text/html" href="https://soonraah.github.io/" hreflang="jp"/>
  <entry>
    <title>Apache Flink の broadcast state pattern を用いた stream data と static data の join</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/flink-join-by-broadcast-state-pattern/</id>
    <updated>2020-08-06T13:00:00Z</updated>
    <published>2020-08-06T13:00:00Z</published>
    <content type="html"><![CDATA[<p>star schema における fact table と dimension table の join のようなことを Apache Flink におけるストリーム処理で行いたい。
stream data と static data の join ということになる。
ただし dimension table 側も更新されるため、完全な static というわけではない。</p>
<p>このポストでは Flink v1.11 を前提とした。</p>
<h2 id="join-の方法">join の方法</h2>
<p>今回は DataStream API でこれを実現することを考える。
Flink のドキュメントを読むと broadcast state pattern でできそうだ。</p>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html">The Broadcast State Pattern</a></li>
</ul>
<p>やり方としては次のようになる。</p>
<ol>
<li>static data のファイルを <code>FileProcessingMode.PROCESS_CONTINUOUSLY</code> で読み込み <code>DataStream</code> 化</li>
<li>1 を <code>broadcast()</code></li>
<li>stream data の <code>DataStream</code> と 2 を <code>connect()</code></li>
</ol>
<p>static data を <code>PROCESS_CONTINUOUSLY</code> で読むのは変更を得るため。
<code>PROCESS_ONCE</code> で読んでしまうとストリーム処理の開始時に1回読むだけになり、dimension table の変更を得られない。
このあたりの仕様については <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html#data-sources">Data Sources</a> を参照。</p>
<p>その後は broadcast state pattern にそのまま従う。
したがって <code>BroadcastProcessFunction</code> or <code>KeyedBroadcastProcessFunction</code> を実装する必要がある。
その中で static data を取り込んで state として持ち、stream data 側で参照すればよい。</p>
<p>2つのデータの各 term に対する関係性を以下に示す。</p>
<table>
<thead>
<tr>
<th>in star schema</th>
<th>stream or static</th>
<th>broadcast or not</th>
</tr>
</thead>
<tbody>
<tr>
<td>dimension table</td>
<td>static data</td>
<td>broadcasted</td>
</tr>
<tr>
<td>fact table</td>
<td>stream data</td>
<td>non-broadcasted</td>
</tr>
</tbody>
</table>
<h2 id="実験">実験</h2>
<h3 id="実験概要">実験概要</h3>
<p>stream data として株価のデータを考える。
適当に乱数で作った株価が &quot;GOOGL&quot; 等の ticker とともに流れてくる。
一方、会社情報が記載された dimension table 的なファイルも用意する。
流れ続ける株価データに対して ticker を key にして会社情報を紐付ける、ということを行う。</p>
<h3 id="コード">コード</h3>
<p>上記を実行するためのコードを以下に示す。
実行可能なプロジェクトを <a href="https://github.com/soonraah/flink_join_test">GitHub</a> に置いたので興味があればどうぞ。</p>
<h4 id="entry-point">Entry Point</h4>
<p><code>main()</code> の実装。
<code>MiniClusterWithClientResource</code> は本来は単体テスト用だが、簡単に local で cluster を動かすためにここで使用している。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">package</span> <span class="nn">example</span>

<span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.MapStateDescriptor</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.java.io.TextInputFormat</span>
<span class="k">import</span> <span class="nn">org.apache.flink.core.fs.Path</span>
<span class="k">import</span> <span class="nn">org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.functions.source.FileProcessingMode</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.scala.</span><span class="o">{</span><span class="nc">DataStream</span><span class="o">,</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">,</span> <span class="n">createTypeInformation</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.apache.flink.test.util.MiniClusterWithClientResource</span>

<span class="k">object</span> <span class="nc">FlinkJoinTest</span> <span class="o">{</span>
  <span class="c1">// See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html#testing-flink-jobs
</span><span class="c1"></span>  <span class="k">private</span> <span class="k">val</span> <span class="nc">FlinkCluster</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MiniClusterWithClientResource</span><span class="o">(</span>
    <span class="k">new</span> <span class="nc">MiniClusterResourceConfiguration</span>
    <span class="o">.</span><span class="nc">Builder</span><span class="o">()</span>
      <span class="o">.</span><span class="n">setNumberSlotsPerTaskManager</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
      <span class="o">.</span><span class="n">setNumberTaskManagers</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
      <span class="o">.</span><span class="n">build</span>
  <span class="o">)</span>

  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">FlinkCluster</span><span class="o">.</span><span class="n">before</span><span class="o">()</span>

    <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
    <span class="n">env</span><span class="o">.</span><span class="n">setParallelism</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>

    <span class="k">val</span> <span class="n">companiesMasterFilePath</span> <span class="k">=</span> <span class="s">&#34;data/companies.csv&#34;</span>

    <span class="k">val</span> <span class="n">companies</span> <span class="k">=</span> <span class="n">readCompaniesMaster</span><span class="o">(</span><span class="n">companiesMasterFilePath</span><span class="o">,</span> <span class="n">env</span><span class="o">)</span>
      <span class="o">.</span><span class="n">broadcast</span><span class="o">(</span><span class="k">new</span> <span class="nc">MapStateDescriptor</span><span class="o">(</span>
        <span class="s">&#34;CompanyState&#34;</span><span class="o">,</span>
        <span class="n">classOf</span><span class="o">[</span><span class="kt">String</span><span class="o">],</span>    <span class="c1">// ticker
</span><span class="c1"></span>        <span class="n">classOf</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span>     <span class="c1">// name
</span><span class="c1"></span>      <span class="o">))</span>

    <span class="c1">// the broadcast state pattern
</span><span class="c1"></span>    <span class="c1">// See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html
</span><span class="c1"></span>    <span class="n">env</span>
      <span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="k">new</span> <span class="nc">UnboundedStocks</span><span class="o">)</span>
      <span class="o">.</span><span class="n">connect</span><span class="o">(</span><span class="n">companies</span><span class="o">)</span>
      <span class="o">.</span><span class="n">process</span><span class="o">(</span><span class="k">new</span> <span class="nc">StockBroadcastProcessFunction</span><span class="o">)</span>
      <span class="o">.</span><span class="n">print</span><span class="o">()</span>

    <span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">(</span><span class="s">&#34;flink join test&#34;</span><span class="o">)</span>

    <span class="nc">FlinkCluster</span><span class="o">.</span><span class="n">after</span><span class="o">()</span>
  <span class="o">}</span>

  <span class="k">private</span> <span class="k">def</span> <span class="n">readCompaniesMaster</span><span class="o">(</span><span class="n">companiesMasterFilePath</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
                                  <span class="n">env</span><span class="k">:</span> <span class="kt">StreamExecutionEnvironment</span><span class="o">)</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Company</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="n">env</span>
      <span class="o">.</span><span class="n">readFile</span><span class="o">(</span>
        <span class="k">new</span> <span class="nc">TextInputFormat</span><span class="o">(</span><span class="k">new</span> <span class="nc">Path</span><span class="o">(</span><span class="n">companiesMasterFilePath</span><span class="o">)),</span>
        <span class="n">companiesMasterFilePath</span><span class="o">,</span>
        <span class="nc">FileProcessingMode</span><span class="o">.</span><span class="nc">PROCESS_CONTINUOUSLY</span><span class="o">,</span>
        <span class="mi">10</span> <span class="o">*</span> <span class="mi">1000</span>
      <span class="o">)</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">line</span> <span class="k">=&gt;</span>
        <span class="k">val</span> <span class="n">items</span> <span class="k">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34;,&#34;</span><span class="o">)</span>
        <span class="nc">Company</span><span class="o">(</span><span class="n">items</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">items</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
      <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h4 id="records">Records</h4>
<p>各種レコードを表す case class。
<code>UnboundedStocks</code> は一定のインターバルで <code>Stock</code> を無限に返す iterator であり、stream data 生成に利用する。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">Company</span><span class="o">(</span><span class="n">ticker</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">name</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="k">case</span> <span class="k">class</span> <span class="nc">Stock</span><span class="o">(</span><span class="n">ticker</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">price</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span>

<span class="cm">/**
</span><span class="cm"> * Iterator to generate unbounded stock data
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">UnboundedStocks</span> <span class="k">extends</span> <span class="nc">Iterator</span><span class="o">[</span><span class="kt">Stock</span><span class="o">]</span> <span class="k">with</span> <span class="nc">Serializable</span> <span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">hasNext</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="kc">true</span>  <span class="c1">// unbounded
</span><span class="c1"></span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">next</span><span class="o">()</span><span class="k">:</span> <span class="kt">Stock</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">Thread</span><span class="o">.</span><span class="n">sleep</span><span class="o">(</span><span class="mi">1000</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">tickers</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">&#34;GOOGL&#34;</span><span class="o">,</span> <span class="s">&#34;AAPL&#34;</span><span class="o">,</span> <span class="s">&#34;FB&#34;</span><span class="o">,</span> <span class="s">&#34;AMZN&#34;</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">ticker</span> <span class="k">=</span> <span class="n">tickers</span><span class="o">(</span><span class="nc">Random</span><span class="o">.</span><span class="n">nextInt</span><span class="o">(</span><span class="n">tickers</span><span class="o">.</span><span class="n">size</span><span class="o">))</span>  <span class="c1">// one of GAFA
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">price</span> <span class="k">=</span> <span class="mi">100</span> <span class="o">+</span> <span class="nc">Random</span><span class="o">.</span><span class="n">nextDouble</span><span class="o">()</span> <span class="o">*</span> <span class="mi">300</span>         <span class="c1">// random price
</span><span class="c1"></span>    <span class="nc">Stock</span><span class="o">(</span><span class="n">ticker</span><span class="o">,</span> <span class="n">price</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h4 id="broadcastprocessfunction">BroadcastProcessFunction</h4>
<p>肝である <code>BroadcastProcessFunction</code> の実装。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">package</span> <span class="nn">example</span>

<span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.MapStateDescriptor</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.functions.co.BroadcastProcessFunction</span>
<span class="k">import</span> <span class="nn">org.apache.flink.util.Collector</span>

<span class="k">class</span> <span class="nc">StockBroadcastProcessFunction</span>
  <span class="k">extends</span> <span class="nc">BroadcastProcessFunction</span><span class="o">[</span><span class="kt">Stock</span>, <span class="kt">Company</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="o">{</span>

  <span class="k">private</span> <span class="k">val</span> <span class="nc">StateDescriptor</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MapStateDescriptor</span><span class="o">(</span>
    <span class="s">&#34;CompanyState&#34;</span><span class="o">,</span>
    <span class="n">classOf</span><span class="o">[</span><span class="kt">String</span><span class="o">],</span>    <span class="c1">// ticker
</span><span class="c1"></span>    <span class="n">classOf</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span>     <span class="c1">// name
</span><span class="c1"></span>  <span class="o">)</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">processElement</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">Stock</span><span class="o">,</span>
                              <span class="n">ctx</span><span class="k">:</span> <span class="kt">BroadcastProcessFunction</span><span class="o">[</span><span class="kt">Stock</span>, <span class="kt">Company</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span><span class="k">#</span><span class="nc">ReadOnlyContext</span><span class="o">,</span>
                              <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">companyName</span> <span class="k">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">getBroadcastState</span><span class="o">(</span><span class="nc">StateDescriptor</span><span class="o">).</span><span class="n">get</span><span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">ticker</span><span class="o">)</span>
    <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">value</span><span class="o">.</span><span class="n">ticker</span><span class="o">,</span> <span class="nc">Option</span><span class="o">(</span><span class="n">companyName</span><span class="o">).</span><span class="n">getOrElse</span><span class="o">(</span><span class="s">&#34;-&#34;</span><span class="o">),</span> <span class="n">value</span><span class="o">.</span><span class="n">price</span><span class="o">))</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">processBroadcastElement</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">Company</span><span class="o">,</span>
                                       <span class="n">ctx</span><span class="k">:</span> <span class="kt">BroadcastProcessFunction</span><span class="o">[</span><span class="kt">Stock</span>, <span class="kt">Company</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span><span class="k">#</span><span class="nc">Context</span><span class="o">,</span>
                                       <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">getBroadcastState</span><span class="o">(</span><span class="nc">StateDescriptor</span><span class="o">).</span><span class="n">put</span><span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">ticker</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">name</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="実行結果">実行結果</h3>
<p>上記を実行すると以下のような stream と static が結合された結果レコードが流れ続ける。</p>
<pre><code>2&gt; (FB,Facebook,158.76057838239333)
1&gt; (GOOGL,Google,288.4271251901199)
2&gt; (AAPL,Apple,191.00515338617706)
1&gt; (FB,Facebook,121.98205452369652)
2&gt; (FB,Facebook,140.05023554456997)
</code></pre><p>この状態で会社情報が記載されている <code>data/companies.csv</code> を更新することを考える。
例えば &quot;GOOGL&quot; の社名を &quot;Google&quot; から &quot;Alphabet&quot; に変更して保存してみた。
するとしばらくしてその修正が反映された結果が流れてくるようになる。</p>
<pre><code>1&gt; (GOOGL,Alphabet,288.1008081843843)
2&gt; (AMZN,Amazon,137.11135563851838)
1&gt; (GOOGL,Alphabet,121.78368168964735)
2&gt; (FB,Facebook,236.53483047124948)
1&gt; (FB,Facebook,220.44300865769645)
</code></pre><p>static data の更新が反映されることが確認できた。
今回は10秒に1回のインターバルで元ファイルを確認するようにファイルを読んでいるため、変更してからそれが反映されるまで最大10秒程度かかる。</p>
<h2 id="懸念点">懸念点</h2>
<p>join はできたが次のような懸念点がある。</p>
<ul>
<li>レコードの削除に対応していない
<ul>
<li>state を上書きしているだけなので <code>companies.csv</code> から削除されたレコードは感知できない</li>
</ul>
</li>
<li>ファイルの更新時に処理が重くなる可能性がある
<ul>
<li><code>companies.csv</code> の更新タイミングでその中の全レコードを処理してしまう</li>
</ul>
</li>
<li>checkpoint が大きくなる
<ul>
<li>state が broadcast されているため、task ごとに重複した state が保存されてしまう</li>
<li>See <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html#important-considerations">Important Considerations</a></li>
</ul>
</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>このように broadcast state pattern によって stream data と static data との join 処理を行うことができた。
ただし、まだちゃんと調べていないが DataStream API ではなく Table API を使えばもう少しカジュアルな感じで近いことができるかもしれない。
気が向いたらそちらも試してみる。</p>]]></content>
  </entry>
  <entry>
    <title>あまり大きな Pull Request を作ってほしくない</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/no-more-huge-pull-request/</id>
    <updated>2020-08-03T13:00:00Z</updated>
    <published>2020-08-03T13:00:00Z</published>
    <content type="html"><![CDATA[<p>GitHub Flow ベースの開発においてはあまり大きな pull request を作ってほしくないという話。
なんというか今更わざわざ言わなくてもいいんだけど…</p>
<p>仕事で何度か大きな pull request が投げられているのを見てしまったので、それはあまりよくないよというのを自分でも指摘しやすくするためにまとめておく。</p>
<h2 id="reference">Reference</h2>
<p>最初に参考資料を挙げておく。</p>
<ul>
<li><a href="http://kurtfunai.com/2017/12/100-duck-sized-pull-requests.html">100 Duck-Sized Pull Requests</a></li>
<li><a href="https://techracho.bpsinc.jp/hachi8833/2018_02_07/51095">「巨大プルリク1件vs細かいプルリク100件」問題を考える（翻訳）</a></li>
<li><a href="https://smallbusinessprogramming.com/optimal-pull-request-size/">Optimal pull request size</a></li>
</ul>
<p>これらを読めば特に私から言うこともないのだが…
これらに書いてあるとおりだが補足しておく。</p>
<h2 id="pull-request-を小分けにしたときのメリット">Pull Request を小分けにしたときのメリット</h2>
<h3 id="module-を適切に切り出すモチベーションが得られる">module を適切に切り出すモチベーションが得られる</h3>
<p>次のような話がある。</p>
<ul>
<li><a href="https://hachibeechan.hateblo.jp/entry/dont-think-just-make-it-abstract">共通化という考え方はアンチパターンを生み出すだけ説</a></li>
</ul>
<p>これを理由に module を分けるべきところが分けられず、いろんなことができてしまうベタッと大きな module が生まれるのを見た。</p>
<p>もちろんよく考えられていない共通化は駄目だが、上記ポストでは一方で</p>

<blockquote>
  <p>ただし共通化という名の下におこなわれるのは「同じロジックを持つコードをまとめる」行為であって、抽象化のようにそのコード単位の意味を捉える作業はその範疇にない。抽象化というのはロジックを意味単位ごとにひとくくりにしていく行為で、これがどういうことなのかは次以降で述べていく。</p>
  <footer>&#8212; はっちん <cite title="共通化という考え方はアンチパターンを生み出すだけ説 - タオルケット体操"><a rel="noopener nofollow" href="https://hachibeechan.hateblo.jp/entry/dont-think-just-make-it-abstract">共通化という考え方はアンチパターンを生み出すだけ説 - タオルケット体操</a></cite></footer>
</blockquote>

<p>と述べており抽象化、つまりコードの意味を考慮した上で適切な単位でまとめておくことは否定していない。</p>
<p>pull request を小さく分けるという行為のためには module や機能を適度なまとまりで切り分けること、つまり抽象化を考えていくことが必要となる。
したがって何でもできてしまう大きな module が生まれるのを防ぐ方向に働く。
(もちろんここで駄目な共通化がなされてしまうこともあるだろう)</p>
<h3 id="リリースまでの期間が短く済む">リリースまでの期間が短く済む</h3>
<p>前述の参考資料においても pull request が大きいと</p>

<blockquote>
  <p>Developers would put off reviews until they had time/energy and the development process would come to a halt.</p>
  <footer>&#8212; Kurtis Funai <cite title="100 Duck-Sized Pull Requests"><a rel="noopener nofollow" href="http://kurtfunai.com/2017/12/100-duck-sized-pull-requests.html">100 Duck-Sized Pull Requests</a></cite></footer>
</blockquote>


<blockquote>
  <p>development continues on the master branch, it often results in merge conflicts, rebases, and other fun.</p>
  <footer>&#8212; Kurtis Funai <cite title="100 Duck-Sized Pull Requests"><a rel="noopener nofollow" href="http://kurtfunai.com/2017/12/100-duck-sized-pull-requests.html">100 Duck-Sized Pull Requests</a></cite></footer>
</blockquote>

<p>となると述べられている。
仮にすぐにレビューされ、かつ conflict なども発生しなかっとしても大きな1つの pull request をレビューする場合とそれを3つに分けた場合の開発プロセスの進行を比較すると</p>
<ul>
<li>大きな1つの pull request
<ol>
<li>全体の開発</li>
<li>全体のレビュー</li>
</ol>
</li>
<li>3つに分割された pull request
<ol>
<li>part-1 の開発</li>
<li>part-1 のレビュー &amp; part-2 の開発</li>
<li>part-2 のレビュー &amp; part-3 の開発</li>
<li>part-3 のレビュー</li>
</ol>
</li>
</ul>
<p>のようになり、開発とレビューを並列で進められる部分があるので全体としての開発期間を短くすることができる。
もちろんこれはうまく噛み合ったときの例だが。
早めにフィードバックが得られるのも大きい。</p>
<h2 id="pull-request-を小分けにしたときのデメリット">Pull Request を小分けにしたときのデメリット</h2>
<p>小分けにすることでレビュワーが全体感をつかみにくくなるというのはあるかもしれない。
part-1 と part-2 で同じレビュワーになるとも限らない。</p>
<p>これに対しては開発に着手する前にまず全体のざっくりとした設計についてレビューを受けることで対応できる。
このステップを入れることで「とりあえず手を動かそう」となりにくくなる。
手を動かすと仕事してる感が出るのでとりあえず手を動かしたくなりがちだが、ちょっと待てよと。
まず何を作るか、全体の工程を考えましょうと。</p>

<blockquote>
  <p>Working under these constraints causes developers to break problems down into incremental deliverables. It helps avoid the temptation of jumping into development without a clear plan.</p>
  <footer>&#8212; Kurtis Funai <cite title="100 Duck-Sized Pull Requests"><a rel="noopener nofollow" href="http://kurtfunai.com/2017/12/100-duck-sized-pull-requests.html">100 Duck-Sized Pull Requests</a></cite></footer>
</blockquote>

<h2 id="まとめ">まとめ</h2>
<p>pull request を分けてくれ、頼む。</p>]]></content>
  </entry>
  <entry>
    <title>勉強会メモ: Spark Meetup Tokyo #3 Online</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/spark-meetup-tokyo-3/</id>
    <updated>2020-08-01T06:00:00Z</updated>
    <published>2020-08-01T06:00:00Z</published>
    <content type="html"><![CDATA[<p>2020-07-31 にオンライン開催された <a href="https://spark-meetup-tokyo.connpass.com/event/181422/">Spark Meetup Tokyo #3 Online</a> に参加した。
感想などメモしておく。</p>
<h1 id="全体感">全体感</h1>
<p>トピックとしては主に</p>
<ul>
<li>Spark 3.0</li>
<li>Spark + AI Summit 2020</li>
<li>Spark 周辺要素</li>
</ul>
<p>といったところだろうか。
最近のコミュニティの動向や関心を日本語で聞くことができてよかった。</p>
<p>運営 &amp; スピーカーの皆様、ありがとうございます。</p>
<h1 id="発表">発表</h1>
<p>発表資料は公開されたら追加していく。</p>
<h2 id="sparkai-summit-2020-イベント概要">SPARK+AI Summit 2020 イベント概要</h2>
<p>スピーカー: <a href="https://twitter.com/tokyodataguy">@tokyodataguy</a> さん</p>
<ul>
<li>Summit は金融業界の参加者が増えているらしい</li>
<li>Spark で最も使われている言語は Python とのことだったが、Databricks の notebook サービスの話だった
<ul>
<li>プロダクションコードではまた違うのだろう</li>
</ul>
</li>
<li>Spark 3.0 の update をざっくりと</li>
<li>Spark 周辺要素の話をざっくりと
<ul>
<li>Koalas</li>
<li>Delta Lake</li>
<li>Redash</li>
<li>MLflow</li>
</ul>
</li>
</ul>
<h2 id="introducing-koalas-10">Introducing Koalas 1.0</h2>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/42rsX5ywWZbk5b" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/ueshin/introducing-koalas-10-and-11" title="Introducing Koalas 1.0 (and 1.1)" target="_blank">Introducing Koalas 1.0 (and 1.1)</a> </strong> from <strong><a href="//www.slideshare.net/ueshin" target="_blank">Takuya UESHIN</a></strong> </div>
<p>スピーカー: <a href="https://twitter.com/ueshin">@ueshin</a> さん</p>
<ul>
<li>What's Koalas
<ul>
<li>open source の pure Python library</li>
<li>pandas の API で Spark を動かせるようにする</li>
<li>小さいデータも大きなデータも同じように扱えるように</li>
</ul>
</li>
<li>最近 v1.1 をリリース</li>
<li>(個人的には pandas の API があまり好きではなく…)</li>
</ul>
<h2 id="sparkai-summit-2020-のセッションハイライト">SPARK+AI Summit 2020 のセッションハイライト</h2>
<p>スピーカー: <a href="https://twitter.com/masaru_dobashi">@masaru_dobashi</a> さん</p>
<ul>
<li>Summit のセッションから case study 的なセッションを2つピックアップ</li>
<li>USCIS の例
<ul>
<li><a href="https://databricks.com/jp/session_na20/lessons-learned-from-modernizing-uscis-data-analytics-platform">Lessons Learned from Modernizing USCIS Data Analytics Platform</a></li>
<li>古き良き Data Warehouse から Data Lake への移行</li>
<li>injection には Kafka も利用</li>
<li>諸々気にせずに気軽にデータをストレージに置きたい</li>
</ul>
</li>
<li>Alibaba の例
<ul>
<li>Spark Structured Streming の上に SQL-like なものを載せた？</li>
<li>ストリームの mini-batch 処理の合間で compaction を行う (つらそう)</li>
</ul>
</li>
<li>スライド中の「パイプラインの途中でモダンな技術に流し込めるかどうか？」という言葉が印象的だった
<ul>
<li>モダンなものに移行するとき現実的に重要</li>
</ul>
</li>
</ul>
<h2 id="spark-v30の紹介">Spark v3.0の紹介</h2>
<h3 id="前半">前半</h3>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/j4RUIhcche96WO" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/ishizaki/introduction-new-features-in-spark-30" title="Introduction new features in Spark 3.0" target="_blank">Introduction new features in Spark 3.0</a> </strong> from <strong><a href="//www.slideshare.net/ishizaki" target="_blank">Kazuaki Ishizaki</a></strong> </div>
<p>スピーカー: <a href="https://twitter.com/kiszk">@kiszk</a> さん</p>
<ul>
<li>SQL の性能に関わる7大機能の話
<ol>
<li>Query planの新しい表示方法</li>
<li>Join hintsの強化</li>
<li>Adaptive query execution</li>
<li>Dynamic partitioning pruning</li>
<li>nested column pruning &amp; pushdown の強化</li>
<li>Aggregation のコード生成の改良</li>
<li>ScalaとJavaの新バージョンのサポート</li>
</ol>
</li>
<li>実行計画が見やすくなるのは本当にうれしい</li>
<li>人がチューニングしていた部分が Adaptive Query Plan や Dynamic Partition Pruning で自動化されるのもうれしい</li>
</ul>
<h3 id="後半">後半</h3>
<p>スピーカー: <a href="https://twitter.com/raspberry1123">@raspberry1123</a> さん</p>
<ul>
<li>Accelarator Aware Scheduling
<ul>
<li>Project Hydrogen</li>
</ul>
</li>
<li>プラグイン機能
<ul>
<li>executor や driver の機能を user が拡張できる</li>
<li>executor については Spark 2.4 からあったそうだが知らなかった…</li>
</ul>
</li>
<li>Structured Straming web UI
<ul>
<li>ストリーム処理にはこういった chart が必要だと思う</li>
</ul>
</li>
</ul>
<h2 id="sparkにprを投げてみた">SparkにPRを投げてみた</h2>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/r8MSqiNNAd73Wk" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/ssuserca76a5/spark-237429607" title="Sparkにプルリク投げてみた" target="_blank">Sparkにプルリク投げてみた</a> </strong> from <strong><a href="//www.slideshare.net/ssuserca76a5" target="_blank">Noritaka Sekiyama</a></strong> </div>
<p>スピーカー: <a href="https://twitter.com/moomindani">@moomindani</a> さん</p>
<ul>
<li>スピーカーは AWS で Glue の開発をしている方</li>
<li>Glue は Spark に強く依存しているため、機能追加にモチベーションがあったとのこと</li>
<li>私も投げてみたい！</li>
</ul>
<h2 id="lt-td-spark-internals-airframeでsparkの機能を拡張するテクニック">LT: td-spark internals: AirframeでSparkの機能を拡張するテクニック</h2>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/jWHWpIeuyCY9mq" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/taroleo/tdspark-internals-extending-spark-with-airframe-spark-meetup-tokyo-3-2020" title="td-spark internals: Extending Spark with Airframe - Spark Meetup Tokyo #3 2020" target="_blank">td-spark internals: Extending Spark with Airframe - Spark Meetup Tokyo #3 2020</a> </strong> from <strong><a href="//www.slideshare.net/taroleo" target="_blank">Taro L. Saito</a></strong> </div>
<p>スピーカー: <a href="https://twitter.com/taroleo">@taroleo</a> さん</p>
<ul>
<li><a href="https://github.com/wvlet/airframe">Airframe</a> とは Scala の application 開発用ツール群のようなもの？</li>
<li>Spark の機能を拡張</li>
</ul>
<h2 id="lt-spark-31-feature-expectation">LT: Spark 3.1 Feature Expectation</h2>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/lwQplIU1K7gMBl" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/maropu0804/lt-spark-31-feature-expectation" title="LT: Spark 3.1 Feature Expectation" target="_blank">LT: Spark 3.1 Feature Expectation</a> </strong> from <strong><a href="//www.slideshare.net/maropu0804" target="_blank">Takeshi Yamamuro</a></strong> </div>
<p>スピーカー: <a href="https://twitter.com/maropu">@maropu</a> さん</p>
<ul>
<li>新機能
<ul>
<li>Support Filter Pushdown JSON
<ul>
<li>JSON の読み取りが早くなる</li>
</ul>
</li>
<li>Better Handling for Node Shutdown
<ul>
<li>node 離脱時にその node が保持する shuffle や cache の情報が失われていた</li>
<li>計画された node の離脱に対して shuffle や cache ブロックを別 node に委譲</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="まとめ">まとめ</h1>
<p>どんどん便利になっていくなという印象。
手でパフォーマンスチューニングする要素はどんどん減っていくのだろう。</p>
<p>Delta Lake が盛り上がっているように感じた。
データレイクについて勉強しないと…</p>]]></content>
  </entry>
  <entry>
    <title>Spark DataFrame クエリの弱い分離レベル</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/weak_isolation_level_of_dataframe/</id>
    <updated>2020-07-19T13:00:00Z</updated>
    <published>2020-07-19T13:00:00Z</published>
    <content type="html"><![CDATA[<p>Spark バッチ処理の問題を調べていたら分離レベルという概念にたどりついた。
分離レベルについて調べたので、Spark の問題の内容と絡めて記しておく。
考えてみれば当たり前でたいした話ではない。</p>
<h1 id="分離レベルとは">分離レベルとは</h1>
<h2 id="トランザクションの挙動についての暗黙の理解">トランザクションの挙動についての暗黙の理解</h2>
<p>アドホックな分析クエリやプロダクションコード中のクエリを書くとき、その単一のクエリのトランザクションにおいて「同時に実行されている別のクエリの commit 前の状態や commit 結果に影響され、このクエリの結果がおかしくなるかもしれない」ということは通常考えない。
トランザクションはデータベースのある時点の状態に対して正しく処理される、というほぼ無意識の理解をおそらくほとんどの開発者が持っている。</p>
<p>多くの場合この理解は間違っていない。
それはなぜかというと DB 等のデータ処理フレームワークがある強さの分離レベルを提供しているからである。</p>
<h2 id="いろいろな分離レベル">いろいろな分離レベル</h2>
<p>ACID 特性のうちの1つ、分離性 (Isolation) の程度を表すのが分離レベル。</p>

<blockquote>
  <p>トランザクション中に行われる操作の過程が他の操作から隠蔽されることを指し、日本語では分離性、独立性または隔離性ともいう。より形式的には、独立性とはトランザクション履歴が直列化されていることと言える。この性質と性能はトレードオフの関係にあるため、一般的にはこの性質の一部を緩和して実装される場合が多い。</p>
  <footer>&#8212; Wikipedia <cite title="ACID (コンピュータ科学)"><a rel="noopener nofollow" href="https://ja.wikipedia.org/wiki/ACID_%28%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF%E7%A7%91%E5%AD%A6%29">ACID (コンピュータ科学)</a></cite></footer>
</blockquote>

<p>分離レベルには名前のついたものがいくつかあり、分離性の保証の強さが異なる。
具体的にはトランザクションの並行性の問題への対応力が異なる。
名著「<a href="https://www.oreilly.co.jp/books/9784873118703/">データ指向アプリケーションデザイン</a>」の第7章で分離レベルについて詳しく述べられているので、以下ではそちらからの引用。
分離レベルを弱い順に並べる。</p>
<ul>
<li>
<p>read uncommitted</p>
<blockquote>
<p>このレベルではダーティライトは生じませんが、ダーティリードは妨げられません。</p>
</blockquote>
</li>
<li>
<p>read committed</p>
<blockquote>
<ol>
<li>データベースからの読み取りを行った際に見えるデータは、コミットされたもののみであること（ダーティリードは生じない）。</li>
<li>データベースへの書き込みを行う場合、上書きするのはコミットされたデータのみであること（ダーティライトは生じない）。</li>
</ol>
</blockquote>
</li>
<li>
<p>snapshot isolation</p>
<blockquote>
<p>スナップショット分離の考え方は、それぞれのトランザクションがデータベースの一貫性のあるスナップショットから読み取りを行うというものです。すなわち、トランザクションが読み取るデータは、すべてそのトランザクションの開始時点のデータベースにコミット済みのものだけということです。</p>
</blockquote>
</li>
<li>
<p>serializability</p>
<blockquote>
<p>この分離レベルはトランザクションが並行して実行されていても、最終的な答えはそれぞれが1つずつ順番に、並行ではなく実行された場合と同じになることを保証します。</p>
</blockquote>
</li>
</ul>
<p>日本語で「分離レベル」を検索すると snapshot isolation の代わりに repeatable read が出てくる事が多い。
しかし repeatable read の名前は実装によって意味が違っていたりして扱いが難しいらしい。</p>
<h2 id="分離レベルと-race-condition-の関係">分離レベルと race condition の関係</h2>
<p>以下に各分離レベルとトランザクションの並行性の問題 (race condition) の関係を示す。
各 race condition の説明については割愛するが、複数のトランザクションが並行して実行されることにより起こりうる期待されていない挙動だと思えばよい。
○はその分離レベルにおいてその race condition が発生しないことを示す。
△は条件によっては発生する。</p>
<table>
<thead>
<tr>
<th></th>
<th>dirty read</th>
<th>dirty write</th>
<th>read skew (nonrepeatable read)</th>
<th>lost update</th>
<th>write skew</th>
<th>phantom read</th>
</tr>
</thead>
<tbody>
<tr>
<td>read uncommitted</td>
<td>○</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>read committed</td>
<td>○</td>
<td>○</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>snapshot isolation</td>
<td>○</td>
<td>○</td>
<td>○</td>
<td>△</td>
<td>-</td>
<td>△</td>
</tr>
<tr>
<td>serializability</td>
<td>○</td>
<td>○</td>
<td>○</td>
<td>○</td>
<td>○</td>
<td>○</td>
</tr>
</tbody>
</table>
<p>下に行くほど強い分離レベルとなっている。
分離レベルが強くなるほど race condition が発生しにくくなるが、一方で lock 等によりパフォーマンスが下がっていくというトレードオフがある。</p>
<h1 id="各種データベースの分離レベル">各種データベースの分離レベル</h1>
<p>ここでは MySQL と Hive においてどの分離レベルが提供されているかを見てみる。</p>
<h2 id="mysql-の場合">MySQL の場合</h2>
<p>MySQL の分離レベルについては以下のドキュメントで述べられている。</p>
<ul>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html">15.7.2.1 Transaction Isolation Levels</a></li>
</ul>
<p>MySQL (というか InnoDB?) では次の4つの分離レベルを設定することができる。</p>
<ul>
<li><code>READ UNCOMMITTED</code></li>
<li><code>READ COMMITTED</code></li>
<li><code>REPEATABLE READ</code> (default)</li>
<li><code>SERIALIZABLE</code></li>
</ul>
<p>デフォルトの分離レベルは <code>REPEATABLE READ</code> だが、これは前述の snapshot isolation に相当するらしい。
分離レベルは、例えば <code>set transaction</code> 構文により次のようにして指定できる。</p>
<div class="highlight"><pre class="chroma"><code class="language-mysql" data-lang="mysql"><span class="kt">set</span> <span class="n">transaction</span> <span class="n">isolation</span> <span class="n">level</span> <span class="n">SERIALIZABLE</span><span class="p">;</span>
</code></pre></div><p>この場合は現在のセッション内で実行される次のトランザクションについて適用される。
すべてのセッションやすべてのトランザクション等の指定もできる。
詳しくは以下。</p>
<ul>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/set-transaction.html">13.3.7 SET TRANSACTION Statement</a></li>
</ul>
<h2 id="hive-の場合">Hive の場合</h2>
<p>Hive についてはドキュメントに次のような記載がある。</p>

<blockquote>
  <p>At this time only snapshot level isolation is supported.  When a given query starts it will be provided with a consistent snapshot of the data.</p>
  <footer>&#8212; Apache Hive <cite title="Hive Transactions"><a rel="noopener nofollow" href="https://cwiki.apache.org/confluence/display/Hive/Hive&#43;Transactions#HiveTransactions-Limitations">Hive Transactions</a></cite></footer>
</blockquote>

<p>Hive は snapshot isolation のみ提供しているとのこと。</p>

<blockquote>
  <p>The default DummyTxnManager emulates behavior of old Hive versions: has no transactions and uses hive.lock.manager property to create lock manager for tables, partitions and databases.</p>
  <footer>&#8212; Apache Hive <cite title="Hive Transactions"><a rel="noopener nofollow" href="https://cwiki.apache.org/confluence/display/Hive/Hive&#43;Transactions#HiveTransactions-Transaction/LockManager">Hive Transactions</a></cite></footer>
</blockquote>

<p>lock は小さくとも partition の単位になるのだろうか。
であるとすると予想通りだが MySQL よりもいかつい挙動になっている。</p>
<p>このように多くの DB では snapshot isolation の分離レベルが基本となっている。</p>
<h1 id="spark-クエリの分離レベル">Spark クエリの分離レベル</h1>
<p>では Spark のクエリはどうだろうか。
ここからようやく本題となる。</p>
<h2 id="read-committed-相当">read committed 相当</h2>
<p>Spark において <code>DataFrame</code> を用いたデータ処理を記述するとき、それは1つの SQL クエリを書くのと近い感覚になる。
そもそも <code>DataFrame</code> は SQL-like な使い心地を目的として作られた API だから当然だ。</p>
<p><code>DataFrame</code> で記述された処理は実行時に <code>RDD</code> として翻訳されるが、分離レベルを考えるにあたって <code>RDD</code> の特性がキーとなってくる。</p>

<blockquote>
  <p>By default, each transformed RDD may be recomputed each time you run an action on it.</p>
  <footer>&#8212; Apache Spark <cite title="RDD Operations"><a rel="noopener nofollow" href="https://spark.apache.org/docs/3.0.0/rdd-programming-guide.html#rdd-operations">RDD Operations</a></cite></footer>
</blockquote>

<p>あまり良い説明を見つけられなかったが、1回の action を伴う処理においても同じ <code>RDD</code> が複数回参照されるとき、その <code>RDD</code> までの計算は通常やり直されることになる。</p>
<p>したがって例えば self join のようなことをするとき、同じデータソースを2回読みに行くということが起こってしまう。
HDFS や S3 上のファイル、JDBC 経由の外部 DB など Spark は様々なデータソースを扱うことができるが、通常 Spark がその2回の読み込みに対して lock をかけたりすることはできない。</p>
<p>つまり non-repeatable read や phantom read を防ぐことができない。
read committed という弱い分離レベルに相当するということになってしまう。</p>
<p>分離レベルという言葉はトランザクションという概念に対して使われるものであり、<code>DataFrame</code> のクエリをトランザクションと呼んでいいのかはわからない。
なので分離レベルという言葉をここで使うのが適切でないかもしれないということは述べておく。</p>
<h2 id="検証">検証</h2>
<p>MySQL からデータを読み取り Spark で処理することを考える。
まず local の MySQL で次のような table を用意する。</p>
<pre><code>mysql&gt; describe employees;
+---------------+---------+------+-----+---------+-------+
| Field         | Type    | Null | Key | Default | Extra |
+---------------+---------+------+-----+---------+-------+
| id            | int(11) | NO   | PRI | NULL    |       |
| salary        | int(11) | YES  |     | NULL    |       |
| department_id | int(11) | YES  |     | NULL    |       |
+---------------+---------+------+-----+---------+-------+
3 rows in set (0.03 sec)
</code></pre><p>部署 (department) ごとの給料 (salary) の平均から各従業員の給料がどれくらい離れているかの差分を見たいものとする。
Spark のコードは次のようになる。
Spark のバージョンはこれを書いている時点での最新 3.0.0 とした。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">package</span> <span class="nn">com.example</span>

<span class="k">import</span> <span class="nn">org.apache.spark.sql.functions.avg</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span>

<span class="k">object</span> <span class="nc">IsolationLevelExperiment</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="c1">// Prepare SparkSession
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">spark</span> <span class="k">=</span> <span class="nc">SparkSession</span>
      <span class="o">.</span><span class="n">builder</span><span class="o">()</span>
      <span class="o">.</span><span class="n">appName</span><span class="o">(</span><span class="s">&#34;Isolation Level Experiment&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">master</span><span class="o">(</span><span class="s">&#34;local[*]&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">getOrCreate</span><span class="o">()</span>

    <span class="k">import</span> <span class="nn">spark.implicits._</span>

    <span class="c1">// Read from MySQL
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">dfEmployee</span> <span class="k">=</span> <span class="n">spark</span>
      <span class="o">.</span><span class="n">read</span>
      <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&#34;jdbc&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&#34;url&#34;</span><span class="o">,</span> <span class="s">&#34;jdbc:mysql://localhost&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&#34;dbtable&#34;</span><span class="o">,</span> <span class="s">&#34;db_name.employees&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&#34;user&#34;</span><span class="o">,</span> <span class="s">&#34;user_name&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&#34;password&#34;</span><span class="o">,</span> <span class="s">&#34;********&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&#34;driver&#34;</span><span class="o">,</span> <span class="s">&#34;com.mysql.cj.jdbc.Driver&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">load</span>
      <span class="o">.</span><span class="n">cache</span>

    <span class="c1">// Get average salary
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">dfAvg</span> <span class="k">=</span> <span class="n">dfEmployee</span>
      <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;department_id&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">agg</span><span class="o">(</span><span class="n">avg</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;salary&#34;</span><span class="o">).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;avg_salary&#34;</span><span class="o">))</span>

    <span class="c1">// Calculate diff
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">dfResult</span> <span class="k">=</span> <span class="n">dfEmployee</span>
      <span class="o">.</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;e&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">join</span><span class="o">(</span>
        <span class="n">dfAvg</span><span class="o">.</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">),</span>
        <span class="n">$</span><span class="s">&#34;e.department_id&#34;</span> <span class="o">===</span> <span class="n">$</span><span class="s">&#34;a.department_id&#34;</span><span class="o">,</span>
        <span class="s">&#34;left_outer&#34;</span>
      <span class="o">)</span>
      <span class="o">.</span><span class="n">select</span><span class="o">(</span>
        <span class="n">$</span><span class="s">&#34;e.id&#34;</span><span class="o">,</span>
        <span class="n">$</span><span class="s">&#34;e.department_id&#34;</span><span class="o">,</span>
        <span class="o">(</span><span class="n">$</span><span class="s">&#34;e.salary&#34;</span> <span class="o">-</span> <span class="n">$</span><span class="s">&#34;a.avg_salary&#34;</span><span class="o">).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;salary_diff&#34;</span><span class="o">)</span>
      <span class="o">)</span>

    <span class="c1">// Output results
</span><span class="c1"></span>    <span class="n">dfResult</span><span class="o">.</span><span class="n">show</span>

    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="o">()</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>このコードを実行する前に MySQL の <a href="https://dev.mysql.com/doc/refman/8.0/en/query-log.html">general query log</a> を ON にする。</p>
<pre><code>mysql&gt; set global general_log = 'ON';
Query OK, 0 rows affected (0.02 sec)

mysql&gt; show global variables like 'general_log%';
+------------------+--------------------------------------+
| Variable_name    | Value                                |
+------------------+--------------------------------------+
| general_log      | ON                                   |
| general_log_file | /usr/local/var/mysql/MacBook-Pro.log |
+------------------+--------------------------------------+
2 rows in set (0.01 sec)
</code></pre><p>これによって MySQL に対して発行されたクエリがログとして記録されるようになる。</p>
<p>直感的に snapshot isolation になっているのであれば MySQL に対する select 文は1回だけ発行されるはずである。
しかし前述のとおり <code>RDD</code> や <code>DataFrame</code> の処理は途中の状態を通常保存せず、同じ <code>RDD</code> や <code>DataFrame</code> を参照していたとしても再計算される。
上記コードの例だと <code>dfEmployee</code> が2回参照されている。</p>
<p>コードを実行すると general query log には次のように、データ取得のための select 文が2つ記録されていた。
それぞれ <code>join()</code> の左右の table のデータソースを示している。</p>
<pre><code>8 Query     SELECT `id`,`salary`,`department_id` FROM test_fout_dsp.employees
7 Query     SELECT `salary`,`department_id` FROM test_fout_dsp.employees WHERE (`department_id` IS NOT NULL)
</code></pre><p>2つの select 文はそれぞれ別のクエリ、トランザクションとして発行されている。
したがって前者の select 文が実行された後、後者の select 文が実行される前に別のトランザクションにより <code>employees</code> が更新されたり挿入・削除されたりすると non-repeatable read や phantom read が発生してしまうのである。</p>
<p>今回はデータソースへのアクセスを確認するためにデータソースとして MySQL を使ったが、同じことはファイルや他の DB など別のデータソースで起こりうる。</p>
<h2 id="回避策">回避策</h2>
<p>プロダクト運用上、non-repeatable read や phantom read が発生しうる状況というのは多くの場合で厄介である。
一時的なデータソースの状態に依存して問題が発生するため、バグの原因追求がとても困難だからだ。
見た目上の分離レベルを強くし、これらを避けるには2つの方法が考えられる。</p>
<h3 id="immutable-なデータにのみアクセスする">immutable なデータにのみアクセスする</h3>
<p>単純な話で non-repeatable read や phantom read が発生するようなデータソースを参照しなければよい。
例えばユーザの行動ログのような蓄積されていくデータが hour 単位で partitioning されているような場合、基本的に過去の partition に対しては変更や挿入・削除は行われない。
このような partition にアクセスする分には前述のような厄介な問題は起こらない。</p>
<h3 id="cache-する">cache する</h3>
<p>データソースから読み取った結果の <code>DataFrame</code> に対して <code>cache()</code> または <code>persist()</code> をするとよい。</p>

<blockquote>
  <p>Spark SQL can cache tables using an in-memory columnar format by calling spark.catalog.cacheTable(&#34;tableName&#34;) or dataFrame.cache().</p>
  <footer>&#8212; Apache Spark <cite title="Caching Data In Memory"><a rel="noopener nofollow" href="https://spark.apache.org/docs/latest/sql-performance-tuning.html#caching-data-in-memory">Caching Data In Memory</a></cite></footer>
</blockquote>

<p>前述のコードで <code>dfEmployee</code> に対して <code>.cache()</code> をした場合、MySQL へのデータ取得のための select 文の発行は1回になった。
大きなデータソースを <code>cache()</code> するときだけメモリや HDD 容量に気をつけておきたい。</p>
<h1 id="まとめ">まとめ</h1>
<p><code>DataFrame</code> はいわゆる RDBMS を操作しているように見えてしまうところが難しいところだろうか。
「データ指向アプリケーションデザイン」に至極真っ当なことが書いてあったので、その言葉を借りて締めておく。</p>

<blockquote>
  <p>何も考えずにツールを信じて依存するのではなく、並行性の問題にはどういったものがあるのか、そしてそれらを回避するにはどうしたら良いのかを、しっかり理解しなければなりません。そうすれば、信頼性があり、正しく動作するアプリケーションを手の届くツールを使って構築できるようになります。</p>
  <footer>&#8212; Martin Kleppmann <cite title="データ指向アプリケーションデザイン"><a rel="noopener nofollow" href="https://www.oreilly.co.jp/books/9784873118703/">データ指向アプリケーションデザイン</a></cite></footer>
</blockquote>]]></content>
  </entry>
  <entry>
    <title>Apache Spark 3.0.0 について調べた</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/study-spark-3-0-0/</id>
    <updated>2020-07-12T00:00:00Z</updated>
    <published>2020-07-12T00:00:00Z</published>
    <content type="html"><![CDATA[<h2 id="はじめに">はじめに</h2>
<p>Apache Spark 3.0.0 がリリースされました。</p>
<ul>
<li><a href="https://spark.apache.org/releases/spark-release-3-0-0.html">Spark Release 3.0.0</a></li>
</ul>
<p>release note を見て個人的に気になったところなど簡単に調べました。
書いてみると Databricks の記事へのリンクばっかになってしまった…</p>
<h2 id="全体感">全体感</h2>
<p>こちらの記事を読めば全体感は OK.</p>
<ul>
<li><a href="https://databricks.com/jp/blog/2020/06/18/introducing-apache-spark-3-0-now-available-in-databricks-runtime-7-0.html">Introducing Apache Spark 3.0</a></li>
</ul>
<p>公式の release note には</p>
<blockquote>
<p>Python is now the most widely used language on Spark.</p>
</blockquote>
<p>とあってそうなん？ってなったけど、こちらの記事だと</p>
<blockquote>
<p>Python is now the most widely used language on Spark and, consequently, was a key focus area of Spark 3.0 development. 68% of notebook commands on Databricks are in Python.</p>
</blockquote>
<p>と書いてありどうやら Databricks の notebook の話らしく、だったらまあそうかなという感じ。
プロダクトコードへの実装というよりは、アドホック分析や検証用途の話なんでしょう。</p>
<h2 id="project-hydrogen-accelerator-aware-scheduler">[Project Hydrogen] Accelerator-aware Scheduler</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-24615">SPARK-24615</a></li>
</ul>
<p>Spark 上で deep learning できるようにすることを目指す Project Hydrogen、その3つの大きな目標のうちの一つ。</p>
<ul>
<li><a href="https://www.slideshare.net/MatthewStubbs6/big-data-ldn-2018-project-hydrogen-unifying-ai-with-apache-spark/26">Big Data LDN 2018: PROJECT HYDROGEN: UNIFYING AI WITH APACHE SPARK</a></li>
</ul>
<p>YARN や Kubernetes では GPU や FPGA を扱えるようになっているので Spark でも扱えるようにしたいというモチベーション。
<a href="https://spark.apache.org/docs/3.0.0/running-on-yarn.html#resource-allocation-and-configuration-overview">Spark のドキュメント</a> によると</p>
<blockquote>
<p>For example, the user wants to request 2 GPUs for each executor. The user can just specify <code>spark.executor.resource.gpu.amount=2</code> and Spark will handle requesting <code>yarn.io/gpu</code> resource type from YARN.</p>
</blockquote>
<p>のようにして executor に GPU リソースを要求できるみたいです。</p>
<h2 id="adaptive-query-execution">Adaptive Query Execution</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-31412">SPARK-31412</a></li>
</ul>
<p>平たく言うと実行時に得られる統計情報を使って plan を最適化すると、静的に生成された plan より効率化できるよねという話。
<code>spark.sql.adaptive.enabled=true</code> にすることで有効になります。</p>
<p>処理の途中で中間生成物が materialize されるタイミングで、その時点の統計情報を使って残りの処理を最適化する、というのを繰り返します。</p>
<p>Spark 3.0.0 では以下3つの AQE が実装されました。</p>
<ul>
<li>Coalescing Post Shuffle Partitions</li>
<li>Converting sort-merge join to broadcast join</li>
<li>Optimizing Skew Join</li>
</ul>
<p>Spark 2 以前だとこのあたりは実行しつつチューニングするような運用になりがちでした。
特に skew の解消は salt を追加したりなど面倒だったりします。
これらが自動で最適化されるというのは運用上うれしいところ。
急なデータ傾向の変化に対しても自動で最適化して対応できるという面があります。</p>
<p>AQE に関してもやはり Databricks の解説記事がわかりやすいです。
図もいい感じ。</p>
<ul>
<li><a href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html">Adaptive Query Execution: Speeding Up Spark SQL at Runtime</a></li>
</ul>
<h2 id="dynamic-partition-pruning">Dynamic Partition Pruning</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-11150">SPARK-11150</a></li>
</ul>
<p>こちらも AQE 同様にクエリのパフォーマンスを改善する目的で導入されたもの。
改善幅は AQE より大きいようです。
やはり実行時の情報を使って partition pruning を行い、不要な partition の参照を減らすという方法。</p>
<p>主に <a href="https://ja.wikipedia.org/wiki/%E3%82%B9%E3%82%BF%E3%83%BC%E3%82%B9%E3%82%AD%E3%83%BC%E3%83%9E">star schema</a> における join 時のように、静的には partition pruning が行えない場合を想定しています。
比較的小さいことが多いと思われる dimension table 側を broadcast する broadcast join において、broadcast された情報を fact table の partition pruning に利用するというやり口。</p>
<ul>
<li><a href="https://www.slideshare.net/databricks/dynamic-partition-pruning-in-apache-spark">Dynamic Partition Pruning in Apache Spark</a></li>
</ul>
<h2 id="structured-streaming-ui">Structured Streaming UI</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-29543">SPARK-29543</a></li>
</ul>
<p>&quot;Structured Streaming&quot; というタブが UI に追加された件。
Spark のドキュメントに例があります。</p>
<blockquote>
<p><img src="https://spark.apache.org/docs/3.0.0/img/webui-structured-streaming-detail.png" alt="Structured Streaming Query Statistics"></p>
</blockquote>

<blockquote>
  <p> </p>
  <footer>&#8212; Apache Spark <cite title="Structured Streaming Tab"><a rel="noopener nofollow" href="https://spark.apache.org/docs/3.0.0/web-ui.html#structured-streaming-tab">Structured Streaming Tab</a></cite></footer>
</blockquote>

<p>Spark 2.4 系では Structured Streaming を動かしていてもせいぜい job や stage が増えていくという味気ないものしか見えませんでした。
Spark 3.0.0 で実際に動かしてみたけど欲しかったやつ！という感じ。
ストリーム処理では入力データ量の変化の可視化がマストだと思ってます。</p>
<h2 id="catalog-plugin-api">Catalog plugin API</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-31121">SPARK-31121</a></li>
<li><a href="https://docs.google.com/document/d/1zLFiA1VuaWeVxeTDXNg8bL6GP3BVoOZBkewFtEnjEoo/edit">SPIP: Spark API for Table Metadata</a></li>
</ul>
<p>これまでは CTAS (Create Table As Select) の操作はあったが、外部のデータソースに対して DDL 的な操作をする API が足りていませんでした。
CTAS も挙動に実装依存の曖昧さがありました。
そこで <code>create</code>, <code>alter</code>, <code>load</code>, <code>drop</code> 等のテーブル操作をできるようにしたという話。</p>
<p>ドキュメントの <a href="https://spark.apache.org/docs/3.0.0/sql-ref-syntax.html#ddl-statements">DDL Statements</a> のあたりを読め何ができるかわかります。
以前のバージョンでも一部のデータソースについてはできた模様 (ex. <a href="https://spark.apache.org/docs/2.4.6/sql-migration-guide-hive-compatibility.html#supported-hive-features">Hive</a>)。</p>
<p>今の自分の業務では Spark から DDL を扱うようなことはしないのでそれほど恩恵は感じられません。
notebook からアドホックな Spark のバッチを動かすというような使い方をしていればうれしいかもしれません。</p>
<h2 id="add-an-api-that-allows-a-user-to-define-and-observe-arbitrary-metrics-on-batch-and-streaming-queries">Add an API that allows a user to define and observe arbitrary metrics on batch and streaming queries</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-29345">SPARK-29345</a></li>
</ul>
<p>クエリの途中の段階で何らかの metrics を仕込んでおいて callback 的にその metrics にアクセスできる仕組み。
<a href="https://spark.apache.org/docs/3.0.0/api/scala/org/apache/spark/sql/Dataset.html#observe(name:String,expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset%5BT%5D">Dataset#observe() の API ドキュメント</a> を読むのが一番早いです。
この例ではストリーム処理を扱っているが、バッチ処理の例を自分で書いて試してみました。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Register listener
</span><span class="c1"></span><span class="n">spark</span>
  <span class="o">.</span><span class="n">listenerManager</span>
  <span class="o">.</span><span class="n">register</span><span class="o">(</span><span class="k">new</span> <span class="nc">QueryExecutionListener</span> <span class="o">{</span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">onSuccess</span><span class="o">(</span><span class="n">funcName</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">qe</span><span class="k">:</span> <span class="kt">QueryExecution</span><span class="o">,</span> <span class="n">durationNs</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">num</span> <span class="k">=</span> <span class="n">qe</span><span class="o">.</span><span class="n">observedMetrics</span>
        <span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="s">&#34;my_metrics&#34;</span><span class="o">)</span>
        <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getAs</span><span class="o">[</span><span class="kt">Long</span><span class="o">](</span><span class="s">&#34;num&#34;</span><span class="o">))</span>
        <span class="o">.</span><span class="n">getOrElse</span><span class="o">(-</span><span class="mf">100.0</span><span class="o">)</span>

      <span class="n">println</span><span class="o">(</span><span class="s">s&#34;num of data: </span><span class="si">$num</span><span class="s">&#34;</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">onFailure</span><span class="o">(</span><span class="n">funcName</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">qe</span><span class="k">:</span> <span class="kt">QueryExecution</span><span class="o">,</span> <span class="n">exception</span><span class="k">:</span> <span class="kt">Exception</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{}</span>
  <span class="o">})</span>

<span class="c1">// Make DataFrame
</span><span class="c1"></span><span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="nc">Seq</span>
  <span class="o">.</span><span class="n">range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1000</span><span class="o">)</span>
  <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="k">_</span><span class="o">,</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="s">&#34;c&#34;</span><span class="o">)(</span><span class="nc">Random</span><span class="o">.</span><span class="n">nextInt</span><span class="o">(</span><span class="mi">3</span><span class="o">)),</span> <span class="n">math</span><span class="o">.</span><span class="n">random</span><span class="o">()))</span>
  <span class="o">.</span><span class="n">toDF</span><span class="o">(</span><span class="s">&#34;id&#34;</span><span class="o">,</span> <span class="s">&#34;type&#34;</span><span class="o">,</span> <span class="s">&#34;value&#34;</span><span class="o">)</span>

<span class="c1">// Observe and process
</span><span class="c1"></span><span class="k">val</span> <span class="n">dfResult</span> <span class="k">=</span> <span class="n">df</span>
  <span class="o">.</span><span class="n">observe</span><span class="o">(</span><span class="s">&#34;my_metrics&#34;</span><span class="o">,</span> <span class="n">count</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;*&#34;</span><span class="o">).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;num&#34;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;type&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">agg</span><span class="o">(</span><span class="n">avg</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;value&#34;</span><span class="o">).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;avg_value&#34;</span><span class="o">))</span>

<span class="c1">// Run
</span><span class="c1"></span><span class="n">dfResult</span><span class="o">.</span><span class="n">show</span>
</code></pre></div><p>これを動かしたときの出力は次のようになりました。</p>
<pre><code>+----+------------------+
|type|         avg_value|
+----+------------------+
|   c|0.5129435063033314|
|   b|0.4693004460694317|
|   a|0.4912087482418599|
+----+------------------+

num of data: 1000
</code></pre><p><code>observe()</code> はその出力の DataFrame に対して schema やデータの中身を変更することはありません。
metrics を仕込むのみ。
logical plan を出力してみると <code>observe()</code> を入れることにより途中に <code>CollectMetrics</code> という plan が挿入されていました。
ソースを見ると accumulator を使っている模様。
なので <code>observe()</code> の集計のみで一度 job が動くわけではなく、クエリが2回走るという感じではありません。
全体の処理の中でひっそりと accumulator で脇で集計しておくといった趣でしょうか。</p>
<p>これは結構有用だと思います。
例えば何らかの集計をやるにしてもその最初や途中で、例えば入力データは何件あったか？みたいなことをログに出しておきたいことがあります。
というか accumulator で頑張ってそういうものを作ったことがある…
これがフレームワーク側でサポートされるのはうれしいです。</p>
<h2 id="まとめ">まとめ</h2>
<p>2つのダイナミックな最適化に期待大。
気が向いたら追加でまた調べるかもしれません。</p>]]></content>
  </entry>
</feed>
