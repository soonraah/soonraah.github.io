<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="jp">
  <title>Froglog</title>
  <subtitle></subtitle>
  <id>https://soonraah.github.io/</id>
  <author>
    <name>Froglog</name>
    <uri>https://soonraah.github.io/</uri>
  </author>
  <updated>2020-09-06T07:49:29Z</updated>
  <link rel="self" type="application/atom+xml" href="https://soonraah.github.io/feed.atom" hreflang="jp"/>
  <link rel="alternate" type="text/html" href="https://soonraah.github.io/" hreflang="jp"/>
  <entry>
    <title>バッチ処理おじさんがストリーム処理のシステムを開発するにあたって調べたこと</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/study-streaming-system/</id>
    <updated>2020-09-06T07:30:00Z</updated>
    <published>2020-09-06T07:30:00Z</published>
    <content type="html"><![CDATA[

<figure class="center">
<img
class="mb-2 mx-auto leading-none shadow-xl"
src="/image/photo/jon-flobrant-rB7-LCa_diU-unsplash.jpg"
alt="Photo by Jon Flobrant on">
<figcaption class="text-sm text-right text-raven-500">
<p>Photo by Jon Flobrant on <a href="https://unsplash.com/photos/rB7-LCa_diU">Unsplash</a></p>
</figcaption>
</figure>

<p>ほとんどバッチ処理しか書いたことのない者だがストリーム処理のシステムを開発することになった。<br>
それにあたって独学で調べたことなどまとめておく。</p>
<h2 id="ストリーム処理とは">ストリーム処理とは</h2>
<p>そもそも &quot;ストリーム処理&quot; とは何を指しているのか。<br>
以下の引用が簡潔に示している。</p>

<blockquote>
  <p>a type of data processing engine that is designed with infinite data sets in mind. Nothing more.</p>
  <footer>&#8212; Tyler Akidau <cite title="Streaming 101: The world beyond batch"><a rel="noopener nofollow" href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/">Streaming 101: The world beyond batch</a></cite></footer>
</blockquote>

<p>こちらは &quot;streaming system&quot; について述べたものだが、つまり終わりのないデータを扱うのがストリーム処理ということである。</p>
<p>例えば web サービスから生まれ続けるユーザ行動ログを逐次的に処理するというのがストリーム処理。<br>
web サービスが終了しないかぎりはユーザ行動ログの生成には終わりがない。</p>
<p>これに対して &quot;1日分のユーザ行動ログ&quot; 等のように有限の量のデータを切り出して処理する場合、これはバッチ処理となる。<br>
ストリーム処理とバッチ処理の違いは扱うデータが無限なのか有限なのかということだ。<br>
この後触れていくが、この終わりのないデータを継続的に処理し続けるというところにバッチ処理にはない難しさがある。</p>
<h2 id="なぜストリーム処理なのか">なぜストリーム処理なのか</h2>
<p>なぜストリーム処理なのか。<br>
ひとえに逐次的な入力データに対する迅速なフィードバックが求められているからと言えるだろう。<br>
迅速なフィードバックがビジネス上のメリットとなることは自明だ。</p>
<ul>
<li>SNS の配信</li>
<li>カーシェアリングにおける配車や料金設定</li>
<li>クレジットカードや広告クリックなどの不正検知</li>
</ul>
<p>もしこれらの application が例えば hourly のバッチ処理で実装されていたらどうだろうか。<br>
まあ待っていられない。</p>
<h2 id="一般的なストリーム処理の構成">一般的なストリーム処理の構成</h2>
<p>モダンな…と言っていいのかわからないが、ストリーム処理を行うための一般的なシステムは次の3つの要素で構成される。</p>
<ul>
<li>producer</li>
<li>broker</li>
<li>consumer</li>
</ul>
<p>producer は最初にレコードを生成する、ストリームデータの発生源となるものである。<br>
例えばログを生成する web application であったり、何らかのセンサーを持つ IoT 機器であったりがこれに該当する。<br>
producer は絶え間なくログを生成し、それを broker へと送る。</p>
<p>broker は producer から送られたログを格納し、任意のタイミングで取り出せるようにするものである。<br>
誤解を恐れずに言うとメッセージキューに近いイメージだ。<br>
Apache Kafka クラスタや Amazon Kinesis Data Streams 等がこれに該当する。</p>
<p>consumer は broker からログを取り出し、それに対し何かしらの処理を行うものだ。<br>
time window 集計であったりログからの異常検知であったり、処理した結果として何かビジネス上意味があるものを得るのである。<br>
これを行うフレームワークとしては Spark Streaming や Apache Flink 等がメジャーなのだろうか。</p>
<p>producer と consumer の間に broker を挟むメリットとしては次のようなことが挙げられる。</p>
<ul>
<li>producer が M 個、consumer が N 個の場合に M * N の関係になるところを broker を挟めば M + N にできる
<ul>
<li>producer, consumer に多数のシステムがあったとしても各自は broker との接続だけを考えればよい</li>
</ul>
</li>
<li>任意のタイミングでデータを読み出せる</li>
<li>producer または consumer に問題が発生してもデータロスが起こりにくくできる
<ul>
<li>その分 broker には高い可用性が求められる
<ul>
<li>Kafka はクラスタで冗長構成</li>
<li>Kinesis Data Streams は複数 AZ でレプリケーション</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="時間の概念">時間の概念</h2>
<p>ストリーム処理では時間の概念がいくつかあり、集計などの処理をどの時間をベースにして実行するのか、意識する必要がある。</p>
<ul>
<li>event time
<ul>
<li>producer 側でログイベントが発生した時間</li>
</ul>
</li>
<li>ingestion time
<ul>
<li>broker にそのログイベントのレコードが挿入された時間</li>
</ul>
</li>
<li>processing time
<ul>
<li>consumer 側でレコードを処理した時間</li>
</ul>
</li>
</ul>
<p>processing time を使うのが一番簡単なのだが、おそらく分析系の処理であれば window 集計等では event time を使うことが多いのではないだろうか。</p>
<p>ingestion time はおそらく実際のプロダクトではあまり使われないのではと思われる。<br>
(ネットワークのパフォーマンスを見るぐらい？)</p>
<h2 id="windowing">Windowing</h2>
<p>ストリーム処理の中で <code>sum</code>, <code>count</code> などを伴う集計処理を行う場合、通常は時間方向の window で切って処理するということになるのではないだろうか。<br>
window で切らずに完全なデータセットがそろうまで待つことはできないし、データが来るたびに逐次的に全体の結果を更新するしていくというのも割に合わない。</p>
<p>window の切り方もいくつかある。</p>
<ul>
<li>tumbling window
<ul>
<li>固定長でオーバーラップしない</li>
</ul>
</li>
<li>sliding window
<ul>
<li>固定長でオーバーラップを含む</li>
</ul>
</li>
<li>session window
<ul>
<li>いわゆる web の session のように、ある種のイベントがある期間発生しないことにより window が区切られる</li>
</ul>
</li>
</ul>
<p>これらについては <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html">Flink のドキュメント</a>が図もあってわかりやすい。</p>
<p>個人的な感想だが、この time window の集計がない単なる <code>map</code> 的なストリーム処理であれば traditional なアーキテクチャでも難しくはない。<br>
しかし time window 集計が必要となった場合は Spark Streaming 等のモダンなフレームワークが威力を発揮してくる。</p>
<h2 id="watermark">Watermark</h2>
<p>時間で window を切るときは、前述のどの時間の定義を用いるかを考えなければいけない。<br>
processing time を用いる場合は簡単だが event time はやや難しい。<br>
consumer 側では event のレコードがどれくらい遅れてやってくるかわからないためだ。</p>
<p>ネットワークその他の影響により、event のレコードが producer -&gt; broker -&gt; consumer という経路で consumer に届くまでの時間というのは一定にはならない。<br>
また、古い event が新しい event より後に届くというように順番が前後することも起こりうる。<br>
ここで &quot;watermark&quot; という考え方が必要になってくる。</p>

<blockquote>
  <p>A watermark with a value of time X makes the statement: &#34;all input data with event times less than X have been observed.&#34;</p>
  <footer>&#8212; Tyler Akidau <cite title="Streaming 102: The world beyond batch"><a rel="noopener nofollow" href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-102/">Streaming 102: The world beyond batch</a></cite></footer>
</blockquote>

<p>ある processing time において「event time X より前のレコードはすべて到着したよ」というのが watermark である。<br>
別の言い方をすると watermark により event のレコードがどの程度遅延してもよいかが定義される。</p>
<p>event time X より前のレコードが真の意味ですべて到着した、というのは難しい。<br>
実際には heuristic にどの程度遅れていいかを決め、それより遅れた場合はある event time 期間における window 処理には含めないということになる。</p>
<p>watermark の決め方はフレームワーク次第だろうか。<br>
例えば <a href="https://spark.apache.org/docs/3.0.0/structured-streaming-programming-guide.html#handling-late-data-and-watermarking">Spark Structured Streaming</a> の例だと図もあって比較的わかりやすい。</p>
<h2 id="schema-evolution">Schema Evolution</h2>
<p>何らかの業務システムや web システム等をある程度運用したことがある人ならわかると思うが、データの schema というのはナマモノだ。<br>
一度決めたら終わりというわけではなくプロダクトやビジネスの変化に応じて変化していく。<br>
カラムが増えたり、削除されたり、名前や型が変わったり…<br>
このようにデータの構造が変化していくこと、またはそれを扱うことを &quot;schema evolution&quot; という。</p>
<p>バッチ処理において schema の変更に追従することを考えるのはそれほど難しくない。<br>
hourly のバッチ処理であったとしても、バッチ処理とバッチ処理の間の時間で application を更新すればいいだけだ。<br>
(が、実際に行うのは困難が伴うことも多い)</p>
<p>ではストリーム処理ではどうだろうか。<br>
いわゆるストリーム処理においては処理と処理の間というものがなく、application がずっと稼働しっぱなしということになる。<br>
バッチ処理のような更新はできない。<br>
もっと言うと producer で生まれた新しい schema のレコードがいつ届くかもわからない。</p>
<p>おそらくこの問題には2つの対応方法がある。<br>
1つめは consumer 側のシステムで前方互換性を保つという方法である。<br>
この場合、新しいフィールドは必ず末尾に追加される等、producer 側での schema 更新についてある程度のルールが必要となるだろう。<br>
producer 側で生成されるレコードの schema の変更が必ず事前にわかるというのであれば後方互換性でもいいが、多くの場合は難しい。<br>
ところで<a href="https://ja.wikipedia.org/wiki/%E4%BA%92%E6%8F%9B%E6%80%A7">前方互換と後方互換</a>、どっちがどっちなのか覚えられません。</p>
<p>2つめの方法として schema 情報をレコード自体に入れ込んでしまうという方法もある。<br>
<a href="https://avro.apache.org/">Apach Avro</a> のような serialization の方法を取っているとレコード自体に schema の情報を付与することができる。</p>
<p>おそらく最もエレガントにこれをやるのが Confluent の <a href="https://docs.confluent.io/current/schema-registry/index.html">Schema Registry</a> という機能だ。<br>
producer から送出されるレコードには schema ID を付与する。<br>
schema の実体は Schema Registry という broker とは別の場所で管理されており、consumer 側では受け取ったレコードに付与されている schema ID と Schema Registry に登録されている shcema の実体を参照してレコードを deserialize することができる。</p>
<h2 id="deploy">Deploy</h2>
<p>ストリーム処理を行うシステムは終わりのないデータを処理するためのものであり、ずっと動き続けることが期待されている。<br>
しかし通常システムは一度立ち上げれば終わりということではなく、運用されている中で更新していく必要がある。</p>
<p>ずっと動かしながらどのように deploy, release するのか。<br>
この問題は主に consumer 側のシステムで配慮が必要になると思われる。<br>
正直これについてはちゃんと調べられていないが、2点ほど述べておきたい。</p>
<p>まず1点目、application を中断・更新・再開するのにどの程度の時間がかかるのかを知っておく必要があるということ。<br>
アーキテクチャやフレームワーク、処理の内容や checkpoint (後述) を使うか等によりこの時間は変わってくる。<br>
一例だが、AWS 環境において</p>
<ul>
<li>AWS Glue + Spark Structured Streaming</li>
<li>Amazon Kinesis Data Analytics + Flink</li>
</ul>
<p>の比較をしたことがある。<br>
前者は再開に数分かかったのに対し、後者は1分未満で再開できた。<br>
再開までの時間が十分に短いと判断できるのであればそのまま deploy, release してしまっていいだろう。</p>
<p>一方そうでない場合はどうすべきかという話が2点目。<br>
再開までの時間が長く、システム要件的に許容できないというのであれば、release 時は二重で動かすというような措置が必要かもしれない。<br>
おそらく <a href="https://www.publickey1.jp/blog/14/blue-green_deployment.html">Blue-Green Deployment</a> のようなことを考えることになるだろう。</p>
<h2 id="checkpoint">Checkpoint</h2>
<p>前述のとおり、ストリーム処理を行うシステムはずっと動き続けることが期待されている。<br>
しかし予定された application の更新や不測のエラー等、何らかの理由で一時的に中断されるということが実際の運用中には起こる。</p>
<p>中断されたとき速やかに復帰する仕組みとして &quot;checkpoint&quot; というものがいくつかの consumer 側のフレームワークで提供されている。<br>
雑に説明すると、処理のある時点における進捗や内部状態などをディスク等に永続化し、そこから処理を再開できるようにするものである。</p>
<ul>
<li><a href="https://spark.apache.org/docs/3.0.0/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing">Recovering from Failures with Checkpointing - Apache Spark</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/checkpointing.html#checkpointing">Checkpointing - Apache Flink</a></li>
</ul>
<p>上記は Spark Structured Streaming と Flink の例だ。<br>
checkpoint には次のようなメリットがあり、運用上有用だと言える。</p>
<ul>
<li>内部の状態を保持しているため、速やかに復帰できる</li>
<li>中断した位置から再開できるので出力に穴が開かない</li>
</ul>
<p>一方で落とし穴もある。<br>
checkpoint では内部の状態が永続化されるわけだが、内部の状態というのは当然 application の実装が決めているものである。<br>
application のコードを変更したとき、変更の内容によっては永続化された checkpoint と application が合わなくなることがあるのだ。<br>
未定義の挙動となることもあるので、checkpoint の運用には十分に配慮する必要がある。<br>
どのような変更なら checkpoint が安全に利用できるのかはフレームワークのドキュメントに記載があるので確認しておきたい。</p>
<h2 id="rdb-の世界との折り合い">RDB の世界との折り合い</h2>
<p>みんな大好きな RDB の世界では table を操作してデータの処理を行う。<br>
基本的には table というものはある時点における完全なデータセットを表すものである。 (ex. isolation)<br>
他方、ストリーム処理はやってきたデータを逐次的に処理するものである (mini-batch の場合もあるが)。</p>
<p>直感的にこの2つは相性が悪そうに見える。<br>
しかし Spark や Flink では table ベースの操作でストリーム処理を行うための API が提供されている。<br>
おそらく</p>
<ul>
<li>ストリーム処理の周辺のデータソースとして RDB が存在する</li>
<li>RDB 的な table 操作があまりにも浸透している</li>
</ul>
<p>というところが API が必要である理由なのだろう。</p>
<p>ストリームデータを table 的に扱うというのが、やや直感的な理解をしにくいものとなっている。<br>
フレームワークのドキュメントを確認しておきたい。</p>
<p>例えば Spark Structured Streaming であれば処理の出力のための3つの <a href="https://spark.apache.org/docs/3.0.0/structured-streaming-programming-guide.html#output-modes">output mode</a> が示されている。</p>
<ul>
<li>Append mode: 追加された行だけ出力</li>
<li>Complete mode: table 全体を出力</li>
<li>Update mode: 更新された行だけ出力</li>
</ul>
<p>どれを選ぶかにより必要とする内部メモリの大きさも影響される。</p>
<h2 id="まとめ">まとめ</h2>
<p>思ったより長文になってしまった。<br>
結局ストリーム処理の難しさは以下の2点に尽きるだろう。</p>
<ul>
<li>複数の時間の概念</li>
<li>常時稼働のシステム</li>
</ul>
<p>独学なので抜け漏れがあったり、話が新しくなかったりすることもあると思われる。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/">Streaming 101: The world beyond batch</a>
<ul>
<li>Apache Beam PMC によるストリーム処理の解説ポスト。必読</li>
</ul>
</li>
<li><a href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-102/">Streaming 102: The world beyond batch</a>
<ul>
<li>上の続きであり watermark について触れている</li>
</ul>
</li>
<li><a href="https://d1.awsstatic.com/whitepapers/architecture/wellarchitected-Analytics-Lens.pdf">Analytics Lens - AWS Well-Architected Framework</a>
<ul>
<li>AWS の資料。ストリーム処理のシステムの全体感がつかめる</li>
</ul>
</li>
<li><a href="https://spark.apache.org/docs/3.0.0/structured-streaming-programming-guide.html">Structured Streaming Programming Guide - Apache Spark</a>
<ul>
<li>Spark Structured Streaming のドキュメント。consumer の気持ちがわかる</li>
</ul>
</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html">Flink DataStream API Programming Guide - Apache Flink</a>
<ul>
<li>Flink のドキュメントの方がより詳しい。DataStream API の解説を中心に読むとよい</li>
</ul>
</li>
</ul>]]></content>
  </entry>
  <entry>
    <title>A/B テストの運用が重くてつらいという話</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/heavy-ab-testing-operation/</id>
    <updated>2020-08-23T03:30:00Z</updated>
    <published>2020-08-23T03:30:00Z</published>
    <content type="html"><![CDATA[

<figure class="center">
<img
class="mb-2 mx-auto leading-none shadow-xl"
src="/image/photo/jason-dent-JVD3XPqjLaQ-unsplash.jpg"
alt="Photo by Jason Dent on">
<figcaption class="text-sm text-right text-raven-500">
<p>Photo by Jason Dent on <a href="https://unsplash.com/photos/JVD3XPqjLaQ">Unsplash</a></p>
</figcaption>
</figure>

<h2 id="前提">前提</h2>
<p>ここでは web システムで使われている機械学習のモデルやアルゴリズムを改善するための online の A/B テストを考える。<br>
具体的に述べると web 広告における CTR 予測や EC サイトのレコメンデーション等が対象である。<br>
よくあるやつ。</p>
<p>web システムにおいて online の A/B テストは KPI 改善の根幹でありとても重要だ。<br>
それが重くなるとつらい、という話。<br>
ここで「重い」と言っているのは計算資源のことではなく、A/B テストを実施する担当者の運用コストについて。</p>
<h2 id="ab-テストの運用が重い場合のデメリット">A/B テストの運用が重い場合のデメリット</h2>
<h3 id="デメリット-1-kpi-改善が遅くなる">デメリット 1. KPI 改善が遅くなる</h3>
<p>デメリットと言えばこれが一番大きい。<br>
単純に A/B テストを1回まわすのに時間がかかってしまうし、それがゆえに online の A/B テストに入るまでの offline のテストが厚くなりここでも時間がかかってしまう。<br>
KPI 改善に時間がかかるというのはつまり売上や利益を大きくするのに時間がかかってしまうということである。</p>
<h3 id="デメリット-2-kpi-改善における-offline-テストの比重が大きくなる">デメリット 2. KPI 改善における offline テストの比重が大きくなる</h3>
<p>前述のとおりだが online の A/B テストが重いとそこで失敗できなくなり、結果としてその前段の offline のテストを厚くするということになる。<br>
offline のテストが厚いことの何が問題だろうか。</p>
<p>ここで前提としている CTR 予測やレコメンデーションのようなタスクの場合、offline のデータは既存のモデルやアルゴリズムの影響を受けることになる。<br>
例えばレコメンデーションの場合を考えると、新しいモデルを offline で評価するための実験データの正例 (コンテンツの閲覧等) は既存モデルによって生み出される。<br>
既存モデルが「このコンテンツがいいよ」といってユーザに出したリスト、その中からコンテンツの閲覧が行われ正例となるからだ。<br>
このような状況下での offline テストにおいては既存モデルと近い好みを持ったモデルのスコアが高くなる傾向がある。</p>
<p>もしかしたら既存モデルの影響を受けないようなデータをあえて用意しているような場合もあるかもしれないが、レアケースだろう。<br>
既存モデルの影響を受けないということは恩恵を受けないということなのでトレードオフでもある。</p>
<h3 id="デメリット-3-新モデルアルゴリズムを却下しにくくなる">デメリット 3. 新モデル／アルゴリズムを却下しにくくなる</h3>
<p>A/B テストの運用が重いと何度も繰り返すことができない。<br>
なので一発の A/B テストで既存モデル／アルゴリズムを置き換える成果を出さなければならないという圧が強くかかってしまう。<br>
既存モデル／アルゴリズムが勝って新しいものが負けてしまったときの埋没費用が大きいからだ。</p>
<p>そうなると次のような事が起こる。</p>
<ul>
<li>「新モデルをチューニングしてみよう！」
<ul>
<li>数ヶ月前のデータでチューニングされていない既存モデル vs. 直近のデータでチューニングされた新モデル</li>
</ul>
</li>
<li>「新モデルが KPI で勝っている間に意思決定しよう！」
<ul>
<li>勝ったり負けたりする中で…</li>
</ul>
</li>
</ul>
<p>つまりフェアなテストではなくなってしまう。</p>
<h2 id="なぜ-ab-テスト運用が重くなるのか">なぜ A/B テスト運用が重くなるのか</h2>
<h3 id="理由-1-リリース作業のコスト">理由 1. リリース作業のコスト</h3>
<p>A/B テストの運用が重くなる理由の1として、A/B テストを始めるときのオペレーション、つまり新しいモデル／アルゴリズムをリリースする際の作業コストが重いことが挙げられる。</p>
<p>これにはモデル／アルゴリズムが web のシステムに対してどのような形でデプロイされているかが影響する。<br>
mercari が次のような資料を公開しているので参考にしたい。</p>
<ul>
<li><a href="https://engineering.mercari.com/blog/entry/ml-system-design/">機械学習システムの設計パターンを公開します。</a></li>
</ul>
<p>例えば <a href="https://mercari.github.io/ml-system-design-pattern/Serving-patterns/Synchronous-pattern/design_ja.html">Synchronous pattern</a> のように web サーバと推論をする場所が分離している場合に比べて <a href="https://mercari.github.io/ml-system-design-pattern/Serving-patterns/Web-single-pattern/design_ja.html">Web single pattern</a> のように一体化している場合は新しいモデル／アルゴリズムのリリースに繊細にならざるを得ないのではないだろうか。<br>
コードベースもそうだし、マシンリソースの管理も後者の方が難しい。<br>
特にリリース担当者がデータサイエンティスト的な人だった場合、リリースの心理的障壁が上がる。</p>
<h3 id="理由-2-はっきりしない指標">理由 2. はっきりしない指標</h3>
<p>A/B テストするときはテストの指標として何らかの KPI を置く。<br>
この KPI が複数ある場合がある。<br>
例えば web 広告における CTR 予測モデルであれば CTR と impression 等。<br>
現実のビジネスは複雑なので複数の KPI があるのは珍しくないと思われる。</p>
<p>一方で KPI が複数になると評価が難しくなる。<br>
「KPI A は5上がったけど B は3下がった、この場合はどうなの？」ということになり &quot;マネージャの肌感&quot; みたいなものが必要になってしまう。<br>
当然機械的に判定することによる自動化なども行えない。<br>
A/B テストの経過における現状の良し悪しの判断にコストがかかってしまうことになる。<br>
また判断基準が複雑なことにより<a href="https://ja.wikipedia.org/wiki/%E3%81%8A%E3%81%A8%E3%82%8A%E5%8A%B9%E6%9E%9C">お取り効果</a>のようなことも発生しうるだろう。</p>
<p>これについての1つの解として OEC: Overall Evaluation Criterion という考え方がある。</p>
<ul>
<li><a href="https://www.analytics-toolkit.com/glossary/overall-evaluation-criterion/">What does &quot;Overall Evaluation Criterion&quot; mean?</a></li>
</ul>
<p>複数の KPI を重みをつけて組み合わせてただ1つの値として評価できるようにする、というものらしい。<br>
逆に考えると OEC を定義せずに複数の KPI で A/B テストをするということは KPI 間のバランスの合意なしで走ってしまっているということになる。</p>
<h3 id="理由-3-煩雑な流量オペレーション">理由 3. 煩雑な流量オペレーション</h3>
<p>これは実際に見たことだが、A/B テストの運用に際して組織の一部の人間しか理解していない煩雑なオペレーションを伴うことがある。</p>
<p>最初は control group を 0.1% だけ流して、1日経って問題がなければ 1% に上げて、その次は5%で…などのような操作を求められるのである。<br>
担当者にすればまあまあのストレスだし、時間がかかってしまう。</p>
<h2 id="まとめ">まとめ</h2>
<p>つらいね。</p>]]></content>
  </entry>
  <entry>
    <title>Apache Flink の Temporary Table Function を用いた stream data と static data の join</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/flink-join-by-temporal-table-function/</id>
    <updated>2020-08-15T15:30:00Z</updated>
    <published>2020-08-15T15:30:00Z</published>
    <content type="html"><![CDATA[

<figure class="center">
<img
class="mb-2 mx-auto leading-none shadow-xl"
src="/image/logo/flink_squirrel_200_color.png">
<figcaption class="text-sm text-right text-raven-500">
<p> <a href="https://flink.apache.org/">Apache Flink</a></p>
</figcaption>
</figure>

<p><a href="https://soonraah.github.io/flink-join-by-broadcast-state-pattern/">前回の記事</a> では Apache Flink における stream data と static data の join において、DataStream API における broadcast state pattern を使う方法を示した。<br>
今回の記事では Table API の temporal table function を用いた実験を行う。</p>
<h2 id="table-api">Table API</h2>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/">Table API</a> は名前のとおりで class <code>Table</code> を中心として SQL-like な DSL により処理を記述するという、DataStream API より high-level な API となっている。<br>
これらの関係は Apaceh Spark の <code>RDD</code> と <code>DataFrame</code> (<code>DataSet</code>) の関係に似ている。<br>
SQL-like な API で記述された処理が実行時に最適化されて low-level API の処理に翻訳されるところも同じだ。</p>
<p>RDB の table の概念を元にしているものと考えられるが、本質的に table の概念とストリーム処理はあまりマッチしないと思う。<br>
table はある時点のデータセット全体を表すのに対し、ストリーム処理ではやってくるレコードを逐次的に処理したい。<br>
ここを合わせているため、ストリーム処理における Table API による処理の挙動の理解には注意が必要だ。<br>
<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/">Streaming Concepts</a> 以下のドキュメントを確認しておきたい。</p>
<h2 id="temporal-table-function">Temporal Table Function</h2>
<p>star schema における、変更されうる dimension table を stream data と結合する方法として、<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html">temporal table</a> という仕組みが提供されている。<br>
ドキュメントでは為替レートの例が示されている。<br>
stream でやってくる fact table 的なレコードに対して、為替のように時々刻々と変化する dimension table をそのレコードの時刻における snap shot としてぶつけるような形となる。</p>
<p>レコードの時刻としては processing time または event time を扱うことができる。<br>
event time の場合であっても watermark で遅延の許容を定義できるため dimension table のすべての履歴を状態として保持する必要はなく、processing time または event time の watermark に応じて過去の履歴は捨てることが可能となっている。</p>
<p>Table API において temporal table を使うには temporal table function という形を取ることになる。</p>
<h2 id="実験">実験</h2>
<h3 id="実験概要">実験概要</h3>
<p>やることは <a href="https://soonraah.github.io/flink-join-by-broadcast-state-pattern/">前回の記事</a> とまったく同じで乱数で作った株価のデータを扱う。<br>
前回と違うのは DataStream API ではなく Table API で処理を記述したところである。</p>
<h3 id="コード">コード</h3>
<p>上記を実行するためのコードを以下に示す。<br>
実行可能なプロジェクトは <a href="https://github.com/soonraah/flink_join_test">GitHub</a> に置いておいた。</p>
<h4 id="entry-point">Entry Point</h4>
<p><code>toTable()</code> により入力データの <code>DataStream</code> を <code>Table</code> に変換した後、処理を記述した。<br>
<code>func</code> が temporal table function に当たる。<br>
今回は processing time を基準として join しているが、実際のシステムでは event time を基準としたいことが多いのではないだろうか。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">package</span> <span class="nn">example</span>

<span class="k">import</span> <span class="nn">org.apache.flink.api.java.io.TextInputFormat</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.scala._</span>
<span class="k">import</span> <span class="nn">org.apache.flink.core.fs.Path</span>
<span class="k">import</span> <span class="nn">org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.TimeCharacteristic</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.functions.source.FileProcessingMode</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.scala.</span><span class="o">{</span><span class="nc">DataStream</span><span class="o">,</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api.bridge.scala.</span><span class="o">{</span><span class="nc">StreamTableEnvironment</span><span class="o">,</span> <span class="k">_</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api.</span><span class="o">{</span><span class="nc">AnyWithOperations</span><span class="o">,</span> <span class="nc">EnvironmentSettings</span><span class="o">,</span> <span class="nc">FieldExpression</span><span class="o">,</span> <span class="n">call</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.apache.flink.test.util.MiniClusterWithClientResource</span>

<span class="k">object</span> <span class="nc">FlinkTableJoinTest</span> <span class="o">{</span>
  <span class="c1">// See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html#testing-flink-jobs
</span><span class="c1"></span>  <span class="k">private</span> <span class="k">val</span> <span class="nc">FlinkCluster</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MiniClusterWithClientResource</span><span class="o">(</span>
    <span class="k">new</span> <span class="nc">MiniClusterResourceConfiguration</span>
    <span class="o">.</span><span class="nc">Builder</span><span class="o">()</span>
      <span class="o">.</span><span class="n">setNumberSlotsPerTaskManager</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
      <span class="o">.</span><span class="n">setNumberTaskManagers</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
      <span class="o">.</span><span class="n">build</span>
  <span class="o">)</span>

  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">FlinkCluster</span><span class="o">.</span><span class="n">before</span><span class="o">()</span>

    <span class="c1">// for batch programs use ExecutionEnvironment instead of StreamExecutionEnvironment
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
    <span class="n">env</span><span class="o">.</span><span class="n">setStreamTimeCharacteristic</span><span class="o">(</span><span class="nc">TimeCharacteristic</span><span class="o">.</span><span class="nc">ProcessingTime</span><span class="o">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">setParallelism</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>

    <span class="c1">// create settings
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">setting</span> <span class="k">=</span> <span class="nc">EnvironmentSettings</span>
      <span class="o">.</span><span class="n">newInstance</span><span class="o">()</span>
      <span class="o">.</span><span class="n">useBlinkPlanner</span><span class="o">()</span>
      <span class="o">.</span><span class="n">inStreamingMode</span><span class="o">()</span>
      <span class="o">.</span><span class="n">build</span><span class="o">()</span>

    <span class="c1">// create a TableEnvironment
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">,</span> <span class="n">setting</span><span class="o">)</span>

    <span class="c1">// create a Table instance for Company
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">companiesMasterFilePath</span> <span class="k">=</span> <span class="s">&#34;data/companies.csv&#34;</span>
    <span class="k">val</span> <span class="n">companies</span> <span class="k">=</span> <span class="n">readCompaniesMaster</span><span class="o">(</span><span class="n">companiesMasterFilePath</span><span class="o">,</span> <span class="n">env</span><span class="o">)</span>
      <span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;ticker&#34;</span><span class="o">.</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;c_ticker&#34;</span><span class="o">),</span> <span class="n">$</span><span class="s">&#34;name&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c_proc_time&#34;</span><span class="o">.</span><span class="n">proctime</span><span class="o">)</span>

    <span class="c1">// temporal table function
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">func</span> <span class="k">=</span> <span class="n">companies</span><span class="o">.</span><span class="n">createTemporalTableFunction</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;c_proc_time&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c_ticker&#34;</span><span class="o">)</span>

    <span class="c1">// create a Table instance for Stock
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">stocks</span> <span class="k">=</span> <span class="n">env</span>
      <span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="k">new</span> <span class="nc">UnboundedStocks</span><span class="o">)</span>
      <span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;ticker&#34;</span><span class="o">.</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;s_ticker&#34;</span><span class="o">),</span> <span class="n">$</span><span class="s">&#34;price&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;s_proc_time&#34;</span><span class="o">.</span><span class="n">proctime</span><span class="o">)</span>

    <span class="c1">// join with a temporal table function
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">results</span> <span class="k">=</span> <span class="n">stocks</span>
      <span class="o">.</span><span class="n">joinLateral</span><span class="o">(</span><span class="n">call</span><span class="o">(</span><span class="n">func</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;s_proc_time&#34;</span><span class="o">),</span> <span class="n">$</span><span class="s">&#34;s_ticker&#34;</span> <span class="o">===</span> <span class="n">$</span><span class="s">&#34;c_ticker&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;s_ticker&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;name&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;price&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">toAppendStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span>
      <span class="o">.</span><span class="n">print</span>

    <span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>

    <span class="nc">FlinkCluster</span><span class="o">.</span><span class="n">after</span><span class="o">()</span>
  <span class="o">}</span>

  <span class="k">private</span> <span class="k">def</span> <span class="n">readCompaniesMaster</span><span class="o">(</span><span class="n">companiesMasterFilePath</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
                                  <span class="n">env</span><span class="k">:</span> <span class="kt">StreamExecutionEnvironment</span><span class="o">)</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Company</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="n">env</span>
      <span class="o">.</span><span class="n">readFile</span><span class="o">(</span>
        <span class="k">new</span> <span class="nc">TextInputFormat</span><span class="o">(</span><span class="k">new</span> <span class="nc">Path</span><span class="o">(</span><span class="n">companiesMasterFilePath</span><span class="o">)),</span>
        <span class="n">companiesMasterFilePath</span><span class="o">,</span>
        <span class="nc">FileProcessingMode</span><span class="o">.</span><span class="nc">PROCESS_CONTINUOUSLY</span><span class="o">,</span>
        <span class="mi">10</span> <span class="o">*</span> <span class="mi">1000</span>
      <span class="o">)</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">line</span> <span class="k">=&gt;</span>
        <span class="k">val</span> <span class="n">items</span> <span class="k">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34;,&#34;</span><span class="o">)</span>
        <span class="nc">Company</span><span class="o">(</span><span class="n">items</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">items</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
      <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="実行結果">実行結果</h3>
<p>上記を実行すると以下のような stream と static が結合された結果レコードが流れ続ける。</p>
<pre><code>2&gt; (AMZN,Amazon,110.05826176785374)
2&gt; (AMZN,Amazon,237.82717323588966)
1&gt; (FB,Facebook,147.96046700184428)
1&gt; (GOOGL,Google,393.58555322242086)
2&gt; (AMZN,Amazon,104.18843434881401)
</code></pre><p>前回と同様に <code>data/companies.csv</code> の中身を更新するとその結果が反映される。<br>
削除が反映されないのも同じだった。<br>
おそらく physical な処理としてはほぼ同じようになっていると思われる。</p>
<h2 id="まとめ">まとめ</h2>
<p>前回と同様の stream data と static data の join を、Table API + temporal table function で行えることを確認した。<br>
temporal table function の概念さえ把握できれば Straem API のときに比べて簡潔に処理を記述できた。</p>]]></content>
  </entry>
  <entry>
    <title>Apache Flink の Broadcast State Pattern を用いた stream data と static data の join</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/flink-join-by-broadcast-state-pattern/</id>
    <updated>2020-08-15T15:30:00Z</updated>
    <published>2020-08-06T13:00:00Z</published>
    <content type="html"><![CDATA[

<figure class="center">
<img
class="mb-2 mx-auto leading-none shadow-xl"
src="/image/logo/flink_squirrel_200_color.png">
<figcaption class="text-sm text-right text-raven-500">
<p> <a href="https://flink.apache.org/">Apache Flink</a></p>
</figcaption>
</figure>

<p>star schema における fact table と dimension table の join のようなことを Apache Flink におけるストリーム処理で行いたい。<br>
stream data と static data の join ということになる。<br>
ただし dimension table 側も更新されるため、完全な static というわけではない。</p>
<p>このポストでは Flink v1.11 を前提とした。</p>
<h2 id="join-の方法">join の方法</h2>
<p>今回は DataStream API でこれを実現することを考える。<br>
Flink のドキュメントを読むと broadcast state pattern でできそうだ。</p>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html">The Broadcast State Pattern</a></li>
</ul>
<p>やり方としては次のようになる。</p>
<ol>
<li>static data のファイルを <code>FileProcessingMode.PROCESS_CONTINUOUSLY</code> で読み込み <code>DataStream</code> 化</li>
<li>1 を <code>broadcast()</code></li>
<li>stream data の <code>DataStream</code> と 2 を <code>connect()</code></li>
</ol>
<p>static data を <code>PROCESS_CONTINUOUSLY</code> で読むのは変更を得るため。<br>
<code>PROCESS_ONCE</code> で読んでしまうとストリーム処理の開始時に1回読むだけになり、dimension table の変更を得られない。<br>
このあたりの仕様については <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html#data-sources">Data Sources</a> を参照。</p>
<p>その後は broadcast state pattern にそのまま従う。<br>
したがって <code>BroadcastProcessFunction</code> or <code>KeyedBroadcastProcessFunction</code> を実装する必要がある。<br>
その中で static data を取り込んで state として持ち、stream data 側で参照すればよい。</p>
<p>2つのデータの各 term に対する関係性を以下に示す。</p>
<table>
<thead>
<tr>
<th>in star schema</th>
<th>stream or static</th>
<th>broadcast or not</th>
</tr>
</thead>
<tbody>
<tr>
<td>dimension table</td>
<td>static data</td>
<td>broadcasted</td>
</tr>
<tr>
<td>fact table</td>
<td>stream data</td>
<td>non-broadcasted</td>
</tr>
</tbody>
</table>
<h2 id="実験">実験</h2>
<h3 id="実験概要">実験概要</h3>
<p>stream data として株価のデータを考える。<br>
適当に乱数で作った株価が &quot;GOOGL&quot; 等の ticker とともに流れてくる。<br>
一方、会社情報が記載された dimension table 的なファイルも用意する。<br>
流れ続ける株価データに対して ticker を key にして会社情報を紐付ける、ということを行う。</p>
<h3 id="コード">コード</h3>
<p>上記を実行するためのコードを以下に示す。<br>
実行可能なプロジェクトを <a href="https://github.com/soonraah/flink_join_test">GitHub</a> に置いたので興味があればどうぞ。</p>
<h4 id="entry-point">Entry Point</h4>
<p><code>main()</code> の実装。<br>
<code>MiniClusterWithClientResource</code> は本来は単体テスト用だが、簡単に local で cluster を動かすためにここで使用している。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">package</span> <span class="nn">example</span>

<span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.MapStateDescriptor</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.java.io.TextInputFormat</span>
<span class="k">import</span> <span class="nn">org.apache.flink.core.fs.Path</span>
<span class="k">import</span> <span class="nn">org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.functions.source.FileProcessingMode</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.scala.</span><span class="o">{</span><span class="nc">DataStream</span><span class="o">,</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">,</span> <span class="n">createTypeInformation</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.apache.flink.test.util.MiniClusterWithClientResource</span>

<span class="k">object</span> <span class="nc">FlinkJoinTest</span> <span class="o">{</span>
  <span class="c1">// See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html#testing-flink-jobs
</span><span class="c1"></span>  <span class="k">private</span> <span class="k">val</span> <span class="nc">FlinkCluster</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MiniClusterWithClientResource</span><span class="o">(</span>
    <span class="k">new</span> <span class="nc">MiniClusterResourceConfiguration</span>
    <span class="o">.</span><span class="nc">Builder</span><span class="o">()</span>
      <span class="o">.</span><span class="n">setNumberSlotsPerTaskManager</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
      <span class="o">.</span><span class="n">setNumberTaskManagers</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
      <span class="o">.</span><span class="n">build</span>
  <span class="o">)</span>

  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">FlinkCluster</span><span class="o">.</span><span class="n">before</span><span class="o">()</span>

    <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
    <span class="n">env</span><span class="o">.</span><span class="n">setParallelism</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>

    <span class="k">val</span> <span class="n">companiesMasterFilePath</span> <span class="k">=</span> <span class="s">&#34;data/companies.csv&#34;</span>

    <span class="k">val</span> <span class="n">companies</span> <span class="k">=</span> <span class="n">readCompaniesMaster</span><span class="o">(</span><span class="n">companiesMasterFilePath</span><span class="o">,</span> <span class="n">env</span><span class="o">)</span>
      <span class="o">.</span><span class="n">broadcast</span><span class="o">(</span><span class="k">new</span> <span class="nc">MapStateDescriptor</span><span class="o">(</span>
        <span class="s">&#34;CompanyState&#34;</span><span class="o">,</span>
        <span class="n">classOf</span><span class="o">[</span><span class="kt">String</span><span class="o">],</span>    <span class="c1">// ticker
</span><span class="c1"></span>        <span class="n">classOf</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span>     <span class="c1">// name
</span><span class="c1"></span>      <span class="o">))</span>

    <span class="c1">// the broadcast state pattern
</span><span class="c1"></span>    <span class="c1">// See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html
</span><span class="c1"></span>    <span class="n">env</span>
      <span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="k">new</span> <span class="nc">UnboundedStocks</span><span class="o">)</span>
      <span class="o">.</span><span class="n">connect</span><span class="o">(</span><span class="n">companies</span><span class="o">)</span>
      <span class="o">.</span><span class="n">process</span><span class="o">(</span><span class="k">new</span> <span class="nc">StockBroadcastProcessFunction</span><span class="o">)</span>
      <span class="o">.</span><span class="n">print</span><span class="o">()</span>

    <span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">(</span><span class="s">&#34;flink join test&#34;</span><span class="o">)</span>

    <span class="nc">FlinkCluster</span><span class="o">.</span><span class="n">after</span><span class="o">()</span>
  <span class="o">}</span>

  <span class="k">private</span> <span class="k">def</span> <span class="n">readCompaniesMaster</span><span class="o">(</span><span class="n">companiesMasterFilePath</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
                                  <span class="n">env</span><span class="k">:</span> <span class="kt">StreamExecutionEnvironment</span><span class="o">)</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Company</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="n">env</span>
      <span class="o">.</span><span class="n">readFile</span><span class="o">(</span>
        <span class="k">new</span> <span class="nc">TextInputFormat</span><span class="o">(</span><span class="k">new</span> <span class="nc">Path</span><span class="o">(</span><span class="n">companiesMasterFilePath</span><span class="o">)),</span>
        <span class="n">companiesMasterFilePath</span><span class="o">,</span>
        <span class="nc">FileProcessingMode</span><span class="o">.</span><span class="nc">PROCESS_CONTINUOUSLY</span><span class="o">,</span>
        <span class="mi">10</span> <span class="o">*</span> <span class="mi">1000</span>
      <span class="o">)</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">line</span> <span class="k">=&gt;</span>
        <span class="k">val</span> <span class="n">items</span> <span class="k">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34;,&#34;</span><span class="o">)</span>
        <span class="nc">Company</span><span class="o">(</span><span class="n">items</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">items</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
      <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h4 id="records">Records</h4>
<p>各種レコードを表す case class。<br>
<code>UnboundedStocks</code> は一定のインターバルで <code>Stock</code> を無限に返す iterator であり、stream data 生成に利用する。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">Company</span><span class="o">(</span><span class="n">ticker</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">name</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="k">case</span> <span class="k">class</span> <span class="nc">Stock</span><span class="o">(</span><span class="n">ticker</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">price</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span>

<span class="cm">/**
</span><span class="cm"> * Iterator to generate unbounded stock data
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">UnboundedStocks</span> <span class="k">extends</span> <span class="nc">Iterator</span><span class="o">[</span><span class="kt">Stock</span><span class="o">]</span> <span class="k">with</span> <span class="nc">Serializable</span> <span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">hasNext</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="kc">true</span>  <span class="c1">// unbounded
</span><span class="c1"></span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">next</span><span class="o">()</span><span class="k">:</span> <span class="kt">Stock</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">Thread</span><span class="o">.</span><span class="n">sleep</span><span class="o">(</span><span class="mi">1000</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">tickers</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">&#34;GOOGL&#34;</span><span class="o">,</span> <span class="s">&#34;AAPL&#34;</span><span class="o">,</span> <span class="s">&#34;FB&#34;</span><span class="o">,</span> <span class="s">&#34;AMZN&#34;</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">ticker</span> <span class="k">=</span> <span class="n">tickers</span><span class="o">(</span><span class="nc">Random</span><span class="o">.</span><span class="n">nextInt</span><span class="o">(</span><span class="n">tickers</span><span class="o">.</span><span class="n">size</span><span class="o">))</span>  <span class="c1">// one of GAFA
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">price</span> <span class="k">=</span> <span class="mi">100</span> <span class="o">+</span> <span class="nc">Random</span><span class="o">.</span><span class="n">nextDouble</span><span class="o">()</span> <span class="o">*</span> <span class="mi">300</span>         <span class="c1">// random price
</span><span class="c1"></span>    <span class="nc">Stock</span><span class="o">(</span><span class="n">ticker</span><span class="o">,</span> <span class="n">price</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h4 id="broadcastprocessfunction">BroadcastProcessFunction</h4>
<p>肝である <code>BroadcastProcessFunction</code> の実装。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">package</span> <span class="nn">example</span>

<span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.MapStateDescriptor</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.functions.co.BroadcastProcessFunction</span>
<span class="k">import</span> <span class="nn">org.apache.flink.util.Collector</span>

<span class="k">class</span> <span class="nc">StockBroadcastProcessFunction</span>
  <span class="k">extends</span> <span class="nc">BroadcastProcessFunction</span><span class="o">[</span><span class="kt">Stock</span>, <span class="kt">Company</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="o">{</span>

  <span class="k">private</span> <span class="k">val</span> <span class="nc">StateDescriptor</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MapStateDescriptor</span><span class="o">(</span>
    <span class="s">&#34;CompanyState&#34;</span><span class="o">,</span>
    <span class="n">classOf</span><span class="o">[</span><span class="kt">String</span><span class="o">],</span>    <span class="c1">// ticker
</span><span class="c1"></span>    <span class="n">classOf</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span>     <span class="c1">// name
</span><span class="c1"></span>  <span class="o">)</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">processElement</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">Stock</span><span class="o">,</span>
                              <span class="n">ctx</span><span class="k">:</span> <span class="kt">BroadcastProcessFunction</span><span class="o">[</span><span class="kt">Stock</span>, <span class="kt">Company</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span><span class="k">#</span><span class="nc">ReadOnlyContext</span><span class="o">,</span>
                              <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">companyName</span> <span class="k">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">getBroadcastState</span><span class="o">(</span><span class="nc">StateDescriptor</span><span class="o">).</span><span class="n">get</span><span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">ticker</span><span class="o">)</span>
    <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">value</span><span class="o">.</span><span class="n">ticker</span><span class="o">,</span> <span class="nc">Option</span><span class="o">(</span><span class="n">companyName</span><span class="o">).</span><span class="n">getOrElse</span><span class="o">(</span><span class="s">&#34;-&#34;</span><span class="o">),</span> <span class="n">value</span><span class="o">.</span><span class="n">price</span><span class="o">))</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">processBroadcastElement</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">Company</span><span class="o">,</span>
                                       <span class="n">ctx</span><span class="k">:</span> <span class="kt">BroadcastProcessFunction</span><span class="o">[</span><span class="kt">Stock</span>, <span class="kt">Company</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span><span class="k">#</span><span class="nc">Context</span><span class="o">,</span>
                                       <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">getBroadcastState</span><span class="o">(</span><span class="nc">StateDescriptor</span><span class="o">).</span><span class="n">put</span><span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">ticker</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">name</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="実行結果">実行結果</h3>
<p>上記を実行すると以下のような stream と static が結合された結果レコードが流れ続ける。</p>
<pre><code>2&gt; (FB,Facebook,158.76057838239333)
1&gt; (GOOGL,Google,288.4271251901199)
2&gt; (AAPL,Apple,191.00515338617706)
1&gt; (FB,Facebook,121.98205452369652)
2&gt; (FB,Facebook,140.05023554456997)
</code></pre><p>この状態で会社情報が記載されている <code>data/companies.csv</code> を更新することを考える。<br>
例えば &quot;GOOGL&quot; の社名を &quot;Google&quot; から &quot;Alphabet&quot; に変更して保存してみた。<br>
するとしばらくしてその修正が反映された結果が流れてくるようになる。</p>
<pre><code>1&gt; (GOOGL,Alphabet,288.1008081843843)
2&gt; (AMZN,Amazon,137.11135563851838)
1&gt; (GOOGL,Alphabet,121.78368168964735)
2&gt; (FB,Facebook,236.53483047124948)
1&gt; (FB,Facebook,220.44300865769645)
</code></pre><p>static data の更新が反映されることが確認できた。<br>
今回は10秒に1回のインターバルで元ファイルを確認するようにファイルを読んでいるため、変更してからそれが反映されるまで最大10秒程度かかる。</p>
<h2 id="懸念点">懸念点</h2>
<p>join はできたが次のような懸念点がある。</p>
<ul>
<li>レコードの削除に対応していない
<ul>
<li>state を上書きしているだけなので <code>companies.csv</code> から削除されたレコードは感知できない</li>
</ul>
</li>
<li>ファイルの更新時に処理が重くなる可能性がある
<ul>
<li><code>companies.csv</code> の更新タイミングでその中の全レコードを処理してしまう</li>
</ul>
</li>
<li>checkpoint が大きくなる
<ul>
<li>state が broadcast されているため、task ごとに重複した state が保存されてしまう</li>
<li>See <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html#important-considerations">Important Considerations</a></li>
</ul>
</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>このように broadcast state pattern によって stream data と static data との join 処理を行うことができた。<br>
ただし、まだちゃんと調べていないが DataStream API ではなく Table API を使えばもう少しカジュアルな感じで近いことができるかもしれない。<br>
気が向いたらそちらも試してみる。</p>
<h3 id="追記">追記</h3>
<p>Table API を使った場合についての記事を追加しました。</p>
<ul>
<li><a href="https://soonraah.github.io/flink-join-by-temporal-table-function/">Apache Flink の Temporary Table Function を用いた stream data と static data の join</a></li>
</ul>]]></content>
  </entry>
  <entry>
    <title>あまり大きな Pull Request を作ってほしくない</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/no-more-huge-pull-request/</id>
    <updated>2020-08-03T13:00:00Z</updated>
    <published>2020-08-03T13:00:00Z</published>
    <content type="html"><![CDATA[<p>GitHub Flow ベースの開発においてはあまり大きな pull request を作ってほしくないという話。<br>
なんというか今更わざわざ言わなくてもいいんだけど…</p>
<p>仕事で何度か大きな pull request が投げられているのを見てしまったので、それはあまりよくないよというのを自分でも指摘しやすくするためにまとめておく。</p>
<h2 id="reference">Reference</h2>
<p>最初に参考資料を挙げておく。</p>
<ul>
<li><a href="http://kurtfunai.com/2017/12/100-duck-sized-pull-requests.html">100 Duck-Sized Pull Requests</a></li>
<li><a href="https://techracho.bpsinc.jp/hachi8833/2018_02_07/51095">「巨大プルリク1件vs細かいプルリク100件」問題を考える（翻訳）</a></li>
<li><a href="https://smallbusinessprogramming.com/optimal-pull-request-size/">Optimal pull request size</a></li>
</ul>
<p>これらを読めば特に私から言うこともないのだが…<br>
これらに書いてあるとおりだが補足しておく。</p>
<h2 id="pull-request-を小分けにしたときのメリット">Pull Request を小分けにしたときのメリット</h2>
<h3 id="module-を適切に切り出すモチベーションが得られる">module を適切に切り出すモチベーションが得られる</h3>
<p>次のような話がある。</p>
<ul>
<li><a href="https://hachibeechan.hateblo.jp/entry/dont-think-just-make-it-abstract">共通化という考え方はアンチパターンを生み出すだけ説</a></li>
</ul>
<p>これを理由に module を分けるべきところが分けられず、いろんなことができてしまうベタッと大きな module が生まれるのを見た。</p>
<p>もちろんよく考えられていない共通化は駄目だが、上記ポストでは一方で</p>

<blockquote>
  <p>ただし共通化という名の下におこなわれるのは「同じロジックを持つコードをまとめる」行為であって、抽象化のようにそのコード単位の意味を捉える作業はその範疇にない。抽象化というのはロジックを意味単位ごとにひとくくりにしていく行為で、これがどういうことなのかは次以降で述べていく。</p>
  <footer>&#8212; はっちん <cite title="共通化という考え方はアンチパターンを生み出すだけ説 - タオルケット体操"><a rel="noopener nofollow" href="https://hachibeechan.hateblo.jp/entry/dont-think-just-make-it-abstract">共通化という考え方はアンチパターンを生み出すだけ説 - タオルケット体操</a></cite></footer>
</blockquote>

<p>と述べており抽象化、つまりコードの意味を考慮した上で適切な単位でまとめておくことは否定していない。</p>
<p>pull request を小さく分けるという行為のためには module や機能を適度なまとまりで切り分けること、つまり抽象化を考えていくことが必要となる。<br>
したがって何でもできてしまう大きな module が生まれるのを防ぐ方向に働く。<br>
(もちろんここで駄目な共通化がなされてしまうこともあるだろう)</p>
<h3 id="リリースまでの期間が短く済む">リリースまでの期間が短く済む</h3>
<p>前述の参考資料においても pull request が大きいと</p>

<blockquote>
  <p>Developers would put off reviews until they had time/energy and the development process would come to a halt.</p>
  <footer>&#8212; Kurtis Funai <cite title="100 Duck-Sized Pull Requests"><a rel="noopener nofollow" href="http://kurtfunai.com/2017/12/100-duck-sized-pull-requests.html">100 Duck-Sized Pull Requests</a></cite></footer>
</blockquote>


<blockquote>
  <p>development continues on the master branch, it often results in merge conflicts, rebases, and other fun.</p>
  <footer>&#8212; Kurtis Funai <cite title="100 Duck-Sized Pull Requests"><a rel="noopener nofollow" href="http://kurtfunai.com/2017/12/100-duck-sized-pull-requests.html">100 Duck-Sized Pull Requests</a></cite></footer>
</blockquote>

<p>となると述べられている。<br>
仮にすぐにレビューされ、かつ conflict なども発生しなかっとしても大きな1つの pull request をレビューする場合とそれを3つに分けた場合の開発プロセスの進行を比較すると</p>
<ul>
<li>大きな1つの pull request
<ol>
<li>全体の開発</li>
<li>全体のレビュー</li>
</ol>
</li>
<li>3つに分割された pull request
<ol>
<li>part-1 の開発</li>
<li>part-1 のレビュー &amp; part-2 の開発</li>
<li>part-2 のレビュー &amp; part-3 の開発</li>
<li>part-3 のレビュー</li>
</ol>
</li>
</ul>
<p>のようになり、開発とレビューを並列で進められる部分があるので全体としての開発期間を短くすることができる。<br>
もちろんこれはうまく噛み合ったときの例だが。<br>
早めにフィードバックが得られるのも大きい。</p>
<h2 id="pull-request-を小分けにしたときのデメリット">Pull Request を小分けにしたときのデメリット</h2>
<p>小分けにすることでレビュワーが全体感をつかみにくくなるというのはあるかもしれない。<br>
part-1 と part-2 で同じレビュワーになるとも限らない。</p>
<p>これに対しては開発に着手する前にまず全体のざっくりとした設計についてレビューを受けることで対応できる。<br>
このステップを入れることで「とりあえず手を動かそう」となりにくくなる。<br>
手を動かすと仕事してる感が出るのでとりあえず手を動かしたくなりがちだが、ちょっと待てよと。<br>
まず何を作るか、全体の工程を考えましょうと。</p>

<blockquote>
  <p>Working under these constraints causes developers to break problems down into incremental deliverables. It helps avoid the temptation of jumping into development without a clear plan.</p>
  <footer>&#8212; Kurtis Funai <cite title="100 Duck-Sized Pull Requests"><a rel="noopener nofollow" href="http://kurtfunai.com/2017/12/100-duck-sized-pull-requests.html">100 Duck-Sized Pull Requests</a></cite></footer>
</blockquote>

<h2 id="まとめ">まとめ</h2>
<p>pull request を分けてくれ、頼む。</p>]]></content>
  </entry>
  <entry>
    <title>勉強会メモ: Spark Meetup Tokyo #3 Online</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/spark-meetup-tokyo-3/</id>
    <updated>2020-08-23T04:00:00Z</updated>
    <published>2020-08-01T06:00:00Z</published>
    <content type="html"><![CDATA[

<figure class="center">
<img
class="mb-2 mx-auto leading-none shadow-xl"
src="/image/logo/Apache_Spark_logo.png">
<figcaption class="text-sm text-right text-raven-500">
<p> <a href="https://spark.apache.org/">Apache Spark</a></p>
</figcaption>
</figure>

<p>2020-07-31 にオンライン開催された <a href="https://spark-meetup-tokyo.connpass.com/event/181422/">Spark Meetup Tokyo #3 Online</a> に参加した。<br>
感想などメモしておく。</p>
<h1 id="全体感">全体感</h1>
<p>トピックとしては主に</p>
<ul>
<li>Spark 3.0</li>
<li>Spark + AI Summit 2020</li>
<li>Spark 周辺要素</li>
</ul>
<p>といったところだろうか。<br>
最近のコミュニティの動向や関心を日本語で聞くことができてよかった。</p>
<p>運営 &amp; スピーカーの皆様、ありがとうございます。</p>
<h1 id="発表">発表</h1>
<p>発表資料は公開されたら追加していく。</p>
<h2 id="sparkai-summit-2020-イベント概要">SPARK+AI Summit 2020 イベント概要</h2>
<p>スピーカー: <a href="https://twitter.com/tokyodataguy">@tokyodataguy</a> さん</p>
<ul>
<li>Summit は金融業界の参加者が増えているらしい</li>
<li>Spark で最も使われている言語は Python とのことだったが、Databricks の notebook サービスの話だった
<ul>
<li>プロダクションコードではまた違うのだろう</li>
</ul>
</li>
<li>Spark 3.0 の update をざっくりと</li>
<li>Spark 周辺要素の話をざっくりと
<ul>
<li>Koalas</li>
<li>Delta Lake</li>
<li>Redash</li>
<li>MLflow</li>
</ul>
</li>
</ul>
<h2 id="introducing-koalas-10">Introducing Koalas 1.0</h2>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/42rsX5ywWZbk5b" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/ueshin/introducing-koalas-10-and-11" title="Introducing Koalas 1.0 (and 1.1)" target="_blank">Introducing Koalas 1.0 (and 1.1)</a> </strong> from <strong><a href="//www.slideshare.net/ueshin" target="_blank">Takuya UESHIN</a></strong> </div>
<p>スピーカー: <a href="https://twitter.com/ueshin">@ueshin</a> さん</p>
<ul>
<li>What's Koalas
<ul>
<li>open source の pure Python library</li>
<li>pandas の API で Spark を動かせるようにする</li>
<li>小さいデータも大きなデータも同じように扱えるように</li>
</ul>
</li>
<li>最近 v1.1 をリリース</li>
<li>(個人的には pandas の API があまり好きではなく…)</li>
</ul>
<h2 id="sparkai-summit-2020-のセッションハイライト">SPARK+AI Summit 2020 のセッションハイライト</h2>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/BQQto5WPtl7pgE" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/nttdata-tech/spark-ai-summit-2020-trend-highlight-nttdata-dobashi" title="Spark + AI Summit 2020セッションのハイライト（Spark Meetup Tokyo #3 Online発表資料）" target="_blank">Spark + AI Summit 2020セッションのハイライト（Spark Meetup Tokyo #3 Online発表資料）</a> </strong> from <strong><a href="//www.slideshare.net/nttdata-tech" target="_blank">NTT DATA Technology &amp; Innovation</a></strong> </div>
<p>スピーカー: <a href="https://twitter.com/masaru_dobashi">@masaru_dobashi</a> さん</p>
<ul>
<li>Summit のセッションから case study 的なセッションを2つピックアップ</li>
<li>USCIS の例
<ul>
<li><a href="https://databricks.com/jp/session_na20/lessons-learned-from-modernizing-uscis-data-analytics-platform">Lessons Learned from Modernizing USCIS Data Analytics Platform</a></li>
<li>古き良き Data Warehouse から Data Lake への移行</li>
<li>injection には Kafka も利用</li>
<li>諸々気にせずに気軽にデータをストレージに置きたい</li>
</ul>
</li>
<li>Alibaba の例
<ul>
<li>Spark Structured Streming の上に SQL-like なものを載せた？</li>
<li>ストリームの mini-batch 処理の合間で compaction を行う (つらそう)</li>
</ul>
</li>
<li>スライド中の「パイプラインの途中でモダンな技術に流し込めるかどうか？」という言葉が印象的だった
<ul>
<li>モダンなものに移行するとき現実的に重要</li>
</ul>
</li>
</ul>
<h2 id="spark-v30の紹介">Spark v3.0の紹介</h2>
<h3 id="前半">前半</h3>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/j4RUIhcche96WO" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/ishizaki/introduction-new-features-in-spark-30" title="Introduction new features in Spark 3.0" target="_blank">Introduction new features in Spark 3.0</a> </strong> from <strong><a href="//www.slideshare.net/ishizaki" target="_blank">Kazuaki Ishizaki</a></strong> </div>
<p>スピーカー: <a href="https://twitter.com/kiszk">@kiszk</a> さん</p>
<ul>
<li>SQL の性能に関わる7大機能の話
<ol>
<li>Query planの新しい表示方法</li>
<li>Join hintsの強化</li>
<li>Adaptive query execution</li>
<li>Dynamic partitioning pruning</li>
<li>nested column pruning &amp; pushdown の強化</li>
<li>Aggregation のコード生成の改良</li>
<li>ScalaとJavaの新バージョンのサポート</li>
</ol>
</li>
<li>実行計画が見やすくなるのは本当にうれしい</li>
<li>人がチューニングしていた部分が Adaptive Query Plan や Dynamic Partition Pruning で自動化されるのもうれしい</li>
</ul>
<h3 id="後半">後半</h3>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/kPZTgORzPFZ2Ms" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/nttdata-tech/spark-meetup3-spark30-update-nttdata-sarutak" title="Apache Spark 3.0新機能紹介 - 拡張機能やWebUI関連のアップデート（Spark Meetup Tokyo #3 Online）" target="_blank">Apache Spark 3.0新機能紹介 - 拡張機能やWebUI関連のアップデート（Spark Meetup Tokyo #3 Online）</a> </strong> from <strong><a href="//www.slideshare.net/nttdata-tech" target="_blank">NTT DATA Technology &amp; Innovation</a></strong> </div>
<p>スピーカー: <a href="https://twitter.com/raspberry1123">@raspberry1123</a> さん</p>
<ul>
<li>Accelarator Aware Scheduling
<ul>
<li>Project Hydrogen</li>
</ul>
</li>
<li>プラグイン機能
<ul>
<li>executor や driver の機能を user が拡張できる</li>
<li>executor については Spark 2.4 からあったそうだが知らなかった…</li>
</ul>
</li>
<li>Structured Straming web UI
<ul>
<li>ストリーム処理にはこういった chart が必要だと思う</li>
</ul>
</li>
</ul>
<h2 id="sparkにprを投げてみた">SparkにPRを投げてみた</h2>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/r8MSqiNNAd73Wk" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/ssuserca76a5/spark-237429607" title="Sparkにプルリク投げてみた" target="_blank">Sparkにプルリク投げてみた</a> </strong> from <strong><a href="//www.slideshare.net/ssuserca76a5" target="_blank">Noritaka Sekiyama</a></strong> </div>
<p>スピーカー: <a href="https://twitter.com/moomindani">@moomindani</a> さん</p>
<ul>
<li>スピーカーは AWS で Glue の開発をしている方</li>
<li>Glue は Spark に強く依存しているため、機能追加にモチベーションがあったとのこと</li>
<li>私も投げてみたい！</li>
</ul>
<h2 id="lt-td-spark-internals-airframeでsparkの機能を拡張するテクニック">LT: td-spark internals: AirframeでSparkの機能を拡張するテクニック</h2>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/jWHWpIeuyCY9mq" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/taroleo/tdspark-internals-extending-spark-with-airframe-spark-meetup-tokyo-3-2020" title="td-spark internals: Extending Spark with Airframe - Spark Meetup Tokyo #3 2020" target="_blank">td-spark internals: Extending Spark with Airframe - Spark Meetup Tokyo #3 2020</a> </strong> from <strong><a href="//www.slideshare.net/taroleo" target="_blank">Taro L. Saito</a></strong> </div>
<p>スピーカー: <a href="https://twitter.com/taroleo">@taroleo</a> さん</p>
<ul>
<li><a href="https://github.com/wvlet/airframe">Airframe</a> とは Scala の application 開発用ツール群のようなもの？</li>
<li>Spark の機能を拡張</li>
</ul>
<h2 id="lt-spark-31-feature-expectation">LT: Spark 3.1 Feature Expectation</h2>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/lwQplIU1K7gMBl" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/maropu0804/lt-spark-31-feature-expectation" title="LT: Spark 3.1 Feature Expectation" target="_blank">LT: Spark 3.1 Feature Expectation</a> </strong> from <strong><a href="//www.slideshare.net/maropu0804" target="_blank">Takeshi Yamamuro</a></strong> </div>
<p>スピーカー: <a href="https://twitter.com/maropu">@maropu</a> さん</p>
<ul>
<li>新機能
<ul>
<li>Support Filter Pushdown JSON
<ul>
<li>JSON の読み取りが早くなる</li>
</ul>
</li>
<li>Better Handling for Node Shutdown
<ul>
<li>node 離脱時にその node が保持する shuffle や cache の情報が失われていた</li>
<li>計画された node の離脱に対して shuffle や cache ブロックを別 node に委譲</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="まとめ">まとめ</h1>
<p>どんどん便利になっていくなという印象。<br>
手でパフォーマンスチューニングする要素はどんどん減っていくのだろう。</p>
<p>Delta Lake が盛り上がっているように感じた。<br>
データレイクについて勉強しないと…</p>]]></content>
  </entry>
  <entry>
    <title>Spark DataFrame クエリの弱い分離レベル</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/weak_isolation_level_of_dataframe/</id>
    <updated>2020-07-19T13:00:00Z</updated>
    <published>2020-07-19T13:00:00Z</published>
    <content type="html"><![CDATA[

<figure class="center">
<img
class="mb-2 mx-auto leading-none shadow-xl"
src="/image/logo/Apache_Spark_logo.png">
<figcaption class="text-sm text-right text-raven-500">
<p> <a href="https://spark.apache.org/">Apache Spark</a></p>
</figcaption>
</figure>

<p>Spark バッチ処理の問題を調べていたら分離レベルという概念にたどりついた。<br>
分離レベルについて調べたので、Spark の問題の内容と絡めて記しておく。<br>
考えてみれば当たり前でたいした話ではない。</p>
<h1 id="分離レベルとは">分離レベルとは</h1>
<h2 id="トランザクションの挙動についての暗黙の理解">トランザクションの挙動についての暗黙の理解</h2>
<p>アドホックな分析クエリやプロダクションコード中のクエリを書くとき、その単一のクエリのトランザクションにおいて「同時に実行されている別のクエリの commit 前の状態や commit 結果に影響され、このクエリの結果がおかしくなるかもしれない」ということは通常考えない。<br>
トランザクションはデータベースのある時点の状態に対して正しく処理される、というほぼ無意識の理解をおそらくほとんどの開発者が持っている。</p>
<p>多くの場合この理解は間違っていない。<br>
それはなぜかというと DB 等のデータ処理フレームワークがある強さの分離レベルを提供しているからである。</p>
<h2 id="いろいろな分離レベル">いろいろな分離レベル</h2>
<p>ACID 特性のうちの1つ、分離性 (Isolation) の程度を表すのが分離レベル。</p>

<blockquote>
  <p>トランザクション中に行われる操作の過程が他の操作から隠蔽されることを指し、日本語では分離性、独立性または隔離性ともいう。より形式的には、独立性とはトランザクション履歴が直列化されていることと言える。この性質と性能はトレードオフの関係にあるため、一般的にはこの性質の一部を緩和して実装される場合が多い。</p>
  <footer>&#8212; Wikipedia <cite title="ACID (コンピュータ科学)"><a rel="noopener nofollow" href="https://ja.wikipedia.org/wiki/ACID_%28%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF%E7%A7%91%E5%AD%A6%29">ACID (コンピュータ科学)</a></cite></footer>
</blockquote>

<p>分離レベルには名前のついたものがいくつかあり、分離性の保証の強さが異なる。<br>
具体的にはトランザクションの並行性の問題への対応力が異なる。<br>
名著「<a href="https://www.oreilly.co.jp/books/9784873118703/">データ指向アプリケーションデザイン</a>」の第7章で分離レベルについて詳しく述べられているので、以下ではそちらからの引用。<br>
分離レベルを弱い順に並べる。</p>
<ul>
<li>
<p>read uncommitted</p>
<blockquote>
<p>このレベルではダーティライトは生じませんが、ダーティリードは妨げられません。</p>
</blockquote>
</li>
<li>
<p>read committed</p>
<blockquote>
<ol>
<li>データベースからの読み取りを行った際に見えるデータは、コミットされたもののみであること（ダーティリードは生じない）。</li>
<li>データベースへの書き込みを行う場合、上書きするのはコミットされたデータのみであること（ダーティライトは生じない）。</li>
</ol>
</blockquote>
</li>
<li>
<p>snapshot isolation</p>
<blockquote>
<p>スナップショット分離の考え方は、それぞれのトランザクションがデータベースの一貫性のあるスナップショットから読み取りを行うというものです。すなわち、トランザクションが読み取るデータは、すべてそのトランザクションの開始時点のデータベースにコミット済みのものだけということです。</p>
</blockquote>
</li>
<li>
<p>serializability</p>
<blockquote>
<p>この分離レベルはトランザクションが並行して実行されていても、最終的な答えはそれぞれが1つずつ順番に、並行ではなく実行された場合と同じになることを保証します。</p>
</blockquote>
</li>
</ul>
<p>日本語で「分離レベル」を検索すると snapshot isolation の代わりに repeatable read が出てくる事が多い。<br>
しかし repeatable read の名前は実装によって意味が違っていたりして扱いが難しいらしい。</p>
<h2 id="分離レベルと-race-condition-の関係">分離レベルと race condition の関係</h2>
<p>以下に各分離レベルとトランザクションの並行性の問題 (race condition) の関係を示す。<br>
各 race condition の説明については割愛するが、複数のトランザクションが並行して実行されることにより起こりうる期待されていない挙動だと思えばよい。<br>
○はその分離レベルにおいてその race condition が発生しないことを示す。<br>
△は条件によっては発生する。</p>
<table>
<thead>
<tr>
<th></th>
<th>dirty read</th>
<th>dirty write</th>
<th>read skew (nonrepeatable read)</th>
<th>lost update</th>
<th>write skew</th>
<th>phantom read</th>
</tr>
</thead>
<tbody>
<tr>
<td>read uncommitted</td>
<td>○</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>read committed</td>
<td>○</td>
<td>○</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>snapshot isolation</td>
<td>○</td>
<td>○</td>
<td>○</td>
<td>△</td>
<td>-</td>
<td>△</td>
</tr>
<tr>
<td>serializability</td>
<td>○</td>
<td>○</td>
<td>○</td>
<td>○</td>
<td>○</td>
<td>○</td>
</tr>
</tbody>
</table>
<p>下に行くほど強い分離レベルとなっている。<br>
分離レベルが強くなるほど race condition が発生しにくくなるが、一方で lock 等によりパフォーマンスが下がっていくというトレードオフがある。</p>
<h1 id="各種データベースの分離レベル">各種データベースの分離レベル</h1>
<p>ここでは MySQL と Hive においてどの分離レベルが提供されているかを見てみる。</p>
<h2 id="mysql-の場合">MySQL の場合</h2>
<p>MySQL の分離レベルについては以下のドキュメントで述べられている。</p>
<ul>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html">15.7.2.1 Transaction Isolation Levels</a></li>
</ul>
<p>MySQL (というか InnoDB?) では次の4つの分離レベルを設定することができる。</p>
<ul>
<li><code>READ UNCOMMITTED</code></li>
<li><code>READ COMMITTED</code></li>
<li><code>REPEATABLE READ</code> (default)</li>
<li><code>SERIALIZABLE</code></li>
</ul>
<p>デフォルトの分離レベルは <code>REPEATABLE READ</code> だが、これは前述の snapshot isolation に相当するらしい。<br>
分離レベルは、例えば <code>set transaction</code> 構文により次のようにして指定できる。</p>
<div class="highlight"><pre class="chroma"><code class="language-mysql" data-lang="mysql"><span class="kt">set</span> <span class="n">transaction</span> <span class="n">isolation</span> <span class="n">level</span> <span class="n">SERIALIZABLE</span><span class="p">;</span>
</code></pre></div><p>この場合は現在のセッション内で実行される次のトランザクションについて適用される。<br>
すべてのセッションやすべてのトランザクション等の指定もできる。<br>
詳しくは以下。</p>
<ul>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/set-transaction.html">13.3.7 SET TRANSACTION Statement</a></li>
</ul>
<h2 id="hive-の場合">Hive の場合</h2>
<p>Hive についてはドキュメントに次のような記載がある。</p>

<blockquote>
  <p>At this time only snapshot level isolation is supported.  When a given query starts it will be provided with a consistent snapshot of the data.</p>
  <footer>&#8212; Apache Hive <cite title="Hive Transactions"><a rel="noopener nofollow" href="https://cwiki.apache.org/confluence/display/Hive/Hive&#43;Transactions#HiveTransactions-Limitations">Hive Transactions</a></cite></footer>
</blockquote>

<p>Hive は snapshot isolation のみ提供しているとのこと。</p>

<blockquote>
  <p>The default DummyTxnManager emulates behavior of old Hive versions: has no transactions and uses hive.lock.manager property to create lock manager for tables, partitions and databases.</p>
  <footer>&#8212; Apache Hive <cite title="Hive Transactions"><a rel="noopener nofollow" href="https://cwiki.apache.org/confluence/display/Hive/Hive&#43;Transactions#HiveTransactions-Transaction/LockManager">Hive Transactions</a></cite></footer>
</blockquote>

<p>lock は小さくとも partition の単位になるのだろうか。<br>
であるとすると予想通りだが MySQL よりもいかつい挙動になっている。</p>
<p>このように多くの DB では snapshot isolation の分離レベルが基本となっている。</p>
<h1 id="spark-クエリの分離レベル">Spark クエリの分離レベル</h1>
<p>では Spark のクエリはどうだろうか。<br>
ここからようやく本題となる。</p>
<h2 id="read-committed-相当">read committed 相当</h2>
<p>Spark において <code>DataFrame</code> を用いたデータ処理を記述するとき、それは1つの SQL クエリを書くのと近い感覚になる。<br>
そもそも <code>DataFrame</code> は SQL-like な使い心地を目的として作られた API だから当然だ。</p>
<p><code>DataFrame</code> で記述された処理は実行時に <code>RDD</code> として翻訳されるが、分離レベルを考えるにあたって <code>RDD</code> の特性がキーとなってくる。</p>

<blockquote>
  <p>By default, each transformed RDD may be recomputed each time you run an action on it.</p>
  <footer>&#8212; Apache Spark <cite title="RDD Operations"><a rel="noopener nofollow" href="https://spark.apache.org/docs/3.0.0/rdd-programming-guide.html#rdd-operations">RDD Operations</a></cite></footer>
</blockquote>

<p>あまり良い説明を見つけられなかったが、1回の action を伴う処理においても同じ <code>RDD</code> が複数回参照されるとき、その <code>RDD</code> までの計算は通常やり直されることになる。</p>
<p>したがって例えば self join のようなことをするとき、同じデータソースを2回読みに行くということが起こってしまう。<br>
HDFS や S3 上のファイル、JDBC 経由の外部 DB など Spark は様々なデータソースを扱うことができるが、通常 Spark がその2回の読み込みに対して lock をかけたりすることはできない。</p>
<p>つまり non-repeatable read や phantom read を防ぐことができない。<br>
read committed という弱い分離レベルに相当するということになってしまう。</p>
<p>分離レベルという言葉はトランザクションという概念に対して使われるものであり、<code>DataFrame</code> のクエリをトランザクションと呼んでいいのかはわからない。<br>
なので分離レベルという言葉をここで使うのが適切でないかもしれないということは述べておく。</p>
<h2 id="検証">検証</h2>
<p>MySQL からデータを読み取り Spark で処理することを考える。<br>
まず local の MySQL で次のような table を用意する。</p>
<pre><code>mysql&gt; describe employees;
+---------------+---------+------+-----+---------+-------+
| Field         | Type    | Null | Key | Default | Extra |
+---------------+---------+------+-----+---------+-------+
| id            | int(11) | NO   | PRI | NULL    |       |
| salary        | int(11) | YES  |     | NULL    |       |
| department_id | int(11) | YES  |     | NULL    |       |
+---------------+---------+------+-----+---------+-------+
3 rows in set (0.03 sec)
</code></pre><p>部署 (department) ごとの給料 (salary) の平均から各従業員の給料がどれくらい離れているかの差分を見たいものとする。<br>
Spark のコードは次のようになる。<br>
Spark のバージョンはこれを書いている時点での最新 3.0.0 とした。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">package</span> <span class="nn">com.example</span>

<span class="k">import</span> <span class="nn">org.apache.spark.sql.functions.avg</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span>

<span class="k">object</span> <span class="nc">IsolationLevelExperiment</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="c1">// Prepare SparkSession
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">spark</span> <span class="k">=</span> <span class="nc">SparkSession</span>
      <span class="o">.</span><span class="n">builder</span><span class="o">()</span>
      <span class="o">.</span><span class="n">appName</span><span class="o">(</span><span class="s">&#34;Isolation Level Experiment&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">master</span><span class="o">(</span><span class="s">&#34;local[*]&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">getOrCreate</span><span class="o">()</span>

    <span class="k">import</span> <span class="nn">spark.implicits._</span>

    <span class="c1">// Read from MySQL
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">dfEmployee</span> <span class="k">=</span> <span class="n">spark</span>
      <span class="o">.</span><span class="n">read</span>
      <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&#34;jdbc&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&#34;url&#34;</span><span class="o">,</span> <span class="s">&#34;jdbc:mysql://localhost&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&#34;dbtable&#34;</span><span class="o">,</span> <span class="s">&#34;db_name.employees&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&#34;user&#34;</span><span class="o">,</span> <span class="s">&#34;user_name&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&#34;password&#34;</span><span class="o">,</span> <span class="s">&#34;********&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&#34;driver&#34;</span><span class="o">,</span> <span class="s">&#34;com.mysql.cj.jdbc.Driver&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">load</span>
      <span class="o">.</span><span class="n">cache</span>

    <span class="c1">// Get average salary
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">dfAvg</span> <span class="k">=</span> <span class="n">dfEmployee</span>
      <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;department_id&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">agg</span><span class="o">(</span><span class="n">avg</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;salary&#34;</span><span class="o">).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;avg_salary&#34;</span><span class="o">))</span>

    <span class="c1">// Calculate diff
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">dfResult</span> <span class="k">=</span> <span class="n">dfEmployee</span>
      <span class="o">.</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;e&#34;</span><span class="o">)</span>
      <span class="o">.</span><span class="n">join</span><span class="o">(</span>
        <span class="n">dfAvg</span><span class="o">.</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">),</span>
        <span class="n">$</span><span class="s">&#34;e.department_id&#34;</span> <span class="o">===</span> <span class="n">$</span><span class="s">&#34;a.department_id&#34;</span><span class="o">,</span>
        <span class="s">&#34;left_outer&#34;</span>
      <span class="o">)</span>
      <span class="o">.</span><span class="n">select</span><span class="o">(</span>
        <span class="n">$</span><span class="s">&#34;e.id&#34;</span><span class="o">,</span>
        <span class="n">$</span><span class="s">&#34;e.department_id&#34;</span><span class="o">,</span>
        <span class="o">(</span><span class="n">$</span><span class="s">&#34;e.salary&#34;</span> <span class="o">-</span> <span class="n">$</span><span class="s">&#34;a.avg_salary&#34;</span><span class="o">).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;salary_diff&#34;</span><span class="o">)</span>
      <span class="o">)</span>

    <span class="c1">// Output results
</span><span class="c1"></span>    <span class="n">dfResult</span><span class="o">.</span><span class="n">show</span>

    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="o">()</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>このコードを実行する前に MySQL の <a href="https://dev.mysql.com/doc/refman/8.0/en/query-log.html">general query log</a> を ON にする。</p>
<pre><code>mysql&gt; set global general_log = 'ON';
Query OK, 0 rows affected (0.02 sec)

mysql&gt; show global variables like 'general_log%';
+------------------+--------------------------------------+
| Variable_name    | Value                                |
+------------------+--------------------------------------+
| general_log      | ON                                   |
| general_log_file | /usr/local/var/mysql/MacBook-Pro.log |
+------------------+--------------------------------------+
2 rows in set (0.01 sec)
</code></pre><p>これによって MySQL に対して発行されたクエリがログとして記録されるようになる。</p>
<p>直感的に snapshot isolation になっているのであれば MySQL に対する select 文は1回だけ発行されるはずである。<br>
しかし前述のとおり <code>RDD</code> や <code>DataFrame</code> の処理は途中の状態を通常保存せず、同じ <code>RDD</code> や <code>DataFrame</code> を参照していたとしても再計算される。<br>
上記コードの例だと <code>dfEmployee</code> が2回参照されている。</p>
<p>コードを実行すると general query log には次のように、データ取得のための select 文が2つ記録されていた。<br>
それぞれ <code>join()</code> の左右の table のデータソースを示している。</p>
<pre><code>8 Query     SELECT `id`,`salary`,`department_id` FROM test_fout_dsp.employees
7 Query     SELECT `salary`,`department_id` FROM test_fout_dsp.employees WHERE (`department_id` IS NOT NULL)
</code></pre><p>2つの select 文はそれぞれ別のクエリ、トランザクションとして発行されている。<br>
したがって前者の select 文が実行された後、後者の select 文が実行される前に別のトランザクションにより <code>employees</code> が更新されたり挿入・削除されたりすると non-repeatable read や phantom read が発生してしまうのである。</p>
<p>今回はデータソースへのアクセスを確認するためにデータソースとして MySQL を使ったが、同じことはファイルや他の DB など別のデータソースで起こりうる。</p>
<h2 id="回避策">回避策</h2>
<p>プロダクト運用上、non-repeatable read や phantom read が発生しうる状況というのは多くの場合で厄介である。<br>
一時的なデータソースの状態に依存して問題が発生するため、バグの原因追求がとても困難だからだ。<br>
見た目上の分離レベルを強くし、これらを避けるには2つの方法が考えられる。</p>
<h3 id="immutable-なデータにのみアクセスする">immutable なデータにのみアクセスする</h3>
<p>単純な話で non-repeatable read や phantom read が発生するようなデータソースを参照しなければよい。<br>
例えばユーザの行動ログのような蓄積されていくデータが hour 単位で partitioning されているような場合、基本的に過去の partition に対しては変更や挿入・削除は行われない。<br>
このような partition にアクセスする分には前述のような厄介な問題は起こらない。</p>
<h3 id="cache-する">cache する</h3>
<p>データソースから読み取った結果の <code>DataFrame</code> に対して <code>cache()</code> または <code>persist()</code> をするとよい。</p>

<blockquote>
  <p>Spark SQL can cache tables using an in-memory columnar format by calling spark.catalog.cacheTable(&#34;tableName&#34;) or dataFrame.cache().</p>
  <footer>&#8212; Apache Spark <cite title="Caching Data In Memory"><a rel="noopener nofollow" href="https://spark.apache.org/docs/latest/sql-performance-tuning.html#caching-data-in-memory">Caching Data In Memory</a></cite></footer>
</blockquote>

<p>前述のコードで <code>dfEmployee</code> に対して <code>.cache()</code> をした場合、MySQL へのデータ取得のための select 文の発行は1回になった。<br>
大きなデータソースを <code>cache()</code> するときだけメモリや HDD 容量に気をつけておきたい。</p>
<h1 id="まとめ">まとめ</h1>
<p><code>DataFrame</code> はいわゆる RDBMS を操作しているように見えてしまうところが難しいところだろうか。<br>
「データ指向アプリケーションデザイン」に至極真っ当なことが書いてあったので、その言葉を借りて締めておく。</p>

<blockquote>
  <p>何も考えずにツールを信じて依存するのではなく、並行性の問題にはどういったものがあるのか、そしてそれらを回避するにはどうしたら良いのかを、しっかり理解しなければなりません。そうすれば、信頼性があり、正しく動作するアプリケーションを手の届くツールを使って構築できるようになります。</p>
  <footer>&#8212; Martin Kleppmann <cite title="データ指向アプリケーションデザイン"><a rel="noopener nofollow" href="https://www.oreilly.co.jp/books/9784873118703/">データ指向アプリケーションデザイン</a></cite></footer>
</blockquote>]]></content>
  </entry>
  <entry>
    <title>Apache Spark 3.0.0 について調べた</title>
    <author>
      <name>soonraah</name>
      <uri>https://soonraah.github.io/</uri>
    </author>
    <id>https://soonraah.github.io/study-spark-3-0-0/</id>
    <updated>2020-07-12T00:00:00Z</updated>
    <published>2020-07-12T00:00:00Z</published>
    <content type="html"><![CDATA[

<figure class="center">
<img
class="mb-2 mx-auto leading-none shadow-xl"
src="/image/logo/Apache_Spark_logo.png">
<figcaption class="text-sm text-right text-raven-500">
<p> <a href="https://spark.apache.org/">Apache Spark</a></p>
</figcaption>
</figure>

<h2 id="はじめに">はじめに</h2>
<p>Apache Spark 3.0.0 がリリースされました。</p>
<ul>
<li><a href="https://spark.apache.org/releases/spark-release-3-0-0.html">Spark Release 3.0.0</a></li>
</ul>
<p>release note を見て個人的に気になったところなど簡単に調べました。<br>
書いてみると Databricks の記事へのリンクばっかになってしまった…</p>
<h2 id="全体感">全体感</h2>
<p>こちらの記事を読めば全体感は OK.</p>
<ul>
<li><a href="https://databricks.com/jp/blog/2020/06/18/introducing-apache-spark-3-0-now-available-in-databricks-runtime-7-0.html">Introducing Apache Spark 3.0</a></li>
</ul>
<p>公式の release note には</p>
<blockquote>
<p>Python is now the most widely used language on Spark.</p>
</blockquote>
<p>とあってそうなん？ってなったけど、こちらの記事だと</p>
<blockquote>
<p>Python is now the most widely used language on Spark and, consequently, was a key focus area of Spark 3.0 development. 68% of notebook commands on Databricks are in Python.</p>
</blockquote>
<p>と書いてありどうやら Databricks の notebook の話らしく、だったらまあそうかなという感じ。<br>
プロダクトコードへの実装というよりは、アドホック分析や検証用途の話なんでしょう。</p>
<h2 id="project-hydrogen-accelerator-aware-scheduler">[Project Hydrogen] Accelerator-aware Scheduler</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-24615">SPARK-24615</a></li>
</ul>
<p>Spark 上で deep learning できるようにすることを目指す Project Hydrogen、その3つの大きな目標のうちの一つ。</p>
<ul>
<li><a href="https://www.slideshare.net/MatthewStubbs6/big-data-ldn-2018-project-hydrogen-unifying-ai-with-apache-spark/26">Big Data LDN 2018: PROJECT HYDROGEN: UNIFYING AI WITH APACHE SPARK</a></li>
</ul>
<p>YARN や Kubernetes では GPU や FPGA を扱えるようになっているので Spark でも扱えるようにしたいというモチベーション。<br>
<a href="https://spark.apache.org/docs/3.0.0/running-on-yarn.html#resource-allocation-and-configuration-overview">Spark のドキュメント</a> によると</p>
<blockquote>
<p>For example, the user wants to request 2 GPUs for each executor. The user can just specify <code>spark.executor.resource.gpu.amount=2</code> and Spark will handle requesting <code>yarn.io/gpu</code> resource type from YARN.</p>
</blockquote>
<p>のようにして executor に GPU リソースを要求できるみたいです。</p>
<h2 id="adaptive-query-execution">Adaptive Query Execution</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-31412">SPARK-31412</a></li>
</ul>
<p>平たく言うと実行時に得られる統計情報を使って plan を最適化すると、静的に生成された plan より効率化できるよねという話。<br>
<code>spark.sql.adaptive.enabled=true</code> にすることで有効になります。</p>
<p>処理の途中で中間生成物が materialize されるタイミングで、その時点の統計情報を使って残りの処理を最適化する、というのを繰り返します。</p>
<p>Spark 3.0.0 では以下3つの AQE が実装されました。</p>
<ul>
<li>Coalescing Post Shuffle Partitions</li>
<li>Converting sort-merge join to broadcast join</li>
<li>Optimizing Skew Join</li>
</ul>
<p>Spark 2 以前だとこのあたりは実行しつつチューニングするような運用になりがちでした。<br>
特に skew の解消は salt を追加したりなど面倒だったりします。<br>
これらが自動で最適化されるというのは運用上うれしいところ。<br>
急なデータ傾向の変化に対しても自動で最適化して対応できるという面があります。</p>
<p>AQE に関してもやはり Databricks の解説記事がわかりやすいです。<br>
図もいい感じ。</p>
<ul>
<li><a href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html">Adaptive Query Execution: Speeding Up Spark SQL at Runtime</a></li>
</ul>
<h2 id="dynamic-partition-pruning">Dynamic Partition Pruning</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-11150">SPARK-11150</a></li>
</ul>
<p>こちらも AQE 同様にクエリのパフォーマンスを改善する目的で導入されたもの。<br>
改善幅は AQE より大きいようです。<br>
やはり実行時の情報を使って partition pruning を行い、不要な partition の参照を減らすという方法。</p>
<p>主に <a href="https://ja.wikipedia.org/wiki/%E3%82%B9%E3%82%BF%E3%83%BC%E3%82%B9%E3%82%AD%E3%83%BC%E3%83%9E">star schema</a> における join 時のように、静的には partition pruning が行えない場合を想定しています。<br>
比較的小さいことが多いと思われる dimension table 側を broadcast する broadcast join において、broadcast された情報を fact table の partition pruning に利用するというやり口。</p>
<ul>
<li><a href="https://www.slideshare.net/databricks/dynamic-partition-pruning-in-apache-spark">Dynamic Partition Pruning in Apache Spark</a></li>
</ul>
<h2 id="structured-streaming-ui">Structured Streaming UI</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-29543">SPARK-29543</a></li>
</ul>
<p>&quot;Structured Streaming&quot; というタブが UI に追加された件。<br>
Spark のドキュメントに例があります。</p>
<blockquote>
<p><img src="https://spark.apache.org/docs/3.0.0/img/webui-structured-streaming-detail.png" alt="Structured Streaming Query Statistics"></p>
</blockquote>

<blockquote>
  <p> </p>
  <footer>&#8212; Apache Spark <cite title="Structured Streaming Tab"><a rel="noopener nofollow" href="https://spark.apache.org/docs/3.0.0/web-ui.html#structured-streaming-tab">Structured Streaming Tab</a></cite></footer>
</blockquote>

<p>Spark 2.4 系では Structured Streaming を動かしていてもせいぜい job や stage が増えていくという味気ないものしか見えませんでした。<br>
Spark 3.0.0 で実際に動かしてみたけど欲しかったやつ！という感じ。<br>
ストリーム処理では入力データ量の変化の可視化がマストだと思ってます。</p>
<h2 id="catalog-plugin-api">Catalog plugin API</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-31121">SPARK-31121</a></li>
<li><a href="https://docs.google.com/document/d/1zLFiA1VuaWeVxeTDXNg8bL6GP3BVoOZBkewFtEnjEoo/edit">SPIP: Spark API for Table Metadata</a></li>
</ul>
<p>これまでは CTAS (Create Table As Select) の操作はあったが、外部のデータソースに対して DDL 的な操作をする API が足りていませんでした。<br>
CTAS も挙動に実装依存の曖昧さがありました。<br>
そこで <code>create</code>, <code>alter</code>, <code>load</code>, <code>drop</code> 等のテーブル操作をできるようにしたという話。</p>
<p>ドキュメントの <a href="https://spark.apache.org/docs/3.0.0/sql-ref-syntax.html#ddl-statements">DDL Statements</a> のあたりを読め何ができるかわかります。<br>
以前のバージョンでも一部のデータソースについてはできた模様 (ex. <a href="https://spark.apache.org/docs/2.4.6/sql-migration-guide-hive-compatibility.html#supported-hive-features">Hive</a>)。</p>
<p>今の自分の業務では Spark から DDL を扱うようなことはしないのでそれほど恩恵は感じられません。<br>
notebook からアドホックな Spark のバッチを動かすというような使い方をしていればうれしいかもしれません。</p>
<h2 id="add-an-api-that-allows-a-user-to-define-and-observe-arbitrary-metrics-on-batch-and-streaming-queries">Add an API that allows a user to define and observe arbitrary metrics on batch and streaming queries</h2>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-29345">SPARK-29345</a></li>
</ul>
<p>クエリの途中の段階で何らかの metrics を仕込んでおいて callback 的にその metrics にアクセスできる仕組み。<br>
<a href="https://spark.apache.org/docs/3.0.0/api/scala/org/apache/spark/sql/Dataset.html#observe(name:String,expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset%5BT%5D">Dataset#observe() の API ドキュメント</a> を読むのが一番早いです。<br>
この例ではストリーム処理を扱っているが、バッチ処理の例を自分で書いて試してみました。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Register listener
</span><span class="c1"></span><span class="n">spark</span>
  <span class="o">.</span><span class="n">listenerManager</span>
  <span class="o">.</span><span class="n">register</span><span class="o">(</span><span class="k">new</span> <span class="nc">QueryExecutionListener</span> <span class="o">{</span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">onSuccess</span><span class="o">(</span><span class="n">funcName</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">qe</span><span class="k">:</span> <span class="kt">QueryExecution</span><span class="o">,</span> <span class="n">durationNs</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">num</span> <span class="k">=</span> <span class="n">qe</span><span class="o">.</span><span class="n">observedMetrics</span>
        <span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="s">&#34;my_metrics&#34;</span><span class="o">)</span>
        <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getAs</span><span class="o">[</span><span class="kt">Long</span><span class="o">](</span><span class="s">&#34;num&#34;</span><span class="o">))</span>
        <span class="o">.</span><span class="n">getOrElse</span><span class="o">(-</span><span class="mf">100.0</span><span class="o">)</span>

      <span class="n">println</span><span class="o">(</span><span class="s">s&#34;num of data: </span><span class="si">$num</span><span class="s">&#34;</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">onFailure</span><span class="o">(</span><span class="n">funcName</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">qe</span><span class="k">:</span> <span class="kt">QueryExecution</span><span class="o">,</span> <span class="n">exception</span><span class="k">:</span> <span class="kt">Exception</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{}</span>
  <span class="o">})</span>

<span class="c1">// Make DataFrame
</span><span class="c1"></span><span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="nc">Seq</span>
  <span class="o">.</span><span class="n">range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1000</span><span class="o">)</span>
  <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="k">_</span><span class="o">,</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="s">&#34;c&#34;</span><span class="o">)(</span><span class="nc">Random</span><span class="o">.</span><span class="n">nextInt</span><span class="o">(</span><span class="mi">3</span><span class="o">)),</span> <span class="n">math</span><span class="o">.</span><span class="n">random</span><span class="o">()))</span>
  <span class="o">.</span><span class="n">toDF</span><span class="o">(</span><span class="s">&#34;id&#34;</span><span class="o">,</span> <span class="s">&#34;type&#34;</span><span class="o">,</span> <span class="s">&#34;value&#34;</span><span class="o">)</span>

<span class="c1">// Observe and process
</span><span class="c1"></span><span class="k">val</span> <span class="n">dfResult</span> <span class="k">=</span> <span class="n">df</span>
  <span class="o">.</span><span class="n">observe</span><span class="o">(</span><span class="s">&#34;my_metrics&#34;</span><span class="o">,</span> <span class="n">count</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;*&#34;</span><span class="o">).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;num&#34;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;type&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">agg</span><span class="o">(</span><span class="n">avg</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;value&#34;</span><span class="o">).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;avg_value&#34;</span><span class="o">))</span>

<span class="c1">// Run
</span><span class="c1"></span><span class="n">dfResult</span><span class="o">.</span><span class="n">show</span>
</code></pre></div><p>これを動かしたときの出力は次のようになりました。</p>
<pre><code>+----+------------------+
|type|         avg_value|
+----+------------------+
|   c|0.5129435063033314|
|   b|0.4693004460694317|
|   a|0.4912087482418599|
+----+------------------+

num of data: 1000
</code></pre><p><code>observe()</code> はその出力の DataFrame に対して schema やデータの中身を変更することはありません。<br>
metrics を仕込むのみ。<br>
logical plan を出力してみると <code>observe()</code> を入れることにより途中に <code>CollectMetrics</code> という plan が挿入されていました。<br>
ソースを見ると accumulator を使っている模様。<br>
なので <code>observe()</code> の集計のみで一度 job が動くわけではなく、クエリが2回走るという感じではありません。<br>
全体の処理の中でひっそりと accumulator で脇で集計しておくといった趣でしょうか。</p>
<p>これは結構有用だと思います。<br>
例えば何らかの集計をやるにしてもその最初や途中で、例えば入力データは何件あったか？みたいなことをログに出しておきたいことがあります。<br>
というか accumulator で頑張ってそういうものを作ったことがある…<br>
これがフレームワーク側でサポートされるのはうれしいです。</p>
<h2 id="まとめ">まとめ</h2>
<p>2つのダイナミックな最適化に期待大。<br>
気が向いたら追加でまた調べるかもしれません。</p>]]></content>
  </entry>
</feed>
