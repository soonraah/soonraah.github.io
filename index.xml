<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Froglog</title>
    <link>https://soonraah.github.io/</link>
    <description>Recent content on Froglog</description>
    <image>
      <title>Froglog</title>
      <url>https://soonraah.github.io/47</url>
      <link>https://soonraah.github.io/47</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Thu, 09 May 2024 22:30:00 +0900</lastBuildDate><atom:link href="https://soonraah.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Data Contract CLI から考える Data Contracts ファーストのデータパイプラインの未来</title>
      <link>https://soonraah.github.io/posts/data-contract-cli/</link>
      <pubDate>Thu, 09 May 2024 22:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/data-contract-cli/</guid>
      <description>このポストについて Data Contract CLI を触ってみたところ、面白かったのとこれからのデータパイプライン開発について思うところがあったので書いてみる。
Data Contract CLI とは？ datacontract/datacontract-cli
Data Contract CLI は data contracts を運用するためのオープンソースのコマンドラインツールである。
data contracts の概念については以前の記事で詳しく書いているのでそちらをご参考いただければと。
ただしこちらの記事は1年前のものであり、今回取り上げる Data Contract CLI の登場などを含めて現在では data contracts を取り巻く状況も変わっている可能性があることに注意。
Data Contract CLI は Python で開発されており、pip でインストールすることができる。
この記事を書いている時点では v0.10.3 が最新であり、この記事の内容はこのバージョンに基づいている。
Data Contract CLI で扱う data contracts は YAML で定義される前提となっており、その仕様は datacontract/datacontract-specification で決められている。
この data contracts に対して Data Contract CLI では次のようなことが行える。
lint によるフォーマットチェック データソースに接続した上での schema やデータ品質のテスト data contracts の破壊的な変更の検出 JSON Schema や dbt など、他の形式からの／へのインポートとエクスポート 以下の図がイメージしやすい。</description>
    </item>
    
    <item>
      <title>読書メモ: DMBOK2 第8章 データ統合と相互運用性</title>
      <link>https://soonraah.github.io/posts/dmbok-chapter-8/</link>
      <pubDate>Thu, 04 Apr 2024 06:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/dmbok-chapter-8/</guid>
      <description>このポストについて DMBOK2 を読み進めていくシリーズ。
今回は第8章「データ統合と相互運用性」について。
業務で扱っているデータ基盤はデータ統合が不完全であるため、なんとかしたいと考えている。
以降、特に注釈のない引用は DMBOK2 第8章からの引用とする。
データストレージと相互運用性とは データ統合と相互運用性 (DII: Data Integration and Interoperability) は次のように定義されている。
アプリケーションや組織内および相互間におけるデータの移動と統合を管理する
データの移動を効率的に管理することがそのビジネス上の意義となる。
ほとんどの組織には数多くのデータストアがあり、組織内・組織間でデータを移動させることは IT 組織の重要な任務となっている。
複雑さとそれに伴うコストを管理するために、全社的な視点からデータ統合を設計しなければならない。
データウェアハウスなどのデータハブによりアプリケーション間のインターフェースの数を削減することができる。
DII のゴールは以下
法令を遵守しながら、必要とするフォーマットと時間枠でデータを安全に提供する。 共有のモデルとインターフェースを開発することでソリューションを管理するコストと複雑さを軽減する。 重要なイベントを特定し、アラートとアクションを自動的に起動する。 ビジネスインテリジェンス、アナリティクス、マスターデータ管理、業務効率化の取り組みをサポートする。 概念・用語など 抽出、変換、取込 DII の中心にあるのが、抽出 (Extract)、変換 (Transform)、取込 (Load) のいわゆる ETL という基本プロセス。
抽出 ソースから必要なデータを選択し、抽出する 抽出されたデータはディスク上やメモリ上にステージングされる 業務システムで実行される場合は、少ないリソースを利用するように設計する 変換 ソースデータを変換してターゲットデータストアの構造と互換性を持つようにする フォーマット変更、構造の変更、意味的変換、重複排除、並べ替えなどがある 取込 ターゲットシステムに物理的に格納されるか、提供される ELT ターゲットシステムにより多くの変化機能がある場合は、プロセスの順序を ELT にすることができる データレイクへの取込を行うビッグデータ環境では一般的 レイテンシ ソースシステムでデータが生成されてから、ターゲットシステムでそのデータが利用可能になるまでの時間差。
アプローチによってレイテンシの高低が異なる。
バッチ 利用者や自動的な要求に応えて、定期的にアプリケーションや組織間を一定量まとまって移動させる レイテンシは高いが大量データを処理するときのパフォーマンスがいい 低レイテンシを実現するためのマイクロバッチもある 変更データキャプチャ データの変更 (挿入・変更・削除) のデータセットを監視し、その差分をターゲットのシステムに渡す DBMS のアクティビティログをコピーし、処理する形で行われることもある 準リアルタイムとイベント駆動 設定された予定により1日を通して分割された少量のデータセットで処理されたり、データ更新などのイベントが発生したときに処理されたりする 一般的にエンタープライズ・サービス・バスを利用して実装される 非同期 データ提供側は受信側の更新確認を待たずに処理を続行する リアルタイム、同期 次のトランザクションを実行する前に、他のアプリケーションからの確認を受け取るまで実行プロセスが待機する 非同期と比べて状態管理の負担が少ないが、他のトランザクションをブロックしたり遅延させたりすることもある 低レイテンシまたはストリーミング イベントが発生したときにシステムからリアルタイムで連続して流れる リプリケーション 分析やクエリによるパフォーマンス低下を防ぐために、トランザクション処理環境にリプリケーション (複製) を使用することがある。</description>
    </item>
    
    <item>
      <title>読書メモ: DMBOK2 第6章 データストレージとオペレーション</title>
      <link>https://soonraah.github.io/posts/dmbok-chapter-6/</link>
      <pubDate>Sun, 17 Mar 2024 21:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/dmbok-chapter-6/</guid>
      <description>このポストについて DMBOK2 を読み進めていくシリーズ。
今回は第5章「データストレージとオペレーション」について。
主にデータベースの運用に関する内容となっており、いわゆるデータベースエンジニアの人には当たり前の話？が書かれている。
以降、特に注釈のない引用は DMBOK2 第6章からの引用とする。
データストレージとオペレーションとは 以下のように定義されている。
データの価値を最大化するために、永続化されるデータを設計し、実装し、サポートすること
主にデータベース管理者 (DBA: Database Administrators) が行うことになる。
次の2つのアクティビティが含まれる。
データベースサポート: データベース環境の初期実装からデータの取得、バックアップ、廃棄までのデータライフサイクル関連アクティビティ データベース技術サポート: 技術要件を決め、技術的なアーキテクチャを定義し、技術を実装・管理する 事業がデータに依存する企業においてはデータストレージとオペレーションのアクティビティは事業の継続性のために必要不可欠である。
ゴールは次のとおり
データライフサイクル全体にわたるデータの可用性を管理する データ資産の完全性を保証する データ処理の性能を管理する 概念・用語など データベースアーキテクチャの種類 集中型データベース: 単一システム内で使うデータを一箇所にまとている 分散型データベース: 多数のノードにデータが配置される 連邦型データベース: 自立した複数のデータベースシステムを単一の連邦型データベースに割り当てる 仮想化／クラウドプラットフォーム: クラウド上のデータベースを実装 データベース処理のタイプ ACID: トランザクションの信頼性のための制約 原子性 (Atomicity): 操作はすべて実行されるかまったく実行されないかのどちらか 一貫性 (Consistency): トランザクションはシステムが定義するすべてのルールを常に満たさなければならない 独立性 (Isolation): 各トランザクションは実行中の他のトランザクションの影響を受けない 永続性 (Durability): トランザクションは完了すると元に戻せない BASE: データの量と多様性を受けた、ACID とは異なる考え 基本的に利用可能 (Basically Available): ノードの障害発生時もあるレベル以上の可用性を保証する ソフトステート (Soft State): データは一定の変動状態にある 最終的な一貫性の確保 (Eventual Consistency): データは最終的にすべてのノードで一貫性を保つが、各トランザクションの一貫性が常に確保されているわけではない CAP: 分散システムでは以下のどれか2つしか満たせない 一貫性 (Consistency): システムは常に想定どおり動作できなければならない 可用性 (Availability): システムは要求時に利用可能でなければならに 分断耐性 (Partition Tolerance): システムは部分的な障害の発生時に運用を続行できなければならない データベース構成 上から順により制御された構造であり、かつ古くからあるものとなっている。</description>
    </item>
    
    <item>
      <title>読書メモ: DMBOK2 第5章 データモデリングとデザイン</title>
      <link>https://soonraah.github.io/posts/dmbok-chapter-5/</link>
      <pubDate>Sun, 21 Jan 2024 21:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/dmbok-chapter-5/</guid>
      <description>このポストについて DMBOK2 を読み進めていくシリーズ。
今回は第5章「データモデリングとデザイン」について。
第4章でデータモデリングについて言及されていたが、ここで詳しく見ていく。
以降、特に注釈のない引用は DMBOK2 第5章からの引用とする。
データモデリングとは データモデリングとは、データ要件を洗い出し、分析し、取扱スコープを決めるプロセスであり、データ要件を記述し伝えるために、明確に定義されたデータモデルと呼ばれる様式が用いられる。このプロセスは反復的であり、概念、論理、物理モデルが含まれる。
第4章 データアーキテクチャ ではエンタープライズ・データモデルという言葉が登場したが、この第5章ではそのデータモデルがより具体的に解説されている。
データモデルは効果的なデータマネジメントを行うにあたり必要なものとされている。
次のような意義がある。
データに関する共通語彙を提供する 組織のデータや情報システムに関しての明示的な知識を捉え文書化する プロジェクトにおいて主なコミュニケーションツールとして使われる アプリケーションをカスタマイズ、統合、リプレースする際の出発点となる データモデルは組織が把握した現状、またはあるべき姿として組織のデータを記述する。
カテゴリー情報、リソース情報、業務イベント情報、詳細取引情報がその対象となる。
データモデルの構成要素 ほとんどのデータモデルはエンティティ、リレーションシップ、属性、ドメインにより構成される。
エンティティ エンティティとはある組織が情報を収集する対象のこと。
組織が使う名詞。
以下は Marmaid の Entity Relationship Diagrams でエンティティを表現した例。
学校を例にしており、学生、コース、インストラクタがエンティティとして表されている。
--- title: Entity --- erDiagram Student Course Instructor リレーションシップ リレーションシップはエンティティ間の関連性を表す。
以下の例は学生はコースを &amp;ldquo;履修する&amp;rdquo;、インストラクタはコースを &amp;ldquo;教える&amp;rdquo; ことを表している。
--- title: Relationship --- erDiagram Student }|--|{ Course : take Instructor }|--|{ Course : teach 線の両端の形はリレーションシップの cardinality (多重度) を表している。
この場合ではそれぞれカラスの足状になっているが、それぞれ「1またはそれ以上」という意味で、「学生は1つ以上のコースを履修する」「コースは1人以上の学生に履修される」ということを表している。
リレーションシップの arity (項数) は一般的には二項型 (バイナリリレーションシップ) であることが多く、2つのエンティティ間の関係を表す。</description>
    </item>
    
    <item>
      <title>読書メモ: DMBOK2 第4章 データアーキテクチャ</title>
      <link>https://soonraah.github.io/posts/dmbok-chapter-4/</link>
      <pubDate>Sat, 06 Jan 2024 20:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/dmbok-chapter-4/</guid>
      <description>このポストについて DMBOK2 を読み進めていくシリーズ。
今回は第4章「データアーキテクチャ」について。
やや抽象度が高い内容となっており、理解が難しいと感じた。
以降、特に注釈のない引用は DMBOK2 第4章からの引用とする。
データアーキテクチャとは データアーキテクチャの定義は以下のとおり。
企業の (組織構造に関係なく) データニーズを明確にし、ニーズに合うマスターとなる青写真を設計し、維持する。マスターとなる青写真を使ってデータ統合を手引し、データ資産をコントロールし、ビジネス戦略に合わせてデータへの投資を行う。
業務戦略と技術実装の間を橋渡しすることがデータアーキテクチャの目的。
以下が成果物、つまり前述の定義の「マスターとなる青写真」となる。
データの保存と処理の要件 企業の現在のデータ要件と長期のデータ要件を満たす構造や計画の立案 これらを記述するためにエンタープライズ・データモデル (EDM) とデータフロー設計が必要となる。(後述)
エンタープライズアーキテクチャ データアーキテクチャとビジネス、アプリケーション、テクニカルなど他の領域のアーキテクチャとの違いは下表のようになる。
それぞれが影響を及ぼし合う関係となっており、データアーキテクチャはビジネスアーキテクチャによって決められる。
ドメイン エンタープライズ・ビジネスアーキテクチャ エンタープライズ・データアーキテクチャ エンタープライズ・アプリケーションアーキテクチャ エンタープライズ・テクニカルアーキテクチャ 目的 企業が顧客や他のステークホルダに どのように価値提供しているかを 明らかにする データがどのように整理・管理されるべきか記述する 企業内アプリケーションの 構造と機能を記述する システムを稼働して価値を 提供するために必要な 物理実装技術を記述する 要素 ビジネスモデル、業務プロセス、業務能力、サービス、イベント、戦略、語彙集 データモデル、データ定義、データマッピング仕様、データフロー、構造化データの API ビジネスシステム、ソフトウェアパッケージ、データベース テクニカルプラットフォーム、ネットワーク、セキュリティ、統合ツール 依存関係 他のドメインに対する要件を設定する ビジネスアーキテクチャによって作られ、要求されるデータを管理する ビジネス要件に基づいて特定されたデータに対応する アプリケーションアーキテクチャを提供し実行する 役割 ビジネスアーキテクトとアナリスト、ビジネス・データスチュワード データアーキテクトとモデラー、データスチュワード アプリケーションアーキテクト インフラストラクチャアーキテクト DMBOK2 表6 アーキテクチャ領域
web 系エンジニア的には「アーキテクチャ」という言葉だけ聞くとクラウドサービスのアイコンを使ってアーキテクチャ図をお絵描きするよくあるアレを想像しがちだが、データアーキテクチャはそれではないということ。
(DMBOK2 の分類ではそれはテクニカルアーキテクチャの領域)
エンタープライズ・データモデル (EDM) EDM とは
全体的でエンタープライズレベルの実装に依存しない概念または論理モデルであり、企業全体にわたるデータに関して一貫した共通のビューを提供する。
次の図は概念・論理モデルと物理的なデータモデルとの関係を示す。
垂直方向には対象領域ごとに対応する異なるレベルのモデルが関係している。
水平方向には同じレベルのモデルがあり、対象領域をまたいでエンティティが関係し合う。
DMBOK2 図23 エンタープライズ・データモデル</description>
    </item>
    
    <item>
      <title>読書メモ: DMBOK2 第15章 データマネジメント成熟度アセスメント</title>
      <link>https://soonraah.github.io/posts/dmbok-chapter-15/</link>
      <pubDate>Sat, 30 Dec 2023 01:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/dmbok-chapter-15/</guid>
      <description>このポストについて DMBOK2 を読み進めていくシリーズ。
今回は第15章「データマネジメント成熟度アセスメント」について。
データマネジメントの導入において重要な役割を果たしそうなので早めに確認しておきたかった。
以降、特に注釈のない引用は DMBOK2 第15章からの引用とする。
データマネジメント成熟度アセスメントとは データマネジメント成熟度アセスメント (DMMA: Data Management Maturity Assessment) はその名の通り、組織のデータマネジメントのレベルの評価に基づくプロセス改善の取り組みのこと。
能力成熟度アセスメント (CMA: Capability Maturity Assessment) というものがあり、それのデータマネジメント版が DMMA。
CMA では
成熟度モデルは進化の観点から定義され、それにはプロセスの特性を表すレベルが使用される。組織がプロセスの特性を理解すると、組織は成熟度を測り、その能力を向上させるための計画を立てることができる。(中略) 新しいレベルに上がる度にプロセスの実行はより一貫性を増し、予測可能な状態となり、信頼性が高くなる。
レベルは通常0~5の6段階で表される。
DMMA は
全体的なデータマネジメントを評価するために使用したり、単一の知識領域や、単一のプロセスに焦点を当てて使用したりできる。どのような点に焦点を当てたとしても、DMMA はデータマネジメント業務の健全性と有効性について、業務と IT の視点のギャップを埋めるために役立つ。
組織が DMMA を実施する理由は、規制への対応、データガバナンス、プロセス改善、etc.
DMMA の第一のゴールはデータマネジメント活動の現状を評価することであり、それにより改善計画を立てることができるようになる。
アセスメントレベル 以下はアセスメントレベルの概要。
データマネジメントの各知識領域 (ex. データガバナンス、メタデータ管理、etc.) ごとにレベルが評価される。
level 0: 能力が欠如した状態 データマネジメントの取り組みがない level 1: 初期／場当たり的な状態 限られたツールセットを用いた一般的なデータマネジメント ガバナンスは低レベル データ処理は一部の専門家に依存し、役割や責任は部門別に定義されている level 2: 反復可能な状態 組織は一元化された共通ツールを使い始める 役割は明確化されており、一部の専門家のみに依存しない level 3: 定義された状態 拡張可能なプロセスの導入と制度化 組織全体である程度統制されたデータの複製 データ品質全体の総体的な向上 組織的なポリシー定義と統制 level 4: 管理された状態 新しいプロジェクトやタスクから得られる結果が予測され、リスクの管理が始まる データマネジメントに成果に対する評価尺度が含まれる データマネジメント用の標準ツールが集中管理計画とガバナンス機能に組み合わされている level 5: 最適化された状態 活動の成果は十分予測可能に 組織は継続的な改善に重点を置く 十分理解された評価尺度を使ってデータ品質とプロセスが管理・測定される 次のように各知識領域ごとに可視化することができる。</description>
    </item>
    
    <item>
      <title>現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる</title>
      <link>https://soonraah.github.io/posts/load-csv-data-into-bq-by-dlt/</link>
      <pubDate>Wed, 20 Dec 2023 09:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/load-csv-data-into-bq-by-dlt/</guid>
      <description>このポストについて 前回の記事 dlt 入門 - ELT の Extract と Load を担う data load tool では dlt の概要を説明した。
この記事ではそれを踏まえ、dlt を使って CSV ファイルを BigQuery に load するという一連の開発作業をやってみる。
現実の CSV はそのまま使えなかったりするので、BigQuery に入れるまでに泥臭い前処理のような作業があることが多い。
そのへんもまとめて dlt でやってみるとこんな感じ、というのが示せるとよい。
やりたいこと 個人で管理しているお金の情報を個人用の BigQuery に置きたい、という要件を想定。
データ概要 具体的には MoneyForward のデータを load していく。
個人では API を利用できないので、web UI から export できる CSV のデータで収入・支出詳細と資産推移月次を対象とする。
CSV の export 方法は以下を参照。
入出金履歴はダウンロードできますか – マネーフォワード MEサポートサイト データの内容は次のようになっている。
収入・支出詳細_2023-11-01_2023-11-30.csv &amp;#34;計算対象&amp;#34;,&amp;#34;日付&amp;#34;,&amp;#34;内容&amp;#34;,&amp;#34;金額（円）&amp;#34;,&amp;#34;保有金融機関&amp;#34;,&amp;#34;大項目&amp;#34;,&amp;#34;中項目&amp;#34;,&amp;#34;メモ&amp;#34;,&amp;#34;振替&amp;#34;,&amp;#34;ID&amp;#34; &amp;#34;1&amp;#34;,&amp;#34;2023/11/30&amp;#34;,&amp;#34;AMAZON.CO.JP&amp;#34;,&amp;#34;-2830&amp;#34;,&amp;#34;楽天カード&amp;#34;,&amp;#34;食費&amp;#34;,&amp;#34;食料品&amp;#34;,&amp;#34;&amp;#34;,&amp;#34;0&amp;#34;,&amp;#34;EPv92ZjQcOxgWQx_cLbhD1&amp;#34; &amp;#34;1&amp;#34;,&amp;#34;2023/11/24&amp;#34;,&amp;#34;東京ガス&amp;#34;,&amp;#34;-4321&amp;#34;,&amp;#34;楽天カード&amp;#34;,&amp;#34;水道・光熱費&amp;#34;,&amp;#34;ガス・灯油代&amp;#34;,&amp;#34;&amp;#34;,&amp;#34;0&amp;#34;,&amp;#34;r6wuQPfrIRS6aFpNYZE5Eh&amp;#34; &amp;#34;1&amp;#34;,&amp;#34;2023/11/24&amp;#34;,&amp;#34;給与 カ) フロッグログ&amp;#34;,&amp;#34;700000&amp;#34;,&amp;#34;みずほ銀行&amp;#34;,&amp;#34;収入&amp;#34;,&amp;#34;給与&amp;#34;,&amp;#34;&amp;#34;,&amp;#34;0&amp;#34;,&amp;#34;doettKpYyNp0Tml9KQQXm1&amp;#34; ヘッダーがあり、各列に名前が付いている。
encoding が CP932 であることに注意。</description>
    </item>
    
    <item>
      <title>dlt 入門 - ELT の Extract と Load を担う data load tool</title>
      <link>https://soonraah.github.io/posts/what-is-dlt/</link>
      <pubDate>Mon, 18 Dec 2023 23:50:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/what-is-dlt/</guid>
      <description>このポストについて このポストは datatech-jp Advent Calendar 2023 の18日目の投稿です。
web の記事で見かけた dlt というツールが気になったので調べてみた。
dlt の概要について書いていく。
What is dlt? https://dlthub.com/
dlt とは &amp;ldquo;data load tool&amp;rdquo; の略。
雑に言うとデータパイプラインにおける ELT の Extract と Load を行う ものとなっている。
主にベルリンとニューヨークに拠点を持つ dltHub 社によって開発されており、OSS の Python ライブラリとして提供されている。
次のような特徴を持つ。
プラットフォームではなくあくまでライブラリであることが強調されている つまり Airflow, GitHub Actions, Google Cloud Functions, ローカル環境などどこでも動かすことができる スケールアウト可能な分散処理ではない extract と load にまつわる反復的で平凡な作業をなくすことを目指している schema 推論や schema evolution をサポート 宣言的なコードでメンテナンスを楽にする incremental loading をサポート 豊富な source GA, Salesforce, Kinesis などいろいろ例が挙げられている 要は API からの取得も含めて JSON-like な形式で Python で読めるものなら何でも 豊富な destination BigQuery, Snowflake など主要なクラウド DWH DuckDB はローカルでの動作確認に便利 Airflow, dbt などとの連携がある CLI の提供もある その他、Glossary を見ておくとドキュメントが読みやすくなる。</description>
    </item>
    
    <item>
      <title>読書メモ: DMBOK2 第12章 メタデータ管理</title>
      <link>https://soonraah.github.io/posts/dmbok-chapter-12/</link>
      <pubDate>Sat, 09 Dec 2023 10:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/dmbok-chapter-12/</guid>
      <description>このポストについて DMBOK2 を読み進めていくシリーズ。
今回は第12章「メタデータ管理」について。
仕事でメタデータを扱い始めたので読んでおきたかった。
以降、特に注釈のない引用は DMBOK2 第12章からの引用とする。
メタデータとは 一般的な説明としては「データに関するデータ」とよく言われている。
データに関するデータはすべてメタデータなので、メタデータはとても幅広い内容となっている。
DMBOK2 ではメタデータの説明として図書館の例を挙げている。
そこには数十万の書籍と雑誌があるのに、図書目録がない。図書目録がなければ、読者は特定の本や特定のトピックの検索を開始する方法さえ分からないかもしれない。図書目録は、必要な情報 (図書館が所有する本と資料、保管場所) を提供するだけでなく、利用者が様々な着眼点 (対象分野、著者、タイトル) から資料を見つけることを可能にする。 (中略) メタデータを持たない組織は、図書目録のない図書館のようなものである。
データという資産を管理するためにも、データを利用するためにも、リスクマネジメントのためにもメタデータは必要となる。
メタデータの種類 メタデータはビジネス、テクニカル、オペレーショナルの3つに分類することができる。
ビジネスメタデータ 主にデータの内容と状態に重点を置く。
IT からは独立している。
dataset, table, column の定義と説明 業務ルール、変換ルール、計算方法、導出方法 データモデル etc. テクニカルメタデータ 技術的詳細やシステムに関する情報。
主に IT に関連している。
物理 database の table, column の名称 column のプロパティ アクセス権 etc. オペレーショナルメタデータ データの処理とアクセスの詳細を示す。
運用で得られる情報とも言える。
バッチプログラムのジョブ実行ログ データの抽出とその結果などの履歴 運用スケジュールの以上 etc. 以上、各種のメタデータで例に挙げたのはあくまで一部であり、現実にはもっと多くのメタデータが存在する。
メタデータを管理する意義 図書館の例からもわかるとおり、メタデータなしではデータを管理することはできない。
信頼性が高く管理されたメタデータにより、次のようなことができるようになる。
データのコンテキストを提供し、それによりデータ品質を測定可能にして信頼性を向上させる 業務効率の向上、および古いデータや誤ったデータの利用防止 データ利用者とエンジニアの間のコミュニケーションの改善 法令遵守の支援 etc. メタデータの管理が不十分だと次のようなことが起こる。
一貫性のないデータ利用と誤った定義によるリスク メタデータは複製されて保管されることによる冗長性 利用者の信頼性低下 etc. メタデータアーキテクチャ メタデータの内容は幅広いがしたがってその取得元も幅広く、ビジネス用語集、BI ツール、モデリングツール、等々が挙げられる。</description>
    </item>
    
    <item>
      <title>CDC &#43; Apache Iceberg で Amazon Athena にデータを取り込む</title>
      <link>https://soonraah.github.io/posts/ingest-data-into-athena-by-cdc-and-iceberg/</link>
      <pubDate>Sun, 03 Dec 2023 22:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/ingest-data-into-athena-by-cdc-and-iceberg/</guid>
      <description>このポストについて このポストは Distributed computing Advent Calendar 2023 の3日目の記事になります。
1日目、2日目に続いて Apache Iceberg について書きますが、このポストでは Iceberg の実用例を書きます。
AWS DMS による CDC の結果を Apache Iceberg 形式にして Amazon Athena でクエリできるようにするという内容になります。
やっていることとしては Perform upserts in a data lake using Amazon Athena and Apache Iceberg | AWS Big Data Blog で紹介されている内容と近いですが、実務としての背景や工夫したところなどを書いていきます。
背景 私の所属する事業会社では日々プロダクトから様々なデータが発生しており、プロダクトの分析やレポーティング、ML など様々な用途で利用されている。
それを支える基盤としてデータ基盤が存在している。
データ基盤ではクエリエンジンとして Amazon Athena を使っている。
ストレージとしては S3 を使用しており、主に分析用として Parquet 形式でデータが置かれる。
ここに業務用の operational な database から日次でデータを取り込んでいる。
データソースは RDS (Aurora MySQL) であり、比較的大きなデータとなっている。
これまではこの RDS -&amp;gt; S3 のデータ取り込みには RDS の S3 snapshot export という機能を利用していた。</description>
    </item>
    
    <item>
      <title>読書メモ: DMBOK2 第3章 データガバナンス</title>
      <link>https://soonraah.github.io/posts/dmbok-chapter-3/</link>
      <pubDate>Sun, 19 Nov 2023 17:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/dmbok-chapter-3/</guid>
      <description>このポストについて DMBOK2 を読み進めていくシリーズ。
今回は第3章「データガバナンス」について。
DAMA ホイール図において中心に置かれているので先に読んでおこうと思った次第。
以降、特に注釈のない引用は DMBOK2 第3章からの引用とする。
データガバナンスとは データガバナンス (DG) の定義は、データ資産の管理 (マネジメント) に対して職務権限を通し統制 (コントロール) することである。統制とは計画を立て、実行を監視し、徹底させることを指す。
この定義からデータガバナンスがなぜ DAMA ホイール図の中心に位置しているかがわかる。
データガバナンスにより周囲の各知識領域の計画や実施を統制するという建付けになる。
データガバナンス (DG) のゴールは組織がデータを資産として管理できるようにすることである。DG はデータを資産として管理するための原則、ポリシー、プロセス、フレームワーク、評価指標を提供し、組織の各階層レベルでデータマネジメントアクティビティを牽引する。
これを可能にするためにデータガバナンスは持続可能であり、業務プロセスに組み込まれており、測定可能になっている必要がある。
データガバナンス組織 次の組織構成が一般的なデータガバナンスモデルであるとのこと。
DMBOK2 データガバナンス組織構成
右側がデータマネジメントを実施するロールになっており、左側がデータガバナンスによりデータマネジメントさせるロールになっている。
データガバナンス運営委員会が組織のデータガバナンスの頂点となっており、最も権限が強い。
各部門にはデータガバナンス評議会 (DGC) が置かれており、これらがポリシー・評価指標の開発などのデータガバナンスの取り組みや課題、報告を管理する。
データガバナンスオフィス (DGO) はデータスチュワード (後述) などで構成され、企業レベルのデータ定義とデータマネジメントにフォーカスする。
大規模、かつデータマネジメントの意識が高い組織でないとこういった組織構成はなかなか作れないのではと思った。
ライト版の図も欲しいところ。
DMBOK2 データ問題の報告経路
ポリシーやステークホルダー利害の不一致、権限、データ品質などなど、データに関する問題は上記のような報告経路をたどる。
データスチュワード制 データスチュワード制はデータとプロセスの実行責任と結果責任を表す最も一般的な呼び名であり、データ資産の効率的な統制と利用を確かなものとする。
ちょっとこの説明ではイメージしにくいかもしれない。
データガバナンスの文脈において、データスチュワードは現場を含む組織の各レベルでデータガバナンスを効かせるための活動を行う実務者だと理解した。
データスチュワードという職務名があってもいいが、そうでなくてもよいらしい。
次のようなことをやる。
核となるメタデータの作成と管理 ルールと標準の文書化 データ品質の問題管理 データガバナンス運営アクティビティの実施 データスチュワードについては以下も参考。
参考: データスチュワードとは？役割やメリット、育成方法、選定基準について解説！ | trocco®(トロッコ) データポリシー データポリシーとは、データとインフォーメーションを生成し、取得し、健全性を保ち、セキュリティを守り、品質を維持し、利用することを統制する基本的なルールに、原則と管理糸を盛り込む指示である。
ポリシーはデータガバナンスの &amp;ldquo;What&amp;rdquo; を説明する。
通常はデータマネジメント・プロフェッショナルか業務ポリシー担当者が起草し、最終的に DGC により承認される。
組織に対して効果的に伝達、実施される必要があり、そのためには簡潔で直感的な表現であるべき。
社内ポータルなどオンラインで閲覧できるようになっているのがよい。</description>
    </item>
    
    <item>
      <title>読書メモ: DMBOK2 第1章 データマネジメント</title>
      <link>https://soonraah.github.io/posts/dmbok-chapter-1/</link>
      <pubDate>Mon, 13 Nov 2023 22:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/dmbok-chapter-1/</guid>
      <description>このポストについて なんか個人的にデータマネジメントの機運が高まってきたので、ずっと積ん読していた DAMA-DMBOK を読んでいこうかなと。
で、せっかくなのでデータ関係の皆さんがよくやっているように自分としても読書メモをまとめてみようと思った。
内容を網羅するのではなく、現場のデータエンジニアとして活動した経験を踏まえて自分なりの観点でまとめてみたい。
今回は第1章「データマネジメント」で、それ以降は関心がある章をつまみ食い的に読んでいく。
DAMA-DMBOK とは DAMA とは DAta Management Association の略であり、
世界各地に80の支部を持ち、8,000名を越える会員を擁する全世界のデータ専門家のための国際的な非営利団体です。 特定のベンダーや技術、手法に依存しないことを前提として、データや情報、知識をエンタープライズの重要な資産として管理する必要性の理解を促し、この分野の成長を推進しております。
&amp;ndash; 一般社団法人 データマネジメント協会 日本支部(DAMA Japan)
とのこと。
この DAMA が刊行しているのがデータマネジメント知識体系ガイド (The DAMA Guide to Data Management Body of Knowledge) であり、その略称が DMBOK である。
ＤＡＭＡ ＤＭＢＯＫは、データマネジメントプロフェッショナルにとって有益な資料かつ指針となることを目指し、データ管理のもっとも信頼できる入門書となるよう編集されています
&amp;ndash; 一般社団法人 データマネジメント協会 日本支部(DAMA Japan)
IT 業界歴の長い人なら PMBOK というプロジェクトマネジメントについて書かれた本をご存知かもしれないが、あれのデータマネジメント版だと思っていい。
私としてはデータマネジメントの教科書的なものだと考えている。
2023年現在における最新版は2018年の第2版となっている。
以降では DMBOK2 とする。
5年前なのでやや古いと思うかもしれないが、内容的には特定技術について書かれているわけではなく、ある程度抽象度が高い話になっているので陳腐化はしにくい。
データマネジメントとは 以降、特に注釈のない引用は DMBOK2 第1章からの引用とする。
データマネジメントとは、データとインフォメーションという資産の価値を提供し、管理し、守り、高めるために、それらのライフサイクルを通して計画、方針、スケジュール、手順などを開発、実施、監督することである。
この一文にいろいろと集約されているので見ていこう。
データとインフォーメーション ここでいう「データ」と「インフォメーション」は DIKW モデルにもとづく解釈でよい。
データはインフォメーションの原材料であり、インフォメーションは意味と目的、つまりコンテキストを付与されたデータと言える。
ex. 前四半期の売上レポートは「インフォメーション」、それの元になっている DWH のデータが「データ」 データマネジメントにおいては特にこのデータとインフォーメションを扱う。</description>
    </item>
    
    <item>
      <title>near real time で更新される Apache Iceberg の table のメンテナンス</title>
      <link>https://soonraah.github.io/posts/maintain-iceberg-table-updated-in-near-real-time/</link>
      <pubDate>Sun, 28 May 2023 09:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/maintain-iceberg-table-updated-in-near-real-time/</guid>
      <description>前回のポストでは merge on read で Apache Iceberg の table を near real time で更新するということを行った。
このポストではそのメンテナンスについて触れて、かつそれを実行してみる。
merge on read の課題 merge on read で table を更新する場合、copy on write の場合と違い table 全体を洗い替えする必要はなく差分のみを追記することになる。
したがって更新にかかる時間は copy on write よりも短くなる。
一方で merge on read の名のとおり読み出し時に積み重なった差分とベースを merge して最新の snapshot とするため、読み出しの速度は copy on write より遅くなる。
長時間更新され差分がたくさん存在しているとなおさら遅い。
なので
更新頻度が低く、参照頻度が高いユースケース -&amp;gt; copy on write 更新頻度が高く、参照頻度が低いユースケース -&amp;gt; merge on write という使い分けがよいとされている。
前回ポストの例では一晩更新を続けた後の merge on read の table に対して簡単な select 文を実行したところ、6分程度かかってしまった。</description>
    </item>
    
    <item>
      <title>Apache Iceberg の table を near real time で更新する</title>
      <link>https://soonraah.github.io/posts/update-iceberg-table-in-near-real-time/</link>
      <pubDate>Thu, 11 May 2023 01:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/update-iceberg-table-in-near-real-time/</guid>
      <description>Apache Iceberg の table を near real time に、つまり高頻度で更新するということをやってみた。
Apache Iceberg とは Apache Iceberg (以下 Iceberg) は分散ファイルシステムやクラウドストレージ上の table format であり、Apache Hudi や Delta Lake と並んで data lake や lakehouse architecture で用いられる。
特徴的なのは table とデータ実体 (Parquet, Avro など) の間に metadata file, manifest list, manifest file の抽象的なレイヤーがあり、ファイル単位で table の状態を track できること。
これにより強い isolation level、パフォーマンス、schema evolution など様々な機能・性能を実現できるようになっている。
Apache Iceberg Iceberg Table Spec
詳しくは公式ドキュメントを参照のこと。
最近では SmartNews 社が Iceberg で data lake を構築したことを記事にしていた。
Flink-based Iceberg Real-Time Data Lake in SmartNews (Part I) | by SmartNews | SmartNews, Inc | Apr, 2023 | Medium ベンダー提供の DWH 中心ではなく Lakehouse Architecture を目指すのであれば最有力の table format の1つだと言えそう。</description>
    </item>
    
    <item>
      <title>Data Contract について調べた</title>
      <link>https://soonraah.github.io/posts/looked-into-data-contracts/</link>
      <pubDate>Sat, 08 Apr 2023 17:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/looked-into-data-contracts/</guid>
      <description>データエンジニアリングの領域で少し前から目にするようになった &amp;ldquo;data contract&amp;rdquo; という言葉。
なんとなく今の業務で困っている課題の解決になりそうな気がしつつもよくわかっていなかったので調べてみた。
data contract について語られているいくつかのブログ記事などを参考にしている。
Data Contract とは データの schema というのはナマモノで、いろいろな理由で変更されることがある。
schema を変更する場合、その schema のデータ (table や log) が所属する単一のビジネス機能や application のドメインで行われることになる。
そのドメインの閉じた世界で考える分にはこれで問題ないのだが、DWH や data lake など組織レベルのデータ基盤でデータを流通していた場合はその先のことも考えないといけなくなる。
このようにチームを超える影響というのは、ビジネス機能に責任を持っているチームからは見えにくくなっていることが多い。
上流の application 側で schema を変更したら下流のデータ基盤の ETL 処理がぶっ壊れてしまった、というのはデータ基盤運用あるあるではないだろうか。
というところを解決して平和に過ごせるようにすることが data contract の主なモチベーションだと思われる。
&amp;ldquo;contract&amp;rdquo; は日本語で言うところの「契約」。
組織におけるデータ流通において、データの送り手である producer 側と受け手である consumer 側との間で合意した契約を遵守することにより、前述のような問題を避けることができるというのが data contract である。
組織内のデータの見通しがよくなったり、パイプラインを宣言的に開発することができるようになるというメリットもある。
エンジニアにとっては Datafold のブログ記事の例を読むとイメージしやすいかもしれない。
To provide another analogy, data contracts are what API is for the web services. Say we want to get data from Twitter.</description>
    </item>
    
    <item>
      <title>Glue Schema Registry の導入を断念した話</title>
      <link>https://soonraah.github.io/posts/give-up-on-schema-registry/</link>
      <pubDate>Tue, 13 Dec 2022 00:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/give-up-on-schema-registry/</guid>
      <description>業務で AWS Glue Schema Registry を使おうとしたけど、やっぱりやめたというお話。
Glue Schema Registry What&amp;rsquo;s Schema Registry? AWS Glue Schema Registry は2020年に発表された AWS の機能だ。
Control the evolution of data streams using the AWS Glue Schema Registry 一方、私が最初に schema registry 的なものを見たのは Confluent の例。
Schema Registry の概要 - Confluent AWS の Glue Schema Registry はこれより後のリリースであり、同等のものの AWS マネージド版といったところだろうか。
schema registry で何ができるかは Confluent のリンク先の図がとてもわかりやすいので参考にしていただきたい。
Glue Schema Registry もだいたい同じで、ストリーム処理のための機能である。
Glue Schema Registry で解決したい課題とその機能 データ基盤上のストリーム処理における schema 管理はバッチ処理のそれとは異なる難しさがある。
これは schema evolution と呼ばれる問題で以前のポストでも述べている。</description>
    </item>
    
    <item>
      <title>「データレイク」と「データレイク層」</title>
      <link>https://soonraah.github.io/posts/data-lake-and-data-lake-layer/</link>
      <pubDate>Tue, 21 Jun 2022 08:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/data-lake-and-data-lake-layer/</guid>
      <description>「データレイク」という言葉は使う人によって異なった意味があるように感じており、気になっていた。
このポストではアーキテクチャ目線でのデータレイクと内容物目線でのデータレイクの違いについて書いてみる。
便宜上前者を「データレイク」、後者を「データレイク層」と呼ぶことにする。
アーキテクチャ目線の「データレイク」 「データレイク」については以前こちらのポストで書いたのでここでは詳しく触れない。
詳細はリンク先を見ていただきたい。
ここでキーとなるのが、
加工前データや非構造化データを含むあらゆるデータを保存 一元的なデータ管理 という部分だ。
あらゆるデータを一元的に管理するという思想であり、これができるアーキテクチャがデータレイクということだ。
例えば AWS や Azure のドキュメントを見るとデータレイクの中が zone に分けられており、生データを保持する raw zone や加工されたデータを置いておく curated zone などがある。
(zone の命名にもいくつかの流派があるようだ…)
Reference architecture - Data Analytics Lens Data lake zones and containers - Cloud Adoption Framework | Microsoft Docs 次の Robinhood 社の例でもデータレイク中に生データとその派生データが存在している。
Fresher Data Lake on AWS S3 | by Balaji Varadarajan | Robinhood 内容物目線の「データレイク層」 一方でデータレイクには生データのみを置くべき、という考えもある。
本書におけるデータレイク（DataLake）層とは、元のデータをコピーして、1つのシステムに集約したものを指します。 データソース（＝水源）から流れてきたデータを蓄える場所なのでレイク（湖）と呼びます。
ECサイトの注文履歴データを、分析用DBにコピーしている場合、それがデータレイクと言えます。データレイクのデータは、データソースと一対一の関係にあります。何も加工していない、ただのコピーだからです。
何も加工していない、ただのコピーであることが重要です。仮にデータの中身に誤りがあったとしても、修正や加工をせず、そのまま集約しましょう。
&amp;ndash; ゆずたそ,渡部 徹太郎,伊藤 徹郎. 実践的データ基盤への処方箋〜 ビジネス価値創出のためのデータ・システム・ヒトのノウハウ (Japanese Edition) (pp.</description>
    </item>
    
    <item>
      <title>成熟フェーズの事業におけるデータサイエンティスト</title>
      <link>https://soonraah.github.io/posts/ds-in-maturation-phase/</link>
      <pubDate>Mon, 12 Jul 2021 22:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/ds-in-maturation-phase/</guid>
      <description>ポエムです。
事業フェーズごとのデータサイエンティストの役割 まずはこちらの発表。
事業立ち上げにデータサイエンティストは必要なのか？ | CA BASE NEXT とても納得できる内容だった。
一部抜き出して要約すると
事業の立ち上げフェーズ データがまだなかったり、整備されていない状態 データサイエンスによる改善がしにくい 事業のグロースフェーズ 大規模なデータが使える状態 データサイエンスによる改善がやりやすい とのこと。異論はない。
では事業が立ち上がり、グロースが落ち着いたその後の成熟フェーズではどうなのだろうかという話。
成熟フェーズにおける改善の難しさ 端的に言うと成熟フェーズでは ML によるさらなる改善は困難になってくると思う。
ここで言う成熟フェーズにおいてはプロダクトの進化とともに機械学習もそれなりに適用されてきたものとする。
成熟フェーズということで既存の ML モデル、特にビジネスインパクトが大きい箇所はこれまでいろいろな改善が重ねられてきている。
そのモデルの精度をさらに上げるとなると、より高度なアルゴリズム、より複雑なデータ等を扱う必要がある。
しかし技術的によっぽど大きなブレークスルーがない限りは精度の改善幅はグロースフェーズよりもかなり小さいものとなるだろう。
精度が上がれば上がるほど、次の1%を上げるためのコストは大きくなっていく。
改善が進むほどに次の改善業務は困難になっていく。
(蛇足だがある程度大きな組織でなければ高度で state-of-the-art な ML アルゴリズムは運用しない方がいいと考えている)
では既存ではない新しい適用箇所に ML を使えばいいのではとなるかもしれない。
しかしやはりそれも難しい。
ビジネスインパクトが大きく、かつわかりやすい適用箇所にはおそらくすでに ML が適用されているからだ。
その状態から更によい適用箇所を見つけるには深いドメイン知識が必要になったりする。
という感じでいわゆるキラキラした「ML でビジネスをドライブ！」みたいなことは成熟フェーズでは難しいことが多いのではないか。
しかしデータサイエンティストにやることがないわけではない。
成熟フェーズで何ができるか ぱっと思いつくのは次のような仕事。
データドリブンな施策の立案・評価 これは事業フェーズ問わずあるべき ドメイン知識が必要 ML エンジニアリング パイプラインの改善や属人性をなくすお仕事 ML モデルの受動的なメンテナンス 精度が変化したときの調査 内部的・外部的要因によるデータの変化への対応 やっぱり ML モデルの精度改善 成熟フェーズということでビジネスもスケールしていれば 0.1% の精度改善でも売上的なインパクトは大きいかもしれない いわゆる狭義のデータサイエンスではなく、ドメイン知識であったりアナリストやエンジニア的な視点が絡んだ仕事が増えてくる。
よくある「ML だけじゃなく◯◯もできると強いよね」みたいな話になってしまった。
おわりに …という話が少し前に Twitter で知人との話題に上がった。</description>
    </item>
    
    <item>
      <title>Apache Flink の Backpressure の仕組みについて調べた</title>
      <link>https://soonraah.github.io/posts/backpressure-for-flink/</link>
      <pubDate>Sun, 28 Feb 2021 21:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/backpressure-for-flink/</guid>
      <description>ストリーム処理のフレームワークが備える backpressure という機能がある。
このポストでは Apache Flink の backpressure について調べたことを記載する。
Backpressure の目的 backpressure はストリーム処理システムにおける負荷管理の仕組みの一つ。
一時的な入力データ量の増大に対応する。
インターネットユーザの行動履歴やセンサーデータなどは常に一定量のデータが流れているわけではなく、単位時間あたりのデータ量は常に変動している。
一時的にスパイクしてデータ量が増大するようなことも起こりうる。
複数の operator からなる dataflow graph により構成されるストリーム処理システムにおいては、処理スピードのボトルネックとなる operator が存在する。
一時的に入力データ量が増えてボトルネックの operator の処理速度を上回ってしまった場合に、データの取りこぼしが発生するのを防ぐのが backpressure の目的となる。
Backpressure の仕組み Buffer-based ここでは以前のブログでも紹介した、ストリーム処理で必要とされる機能について書かれた Fragkoulis et al. 1 を引用して一般論としての backpressure について述べたい。
上流／下流の operator をそれぞれ producer, consumer とする。
producer, consumer (それらの subtask と言ってもいいかも) がそれぞれ異なる物理マシンに deploy されているケースが Figure 12b となる。
各 subtask は input と output の buffer を持っており、
producer は処理結果を output buffer に書き出す TCP 等の物理的な接続でデータを送信 consumer 側の output buffer にデータを格納 consumer がそれを読み込んで処理する というような流れになる。</description>
    </item>
    
    <item>
      <title>データレイク関連の OSS - Delta Lake, Apache Hudi, Apache Kudu</title>
      <link>https://soonraah.github.io/posts/oss-for-data-lake/</link>
      <pubDate>Tue, 26 Jan 2021 08:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/oss-for-data-lake/</guid>
      <description>はじめに 前回のポストではデータレイクとはどういうものかというのを調べた。
今回はデータレイクの文脈でどのような OSS が注目されているのかを見ていきたい。
以下は NTT データさんによる講演資料であり、その中で「近年登場してきた、リアルタイム分析に利用可能なOSSストレージレイヤソフト」というのが3つ挙げられている。
大規模データ活用向けストレージレイヤソフトのこれまでとこれから（NTTデータ テクノロジーカンファレンス 2019 講演資料、2019/09/05） from NTT DATA Technology &amp;amp; Innovation Delta Lake Apache Hudi Apache Kudu これらはすべて論理的なストレージレイヤーを担う。
こちらの講演資料に付け足すようなこともないかもしれないが、このポストではデータレイクという文脈から自分で調べて理解した内容をまとめるということを目的にする。
当然 Hadoop, Hive, Spark 等もデータレイクの文脈において超重要だが、「データレイク」という言葉がよく聞かれるようになる前から普及していたのでこのポストでは触れないことにする。
Delta Lake https://delta.io/
Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.
Delta Lake
Delta Lake は Apache Spark の読み書きに ACID な transaction を提供するストレージレイヤーの OSS である。
Databricks が作り、2019年4月に v0.1.0 がリリースされたのが最初だ。</description>
    </item>
    
    <item>
      <title>いまさらながらのデータレイク</title>
      <link>https://soonraah.github.io/posts/what-is-a-data-lake/</link>
      <pubDate>Thu, 31 Dec 2020 20:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/what-is-a-data-lake/</guid>
      <description>最近よく聞かれるようになった「データレイク」という概念にあまりついていけていなかったため、いまさらながらざっと調べてみた。
データレイクとは Wikipedia によると最初にこの言葉を使ったのは Pentaho 社の CTO である James Dixon らしい。
その時の彼のブログ (10年前…) を読むと、既にあったデータマートに対して
Only a subset of the attributes are examined, so only pre-determined questions can be answered. The data is aggregated so visibility into the lowest levels is lost
&amp;ndash;Pentaho, Hadoop, and Data Lakes - James Dixon’s Blog というような問題意識からデータレイクというコンセプトを提案したようだ。
最近？のデータレイクについてはベンダー等の記事が参考になる。
データレイクとは - AWS データレイクとは？ - talend データレイクとは？データレイクの落とし穴と効果 - Informatica 書籍だと『AWSではじめるデータレイク: クラウドによる統合型データリポジトリ構築入門』がいいだろうか。
データレイクの概要と AWS が考えている構築・運用がざっとわかる。
Amazon で検索した限りだと現時点でタイトルに「データレイク」を含む和書はこれのみだった。
上に挙げたような文献ではデータレイクはデータウェアハウスとの対比という形でよく語られている。
共通している内容は概ね以下のとおり。</description>
    </item>
    
    <item>
      <title>Apache Flink の DataStream API 利用時の CSV ファイル読み込み</title>
      <link>https://soonraah.github.io/posts/read-csv-by-flink-datastream-api/</link>
      <pubDate>Tue, 01 Dec 2020 00:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/read-csv-by-flink-datastream-api/</guid>
      <description>ストリーム処理における CSV ファイルの読み込み Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。
しかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。
star schema における dimension table 的な情報をストリームに結合したい場合 等が考えられる。
このポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。
Flink は現時点の stable である v1.11 を想定。
CSV ファイルを読む方法 DataStream API ベースの実装で CSV ファイルを読むには StreamExecutionEnvironment のメソッドである readFile() を使う。
overload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。
まず1つめは FileInputFormat&amp;lt;OUT&amp;gt; inputFormat であり、こちらは data stream の生成に用いる入力フォーマットを指定する。
おそらく最も一般的なのが TextInputFormat だと思われる。
もちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。
PojoCsvInputFormat RowCsvInputFormat TupleCsvInputFormat なんとなく名前でわかると思うが、それぞれ readFile() の結果として返される DataStreamSource が内包する型が異なる。</description>
    </item>
    
    <item>
      <title>機械学習の精度と利益と倫理とイシューと</title>
      <link>https://soonraah.github.io/posts/ml-accuracy-profit-ethic-issue/</link>
      <pubDate>Thu, 12 Nov 2020 09:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/ml-accuracy-profit-ethic-issue/</guid>
      <description>ちょっと昔話 かつて参画したプロジェクトの話。
そのプロジェクトでは他社から受注した受託開発として機械学習系のシステムを開発していた。
当時としては新しいフレームワークを使い、かなり頑張ってなんとか納期内で完成させた。
その中の1つの機能として A/B テストができるようにしていた。
パラメータチューニングによりパフォーマンスを改善することを想定していた。
しかし結局その機能は使われることがなかった。
なぜか。
A/B テストを実施するためのクライアントの追加の予算がつかなかったためである。
受託なのでなおさらなのだが、売上にならなければ工数をかけるこはできない。
工数を使ってパフォーマンス改善することはできなかった。
手はあるのに。
機械学習の精度は必ずしも利益に結びつかない この昔話で何が言いたいかというと、機械学習の精度改善は必ずしも利益に結びつかないということである。
そのことを示しているとても素晴らしい資料がこちら。
機械学習の精度と売上の関係 from Tokoroten Nakayama 前述の昔話の例はこの資料で言うところの③ロジスティック型 (=外注) となる。
いったん売上が立った後、追加予算がつかなかったので精度改善では売上は増えなかったのだ。
倫理感による精度改善 受託開発を主としている組織であれば工数にはシビアなので、売上の立たない工数をかけることはあまりないだろう。
(よっぽどの炎上鎮火とかでなければ)
しかし自社で製品やサービスを作って提供しているような組織の場合、利益にならない精度改善をしているのを時折見かける。
なぜそのようなことが起こるかと言うと多くの場合はデータサイエンティスト／機械学習エンジニアとしての倫理感からなのではないだろうか。
「◯◯予測という機能なのでできるだけ良い予測精度を示すべきだ」
「ユーザには気づかれない部分だが精度が悪いので改善したい」
倫理感や興味が先行してしまっているのだ。
しかしその精度を上げた先に利益があるとは限らない。
機械学習で職を得ている人間は自分の仕事を機械学習の精度を上げるゲームだとみなす傾向があるように思う。
例えばインターネット広告の CTR 予測。
これは予測精度が高いほど利益は改善するし、広告主に価値も提供できる。
精度改善に倫理と利益が伴っている、とても機械学習がハマる例だと思う。
本来はこれらを兼ね備えているのが良い適用先であるはずだ。
イシューは行き渡っているのか 利益に結びつかない、または間接的にしか結びつかないような精度改善をやることが許されるというのは組織に余裕があるということで悪いことではないのかもしれない。
しかし単によいイシューの設定ができてないだけという可能性もある。
自社で製品やサービスを作って提供しているような組織において、単純なロジスティック回帰でコアなところのビジネスを大きく加速させることができた時期を過ぎると機械学習で解くのに適したよい問題を恒常的に見つけ出すのは実は難しいのではないだろうかと最近考えるようになった。
ビジネスの領域拡大よりも既存領域への機械学習の適用の方が速いということは十分ありうる。
もちろんチームの規模にもよる。
機械学習チームの人的リソースの規模に対して機械学習で解くべきよいイシューを見つけ出せているのか、ということだ。
少し前にちょっと話題になったこちらの件もイシューが大事だと言っている。
全ての機械学習の論文は新しいアルゴリズムを提案しているのですか？ - Quora キャリアの行く末 事業会社においてビジネスの領域拡大よりも既存領域への機械学習の適用の方が速く、よいイシューを提供しにくいということがよく起こるのであれば、機械学習チームのリソースは余剰気味になりやすいということになる。
これが続くと今後機械学習しかやらない人材の市場価値は下がっていくのかもしれない。
もしくは自社で製品やサービスを持っている組織ではなく、受託開発やコンサルが主戦場になっていくのかもしれない。
何にせよ特定のプロダクトに commit したいのであれば機械学習エンジニアは機械学習以外のスキルも磨いていく必要があるように思う。
おわりに 見える範囲にいる人が利益にならない精度改善をしているのを横目で見てこのようなことを考えていた。
難しいけどできるだけ金を生んでいきたい。</description>
    </item>
    
    <item>
      <title>ストリーム処理システムに求められる機能性、および Apache Flink におけるその対応</title>
      <link>https://soonraah.github.io/posts/functionality-of-streaming-system/</link>
      <pubDate>Sat, 07 Nov 2020 16:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/functionality-of-streaming-system/</guid>
      <description>はじめに このポストではストリーム処理の survay 論文の話題に対して Apache Flink における例を挙げて紹介する。
論文概要 Fragkoulis, M., Carbone, P., Kalavri, V., &amp;amp; Katsifodimos, A. (2020). A Survey on the Evolution of Stream Processing Systems.
2020年の論文。
過去30年ぐらいのストリーム処理のフレームワークを調査し、その発展を論じている。
ストリーム処理に特徴的に求められるいくつかの機能性 (functionality) についてその実現方法をいくつか挙げ、比較的古いフレームワークと最近のフレームワークでの対比を行っている。
このポストのスコープ このポストでは前述のストリーム処理システムに求められる機能性とそれがなぜ必要となるかについて簡単にまとめる。
論文ではそこからさらにその実現方法がいくつか挙げられるが、ここでは個人的に興味がある Apache Flink ではどのように対処しているかを見ていく。
ちなみに論文中では Apache Flink はモダンなフレームワークの1つとしてちょいちょい引き合いに出されている。
ここでは Flink v1.11 をターゲットとする。
以下では論文で挙げられている機能性に沿って記載していく。
Out-of-order Data Management Out-of-order ストリーム処理システムにやってくるデータの順序は外的・内的要因により期待される順序になっていないことがある。
外的要因としてよくあるのはネットワークの問題。
データソース (producer) からストリーム処理システムに届くまでのルーティング、負荷など諸々の条件により各レコードごとに転送時間は一定にはならない。
各 operator の処理などストリーム処理システムの内的な要因で順序が乱されることもある。
out-of-order は処理の遅延や正しくない結果の原因となることがある。
out-of-order を管理するためにストリーム処理システムは処理の進捗を検出する必要がある。
&amp;ldquo;進捗&amp;rdquo; とはある時間経過でレコードの処理がどれだけ進んだかというもので、レコードの順序を表す属性 A (ex. event time) により定量化される。</description>
    </item>
    
    <item>
      <title>バッチ処理おじさんがストリーム処理のシステムを開発するにあたって調べたこと</title>
      <link>https://soonraah.github.io/posts/study-streaming-system/</link>
      <pubDate>Sun, 06 Sep 2020 16:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/study-streaming-system/</guid>
      <description>ほとんどバッチ処理しか書いたことのない者だがストリーム処理のシステムを開発することになった。
それにあたって独学で調べたことなどまとめておく。
ストリーム処理とは そもそも &amp;ldquo;ストリーム処理&amp;rdquo; とは何を指しているのか。
以下の引用が簡潔に示している。
a type of data processing engine that is designed with infinite data sets in mind. Nothing more.
&amp;ndash; Streaming 101: The world beyond batch
こちらは &amp;ldquo;streaming system&amp;rdquo; について述べたものだが、つまり終わりのないデータを扱うのがストリーム処理ということである。
例えば web サービスから生まれ続けるユーザ行動ログを逐次的に処理するというのがストリーム処理。
web サービスが終了しないかぎりはユーザ行動ログの生成には終わりがない。
これに対して &amp;ldquo;1日分のユーザ行動ログ&amp;rdquo; 等のように有限の量のデータを切り出して処理する場合、これはバッチ処理となる。
ストリーム処理とバッチ処理の違いは扱うデータが無限なのか有限なのかということだ。
この後触れていくが、この終わりのないデータを継続的に処理し続けるというところにバッチ処理にはない難しさがある。
なぜストリーム処理なのか なぜストリーム処理なのか。
ひとえに逐次的な入力データに対する迅速なフィードバックが求められているからと言えるだろう。
迅速なフィードバックがビジネス上のメリットとなることは自明だ。
SNS の配信 カーシェアリングにおける配車や料金設定 クレジットカードや広告クリックなどの不正検知 もしこれらの application が例えば hourly のバッチ処理で実装されていたらどうだろうか。
まあ待っていられない。
一般的なストリーム処理の構成 モダンな…と言っていいのかわからないが、ストリーム処理を行うための一般的なシステムは次の3つの要素で構成される。
producer broker consumer producer は最初にレコードを生成する、ストリームデータの発生源となるものである。
例えばログを生成する web application であったり、何らかのセンサーを持つ IoT 機器であったりがこれに該当する。</description>
    </item>
    
    <item>
      <title>A/B テストの運用が重くてつらいという話</title>
      <link>https://soonraah.github.io/posts/heavy-ab-testing-operation/</link>
      <pubDate>Sun, 23 Aug 2020 12:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/heavy-ab-testing-operation/</guid>
      <description>前提 ここでは web システムで使われている機械学習のモデルやアルゴリズムを改善するための online の A/B テストを考える。
具体的に述べると web 広告における CTR 予測や EC サイトのレコメンデーション等が対象である。
よくあるやつ。
web システムにおいて online の A/B テストは KPI 改善の根幹でありとても重要だ。
それが重くなるとつらい、という話。
ここで「重い」と言っているのは計算資源のことではなく、A/B テストを実施する担当者の運用コストについて。
A/B テストの運用が重い場合のデメリット デメリット 1. KPI 改善が遅くなる デメリットと言えばこれが一番大きい。
単純に A/B テストを1回まわすのに時間がかかってしまうし、それがゆえに online の A/B テストに入るまでの offline のテストが厚くなりここでも時間がかかってしまう。
KPI 改善に時間がかかるというのはつまり売上や利益を大きくするのに時間がかかってしまうということである。
デメリット 2. KPI 改善における offline テストの比重が大きくなる 前述のとおりだが online の A/B テストが重いとそこで失敗できなくなり、結果としてその前段の offline のテストを厚くするということになる。
offline のテストが厚いことの何が問題だろうか。
ここで前提としている CTR 予測やレコメンデーションのようなタスクの場合、offline のデータは既存のモデルやアルゴリズムの影響を受けることになる。
例えばレコメンデーションの場合を考えると、新しいモデルを offline で評価するための実験データの正例 (コンテンツの閲覧等) は既存モデルによって生み出される。
既存モデルが「このコンテンツがいいよ」といってユーザに出したリスト、その中からコンテンツの閲覧が行われ正例となるからだ。</description>
    </item>
    
    <item>
      <title>Apache Flink の Temporary Table Function を用いた stream data と static data の join</title>
      <link>https://soonraah.github.io/posts/flink-join-by-temporal-table-function/</link>
      <pubDate>Sun, 16 Aug 2020 00:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/flink-join-by-temporal-table-function/</guid>
      <description>前回の記事 では Apache Flink における stream data と static data の join において、DataStream API における broadcast state pattern を使う方法を示した。
今回の記事では Table API の temporal table function を用いた実験を行う。
Table API Table API は名前のとおりで class Table を中心として SQL-like な DSL により処理を記述するという、DataStream API より high-level な API となっている。
これらの関係は Apaceh Spark の RDD と DataFrame (DataSet) の関係に似ている。
SQL-like な API で記述された処理が実行時に最適化されて low-level API の処理に翻訳されるところも同じだ。
RDB の table の概念を元にしているものと考えられるが、本質的に table の概念とストリーム処理はあまりマッチしないと思う。
table はある時点のデータセット全体を表すのに対し、ストリーム処理ではやってくるレコードを逐次的に処理したい。
ここを合わせているため、ストリーム処理における Table API による処理の挙動の理解には注意が必要だ。</description>
    </item>
    
    <item>
      <title>Apache Flink の Broadcast State Pattern を用いた stream data と static data の join</title>
      <link>https://soonraah.github.io/posts/flink-join-by-broadcast-state-pattern/</link>
      <pubDate>Thu, 06 Aug 2020 22:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/flink-join-by-broadcast-state-pattern/</guid>
      <description>star schema における fact table と dimension table の join のようなことを Apache Flink におけるストリーム処理で行いたい。
stream data と static data の join ということになる。
ただし dimension table 側も更新されるため、完全な static というわけではない。
このポストでは Flink v1.11 を前提とした。
join の方法 今回は DataStream API でこれを実現することを考える。
Flink のドキュメントを読むと broadcast state pattern でできそうだ。
The Broadcast State Pattern やり方としては次のようになる。
static data のファイルを FileProcessingMode.PROCESS_CONTINUOUSLY で読み込み DataStream 化 1 を broadcast() stream data の DataStream と 2 を connect() static data を PROCESS_CONTINUOUSLY で読むのは変更を得るため。</description>
    </item>
    
    <item>
      <title>あまり大きな Pull Request を作ってほしくない</title>
      <link>https://soonraah.github.io/posts/no-more-huge-pull-request/</link>
      <pubDate>Mon, 03 Aug 2020 22:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/no-more-huge-pull-request/</guid>
      <description>GitHub Flow ベースの開発においてはあまり大きな pull request を作ってほしくないという話。
なんというか今更わざわざ言わなくてもいいんだけど…
仕事で何度か大きな pull request が投げられているのを見てしまったので、それはあまりよくないよというのを自分でも指摘しやすくするためにまとめておく。
Reference 最初に参考資料を挙げておく。
100 Duck-Sized Pull Requests 「巨大プルリク1件vs細かいプルリク100件」問題を考える（翻訳） Optimal pull request size これらを読めば特に私から言うこともないのだが…
これらに書いてあるとおりだが補足しておく。
Pull Request を小分けにしたときのメリット module を適切に切り出すモチベーションが得られる 次のような話がある。
共通化という考え方はアンチパターンを生み出すだけ説 これを理由に module を分けるべきところが分けられず、いろんなことができてしまうベタッと大きな module が生まれるのを見た。
もちろんよく考えられていない共通化は駄目だが、上記ポストでは一方で
ただし共通化という名の下におこなわれるのは「同じロジックを持つコードをまとめる」行為であって、抽象化のようにそのコード単位の意味を捉える作業はその範疇にない。抽象化というのはロジックを意味単位ごとにひとくくりにしていく行為で、これがどういうことなのかは次以降で述べていく。
&amp;ndash; 共通化という考え方はアンチパターンを生み出すだけ説 - タオルケット体操
と述べており抽象化、つまりコードの意味を考慮した上で適切な単位でまとめておくことは否定していない。
pull request を小さく分けるという行為のためには module や機能を適度なまとまりで切り分けること、つまり抽象化を考えていくことが必要となる。
したがって何でもできてしまう大きな module が生まれるのを防ぐ方向に働く。
(もちろんここで駄目な共通化がなされてしまうこともあるだろう)
リリースまでの期間が短く済む 前述の参考資料においても pull request が大きいと
Developers would put off reviews until they had time/energy and the development process would come to a halt.</description>
    </item>
    
    <item>
      <title>勉強会メモ: Spark Meetup Tokyo #3 Online</title>
      <link>https://soonraah.github.io/posts/spark-meetup-tokyo-3/</link>
      <pubDate>Sat, 01 Aug 2020 15:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/spark-meetup-tokyo-3/</guid>
      <description>2020-07-31 にオンライン開催された Spark Meetup Tokyo #3 Online に参加した。
感想などメモしておく。
全体感 トピックとしては主に
Spark 3.0 Spark + AI Summit 2020 Spark 周辺要素 といったところだろうか。
最近のコミュニティの動向や関心を日本語で聞くことができてよかった。
運営 &amp;amp; スピーカーの皆様、ありがとうございます。
発表 発表資料は公開されたら追加していく。
SPARK+AI Summit 2020 イベント概要 スピーカー: @tokyodataguy さん
Summit は金融業界の参加者が増えているらしい Spark で最も使われている言語は Python とのことだったが、Databricks の notebook サービスの話だった プロダクションコードではまた違うのだろう Spark 3.0 の update をざっくりと Spark 周辺要素の話をざっくりと Koalas Delta Lake Redash MLflow Introducing Koalas 1.0 Introducing Koalas 1.0 (and 1.1) from Takuya UESHIN スピーカー: @ueshin さん</description>
    </item>
    
    <item>
      <title>Spark DataFrame クエリの弱い分離レベル</title>
      <link>https://soonraah.github.io/posts/weak_isolation_level_of_dataframe/</link>
      <pubDate>Sun, 19 Jul 2020 22:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/weak_isolation_level_of_dataframe/</guid>
      <description>Spark バッチ処理の問題を調べていたら分離レベルという概念にたどりついた。
分離レベルについて調べたので、Spark の問題の内容と絡めて記しておく。
考えてみれば当たり前でたいした話ではない。
分離レベルとは トランザクションの挙動についての暗黙の理解 アドホックな分析クエリやプロダクションコード中のクエリを書くとき、その単一のクエリのトランザクションにおいて「同時に実行されている別のクエリの commit 前の状態や commit 結果に影響され、このクエリの結果がおかしくなるかもしれない」ということは通常考えない。
トランザクションはデータベースのある時点の状態に対して正しく処理される、というほぼ無意識の理解をおそらくほとんどの開発者が持っている。
多くの場合この理解は間違っていない。
それはなぜかというと DB 等のデータ処理フレームワークがある強さの分離レベルを提供しているからである。
いろいろな分離レベル ACID 特性のうちの1つ、分離性 (Isolation) の程度を表すのが分離レベル。
トランザクション中に行われる操作の過程が他の操作から隠蔽されることを指し、日本語では分離性、独立性または隔離性ともいう。より形式的には、独立性とはトランザクション履歴が直列化されていることと言える。この性質と性能はトレードオフの関係にあるため、一般的にはこの性質の一部を緩和して実装される場合が多い。 &amp;ndash; Wikipedia ACID (コンピュータ科学)
分離レベルには名前のついたものがいくつかあり、分離性の保証の強さが異なる。
具体的にはトランザクションの並行性の問題への対応力が異なる。
名著「データ指向アプリケーションデザイン」の第7章で分離レベルについて詳しく述べられているので、以下ではそちらからの引用。
分離レベルを弱い順に並べる。
read uncommitted
このレベルではダーティライトは生じませんが、ダーティリードは妨げられません。
read committed
データベースからの読み取りを行った際に見えるデータは、コミットされたもののみであること（ダーティリードは生じない）。 データベースへの書き込みを行う場合、上書きするのはコミットされたデータのみであること（ダーティライトは生じない）。 snapshot isolation
スナップショット分離の考え方は、それぞれのトランザクションがデータベースの一貫性のあるスナップショットから読み取りを行うというものです。すなわち、トランザクションが読み取るデータは、すべてそのトランザクションの開始時点のデータベースにコミット済みのものだけということです。
serializability
この分離レベルはトランザクションが並行して実行されていても、最終的な答えはそれぞれが1つずつ順番に、並行ではなく実行された場合と同じになることを保証します。
日本語で「分離レベル」を検索すると snapshot isolation の代わりに repeatable read が出てくる事が多い。
しかし repeatable read の名前は実装によって意味が違っていたりして扱いが難しいらしい。
分離レベルと race condition の関係 以下に各分離レベルとトランザクションの並行性の問題 (race condition) の関係を示す。
各 race condition の説明については割愛するが、複数のトランザクションが並行して実行されることにより起こりうる期待されていない挙動だと思えばよい。
○はその分離レベルにおいてその race condition が発生しないことを示す。</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://soonraah.github.io/about/</link>
      <pubDate>Sun, 12 Jul 2020 12:16:47 +0900</pubDate>
      
      <guid>https://soonraah.github.io/about/</guid>
      <description>東京の IT 企業で働く中年エンジニアが勉強したことや考えていることなどを書きます。
このブログは Hugo で作成しました。
Theme には PaperMod を利用しています。</description>
    </item>
    
    <item>
      <title>Apache Spark 3.0.0 について調べた</title>
      <link>https://soonraah.github.io/posts/study-spark-3-0-0/</link>
      <pubDate>Sun, 12 Jul 2020 11:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/study-spark-3-0-0/</guid>
      <description>はじめに Apache Spark 3.0.0 がリリースされました。
Spark Release 3.0.0 release note を見て個人的に気になったところなど簡単に調べました。
書いてみると Databricks の記事へのリンクばっかになってしまった…
全体感 こちらの記事を読めば全体感は OK.
Introducing Apache Spark 3.0 公式の release note には
Python is now the most widely used language on Spark.
とあってそうなん？ってなったけど、こちらの記事だと
Python is now the most widely used language on Spark and, consequently, was a key focus area of Spark 3.0 development. 68% of notebook commands on Databricks are in Python.
と書いてありどうやら Databricks の notebook の話らしく、だったらまあそうかなという感じ。</description>
    </item>
    
    
    
  </channel>
</rss>
