<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AWS on Froglog</title>
    <link>https://soonraah.github.io/tags/aws/</link>
    <description>Recent content in AWS on Froglog</description>
    <image>
      <title>Froglog</title>
      <url>https://soonraah.github.io/47</url>
      <link>https://soonraah.github.io/47</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sun, 03 Dec 2023 22:00:00 +0900</lastBuildDate><atom:link href="https://soonraah.github.io/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CDC &#43; Apache Iceberg で Amazon Athena にデータを取り込む</title>
      <link>https://soonraah.github.io/posts/ingest-data-into-athena-by-cdc-and-iceberg/</link>
      <pubDate>Sun, 03 Dec 2023 22:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/ingest-data-into-athena-by-cdc-and-iceberg/</guid>
      <description>このポストについて このポストは Distributed computing Advent Calendar 2023 の3日目の記事になります。
1日目、2日目に続いて Apache Iceberg について書きますが、このポストでは Iceberg の実用例を書きます。
AWS DMS による CDC の結果を Apache Iceberg 形式にして Amazon Athena でクエリできるようにするという内容になります。
やっていることとしては Perform upserts in a data lake using Amazon Athena and Apache Iceberg | AWS Big Data Blog で紹介されている内容と近いですが、実務としての背景や工夫したところなどを書いていきます。
背景 私の所属する事業会社では日々プロダクトから様々なデータが発生しており、プロダクトの分析やレポーティング、ML など様々な用途で利用されている。
それを支える基盤としてデータ基盤が存在している。
データ基盤ではクエリエンジンとして Amazon Athena を使っている。
ストレージとしては S3 を使用しており、主に分析用として Parquet 形式でデータが置かれる。
ここに業務用の operational な database から日次でデータを取り込んでいる。
データソースは RDS (Aurora MySQL) であり、比較的大きなデータとなっている。
これまではこの RDS -&amp;gt; S3 のデータ取り込みには RDS の S3 snapshot export という機能を利用していた。</description>
    </item>
    
    <item>
      <title>Glue Schema Registry の導入を断念した話</title>
      <link>https://soonraah.github.io/posts/give-up-on-schema-registry/</link>
      <pubDate>Tue, 13 Dec 2022 00:30:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/give-up-on-schema-registry/</guid>
      <description>業務で AWS Glue Schema Registry を使おうとしたけど、やっぱりやめたというお話。
Glue Schema Registry What&amp;rsquo;s Schema Registry? AWS Glue Schema Registry は2020年に発表された AWS の機能だ。
Control the evolution of data streams using the AWS Glue Schema Registry 一方、私が最初に schema registry 的なものを見たのは Confluent の例。
Schema Registry の概要 - Confluent AWS の Glue Schema Registry はこれより後のリリースであり、同等のものの AWS マネージド版といったところだろうか。
schema registry で何ができるかは Confluent のリンク先の図がとてもわかりやすいので参考にしていただきたい。
Glue Schema Registry もだいたい同じで、ストリーム処理のための機能である。
Glue Schema Registry で解決したい課題とその機能 データ基盤上のストリーム処理における schema 管理はバッチ処理のそれとは異なる難しさがある。
これは schema evolution と呼ばれる問題で以前のポストでも述べている。</description>
    </item>
    
    <item>
      <title>いまさらながらのデータレイク</title>
      <link>https://soonraah.github.io/posts/what-is-a-data-lake/</link>
      <pubDate>Thu, 31 Dec 2020 20:00:00 +0900</pubDate>
      
      <guid>https://soonraah.github.io/posts/what-is-a-data-lake/</guid>
      <description>最近よく聞かれるようになった「データレイク」という概念にあまりついていけていなかったため、いまさらながらざっと調べてみた。
データレイクとは Wikipedia によると最初にこの言葉を使ったのは Pentaho 社の CTO である James Dixon らしい。
その時の彼のブログ (10年前…) を読むと、既にあったデータマートに対して
Only a subset of the attributes are examined, so only pre-determined questions can be answered. The data is aggregated so visibility into the lowest levels is lost
&amp;ndash;Pentaho, Hadoop, and Data Lakes - James Dixon’s Blog というような問題意識からデータレイクというコンセプトを提案したようだ。
最近？のデータレイクについてはベンダー等の記事が参考になる。
データレイクとは - AWS データレイクとは？ - talend データレイクとは？データレイクの落とし穴と効果 - Informatica 書籍だと『AWSではじめるデータレイク: クラウドによる統合型データリポジトリ構築入門』がいいだろうか。
データレイクの概要と AWS が考えている構築・運用がざっとわかる。
Amazon で検索した限りだと現時点でタイトルに「データレイク」を含む和書はこれのみだった。
上に挙げたような文献ではデータレイクはデータウェアハウスとの対比という形でよく語られている。
共通している内容は概ね以下のとおり。</description>
    </item>
    
  </channel>
</rss>
