[{"content":"このポストについて 書籍『クラウドFinOps 第2版』を読んだところ、FinOps にデータマネジメントやデータエンジニアリングに深く関連する内容があるということがわかったのでまとめてみる。\n書籍について J.R. Storment; Mike Fulle. クラウドFinOps 第2版 協調的でリアルタイムなクラウド価値の意思決定 O\u0026rsquo;Reilly Japan. 2025年3月に出版。\nちなみに原著の初版は2019年、第2版は2023年。\nタイトルのとおり FinOps (後述) について書かれた書籍となっている。\n著者は両名とも FinOps Foundation の関係者であり、本文中にも随所に FinOps Foundation についての記載が出てくる。\n私はデータエンジニア、ソフトウェアエンジニアとして日々 AWS その他のクラウドサービスを利用している。\nクラウドサービス上に例えばデータ基盤等を構築し、ビジネス上の価値を提供している。\nその一方でクラウドを使うということは料金的な意味でのコストがかかるということでもある。\nもちろん支払うコストは少ない方がいい。\nそれは分かるのだが、それ以上のクラウドコストについての体系的な考え方を持ち合わせていなかった。\n毎日それなりの額を使ってるのにね。\nというのが本書を読もうと思った理由だった。\nFinOps とは 定義 これを書いている2025年8月現在における FinOps Foundation での定義は以下のようになっている。1\n\u0026ldquo;FinOps is an operational framework and cultural practice which maximizes the business value of cloud and technology, enables timely data-driven decision making, and creates financial accountability through collaboration between engineering, finance, and business teams.\u0026rdquo;\n(訳)\n\u0026ldquo;FinOpsは、クラウドとテクノロジーのビジネス価値を最大化し、データ駆動型の意思決定を迅速に行うことを可能にし、エンジニアリング、財務、ビジネスチーム間の協業を通じて財務的な責任を確立する、運用フレームワークおよび文化的な実践です\u0026rdquo;\n基本原則 また FinOps には経験則から得られた 基本原則 がある。\n各チームは協力する必要がある 事業価値が技術的な意思決定を駆動する すべての人が自分の技術利用にオーナーシップを持つ FinOps のデータはアクセスしやすく、タイムリーかつ正確であるべき FinOps は中央集権的に実施されるべき クラウドの変動費モデルを活用する 各チームは協力する必要がある FinOps は技術レイヤーだけの話ではなく、組織全体を巻き込む文化的な変革活動。\n財務、技術、ビジネスの各部門や経営層が協力し合い、部門横断的な連携が不可欠となる。\n事業価値が技術的な意思決定を駆動する 本書を読む前は FinOps というと節約テクのようなイメージがあったがそうではなかった。\nコスト削減だけではなく、クラウド支出から最大のビジネス価値を引き出すことを目的にしている。\nしたがってビジネス上のメリットがある場合はあえて高いコストを払う選択をすることも許容されている。\nすべての人が自分の技術利用にオーナーシップを持つ クラウドを使う各チームは支出とそこから得られる価値に責任を持つ。\nエンジニアチームはコスト配賦によりクラウドの利用と支出について財務面の説明責任とアーキテクチャの決定権を持つ。\nFinOps のデータはアクセスしやすく、タイムリーかつ正確であるべき データ駆動で意思決定を行うのが FinOps の核と言える。\n例えば各チームにおいて near real time にコストが分かるとプリウス効果 2 により、エンジニアに良い行動変化が促されるらしい。\n月次や四半期単位だと遅い。\nFinOps は中央集権的に実施されるべき FinOps の推進には組織横断の FinOps チームが必要。\n財務と技術の部門間の橋渡し役を担い、FinOps 文化の浸透をリードする。\nコミットメントベースの割引 3 の購入、レポートやコスト削減のための推奨事項の提供は中央 FinOps チームが行うことになる。\nクラウドの変動費モデルを活用する クラウドの変動費モデルはリスクではなく価値提供の機会と見る。\nクラウドコストに対する随時の予測、計画、予算確保などを行う。\nクラウドの支出は 使用量 × 料金 4 の単純な公式で表される。\nクラウドの支出を最適化するために使用量は各チームが不要なリソースの停止やダウンサイジングなどで削減し、時間・リソースあたりの料金は中央の FinOps チームがコミットメント割引などで削減する。\nデータマネジメント、データエンジニアリングとの関連性 本書を読んでデータエンジニアはアプリケーションエンジニア等より FinOps との関連があると思った。\nデータエンジニア目線で見た FinOps とデータマネジメント、データエンジニアリングとの関連性について考えてみる。\nデータ領域における FinOps の導入 データ領域、例えばデータ基盤の開発・運用についてもクラウドのコストがかかる。\nデータ基盤は組織においても扱うデータの規模が大きく、少なくないコストがかかることも多い。\nよって当然本書に書かれているような FinOps の文化や運用に組み込まれて実践した方がいい。\n例えばコスト最適化について考えてみる。\nDatabricks, Snowflake 等で構築されるモダンなデータ基盤にかかるクラウドコストとしては、データ保存のストレージコストと ELT などのコンピュートコストが大きなところだろう。\nAWS 上に構築された Databricks 環境であれば次のようなコスト最適化が考えられる。\n使用量の削減 ストレージコスト データライフサイクルにおける削除条件を定義し、クラウドストレージ (S3) から定期的に不要となったデータを削除する データの圧縮形式を検討する コンピュートコスト 現在使用されていないデータの更新を停止する SQL のワークロードには SQL に最適化された SQL warehouse を使用する 数秒で起動・スケールアップできる (= 起動のための時間に料金がかからない) serverless SQL warehouse 等を利用する 料金の削減 ストレージコスト すぐに参照されることのない古いデータを S3 Glacier Deep Archive などのより安価なストレージクラスにアーカイブする 5 コンピュートコスト serverless のコンピュートが使えない場合は spot instance やコミットメント割引の利用を検討する コスト最適化のベストプラクティス | Databricks Documentation も読んでおきたい。\nクラウド請求データの処理 FinOps のフェーズによってクラウド請求データをどこから取得するかが異なる。\n初期においてはクラウド事業者から送られてくる請求明細書や AWS Cost Explorer 等のネイティブツールを見ていればよいが、高度な FinOps 運用をするためにはより詳細で粒度が小さい請求データが必要となる。\n例えば AWS では CUR (Cost and User Report) として詳細な請求データが提供されている。\n大規模にクラウドを利用している組織では月間数十億行になることもあるとのこと。\nFinOps 運用ではこれを near real time で処理し、各チームが閲覧するレポートにしたい。\nこれは正にデータ基盤におけるデータパイプラインの処理ということになり、いわゆる一般的なデータパイプラインの処理として当てはめることができるだろう。\n当然ここではデータ品質や BI 等の要素も考えることになる。\nデータ品質が悪いとエンジニアはレポートを信用できなくなり、FinOps が浸透しなくなってしまう。\nただし本書では FinOps 導入初期においては自前でデータ処理するよりも FinOps 系のツールやサービスなど third party 製品や OSS 等を使用することを推奨している。\n自前で FinOps データパイプラインを実装するのは、ある程度 FinOps にこなれてからがいい。\n(自前で実装するとたぶん ELT のクエリは複雑になる)\nデータマネジメントとの類似点 FinOps は組織の文化的変革であり、その実装とプロセスには経営層の賛同が必要とのことである。\nDMBOK においてデータマネジメントについても似たようなことが書いてあったのを思い出した。\n本書の第3章「文化的転換とFinOpsチーム」や第6章「FinOpsの導入」においてどうやって組織に新しい文化をインストールするかについて書かれており、データマネジメントの導入においても参考にできそうだと思った。\n例えば第6章では FinOps の成熟度ごとに経営層にどうやって説明するかについて記載されている。\nまとめ 『クラウドFinOps 第2版』、読むのに時間かかったけどクラウドという常に課金される世界におけるコストについての考え方を知ることができたのは良かった。\n自分たちが使っているお金のことはわかっておいた方がいい。\nそしてデータエンジニアとの関わりも深そうだということがわかった。\n余談だけど、データ基盤はコストセンターと見られることが多いと何かで読んだけど、例えば FinOps に絡めることで利益への貢献を訴えることもできるかもしれない。\n原著が書かれた時点から FinOps の定義も基本原則も少し変更が入っているらしく、本書と FinOps Foundation のサイトを見比べると若干の違いがあることが分かる。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n「プリウス効果」とはハイブリッドカーのプリウスに乗るとエコな運転になる効果のこと。運転席のダッシュボードでリアルタイムに現在の消費電力がわかるようになっており、運転手はそれを見て消費電力が少ない運転をするようになるらしい。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nクラウド事業者における Savings Plan, Reserved Instance, 確約利用割引など、一定量のリソース利用料を事前コミットすることで当該リソースを通常より割り引かれた価格で利用できること。奥が深く、難しい。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n「料金」というのは本書ではインスタンスやストレージの時間あたりの料金のこと。この記事でも本書に合わせて単に料金と呼ぶことにする。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDatabricksにおけるアーカイブのサポート | Databricks Documentation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://soonraah.github.io/posts/cloud-finops-for-data-engineer/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003e書籍『クラウドFinOps 第2版』を読んだところ、FinOps にデータマネジメントやデータエンジニアリングに深く関連する内容があるということがわかったのでまとめてみる。\u003c/p\u003e\n\u003ch2 id=\"書籍について\"\u003e書籍について\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.oreilly.co.jp/books/9784814401086/\"\u003eJ.R. Storment; Mike Fulle. クラウドFinOps 第2版 協調的でリアルタイムなクラウド価値の意思決定 O\u0026rsquo;Reilly Japan.\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2025年3月に出版。\u003cbr\u003e\nちなみに原著の初版は2019年、第2版は2023年。\u003c/p\u003e\n\u003cp\u003eタイトルのとおり FinOps (後述) について書かれた書籍となっている。\u003cbr\u003e\n著者は両名とも \u003ca href=\"https://www.finops.org/\"\u003eFinOps Foundation\u003c/a\u003e の関係者であり、本文中にも随所に FinOps Foundation についての記載が出てくる。\u003c/p\u003e\n\u003cp\u003e私はデータエンジニア、ソフトウェアエンジニアとして日々 AWS その他のクラウドサービスを利用している。\u003cbr\u003e\nクラウドサービス上に例えばデータ基盤等を構築し、ビジネス上の価値を提供している。\u003c/p\u003e\n\u003cp\u003eその一方でクラウドを使うということは料金的な意味でのコストがかかるということでもある。\u003cbr\u003e\nもちろん支払うコストは少ない方がいい。\u003cbr\u003e\nそれは分かるのだが、それ以上のクラウドコストについての体系的な考え方を持ち合わせていなかった。\u003cbr\u003e\n毎日それなりの額を使ってるのにね。\u003c/p\u003e\n\u003cp\u003eというのが本書を読もうと思った理由だった。\u003c/p\u003e\n\u003ch2 id=\"finops-とは\"\u003eFinOps とは\u003c/h2\u003e\n\u003ch3 id=\"定義\"\u003e定義\u003c/h3\u003e\n\u003cp\u003eこれを書いている2025年8月現在における \u003ca href=\"https://www.finops.org/introduction/what-is-finops/\"\u003eFinOps Foundation\u003c/a\u003e での定義は以下のようになっている。\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;FinOps is an operational framework and cultural practice which maximizes the business value of cloud and technology, enables timely data-driven decision making, and creates financial accountability through collaboration between engineering, finance, and business teams.\u0026rdquo;\u003c/p\u003e","title":"データエンジニアから見るクラウド FinOps"},{"content":"このポストについて データ基盤移行について書いていくシリーズです。\nシリーズ一覧はこちらから。\n前回 Part 3. アーキテクチャ編ではどういったシステム構成にしたかを書きました。\n今回はその技術スタックへと移行するための苦労と効率化について書きます。\n(次は CI/CD の話をすると書きましたが…スマンありゃウソだった)\nスコープ 今回はやや小さいスコープの話です。\nデータ基盤における ETL (ELT) 処理の移行作業を対象としています。\n移行作業における工数的な課題を AI ワークフローを作って効率化して軽減したという話になります。\nETL 以外の移行作業は今回はスコープ外となります。\n課題 旧データ基盤から新データ基盤へと table およびそれを更新するための処理を移行するにあたり工数面での課題が2つあります。\n技術スタックの移行 column 命名などの標準化 これらについて述べます。\n技術スタックの移行 データ基盤の移行において、新旧の環境で技術スタックは次のようになっています。\n旧データ基盤 ETL: Glue Job 新データ基盤 ELT: dbt-databricks つまり Glue Job の Python コードを dbt model、つまり SQL に翻訳する必要があり、それなりに手間がかかります。\nさらにこの Python コードは次のような問題もあり、移行のハードルを上げます。\nUDF を実装して特殊な処理を行っているケースがある Spark の API だけでなく Glue の API をふんだんに使っている (なるべく Spark に寄せればいいものを…) (ここ数年の業務で見た中で一番というぐらいに) コード品質が低い column 命名などの標準化 旧データ基盤は利用者への配慮があまりない状態で table の schema が作られており、利用者にとって使いにくいものとなっていました。\nそれを改善するため、新データ基盤では次のようなルールを導入しました。\n時刻を表す column は timestamp 型にし、column 名を \u0026lt;過去分詞形\u0026gt;_at とする ex. created_at 複数の要素を表す column は array 型にし、column 名を \u0026lt;複数形\u0026gt; とする ex. features etc. これらルールの適用により利用者にとっての利便性が向上しますが、一方でモデリング時に手間と注意力を要します。\n移行対象の table が数件程度ならこのような対応も人手で行えますが、数十〜の table を対応するには工数がかかりすぎます。(それでも少ない方ですが)\nとはいえ従来的なプログラミングで簡単に変換できるようなものでもありません。\nそこで AI ワークフローを作ることにしました。\nAI ワークフローって？ Anthropic のブログ記事 Building Effective AI Agents \\ Anthropic において、エージェントと対比する形でワークフローについて述べられています。\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths. Agents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks. (エージェントの定義はいろいろあるようですが) 今回作ったのはここで言うエージェントほど自由ではなく、ワークフローの範疇となります。\n同記事で挙げられている Workflow: Evaluator-optimizer を含んだワークフローを構築します。\nソリューション ワークフロー LangGraph を使って次のようなワークフローを実装しました。\nflowchart TD start((Start)) retrieve_target_table[\"①移行対象の旧データ基盤 table の schema を取得\"] retrieve_migrated_table[\"②移行済みの旧データ基盤 table の schema を取得\"] retrieve_migrated_dbt_model[\"③移行済みの dbt model を取得\"] generate_dbt_model[\"④LLM による dbt model 生成\"] sql_evaluator[\"⑤LLM による生成された dbt model の評価\"] end_((End)) start --\u003e retrieve_target_table retrieve_target_table --\u003e generate_dbt_model start --\u003e retrieve_migrated_table retrieve_migrated_table --\u003e retrieve_migrated_dbt_model retrieve_migrated_dbt_model --\u003e generate_dbt_model generate_dbt_model --\u003e sql_evaluator sql_evaluator -- dbt model が適切 --\u003e end_ sql_evaluator -- dbt model が不適切 --\u003e generate_dbt_model AI ワークフローと言いつつ、LLM を使っているステップは④と⑤だけだったりします。\n各ステップを解説します。\n①移行対象の旧データ基盤 table の schema を取得 最初に分岐した左のラインでは移行したい table の情報を取得します。\ntable 情報はどこから取ってきてもいいのですが、OpenMetadata を導入していたためそちらから取ってきました。\nこの table の schema を入力として dbt model を生成します。\n本来は Glue Job の Python コードを入力として与えるのが正当なやり方だと思いますが、ここではそうしません。\n複数ファイルに散らばる Python コードを読ませるのが多少手間なのと、(ここ数年の業務で見た中で一番というぐらいに) コード品質が低いコードを LLM に読ませるのが忍びないからです。\nじゃあ何の情報から dbt model を作るの？というのが右のラインになります。\n②移行済みの旧データ基盤 table の schema を取得 右のラインではすでに移行が完了した table についての情報を取得します。\nまずは移行対象と同様に schema 情報を OpenMetadata から取得します。\n③移行済みの dbt model を取得 次に移行済みの dbt model を取得します。\nこれはローカルに clone された git repository からファイルとして取得する想定です。\nさて、この②と③により dbt model 生成の入出力の例が得られました。\nこれにより dbt model 生成のプロンプトで few-shot prompting ができるようになります。\nfew-shot prompting は簡単に言うと、LLM への指示に対する入出力の例を提示することにより、得たい出力を得やすくするという手法です。\n④LLM による dbt model 生成 ここでようやく LLM の出番です。\nプロンプトには\ndbt model を作れという指示 指示の背景 column 命名などの標準化ルール few-shot prompting の入出力例 入力となる移行対象 table の schema を与えます。\nそうすると何らかの dbt model (SQL) っぽいものが出力されました。\n⑤LLM による生成された dbt model の評価 ④で終わってもよかったのですが、生成物を LLM で評価するというフェーズも追加しました。\n④のプロンプトとその出力の dbt model を LLM に与え、「タスクの実行結果は適切だったと思うか？」と問いかけます。\n適切なら終了します。めでたしめでたし。\n不適切と判断された場合はその理由つけて④に差し戻し、評価結果に配慮させながら④をやり直します。\n無限ループにならないよう N 回繰り返すと失敗するような制御も必要です。\nこの LLM に評価させるというのが前述の Building Effective AI Agents \\ Anthropic における Workflow: Evaluator-optimizer となります。\nagent desing pattern においては self-reflection とも言われます。\n技術スタック ワークフローの実装には次のような技術スタックを使いました。\nLangGraph: ワークフローの構成 LangChain: ワークフローの node 内での prompt 生成、LLM 呼び出し、結果成形などの一連の処理 Amazon Bedrock: LLM は Bedrock 経由で Claude 3.7 Sonnet を利用 これらを使ったワークフロー実装はそこまで難しいものではなく、だいたい1週間ちょっとぐらいでコーディングしました。\nワークフローの導入効果 一部の table 移行で試したところ、ちゃんと測ったわけではありませんが体感で50~80%ぐらいは工数削減できたかなと思います。\n比較的ローコストで大きく工数を削減できたのでこの結果には満足しています。\n勉強したての AI ワークフローのユースケースとして実際に役に立つものを作れたのも良かったです。\nfew-shot prompting だのみなので、他の dbt model と異なる特殊な処理をしているケースには対応が難しく、そういった場合は人間様の出番になります。\nまあ作業を100%置き換えることを目指していたわけではないため、それは良しとしています。\n何回か試した中では評価で不適切とみなされることはありませんでした。\nこれは dbt model 生成とその評価に同じ LLM を使っているからかもしれません。\n人間も自分の仕事を自分で評価すると甘くなります。\nself-reflection ではなく、評価に別の LLM を使うという crossーreflection という agent design pattern にするといいかもしれません。\nQ\u0026amp;A Q. ワークフロー作らんでも Cursor や Cline みたいなエージェントに命令したらええんちゃうん？ A.\nおそらくそのやり方でもできるでしょう。\nもっと言うと Glue Job の Python コードを含むプロジェクトのファイルをまるっとコンテキストとして持てるのであれば、精度も上がるかもしれません。\nただ残念ながら弊社は出遅れており、そういったソフトウェア開発エージェントがまだ利用できない状態です…\nワークフローを自前で作ることのメリットを強いて上げるのであれば、決まった処理の流れを曖昧な自然言語ではなくコードで定義できるというところになるでしょうか。\nQ. Glue Job は Spark やろ？せやったらそのままのコードで Databricks に移行できるんちゃうん？ A.\nApache Spark はプログラミングモデルは DataFrame 形式で非常にとっつきやすいのですが、アーキテクチャを理解していないとパフォーマンスの問題に対応できません。\nアーキテクチャを理解せずに Spark を使っている開発者も割と多いです。\n弊社もそういう状況で Spark を理解するエンジニアを安定的に雇用するのも困難なため、このデータ基盤移行によりコードとしての Spark は捨てて ELT については完全に SQL 化する運用にシフトしようとしています。\nまた、前述のとおり Spark のコードと言っても Glue Job の API が結構入り込んでいてポータビリティが低くなってしまっていました。\nさらに (ここ数年の業務で見た中で一番というぐらいに) コード品質が低く、これを残したくない気持ちがありました。\nQ. AI ワークフローはどないして勉強したらええのん？ A.\n今回の技術選定、ひいてはそもそものワークフロー作成のアイディアは以下の書籍に大きく影響を受けています。\n西見 公宏; 吉田 真吾; 大嶋 勇樹. LangChainとLangGraphによるRAG・AIエージェント［実践］入門 エンジニア選書. 株式会社技術評論社. 個人的にはとても面白い内容でした。\nAI エージェントやワークフローについて学ぶのにいい書籍だと思います。(が、この分野は古くなるのも早いので注意)\nこの書籍について書いたポストもご参照いただければと。\n次回予告 次こそ CI/CD の話をします！\n","permalink":"https://soonraah.github.io/posts/ordinary-data-platform-migration-part-4/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003eデータ基盤移行について書いていくシリーズです。\u003cbr\u003e\nシリーズ一覧は\u003ca href=\"/tags/%E3%81%B5%E3%81%A4%E3%81%86%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E5%9F%BA%E7%9B%A4%E7%A7%BB%E8%A1%8C/\"\u003eこちら\u003c/a\u003eから。\u003c/p\u003e\n\u003cp\u003e前回 \u003ca href=\"/posts/ordinary-data-platform-migration-part-3/\"\u003ePart 3. アーキテクチャ編\u003c/a\u003eではどういったシステム構成にしたかを書きました。\u003cbr\u003e\n今回はその技術スタックへと移行するための苦労と効率化について書きます。\u003c/p\u003e\n\u003cp\u003e(次は CI/CD の話をすると書きましたが…スマンありゃウソだった)\u003c/p\u003e\n\u003ch2 id=\"スコープ\"\u003eスコープ\u003c/h2\u003e\n\u003cp\u003e今回はやや小さいスコープの話です。\u003cbr\u003e\nデータ基盤における ETL (ELT) 処理の移行作業を対象としています。\u003cbr\u003e\n移行作業における工数的な課題を AI ワークフローを作って効率化して軽減したという話になります。\u003cbr\u003e\nETL 以外の移行作業は今回はスコープ外となります。\u003c/p\u003e\n\u003ch2 id=\"課題\"\u003e課題\u003c/h2\u003e\n\u003cp\u003e旧データ基盤から新データ基盤へと table およびそれを更新するための処理を移行するにあたり工数面での課題が2つあります。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e技術スタックの移行\u003c/li\u003e\n\u003cli\u003ecolumn 命名などの標準化\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこれらについて述べます。\u003c/p\u003e\n\u003ch3 id=\"技術スタックの移行\"\u003e技術スタックの移行\u003c/h3\u003e\n\u003cp\u003eデータ基盤の移行において、新旧の環境で技術スタックは次のようになっています。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e旧データ基盤\n\u003cul\u003e\n\u003cli\u003eETL: Glue Job\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e新データ基盤\n\u003cul\u003e\n\u003cli\u003eELT: dbt-databricks\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eつまり Glue Job の Python コードを dbt model、つまり SQL に翻訳する必要があり、それなりに手間がかかります。\u003cbr\u003e\nさらにこの Python コードは次のような問題もあり、移行のハードルを上げます。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUDF を実装して特殊な処理を行っているケースがある\u003c/li\u003e\n\u003cli\u003eSpark の API だけでなく Glue の API をふんだんに使っている (なるべく Spark に寄せればいいものを…)\u003c/li\u003e\n\u003cli\u003e(ここ数年の業務で見た中で一番というぐらいに) コード品質が低い\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"column-命名などの標準化\"\u003ecolumn 命名などの標準化\u003c/h3\u003e\n\u003cp\u003e旧データ基盤は利用者への配慮があまりない状態で table の schema が作られており、利用者にとって使いにくいものとなっていました。\u003cbr\u003e\nそれを改善するため、新データ基盤では次のようなルールを導入しました。\u003c/p\u003e","title":"ふつうのデータ基盤移行 - Part 4. AI ワークフローで移行作業効率化編"},{"content":"このポストについて データ基盤移行について書いていくシリーズです。\nシリーズ一覧はこちらから。\n前回 Part 2. 技術選定編では技術選定について書きました。\n今回はそれを踏まえた結果としてどのようなアーキテクチャになったかを書きます。\nスコープ 前回の記事ではプラットフォームとして Databricks を選定したことやその経緯について記載しました。\n一方、それより詳細な技術スタックを含むシステムアーキテクチャについては示していませんでした。\n例えばデータ基盤では通常次のような技術スタックについて考える必要があります。\nデータ取込 workflow orchestration ELT (or ETL) storage これらについて述べ、またデータ基盤の階層構造についても説明します。\nシステムアーキテクチャ データ基盤のシステム・アーキテクチャです。\nよく混同されがちですが、データアーキテクチャではありません。\nAWS + Databricks の構成をベースとして構築されています。\n概要図 データ取込 現時点ではデータソースとしては S3 に置かれた半構造化データ (JSON)、RDS がメインとなっています。\nこれら2つの取込方法について述べます。\nまず、S3 のデータは SQL の copy into 文により取り込んでいます。\nGet started using COPY INTO to load data | Databricks Documentation Auto Loader を使う方が Databricks 的でありそれも検討したのですが、schema evolution や冪等性など検討した結果として copy into を採用しました。\nRDS からのデータ取込は foreign catalog 経由で行います。\nManage and work with foreign catalogs | Databricks Documentation foreign catalog は RDS など Databricks の外部の DB に対して、Databricks からクエリを投げることができる仕組みです。\nいわゆる federated query であり、Databricks 上で Databricks の table と RDS の table を join できたりします。\n現時点では Fivetran や Airbyte などは使っていませんが、今後検討するかもしれません。\nまた、Databricks さんとしても Lakeflow Connect としてこの領域を頑張り始めたので、そちらも期待しています。\nWhat is Lakeflow Connect? | Databricks Documentation Workflow Orchestration データパイプライン全体の処理の依存関係を管理する workflow orchestration には Databricks Jobs を使いました。\nOrchestration using Databricks Jobs | Databricks Documentation Databricks 上でもっともライトに使える workflow orchestration ということでこれにしています。\nDatabricks Jobs では job の中に複数の task があり、その task の依存関係で処理の流れが構築されます。\nApache Airflow などと似たようなものイメージしていただければと。\n一方で job の定義は Airflow と違い、Terraform などの IaC で行うことになります。\nこれは少し面倒ですね。\n我々の job では後述の dbt やちょっとした Python コードが実行されています。\nELT ELT には dbt を使っています。\nDatabricks には DLT (Delta Live Table の略。こっちの dlt とは別物) という dbt と似たことができる機能がありますが、dbt を選びました。\ndbt 周辺のエコシステム、および dbt の方が移植性があることが選定理由です。\nDatabricks というとかつては Apache Spark のイメージが強かったのですが、最近は dbt から SQL で使うケースも多いみたいです。\nSpark で実装もできなくはないのですが、Spark のアーキテクチャを理解してパフォーマンスチューニングまでできるレベルのエンジニアは少ないので直接 Spark のコードを書く方針は取りませんでした。\n実質的には Photon という Spark の技術をベースにしたエンジンで SQL が実行されるわけではありますが。\nデータパイプラインの大半を dbt でやっておけば、仮に Databricks Jobs より他のがいいってなったときでも workflow orchestration の変更が比較的容易だという打算もあります。\n…なんですが、結局 dbt 以外の「ちょっとした Python コード」も拡大しているのでそんな容易にもならいのでしょうね、きっと。\n以下余談。\n旧データ基盤では ETL で Glue Job が使われていましたが、Spark のアーキテクチャを理解していないエンジニアによって作られていたためひどいパフォーマンスでした。\nSpark では SQL-like なインターフェースが提供されておりプログラミングモデルとしてはとっつきやすいのですが、アーキテクチャを理解して実装しないとすごく無駄な動きをしてしまったりします。\nそういうこともあり、SQL + dbt の枠組みでやるぞとなりました。\nData Catalog Databricks を使うのであれば data catalog としては Unity Catalog を使うのが自然でしょう。\nWhat is Unity Catalog? | Databricks Documentation Unity Catalog は Databricks の中でも中核的な機能であり、全貌を説明するのは難しいです。\ndata catalog における table や column の description は dbt project の YAML でバージョン管理されています。\nStorage データ基盤のユーザーが意識することは少ないですが、データの実体は我々が管理する S3 bucket 上に配置されます。\nこのように storage が computing のリソースから分離されていることにより、データ容量と処理量のそれぞれに応じて別々にスケールすることができます。\nデータは Delta Lake 形式で保存されており、Unity Catalog 経由で SQL などで利用されます。\nDelta Lake 形式であることにより従来の data lake にはなかった ACID transaction が実現され、lakehouse architecture の体をなします。\nIntroduction — Delta Lake Documentation lakehouse architecture については以下の論文がおすすめです。\nArmbrust, M., Ghodsi, A., Xin, R., Zaharia, M., \u0026amp; Berkeley, U. (n.d.). Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics. Retrieved January 16, 2022, from http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf データの階層構造 アーキテクチャかというと微妙ですが、ELT によりデータが加工されて作られる階層構造についても触れておきます。\n概要図 全体 このようにデータ基盤内でデータの処理過程を階層に分けるというのはよくある話ですが、階層の分け方や命名は各組織で結構違いがあると感じています。\nここでは medallion architecture と dbt best practice を参考にして各階層を定義しました。\nメダリオンアーキテクチャ (medallion architecture) | Databricks How we structure our dbt projects | dbt Developer Hub まずは medallion architecture について。\n下記の記事の図を見てほしいのですが、データ基盤における各階層の命名の仕方は本当にいろいろあります。\nMastering Data Layers in data platform | Medium その中から medallion architecture を選んだわけですが、medallion architecture における各階層の命名には以下のメリットがあります。\nbronze, silver, gold という命名は誰にでもわかりやすく、またこの順で発展していることも示せている data lake, data warehouse など、近い分野において別の意味で使われることのある言葉は使われていない (Databricks が提唱している考えであり、Databricks のコンテキストで話がしやすい) これに加え、dbt best practice から \u0026ldquo;staging\u0026rdquo; という層を追加しています。\ndbt を使うことが決まっていたことから dbt の best practice を見ていましたが、dbt が推奨する階層において medallion architecture の定義と一致しないのが staging でした。\nよってその層を medallion architecture に追加しました。\nちなみに各階層は Databricks の schema として実装されます。\nBronze Layer この階層ではほぼ未加工の状態で取り込んだだけのデータを保持します。\n「ほぼ」と書いたのは、データソースや load された時刻など少しのメタデータを加えるためです。\nこれらのメタデータは問題発生時の調査や監査に使用されます。\nStaging Layer この階層では各 column の簡易な変更のみを行います。\nこの後の処理がしやすいよう、データの体裁を整えるようなイメージです。\n簡易な変更とは例えば以下のような処理です。\nデータ基盤の命名規則にもとづいた column の命名の変更 型変換 (ex. string 型で渡された日付を date 型に) 単位の変換 (ex. フィートをメートルに) struct のフラット化 ただしここではビジネスロジックの適用や複雑な計算、join や group by などは行いません。\n他の階層は主に table として実装しますが、この階層のみは view として作ります。\nSilver Layer この階層ではデータを汎用的な分析に利用できるレベルにします。\nビジネスロジックにもとづくフィルタリングや結合、複雑な処理、重複排除などを行います。\nGold Layer この階層では各ユースケースに合わせたデータを生成します。\n特定のレポートや他のシステムへの input のため、必要に応じて集計などを行います。\nその他 本来であれば silver layer あたりで dimensional modeling や data vault などを取り入れたいのですが、現状ではできていません。\n移行前の旧データ基盤ではそういったデータモデリングが行われておらず、データ基盤のアーキテクチャの移行と合わせてデータモデリングの考えを導入するのはハードルが高すぎるとなったためです。\nこのあたりは今後の課題です。\n次回予告 今回アーキテクチャの話ができたので、開発するものがイメージしやすくなったのではと思います。\n次は CI/CD の話でもしますかね。\n","permalink":"https://soonraah.github.io/posts/ordinary-data-platform-migration-part-3/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003eデータ基盤移行について書いていくシリーズです。\u003cbr\u003e\nシリーズ一覧は\u003ca href=\"/tags/%E3%81%B5%E3%81%A4%E3%81%86%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E5%9F%BA%E7%9B%A4%E7%A7%BB%E8%A1%8C/\"\u003eこちら\u003c/a\u003eから。\u003c/p\u003e\n\u003cp\u003e前回 \u003ca href=\"/posts/ordinary-data-platform-migration-part-2/\"\u003ePart 2. 技術選定編\u003c/a\u003eでは技術選定について書きました。\u003cbr\u003e\n今回はそれを踏まえた結果としてどのようなアーキテクチャになったかを書きます。\u003c/p\u003e\n\u003ch2 id=\"スコープ\"\u003eスコープ\u003c/h2\u003e\n\u003cp\u003e前回の記事ではプラットフォームとして Databricks を選定したことやその経緯について記載しました。\u003cbr\u003e\n一方、それより詳細な技術スタックを含むシステムアーキテクチャについては示していませんでした。\u003cbr\u003e\n例えばデータ基盤では通常次のような技術スタックについて考える必要があります。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eデータ取込\u003c/li\u003e\n\u003cli\u003eworkflow orchestration\u003c/li\u003e\n\u003cli\u003eELT (or ETL)\u003c/li\u003e\n\u003cli\u003estorage\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこれらについて述べ、またデータ基盤の階層構造についても説明します。\u003c/p\u003e\n\u003ch2 id=\"システムアーキテクチャ\"\u003eシステムアーキテクチャ\u003c/h2\u003e\n\u003cp\u003eデータ基盤のシステム・アーキテクチャです。\u003cbr\u003e\nよく混同されがちですが、\u003ca href=\"/posts/dmbok-chapter-4/\"\u003eデータアーキテクチャ\u003c/a\u003eではありません。\u003c/p\u003e\n\u003cp\u003eAWS + Databricks の構成をベースとして構築されています。\u003c/p\u003e\n\u003ch3 id=\"概要図\"\u003e概要図\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"system architecture\" loading=\"lazy\" src=\"/image/ordinary-data-plagform-migration/system-architecture.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"データ取込\"\u003eデータ取込\u003c/h3\u003e\n\u003cp\u003e現時点ではデータソースとしては S3 に置かれた半構造化データ (JSON)、RDS がメインとなっています。\u003cbr\u003e\nこれら2つの取込方法について述べます。\u003c/p\u003e\n\u003cp\u003eまず、S3 のデータは SQL の \u003ccode\u003ecopy into\u003c/code\u003e 文により取り込んでいます。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/copy-into/\"\u003eGet started using COPY INTO to load data | Databricks Documentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/auto-loader\"\u003eAuto Loader\u003c/a\u003e を使う方が Databricks 的でありそれも検討したのですが、schema evolution や冪等性など検討した結果として \u003ccode\u003ecopy into\u003c/code\u003e を採用しました。\u003c/p\u003e\n\u003cp\u003eRDS からのデータ取込は foreign catalog 経由で行います。\u003c/p\u003e","title":"ふつうのデータ基盤移行 - Part 3. アーキテクチャ編"},{"content":"書籍について 西見 公宏; 吉田 真吾; 大嶋 勇樹. LangChainとLangGraphによるRAG・AIエージェント［実践］入門 エンジニア選書. 株式会社技術評論社. タイトルのとおりで RAG や AI エージェントについて書かれた本。\n出版は2024年11月。\nLangChain や LangGraph を使ったサンプルコードが豊富に載っている。\n著者の方は3名ともジェネラティブエージェンツ社の方で、AI エージェントを扱う会社らしい。\n前作として『ChatGPT/LangChainによるチャットシステム構築［実践］入門』という本もあり、本書の何章かは前作の内容を引き継いでアップデートしているとのこと。\n個人的にはこれまで LLM や LangChain など追えていなかったので勉強になったし、自分でコードを写経して動かしてみるのがとても面白かった。\nAI エージェントまわりの技術を実感したい人におすすめ。\nちなみにサンプルコードを動かすためには OpenAI をはじめとする様々なサービスに登録する必要があり、料金が発生するものも含まれている。\nこのポストについて 書籍「LangChainとLangGraphによるRAG・AIエージェント［実践］入門」を読んで、面白かったので内容をまとめる。\n各章ごとに内容を挙げていってもいいのだが、ここではそうはせず、本書に登場する言葉や概念をまとめていくことにする。\nちなみにこの記事で紹介するプロンプト等は書籍そのままではなく、少し変更している。\n実際のプロンプトを知りたい場合は書籍を読んでください。\nプロンプトエンジニアリング これは知っている人も多いだろう。\nプロンプトとは主に自然言語で記述される、LLM に与える命令のこと。\nLLM をアプリケーションに組み込む場合はプロンプトはテンプレート化し、入力データをそれに差し込む形となる。\nLLM は必ずしも人間の出す命令に対して望ましい回答を出力してくれるわけではない。\n望ましい回答を出力してもらえるよう、プロンプトを工夫するテクニックがプロンプトエンジニアリングである。\n本書ではプロンプトエンジニアリングの具体的な手法として次の3つが紹介されていた。\nちなみにここで例として示している出力は実際に OpenAI の LLM gpt-4o-mini で出力したものである。\nZero-shot プロンプティング 簡単なタスクであれば、特に追加の情報がなくとも (=Zero-shot) 望ましい回答を得ることができる。\nsystem: 次の日本酒のレビューをポジティブ・ネガティブ・中立のどれかに分類してください。 user: ふくよかで芳醇な香り 出力\nポジティブ Few-shot プロンプティング より複雑なタスクになった場合、デモンストレーションを与えることで回答の精度を高めることができる。\nいくつかのデモンストレーションを示すため、Zero-shot ではなく Few-shot である。\nsystem: 次の内容が日本酒についての話かどうかを判定してください。 user: ふくよかで芳醇な香り assistant: True user: 明日は雨らしい assistant: False user: AI エージェントはソフトウェア開発を変えるだろう 出力\nFalse 回答の精度に加え、出力の形式も示すことができる。\nZero-shot Chain-of-Thought プロンプティング system: 以下の計算の結果のみ出力してください。 user: たかし君が200円のリンゴを3個、100円のミカンを2個買ったら財布に150円残りました。たかし君は最初何円持っていたでしょう？ 出力\nたかし君は最初500円持っていました。 この例では回答が間違っている。(またはたかし君の万引きが発覚したか)\nこのように中間的な推論が必要な場合に有効なのが Zero-shot Chain-of-Thought プロンプティングである。\nプロンプトを少し変えてみる。\nsystem: 以下の計算の結果をステップバイステップで考えてください。 user: たかし君が200円のリンゴを3個、100円のミカンを2個買ったら財布に150円残りました。たかし君は最初何円持っていたでしょう？ 出力\nたかし君の最初の金額を求めるために、次のステップで計算を進めます。 1. **リンゴの合計金額を計算する**： リンゴは1個200円で3個買ったので、 \\[ 200円 \\times 3 = 600円 \\] 2. **ミカンの合計金額を計算する**： ミカンは1個100円で2個買ったので、 \\[ 100円 \\times 2 = 200円 \\] 3. **リンゴとミカンの合計金額を求める**： リンゴとミカンの合計金額は、 \\[ 600円 + 200円 = 800円 \\] 4. **財布に残っている金額を考慮する**： たかし君は買い物をした後に150円残っているので、最初の金額を求めるために800円（使った金額）と150円（残っている金額）を足します。 \\[ 800円 + 150円 = 950円 \\] したがって、たかし君は最初に950円持っていたことになります。 見事正解した。\nこのように「ステップバイステップで考える」ことを示すだけで、回答が正確になるという有効な手法である。\nプロンプトエンジニアリングには上に挙げた以外の手法もある。\n詳しくは Prompt Engineering Guide | Prompt Engineering Guide などを参考にするとよい。\nLangChain LangChain の概要 LangChain とは LLM アプリケーションを開発するためのフレームワークである。\n公式ドキュメントによると、\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment: Build your applications using LangChain\u0026rsquo;s open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support. Productionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence. Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform. のように開発ライフサイクルの各ステージについての機能がある。\nProductionization の LangSmith については後述するとして、ここでは主に Development について述べる。\nちなみに書籍では Deployment の部分についてはあまり触れられていなかった。\nLangChain の構成 LangChain は Python や JavaScript のライブラリとして OSS で提供されている。\n書籍では主に Python で紹介されていたので、ここでもそれにならう。\nLangChain には次のようなパッケージが含まれる。\nlangchain-core ベースとなる抽象化や LCEL (後述) langchain-openai や langchain-anthropic など OpenAI や Anthropic などパートナーとのインテグレーション langchain-community パートナーのパッケージとして独立していない各種インテグレーション langchain LLM アプリケーション特定のユースケースに特化した機能 langchain-text-splitter テキストの chunk への分割 langchain-experimental 研究・実験目的のコードなど コード例 実際にコードを見るとイメージしやすいというところで見てみよう。\nただしセットアップ方法などはこのポストでは省略。\nfrom langchain_core.prompts import ChatPromptTemplate from langchain_core.output_parsers import StrOutputParser from langchain_openai import ChatOpenAI # プロンプトを用意 prompt = ChatPromptTemplate.from_template( \u0026#34;{drink}の原料を教えてください。\u0026#34; ) # LLM を用意 llm = ChatOpenAI(model=\u0026#34;gpt-4o-mini\u0026#34;, temperature=0.0) # parser を用意 output_parser = StrOutputParser() # 用意したものをつなげて処理の連鎖を作成 chain = prompt | llm | output_parser # 実行 result = chain.invoke({\u0026#34;drink\u0026#34;: \u0026#34;日本酒\u0026#34;}) print(result) 出力\n日本酒の主な原料は以下の4つです。 1. **米**: 日本酒の基本的な原料で、特に酒造りに適した「酒米」が使用されます。酒米は、一般的な食用米よりも大きく、デンプン含量が高いのが特徴です。 2. **水**: 日本酒の製造には大量の水が必要です。水の質は日本酒の味に大きく影響するため、清らかでミネラルバランスの良い水が求められます。 3. **酵母**: 発酵を促進するために使用される微生物で、アルコールと香りを生成します。日本酒専用の酵母が多く存在し、それぞれ異なる風味を引き出します。 4. **麹（こうじ）**: 米に麹菌を繁殖させたもので、デンプンを糖に変える役割を果たします。麹は日本酒の発酵過程において非常に重要な役割を担っています。 これらの原料を組み合わせて、発酵・熟成を経て日本酒が作られます。 このように LLM アプリケーションに必要な要素 (Runnable) を | でつなげて鎖のように記述する方法を LangChain Expression Language (LCEL) と言う。\n関数型プログラミングっぽくなっており、プログラミングとしての型が提示されていてとてもいいと思った。\nプロンプトはテンプレート化されているため、例えば日本酒をビールに変えて実行することもできる。\n# 実行 result = chain.invoke({\u0026#34;drink\u0026#34;: \u0026#34;ビール\u0026#34;}) print(result) 上記は非常に簡単な例だが、次のような複雑なこともできる。\n詳しくは書籍を参照。\nPydantic のモデルを出力形式として指定 RunnableLambda で任意の関数を chain に差し込む RunnableParallel で並列処理 RunnablePassthrough で処理をスルー RAG の実装 発展型の Advanced RAG も含む LangSmith LangSmith とはプロダクショングレードの LLM アプリケーション構築のためのプラットフォーム。\nLangChain の開発元、LangChain, Inc. により提供されている web サービスである。\n以下の3つの機能がある。\nObservability: トレーシングなど Evals: LLM アプリケーションのオンライン／オフライン評価 Prompt Engineering: プロンプト管理 ここでは3つのうちの Observability, Evals について述べる。\nObservability LangChain による LLM アプリケーションを実行すると、LangSmith にその情報が送られ、詳細分析ができるというのがこの機能。\nトレーシングのためのセットアップを行っておく必要がある。\n例えば前述の LCEL の例で示したコードの実行結果は次のように表示される。\nLLM への Input/Output が表示されている。\nこの簡単な例ではあまりありがたみがわからないかもしれない。\nしかし後述するエージェントデザインパターンのような、複数の LLM への入出力を経て高度なタスクを実行するようなケースにおいては非常に強力なサポートになる。\nこれがないと各段階で LLM が何をやっているのかがわからない。(もしくは自分でロギングするか…これはこれでしんどい)\nEvals LangSmith を使って LLM アプリケーションのオフライン／オンライン評価を行うことができる。\nここではオフライン評価について見ていこう。\n書籍では Ragas という LLM アプリケーションの評価のためのライブラリを利用した RAG の評価を行っている。\nオフライン評価の流れは以下。\nちなみに書籍ではほとんどのコードは Google Colab で実行することになっている。\n(開発環境などと読み替えてもいいだろう)\nGoogle Colab 上でデータセットを生成 LangSmith にデータセットを登録 Google Colab 上でオフライン評価を実施し、LangSmith に登録 「このデータセットに対する評価」というのが Experiment という単位で LangSmith 上で管理され、LLM アプリケーションの性能改善に伴うメトリクス (評価指標) の変化を追うことができるようになる。\nこの例では context precision および answer relevancy というメトリクスを扱っている。\nRagas にはこの他にも様々なメトリクスがあるので、アプリケーションにあったものを選ぶ必要がある。\nList of available metrics - Ragas こちらのオフライン評価のコードはまあまあ複雑なので割愛。\n書籍のほぼすべてのコードは写経して実行したが、このオフライン評価の部分だけはうまくいかなかったため、\n「LangChainとLangGraphによるRAG・AIエージェント［実践］入門」でつまずいたことメモ:7章 #langchain - Qiita を参考にして作成・実行した。\nLLM アプリケーションのオフラインの性能評価の方法をまったく知らなかったので、個人的には新鮮だった。\n評価データも LLM で作るのか、と。\nAI エージェント 今、世間でも話題になっている AI エージェント。\nAI エージェントの定義はいろいろありそうだが、本書では次のようになっている。\nAIエージェントとは、複雑な目標を自律的に遂行できるAIシステムを指します。従来のAIシステムが特定のタスクに特化していたのに対し、LLMを活用したAIエージェントは、与えられた目標を達成するために必要な行動を自ら決定し、実行することができます。この自律性により、より汎用的で柔軟なLLMアプリの実現が可能になります。\nいくつかの汎用 LLM エージェントのフレームワークが例として挙げられている。\nAutoGPT BabyAGI AutoGen crewAI 書籍ではこれらのフレームワークについてそれぞれ軽い説明が記載されているが、このあたりからエージェントやばいなと思い始めた。\n特にマルチエージェントのアプローチはほぼチームビルディングであり、マネジメントでは？\nマルチエージェントの定義は次のように書かれていた。\nマルチステップなマルチエージェント：一連の処理の中で、複数のシステムプロンプトを使って、役割やステップごとに別々のAIエージェントで処理を行う、ワークフローの最適化を目的とした処理形態 マルチロールなマルチエージェント：異なるペルソナや役割を持たせた複数のエージェントを、目的に向かって協調動作させる形態 マルチエージェントでソフトウェア開発を自動化するためのフレームワークも紹介されていた。\nDevin ChatDev MetaGPT MAGIS Devin などは今すごく話題になっており、これらのエージェントで今後ソフトウェア開発は変わってくんだろうなと思っている。\nLangGraph LangGraph とは LLM を活用した複雑なワークフローを開発するためのライブラリである。\nワークフローといえば Apache Airflow だが、あの DAG のようなものをイメージすればよい。\n(ただし cyclic であることは許されているので DAG ではない)\nDAG の task にあたるところで LLM の呼び出しがあるようなイメージ。\n例えば書籍ではサンプルとして要件定義書生成 AI エージェントの例が記載されている。\nこのエージェントは雑に「◯◯のようなアプリを作りたい」と言うと要件定義書を作成してくれるといったもの。\nワークフローは次のようになっている。\nflowchart TD S@{ shape: circle, label: \"Start\" } generate_personas[\"`generate_personas ペルソナを生成`\"] conduct_intervies[\"`conduct_intervies 生成したペルソナにインタビュー`\"] evaluate_information[\"`evaluate_information インタビュー結果を評価`\"] generate_requirements[\"`generate_requirements 要件定義書を作成`\"] E@{ shape: circle, label: \"End\" } S --\u003e generate_personas generate_personas --\u003e conduct_intervies conduct_intervies --\u003e evaluate_information evaluate_information --\u003e generate_requirements evaluate_information -- 情報量が不十分な場合 --\u003e generate_personas generate_requirements --\u003e E 内部的にアプリのユーザーを想定したペルソナを作成し (generate_personas)、それに対してインタビューを行う (conduct_intervies)。\nそのインタビューの結果を評価し (evaluate_information)、情報が十分なら要件定義書を作成して (generate_requirements) 終了する。\n情報が不十分ならペルソナを追加で作成する。\nLangGraph は名前のとおりこのグラフ構造を表現するものとなっている。\nグラフのノード間のデータの引き渡しには state という概念があり、まずこれを定義してやる必要がある。\nclass InterviewState(BaseModel): user_request: str = Field(..., description=\u0026#34;ユーザーからのリクエスト\u0026#34;) personas: Annotated[list[Persona], operator.add] = Field( default_factory=list, description=\u0026#34;生成されたペルソナのリスト\u0026#34; ) ... 各ノードの処理も実装しておく。\nノード内の処理は LangChain で書くことができる。\nclass PersonaGenerator: def __init__(self, llm: ChatOpenAI, k: int = 5): self.llm = llm.with_structured_output(Personas) self.k = k def run(self, user_request: str) -\u0026gt; Personas: prompt = ChatPromptTemplate.from_messages( # ペルソナ作成用のプロンプト ... ) chain = prompt | self.llm # ペルソナを生成 return chain.invoke({\u0026#34;user_request\u0026#34;: user_request}) ... 各ノードの処理が実装できたらワークフローを構築する。\nclass DocumentationAgent: def __init__(self, llm: ChatOpenAI, k: Optional[int] = None): # 各種ジェネレータの初期化 self.persona_generator = PersonaGenerator(llm=llm, k=k) ... # グラフの作成 self.graph = self._create_graph() def _create_graph(self) -\u0026gt; StateGraph: # グラフの初期化 workflow = StateGraph(InterviewState) # 各ノードの追加 workflow.add_node(\u0026#34;generate_personas\u0026#34;, self._generate_personas) ... # エントリーポイントの設定 workflow.set_entry_point(\u0026#34;generate_personas\u0026#34;) # ノード間のエッジの追加 workflow.add_edge(\u0026#34;generate_personas\u0026#34;, \u0026#34;conduct_interviews\u0026#34;) workflow.add_edge(\u0026#34;conduct_interviews\u0026#34;, \u0026#34;evaluate_information\u0026#34;) # 条件付きエッジの追加 workflow.add_conditional_edges( \u0026#34;evaluate_information\u0026#34;, lambda state: not state.is_information_sufficient and state.iteration \u0026lt; 5, {True: \u0026#34;generate_personas\u0026#34;, False: \u0026#34;generate_requirements\u0026#34;}, ) workflow.add_edge(\u0026#34;generate_requirements\u0026#34;, END) # グラフのコンパイル return workflow.compile() def _generate_personas(self, state: InterviewState) -\u0026gt; dict[str, Any]: # ペルソナの生成 new_personas: Personas = self.persona_generator.run(state.user_request) return { \u0026#34;personas\u0026#34;: new_personas.personas, \u0026#34;iteration\u0026#34;: state.iteration + 1, } ... def run(self, user_request: str) -\u0026gt; str: # 初期状態の設定 initial_state = InterviewState(user_request=user_request) # グラフの実行 final_state = self.graph.invoke(initial_state) # 最終的な要件定義書の取得 return final_state[\u0026#34;requirements_doc\u0026#34;] コードはかなり端折ったが、エッセンスはわかっていただけたと思う。\n最後の run() を実行すれば要件定義書が作成される。\nこれを写経して実際に動かしてみたところ、それっぽい要件定義書ができて感動した。\nLangGraph でちょっとしたコードを書けばこのような高度？なワークフローが実行できるのである。\n内部的にはペルソナ作ってインタビューしてってやっているが、外部のユーザーからはそれはわからないようになっている。\n逆に言うとこの程度の複雑さのワークフローになってくると LangSmith のトレーシングがかなり意味を持つようになってくる。\nエージェントデザインパターン ソフトウェア設計の言葉で「デザインパターン」というものがある。\nこれは設計でよくある問題を解決するための設計思想やアプローチを示すもの。\nデザインパターン (ソフトウェア) - Wikipedia エージェントデザインパターンはこれの AI エージェント版のようなものであり、AI エージェントの設計のパターンを示す。\nYue Liu らによって提案された。\nAgent Design Pattern Catalogue: A Collection of Architectural Patterns for Foundation Model based Agents 全 18 のパターンが紹介されている。\n目標設定と計画設定 Passive Goal Creator Proactive Goal Creator Prompt/Response Optimizer Single-path Plan Generator Multi-path Plan Generator One-shot Model Querying Incremental Model Querying 推論の確実性向上 Retrieval-Augmented Generation (RAG) Self-Reflection Cross-Reflection Human-Reflection Agent Evaluator エージェント間の協調 Voting-based Cooperation Role-based Cooperation Debate-based Cooperation 入出力制御 Multimodal Guardrails Tool/Agent Registry Agent Adapter 書籍ではこのうちのいくつかについてのサンプルコードを示している。\nここでは簡単なところで Passive Goal Creator の例を見てみる。\nPassive Goal Creator はユーザーの入力から具体的な目標を生成するパターンであり、この結果が後続の LLM への入力となる。\n例えば次のようなプロンプトを用いて、目標を具体化する。\nユーザーの入力を分析し、明確で実行可能な目標を生成してください。 要件: 1. 目標は具体的かつ明確であり、実行可能なレベルで詳細化されている必要があります。 2. ... ユーザーの入力: {query} これを LLM に食わせてより具体的な目標を得、それにより後続の LLM が仕事をしやすくなるわけである。\nサンプルコードでは様々なパターンを追加して組み合わせていくような例が示されており、LLM アプリケーションが賢くなっていく様を見て取ることができた。\n体感として Self-Reflection, Cross-Reflection はなかなか有効だと感じた。\n所感 個人的に LLM まわりをあまり追えていなかったこともあり、冒頭に述べたとおりだがとても興味深く、面白く読むことができた。\nとはいえ挙げられていたライブラリなども含め、この分野は日進月歩で発展しているのでここで得た知識が陳腐化するのも早いだろう。\n以下は細かい所感。\nテストが難しそう 必要な情報を渡せていないのか？LLM の性能の問題なのか？の見分けが難しい LangGraph のノード単位でコードを分割して手厚く単体テストしたり、LangSmith で調べたりなどで頑張るのだろうか 独特の不気味さ LangChain のプログラムとしての美しさ、その一方での LLM という得体の知れないもの この2つが組み合わさっていることに何とも言えない不気味さを感じる こんな簡単なプログラミングで抽象度の高い問題が解ける という感動 何かいろんなことができそうな気がする、しらんけど LLM アプリケーションはほぼマネジメントでは？ agent design pattern とかマルチエージェントとか ここで得た知識を業務に適用することを画策中。\n面白いことができたらまた記事にするかも。\n","permalink":"https://soonraah.github.io/posts/reading-note-agent-book/","summary":"\u003ch2 id=\"書籍について\"\u003e書籍について\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://gihyo.jp/book/2024/978-4-297-14530-9\"\u003e西見 公宏; 吉田 真吾; 大嶋 勇樹. LangChainとLangGraphによるRAG・AIエージェント［実践］入門 エンジニア選書. 株式会社技術評論社.\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eタイトルのとおりで RAG や AI エージェントについて書かれた本。\u003cbr\u003e\n出版は2024年11月。\u003cbr\u003e\nLangChain や LangGraph を使ったサンプルコードが豊富に載っている。\u003cbr\u003e\n著者の方は3名ともジェネラティブエージェンツ社の方で、AI エージェントを扱う会社らしい。\u003cbr\u003e\n前作として『\u003ca href=\"https://gihyo.jp/book/2023/978-4-297-13839-4\"\u003eChatGPT/LangChainによるチャットシステム構築［実践］入門\u003c/a\u003e』という本もあり、本書の何章かは前作の内容を引き継いでアップデートしているとのこと。\u003c/p\u003e\n\u003cp\u003e個人的にはこれまで LLM や LangChain など追えていなかったので勉強になったし、自分でコードを写経して動かしてみるのがとても面白かった。\u003cbr\u003e\nAI エージェントまわりの技術を実感したい人におすすめ。\u003cbr\u003e\nちなみにサンプルコードを動かすためには OpenAI をはじめとする様々なサービスに登録する必要があり、料金が発生するものも含まれている。\u003c/p\u003e\n\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003e書籍「LangChainとLangGraphによるRAG・AIエージェント［実践］入門」を読んで、面白かったので内容をまとめる。\u003cbr\u003e\n各章ごとに内容を挙げていってもいいのだが、ここではそうはせず、本書に登場する言葉や概念をまとめていくことにする。\u003c/p\u003e\n\u003cp\u003eちなみにこの記事で紹介するプロンプト等は書籍そのままではなく、少し変更している。\u003cbr\u003e\n実際のプロンプトを知りたい場合は書籍を読んでください。\u003c/p\u003e\n\u003ch2 id=\"プロンプトエンジニアリング\"\u003eプロンプトエンジニアリング\u003c/h2\u003e\n\u003cp\u003eこれは知っている人も多いだろう。\u003cbr\u003e\nプロンプトとは主に自然言語で記述される、LLM に与える命令のこと。\u003cbr\u003e\nLLM をアプリケーションに組み込む場合はプロンプトはテンプレート化し、入力データをそれに差し込む形となる。\u003c/p\u003e\n\u003cp\u003eLLM は必ずしも人間の出す命令に対して望ましい回答を出力してくれるわけではない。\u003cbr\u003e\n望ましい回答を出力してもらえるよう、プロンプトを工夫するテクニックがプロンプトエンジニアリングである。\u003cbr\u003e\n本書ではプロンプトエンジニアリングの具体的な手法として次の3つが紹介されていた。\u003c/p\u003e\n\u003cp\u003eちなみにここで例として示している出力は実際に OpenAI の LLM \u003ccode\u003egpt-4o-mini\u003c/code\u003e で出力したものである。\u003c/p\u003e\n\u003ch3 id=\"zero-shot-プロンプティング\"\u003eZero-shot プロンプティング\u003c/h3\u003e\n\u003cp\u003e簡単なタスクであれば、特に追加の情報がなくとも (=Zero-shot) 望ましい回答を得ることができる。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esystem: 次の日本酒のレビューをポジティブ・ネガティブ・中立のどれかに分類してください。\nuser: ふくよかで芳醇な香り\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e出力\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eポジティブ\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"few-shot-プロンプティング\"\u003eFew-shot プロンプティング\u003c/h3\u003e\n\u003cp\u003eより複雑なタスクになった場合、デモンストレーションを与えることで回答の精度を高めることができる。\u003cbr\u003e\nいくつかのデモンストレーションを示すため、Zero-shot ではなく Few-shot である。\u003c/p\u003e","title":"読書メモ: LangChainとLangGraphによるRAG・AIエージェント［実践］入門"},{"content":"このポストについて データ基盤移行について書いていくシリーズです。\nシリーズ一覧はこちらから。\n前回は戦略策定 (実際は戦術) までのところを書きました。\n今回はそれを踏まえた技術選定、およびその後の予算獲得について書いていきます。\nまた、こちらは Databricks Advent Calendar 2024 シリーズ 2 の16日目の記事にもなっています。\nはいそうです、出落ちですが技術選定として Databricks を選ぶことになります。\nスコープ 前回 Part 1. 戦略策定編では概ねのロードマップが決まり、まずはデータ基盤のリアーキテクチャをやっていくことになりました。\nリアーキテクチャにおいてはどのような技術スタックを使っていくかが重要な選択になります。\nデータ基盤においてはデータ処理のためのストレージとコンピュートの選択がとても重要です。\n以降ではこの2つをあわせた DWH 製品の選定について書いていきます。\n「DHW 製品」という言葉は適切ではないかもしれませんが、ここではストレージ + コンピュートが組み合わさったものぐらいに考えてください。\nもちろんデータ基盤には他の技術要素もあり、それらも軽くない選択ですがこのポストでは割愛します。\n(気が向いたら別記事で書くかも)\n技術選定の目的 まず何のために技術スタックの置き換え、ひいては技術選定をするかの目的を明確にしておく必要があります。\n旧データ基盤では次のような技術スタックになっていました。\nストレージ: S3 コンピュート: Glue Job, Athena この構成には次のような課題がありました。\n主にこれらの課題を解決するために DWH 製品の乗り換えを検討することになりました。\ndbt との親和性の低さ 一貫したガバナンスの欠如 dbt との親和性の低さ 前回作成したロードマップにおいて、dbt の導入が課題解決における重要なポイントになっています。\ndbt の周辺エコシステムがデータ基盤の課題の解決に大きく貢献すると考えています。\nまた、データパイプラインの開発・運用の負荷も dbt 導入で軽減できそうです。\n旧データ基盤では Glue Job と Athena クエリを組み合わせた複雑なパイプラインになっており、table を1つ追加するだけでもいろいろなコードに手をいれる必要があります。\nほぼ SQL で実装でき、かつ宣言的にパイプライン構築できる dbt は魅力的です。\n仮に旧データ基盤に dbt を導入するとなると dbt-athena を使うことになります。\nただ dbt による Athena のサポートはやや弱く、dbt-athena はコミュニティ版から少し前に移管されたものですし、これを書いている2024年12月の時点で dbt Cloud の Athena のサポートはまだプレビューです。\n反論がある方もいらっしゃるかもしれませんが、モダンなデータ基盤構築において Athena はやや影が薄い印象があり、dbt のサポートの弱さもこれが原因だと思います。\n(ただし直近の re:Invent 2024 の内容からすると潮目が変わる可能性もありそうです)\nしたがってより存在感があり、エコシステムに受け入れられている DWH 製品にどこかのタイミングで乗り換えたいと以前から考えていました。\nつまりプロダクトレベルの data gravity にあやかりたい。\n一貫したガバナンスの欠如 旧データ基盤の技術スタックは寄せ集め感が強く、うまく表現できませんが全体として調和した上でガバナンスを持って開発・運用するのが難しいと感じていました。\n複数の AWS サービスを組み合わせて考える必要があり、また初期開発当時のチームの開発スキルの問題もあり、開発上も運用上も一貫したガバナンスを持つことが困難でした。\nモダンな DWH 製品ではこのあたりの配慮があります。\n技術選定で考慮したいこと 技術選定では上記の課題を解決できるように DWH 製品を選ぶわけですが、加えて一般的に DWH 製品に求められる観点も考慮する必要があります。\n次のような観点も見ていきます。\n人的・金銭的コスト (初期導入時／運用時) バックアップとリカバリ スケーラビリティ データ統合のしやすさ セキュリティ UI/UX エコシステム サポート 技術選定の実施 技術選定の流れ 全体としては次のような流れで技術選定を行いました。\nflowchart TD first[\"一次調査 (スクリーニング)\"] second[\"二次調査 (PoC)\"] decision[\"技術スタックの決定\"] first --\u003e second second --\u003e decision 一次調査 (スクリーニング) 一次調査では軽めの調査で広い候補から少数の候補へとスクリーニングを行います。\nここでは旧データ基盤の維持も含め、6つの候補が挙げられました。\n6つの候補は一般的によく利用されているものであり、データエンジニアの皆さんがぱっと想像できるようなアレとかアレとかです。\n一次調査は公式ドキュメントなどのカタログスペックでの調査が主となっており、前述の観点で候補を比較しました。\n結果として Databricks および製品 A に候補が絞られました。\n(ここで AI が作ったタイトル画像をもう一度見ていただきたいのですが、朱色のレンガは焼いて作るので火属性だし、水色の、あの、あれはもちろん氷属性ですね)\n二次調査 (PoC) 二次調査では一次調査の結果を受け、2つの候補でそれぞれ PoC を行い比較します。\nPoC では実際に DWH 製品を導入したことを想定し、DWH 上に dbt でデータパイプラインを構築します。\n業務で使う代表的なワークロードの一部を再現しました。\n特に処理のコストパフォーマンスはワークロード次第なので、組織でよく使うワークロードを想定して実際に動かしてみないとわかりません。\nまた UI/UX の使い勝手なども実際に使ってみないとわかりません。\n使い勝手的なところはアナリストにも評価していただきました。\nPoC の段階ではまだ IaC は導入せず、UI などから必要なリソースを用意しました。\nデータエンジニア2名でそれぞれ Databricks、製品 A で環境を構築し、2〜3週間ぐらいで検証しました。\nちなみにこのとき両製品で同じ dbt プロジェクトを使っていました。\nこのように dbt は比較的高い相互運用性があり、何かあったときの DWH の変更のハードルを下げてくれるということを実感しました。\n(クエリの書き方次第ではある)\n技術スタックの決定 さて、このようにして両製品を比較したわけですが、もともと想定していた観点ではそこまで大きな違いはありませんでした。\nとはいえ以下の点で優れていたことにより Databricks の採用を決めました。\nコストパフォーマンス 我々の代表的なワークロードでは Databricks の方が若干コストが安かった これはワークロードによっても変わると思われる オープン性 データ実体は Delta Lake という open table format また顧客 (=我々) 管理の S3 bucket に保存されるので透明性が高く、データの囲い込みがない OSS 志向も強い 製品 A も open table format を選択できるが、パフォーマンスで劣るとの話があった アナリストからの評価 これはちょっとタイミングの問題もあるのだが、調査時点では以下のような機能が Databricks のみにあってアナリストからの高評価となった SQL の Git 管理機能 Genie などの AI まわり もともと Redash を使っていたので親和性が高いというのもあったかも DatabricksがRedashを買収してデータサイエンティストのためのデータ視覚化を充実 (TechCrunch Japan) とまあ現場レベルではこれで行きましょうという合意ができたわけですが、Databricks を使えるようにするため今度は予算を獲得する必要があります。\n予算獲得 予算獲得までの流れ どのように予算を獲得するかは組織に大きく依存するでしょう。\nウチの場合は以下のような流れでした。\nflowchart TD estimate[\"費用見積\"] stakeholder[\"ステークホルダーへの説明\"] approval[\"承認会議\"] estimate --\u003e stakeholder stakeholder --\u003e approval 費用見積 技術スタックが決まったので費用を見積もります。\n厳密に見積もるのは難しいですが、ある程度の仮定をおいて計算します。\n通常のデータ基盤であればストレージとコンピュートの費用が中心になるでしょう。\n両者はともにデータ量に依存します。\n事業計画にもとづいてデータ量の変化を仮定し、それに対してストレージとコンピュートの費用の見積を出しました。\nまた、費用については新旧のデータ基盤の並行稼働のことも考慮する必要があります。\nバチッと一瞬で切り替えられるわけではなく、ある程度の期間は2つのデータ基盤を並行して運用することになります。\nその間は余分に費用がかかってしまいます。\nステークホルダーへの説明 次に社内のステークホルダーへの説明を行いました。\nまずは一次上長であるマネージャにやりたいことと費用見積の詳細について説明します。\nPart 1 でも少し触れていますが、マネージャはデータエンジニアリングの知見はあまりなかったため、それなりの説明コストがかかりました。\nとはいえちゃんと理解しようとしてくださっていたのでそこは感謝です。\n次にデータ基盤に対する社内のデータプロバイダー、コンシューマーについてやりたいことやその影響の説明をしました。\n3回ぐらい大きめのミーティングを開催して説明しました。\nここにも準備など含めそれなりの時間がかかっています。\n正直、当初はこのステップは本当に必要なのか？と考えていました。\n弊社ぐらいの小さい組織であれば、リーダーシップを持った人が責任と権限を持ち、スピード感を持って決めていく方がいいはずです。\nステークホルダーには決まった後に伝えればいいだろうと。\nただ後になってから、これは必要なステップだったと思うに至りました。\n承認会議 ここまでやってようやく本丸の承認会議です。\n決裁権を持つエライ人の前で説明して承認を得るという会議を開催しました。\n結論として、割とすんなりと承認を得ることができました。\n「たぶんやった方がいいんだろうけど」、その場でエライ人が言ったこの言葉が象徴的でした。\n弊社の場合決裁権を持つマネジメント層にデータまわりに詳しい人がいません。\nもちろん費用見積はちゃんと評価されるわけですが、その上で雰囲気で決まっていく感がありました。(JTC)\nその雰囲気を醸成するためにステークホルダーへの説明が必要だったわけです。\n組織にもよるのでしょうが、データエンジニアをやってるとこのような政治っぽいことを考えることになる場合もあるのだと理解しました。\nこのあたり前述のマネージャにかなり助けられました。\nここまでの課題と今後の対策 Part 1 とかぶりますが、やはりマネジメント層のデータまわりのリテラシーは組織としての課題です。\n外的要因により必要にかられるようなことがない限り、少しずつ啓蒙していくしかないかなと考えています。\nDatabricks について Advent Calendar の記事だということもあり Databricks についてもう少し触れておきます。\nこれを書いている時点では PoC のときよりは Databricks に触っており、その中で体感した良かったところを挙げておきます。\n(あくまである一社での例です)\nOSS ベースであることによる透明性 データ処理は Apache Spark 互換のエンジン Photon で実行される ベースは OSS の Spark なので、Spark の知見があれば実行計画を見るなどして処理のイメージがつきやすい 最適化のポイントもわかりやすい (と思っている) 一貫したガバナンス ML model 等の data product も table などの database object と同じように権限管理できる 同じ user でも workspace 単位でアクセス可能なリソースを制限できる PoC の時点でもカタログとしてわかっていたが、触ってみて実感したところがある AI まわり ChatGPT-like にデータについての問い合わせが行える Genie Nootebook でエラーを出したときに原因や修正案を示してくれる LLM の評価や開発の機能 手厚いサポート 弊社に対して担当がついており、Slack で質問に回答していただける A◯S のサポートと比べても回答のスピード・質ともに高い いつもお世話になってます 🙇‍♂️ とはいえまだすべての機能を確認できていないし、そもそも結構なペースで機能が増えています。\n個人的には Databricks LakeFlow のあたりに期待しています。\n次回予告 次回はまだ未定ですが、何か開発関連の話を書くつもりです。\n以上、現場からでした。\n","permalink":"https://soonraah.github.io/posts/ordinary-data-platform-migration-part-2/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003eデータ基盤移行について書いていくシリーズです。\u003cbr\u003e\nシリーズ一覧は\u003ca href=\"/tags/%E3%81%B5%E3%81%A4%E3%81%86%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E5%9F%BA%E7%9B%A4%E7%A7%BB%E8%A1%8C/\"\u003eこちら\u003c/a\u003eから。\u003c/p\u003e\n\u003cp\u003e前回は戦略策定 (実際は戦術) までのところを書きました。\u003cbr\u003e\n今回はそれを踏まえた技術選定、およびその後の予算獲得について書いていきます。\u003c/p\u003e\n\u003cp\u003eまた、こちらは \u003ca href=\"https://qiita.com/advent-calendar/2024/databricks\"\u003eDatabricks Advent Calendar 2024\u003c/a\u003e シリーズ 2 の16日目の記事にもなっています。\u003cbr\u003e\nはいそうです、出落ちですが技術選定として Databricks を選ぶことになります。\u003c/p\u003e\n\u003ch2 id=\"スコープ\"\u003eスコープ\u003c/h2\u003e\n\u003cp\u003e前回 \u003ca href=\"/posts/ordinary-data-platform-migration-part-1/\"\u003ePart 1. 戦略策定編\u003c/a\u003eでは概ねのロードマップが決まり、まずはデータ基盤のリアーキテクチャをやっていくことになりました。\u003cbr\u003e\nリアーキテクチャにおいてはどのような技術スタックを使っていくかが重要な選択になります。\u003cbr\u003e\nデータ基盤においてはデータ処理のためのストレージとコンピュートの選択がとても重要です。\u003cbr\u003e\n以降ではこの2つをあわせた DWH 製品の選定について書いていきます。\u003cbr\u003e\n「DHW 製品」という言葉は適切ではないかもしれませんが、ここではストレージ + コンピュートが組み合わさったものぐらいに考えてください。\u003c/p\u003e\n\u003cp\u003eもちろんデータ基盤には他の技術要素もあり、それらも軽くない選択ですがこのポストでは割愛します。\u003cbr\u003e\n(気が向いたら別記事で書くかも)\u003c/p\u003e\n\u003ch2 id=\"技術選定の目的\"\u003e技術選定の目的\u003c/h2\u003e\n\u003cp\u003eまず何のために技術スタックの置き換え、ひいては技術選定をするかの目的を明確にしておく必要があります。\u003cbr\u003e\n旧データ基盤では次のような技術スタックになっていました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eストレージ: S3\u003c/li\u003e\n\u003cli\u003eコンピュート: Glue Job, Athena\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこの構成には次のような課題がありました。\u003cbr\u003e\n主にこれらの課題を解決するために DWH 製品の乗り換えを検討することになりました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edbt との親和性の低さ\u003c/li\u003e\n\u003cli\u003e一貫したガバナンスの欠如\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"dbt-との親和性の低さ\"\u003edbt との親和性の低さ\u003c/h3\u003e\n\u003cp\u003e前回作成した\u003ca href=\"/posts/ordinary-data-platform-migration-part-1/#%e3%83%ad%e3%83%bc%e3%83%89%e3%83%9e%e3%83%83%e3%83%97%e4%bd%9c%e6%88%90\"\u003eロードマップ\u003c/a\u003eにおいて、dbt の導入が課題解決における重要なポイントになっています。\u003cbr\u003e\ndbt の周辺エコシステムがデータ基盤の課題の解決に大きく貢献すると考えています。\u003c/p\u003e\n\u003cp\u003eまた、データパイプラインの開発・運用の負荷も dbt 導入で軽減できそうです。\u003cbr\u003e\n旧データ基盤では Glue Job と Athena クエリを組み合わせた複雑なパイプラインになっており、table を1つ追加するだけでもいろいろなコードに手をいれる必要があります。\u003cbr\u003e\nほぼ SQL で実装でき、かつ宣言的にパイプライン構築できる dbt は魅力的です。\u003c/p\u003e\n\u003cp\u003e仮に旧データ基盤に dbt を導入するとなると \u003ca href=\"https://docs.getdbt.com/docs/core/connect-data-platform/athena-setup\"\u003edbt-athena\u003c/a\u003e を使うことになります。\u003cbr\u003e\nただ dbt による Athena のサポートはやや弱く、dbt-athena はコミュニティ版から少し前に移管されたものですし、これを書いている2024年12月の時点で dbt Cloud の Athena のサポートはまだプレビューです。\u003cbr\u003e\n反論がある方もいらっしゃるかもしれませんが、モダンなデータ基盤構築において Athena はやや影が薄い印象があり、dbt のサポートの弱さもこれが原因だと思います。\u003cbr\u003e\n(ただし直近の re:Invent 2024 の内容からすると潮目が変わる可能性もありそうです)\u003c/p\u003e","title":"ふつうのデータ基盤移行 - Part 2. 技術選定編"},{"content":"このポストについて データ基盤移行について書かれた各社の技術ブログなど見かけることがありますが、割とさらっと書かれていることが多いように思います。\n本当はいろんな面で苦労があり、記事に表れていない辛さや工夫などがあるはず。\nということで今自分が経験している普通の会社の普通のデータ基盤移行について、詳しく記事にしてみようと考えました。\n何回かに分けてデータ基盤移行のいろいろな側面を、うまくいったこともいかなかったことも含めて書いていきます。\nとはいえ現在進行形なので、全編書き終わるのはかなり先になりそうです。\nデータ基盤移行のシリーズ一覧はこちらから。\n移行の背景 組織 まずイメージしやすいよう、どういった組織におけるデータ基盤移行なのかについて軽く触れておきます。\n社員規模: 〜100名 web 系の B2C ビジネス データチームの構成 マネージャ: 1名 (データエンジニアリングの経験はほぼない) データエンジニア: 2 -\u0026gt; 3名 (途中で採用) 中小のベンチャー？企業ではありますが、意思決定プロセスは JTC 感があります。\n私はデータエンジニアのポジションとなっており、その視点からの話であることにご留意ください。\n小さい組織ということで私は移行の計画から設計、開発その他のあらゆるフェーズに中心的に関わっています。\nどこもそうだと思いますが、人員的にはまあまあきびしい。\nよくある中小 IT 企業のよくあるデータ基盤移行の話だと思っていただきたく。\n大企業ではないのでそこまでちゃんとはしていません。\n(ちなみに自分のブログで本件を記事にしていいかは上長に確認の上、OK をもらっています)\n旧データ基盤 一連のポストでは移行前のデータ基盤のことを「旧データ基盤」と表記するものとします。\n旧データ基盤は AWS 上で構築されており、アーキテクチャについて簡単に挙げると\nstorage: S3 ETL: Glue Job, Athena SQL engine: Athena workflow orchestration: MWAA のようになっていました。\n旧データ基盤の開発・運用側 (データエンジニア) としても、また社内の利用者側としてもいろいろと問題が挙がってきてはいました。\nしかしそれをうまく集約・言語化できていないという状況でした。\nそんな中でエライ人の鶴の一声で移行しようぜ！ということになり、データ基盤の移行を検討することに相成りました。\n移行計画を考えるにあたり まず考えたこと データ基盤の移行は組織におけるデータマネジメントにおいて重要な位置づけとなるはず。\nしたがって単なる技術スタックの置き換えというスコープで考えるのはもったいないです。\n組織のデータマネジメントの未来を想定して、戦略を持って開発・運用を進めるべきであると考えました。\nそのためにはイシューを明確化しないといけません。\nでもどの抽象度レベルで？\nボトムアップの戦術策定 まずは現場感覚、ボトムアップでの課題を明らかにすることを考えました。\n本来は後述する戦略レベルから先に考えるべきですが、実際に目に見えている課題があり、取り組みやすかったというところで戦術のレベルから考え始めています。(良し悪しはある)\n現状のアーキテクチャと運用では戦略策定への対応が難しいため、せめてそのための地ならしとして今見えている課題に対応できる状態にしたいというのもありました。\nこの視点でイシューを明確化するために2つのサーベイを実施しました。\nデータマネジメント成熟度アセスメントと利用者アンケートです。\nこれらを実施して整理した結果、当面のロードマップのようなものを描くことができました。\nこの2つのサーベイとそこから得られた結果については後ほど詳しく記載します。\nちなみにここでは「戦略」「戦術」という言葉は大雑把に使っています。\n戦略の方が抽象度が高く長期、戦術の方が抽象度が低く短期のものだと思ってください。\nトップダウンの戦略策定 データ戦略やデータマネジメント戦略は事業戦略と整合しているべきというのは複数の書籍で言われていますし、同業者の口からもよく聞きます。\nデータ戦略には、情報を利用することで競争上の優位性を確保し、企業の目標を達成するための事業計画が含まれる。組織にどんなデータが必要で、それをどのように取得し、時間の経過とともにどのように管理し信頼性を担保するか、どう活用するかなど、事業戦略に不可欠なデータの必要性を理解した上で、データ戦略を考えなければいけない。\n\u0026ndash; DAMA International （編著） 『データマネジメント知識体系ガイド第二版』, 第1章　データマネジメント, 日経BP, 2018年\nデータ戦略が意味を持つには、企業のミッション、ビジョン、戦略、KPIとの整合性が不可欠です。組織の目標は、通常はミッションからビジョン、財務目標、部門目標へと段階的に具体化し、最終的にはソーシャルメディア目標や販売目標などのチームごとの具体的な目標へと落とし込んでいきます。\n\u0026ndash; Harvinder Atwal (著) 『実践DataOps』, Chapter 2 データ戦略, 翔泳社, 2024年\n戦術レベルのロードマップに対して戦略とのすり合わせを行おうとしましたが、結論としてこのようなトップダウンの戦略策定は我々の組織ではうまくいきませんでした。\n(つまり記事のタイトルに偽りありです…)\nうまくいかなかったのには2つの理由があります。\nまず第一にマネジメントのケイパビリティです。\n多少のサポートにより経営戦略とひもづけてデータ戦略を主体的に立案することができるような人材が、残念ながらマネジメント層にいませんでした。\nむしろプラットフォームなどというものはビジネスから遠いところにあると思われていて関心は薄いです。\n難しいところです。\n第二に、上記の前提があるとしてデータエンジニア側からうまく戦略策定を誘導することができませんでした。\n第一の理由は悲観的に書かれているように見えるかもしれませんが、世の中小規模の組織では同様の状況になっていることが多いと思われます。\nこれを嘆いていてもどうにもなりません。\nということでエライ人に対してデータ基盤に関係しそうな経営戦略を教えてくれ、それとデータ基盤の戦略を関連させたいというお願いをしてみましたが「？」という雰囲気がリモート会議の画面から漏れ出ていました。\nまたマネージャに対してデータ基盤としてのビジョンを提案したこともありますが、なんで急にそんなお題目みたいなこと言うの？という感じでした。\n今これを書いてて思ったのは、おそらく2人ともデータ基盤移行を単なる技術スタックの置き換えだと思っていたのかもしれません。\n関係性の構築が不十分、かつデータ基盤移行で目指すところの意味をあまり議論できていなかったということが反省点です。\n戦術策定のためのサーベイ データマネジメント成熟度アセスメント (DMMA) 組織のデータマネジメントの進め方の指針として DMBOK Pyramid (Peter Aiken’s Framework) があります。\nPeter Aiken\u0026rsquo;s Framework, Why HR Data Management Strategy is important in your HR Transformation | SAP Blogs (元ネタは DMBOK2)\n自分たちの立ち位置としてこの図の Phase 1 (青色) と Phase 2 (黄色) の間ぐらいだとなんとなく考えていましたが、社内のステークホルダーに話すためには定量的に立ち位置を確認する必要があります。\nこれをやるのがデータマネジメント成熟度アセスメント (DMMA: Data Management Matulity Accessment) です。\nDMMA は DMBOK2 にも記載されている、組織のデータマネジメントの現状を評価する方法です。\nこちらの記事も参考にしてください。\nここで困ったのがアセスメントのための具体的な質問をどうやって用意するかです。\nDMBOK2 に記載されているのはリンクがもう古かったり、大げさだったりで使いにくいものばかりでした。\nそこで ChatGPT を利用しました。\n(プロンプト力はあまり高くないですが) 例えば次のように指示しました。\nDMMA で DAMA DMBOK のそれぞれの知識領域について定量的にスコアリングするために、質問集を用意しようと考えています。 定量的に評価したいので知識領域ごとに closed question が複数必要です。 DAMA DMBOK 第3章「データガバナンス」を評価するための質問をいくつか挙げてください。 次のような結果が得られました。\nDAMA DMBOKの第3章「データガバナンス」を評価するための質問を考える際、組織のデータガバナンスの成熟度と効果を定量的に評価するために、以下のようなクローズドクエスチョン（選択肢から選ぶ質問）を用意することが考えられます。これらの質問は、データガバナンスのさまざまな側面をカバーし、回答者が具体的な選択肢から選ぶ形式で答えられるようになっています。\nデータガバナンスポリシーの存在: 組織には明確なデータガバナンスポリシーがありますか？ データガバナンスフレームワークの実装状況: 組織のデータガバナンスフレームワークは完全に実装されていますか？\n\u0026hellip; 割といい感じではないでしょうか？\nこのようにしてアセスメントのための質問を作成していきました。\n質問への回答はデータエンジニア2名でそれぞれ実施し、5段階評価で回答します。\nスコープはもちろん組織全体ではなくデータ基盤です。\nそれらを相談してすり合わせた結果を DMMA の結果としました。\n各知識領域ごとの各質問に対するスコアが得られ、結果として以下のようなレーダーチャートが得られます。\n(こちらのレーダーチャートは本当の結果ではなく、例となっています)\n予想はしていましたが、この時点でのスコアは低いです。\nDMMA は定期的に実行してデータマネジメントの進捗を確かめるものなので、データ基盤移行が終わったあとにどうなるかが楽しみです。\n利用者アンケート DMMA はデータエンジニアで実施しているため、どちらかというと運用側の視点です。\n利用者側の視点も得るため、社内のデータ基盤の利用者に対してアンケートを実施しました。\nここでも ChatGPT が活躍します。\n私は IT 企業のデータエンジニアであり、社内のデータ基盤の刷新を検討しています。 その参考とするために、社内のデータ基盤利用者に対して Google Form でアンケートを実施しようとしています。 アンケートの目的は、データエンジニアが把握していないデータ基盤の利用ニーズを理解することで、その気付きを新しいデータ基盤の技術スタック選定や運用改善に活かしたいと考えています。 アンケート項目を考えてください。 この結果を元に多少修正し、Google Forms にしてアンケートを作成しました。\n利用者に広くアンケートへの回答を依頼し、ご理解もあって思ったより多くのご回答をいただくことができました。\nここで面白かったのが、DMMA 結果における課題感とアンケート結果の課題感が概ね一致していたことです。\n当たり前と言えば当たり前なのですが、データエンジニアが利用者側の課題をある程度把握できていると見ることができます。\nロードマップ作成 サーベイ結果を集約して整理したところ、組織のデータまわりでは数個の大きな課題があることがわかりました。\n例として1つ挙げると「データ品質テストの不足」です。\nアンケート結果を集約すると、こういった大きな課題にまとめることができました。\n解決すべき課題がわかった上で、データ関連の技術スタック、およびそれによる解決可能性を検討しました。\nその上で次のような形で大まかなロードマップを定めました。\nflowchart LR current([\"現状\"]) --\u003e dwh[\"DWH 製品の導入\"] subgraph rearchitecture[\"データ基盤のリアーキテクチャ\"] dwh --\u003e dbt[\"dbt の導入\"] end dwh --\u003e solution_1[\"solution 1\"] dwh --\u003e issue_1\u003e\"issue 1 の解決\"] solution_1 --\u003e issue_1 dbt --\u003e solution_2[\"solution 2\"] solution_2 --\u003e issue_2\u003e\"issue 2 の解決\"] dbt --\u003e issue_3\u003e\"issue 3 の解決\"] current --\u003e solution_4_1[\"solution 4-1\"] solution_4_1 --\u003e issue_4\u003e\"issue 4 の解決\"] current --\u003e solution_4_2[\"solution 4-2\"] solution_4_2 --\u003e issue_4 dbt --\u003e solution_5[\"solution 5\"] solution_5 --\u003e issue_5\u003e\"issue 5 の解決\"] dbt --\u003e implement_test[\"データ品質テストの実装\"] implement_test --\u003e improve_quality\u003e\"データ品質テストによる品質改善\"] (図が見にくい場合はライトモードにしてください)\nDWH 製品を新しく導入すること、および dbt の導入により多くのソリューションが実現できると考えました。\nよってこの2つをまとめてデータ基盤のリアーキテクチャとみなし、当面はここに取り組んでいくことにしました。\nここまでの課題と今後の対策 ここまでの取り組みで一番大きな課題だと感じたのは組織としてのデータマネジメント戦略をトップダウンで作れていないことです。\nマネジメント層を含めほとんどの人が「データマネジメント？ナニソレオイシイノ？」という状態だったため、せめて組織にデータマネジメントを浸透させたいと考えました。\n組織が今よりもう少しデータマネジメントの言葉を使って会話できるようになりたい。\nそこで社内で DMBOK2 の輪読会を開催することにしました。\n幸い思ったより多くの方に関心を持っていただくことができ、その中にはマネージャ数名も含まれています。\n輪読会は週1回開催で発表者を決め、DMBOK2 を1章ずつ読み進める形としています。\n量が多く時間はかかりましたが、年末もしくは年明けぐらいに終わる見込みです。\nこれが終われば組織がデータマネジメントを学習でき、一段抽象度の高い戦略の議論をする下地ができるのではと考えています。\n次回予告 次回のポストでは DWH (という呼称は適切でないかもしれませんが) の技術選定、そしてその結果を受けての予算獲得について書く予定です。\n以上、現場からでした。\n","permalink":"https://soonraah.github.io/posts/ordinary-data-platform-migration-part-1/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003eデータ基盤移行について書かれた各社の技術ブログなど見かけることがありますが、割とさらっと書かれていることが多いように思います。\u003cbr\u003e\n本当はいろんな面で苦労があり、記事に表れていない辛さや工夫などがあるはず。\u003c/p\u003e\n\u003cp\u003eということで今自分が経験している普通の会社の普通のデータ基盤移行について、詳しく記事にしてみようと考えました。\u003cbr\u003e\n何回かに分けてデータ基盤移行のいろいろな側面を、うまくいったこともいかなかったことも含めて書いていきます。\u003cbr\u003e\nとはいえ現在進行形なので、全編書き終わるのはかなり先になりそうです。\u003c/p\u003e\n\u003cp\u003eデータ基盤移行のシリーズ一覧は\u003ca href=\"/tags/%E3%81%B5%E3%81%A4%E3%81%86%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E5%9F%BA%E7%9B%A4%E7%A7%BB%E8%A1%8C/\"\u003eこちら\u003c/a\u003eから。\u003c/p\u003e\n\u003ch2 id=\"移行の背景\"\u003e移行の背景\u003c/h2\u003e\n\u003ch3 id=\"組織\"\u003e組織\u003c/h3\u003e\n\u003cp\u003eまずイメージしやすいよう、どういった組織におけるデータ基盤移行なのかについて軽く触れておきます。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e社員規模: 〜100名\u003c/li\u003e\n\u003cli\u003eweb 系の B2C ビジネス\u003c/li\u003e\n\u003cli\u003eデータチームの構成\n\u003cul\u003e\n\u003cli\u003eマネージャ: 1名 (データエンジニアリングの経験はほぼない)\u003c/li\u003e\n\u003cli\u003eデータエンジニア: 2 -\u0026gt; 3名 (途中で採用)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e中小のベンチャー？企業ではありますが、意思決定プロセスは JTC 感があります。\u003cbr\u003e\n私はデータエンジニアのポジションとなっており、その視点からの話であることにご留意ください。\u003cbr\u003e\n小さい組織ということで私は移行の計画から設計、開発その他のあらゆるフェーズに中心的に関わっています。\u003cbr\u003e\nどこもそうだと思いますが、人員的にはまあまあきびしい。\u003c/p\u003e\n\u003cp\u003eよくある中小 IT 企業のよくあるデータ基盤移行の話だと思っていただきたく。\u003cbr\u003e\n大企業ではないのでそこまでちゃんとはしていません。\u003c/p\u003e\n\u003cp\u003e(ちなみに自分のブログで本件を記事にしていいかは上長に確認の上、OK をもらっています)\u003c/p\u003e\n\u003ch3 id=\"旧データ基盤\"\u003e旧データ基盤\u003c/h3\u003e\n\u003cp\u003e一連のポストでは移行前のデータ基盤のことを「旧データ基盤」と表記するものとします。\u003cbr\u003e\n旧データ基盤は AWS 上で構築されており、アーキテクチャについて簡単に挙げると\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003estorage: S3\u003c/li\u003e\n\u003cli\u003eETL: Glue Job, Athena\u003c/li\u003e\n\u003cli\u003eSQL engine: Athena\u003c/li\u003e\n\u003cli\u003eworkflow orchestration: MWAA\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eのようになっていました。\u003c/p\u003e\n\u003cp\u003e旧データ基盤の開発・運用側 (データエンジニア) としても、また社内の利用者側としてもいろいろと問題が挙がってきてはいました。\u003cbr\u003e\nしかしそれをうまく集約・言語化できていないという状況でした。\u003cbr\u003e\nそんな中でエライ人の鶴の一声で移行しようぜ！ということになり、データ基盤の移行を検討することに相成りました。\u003c/p\u003e\n\u003ch2 id=\"移行計画を考えるにあたり\"\u003e移行計画を考えるにあたり\u003c/h2\u003e\n\u003ch3 id=\"まず考えたこと\"\u003eまず考えたこと\u003c/h3\u003e\n\u003cp\u003eデータ基盤の移行は組織におけるデータマネジメントにおいて重要な位置づけとなるはず。\u003cbr\u003e\nしたがって単なる技術スタックの置き換えというスコープで考えるのはもったいないです。\u003cbr\u003e\n組織のデータマネジメントの未来を想定して、戦略を持って開発・運用を進めるべきであると考えました。\u003c/p\u003e\n\u003cp\u003eそのためにはイシューを明確化しないといけません。\u003cbr\u003e\nでもどの抽象度レベルで？\u003c/p\u003e\n\u003ch3 id=\"ボトムアップの戦術策定\"\u003eボトムアップの戦術策定\u003c/h3\u003e\n\u003cp\u003eまずは現場感覚、ボトムアップでの課題を明らかにすることを考えました。\u003cbr\u003e\n本来は後述する戦略レベルから先に考えるべきですが、実際に目に見えている課題があり、取り組みやすかったというところで戦術のレベルから考え始めています。(良し悪しはある)\u003cbr\u003e\n現状のアーキテクチャと運用では戦略策定への対応が難しいため、せめてそのための地ならしとして今見えている課題に対応できる状態にしたいというのもありました。\u003c/p\u003e","title":"ふつうのデータ基盤移行 - Part 1. 戦略策定編"},{"content":"このポストについて ​\nDMBOK2 を読み進めていくシリーズ。\n今回は第13章「データ品質」について。\nこれまで業務で「データ品質」という言葉が使われることがあったが、意味が限定的だったり人によって定義が違ったりしていた。\nそのあたりクリアにできるとよい。\n​\n内容紹介 ​\nデータ品質の定義 ​\nデータ品質の簡潔な定義は「目的に適合している」。\nデータ品質管理の定義は「データを収集し扱うための技法を適用し、企業や地域のデータ利用者の、ニーズや利用に適したデータとすることを保証する活動を計画し、実施し、管理する」。\n​\nデータ基盤担当の仕事柄、データ品質というとどちらかというと上流であるデータソース側の定義が重要だと思っていたが、そうではなく下流であるデータ利用においての観点が起点になるというのが気づきだった。(でも考えてみれば当たり前)\n​\nビジネス上の意義 ​\nステークホルダーの体験と組織の評判を高める ex. データが正しいことを顧客が信頼し、組織との取引に自信を持てる 組織がより有効な成果を出せるようにする ex. ビジネスチャンスの特定と効果的な請求により売上を獲得できる 低品質なデータによるリスクとコストを削減する ex. データが正しいかどうかをスタッフが見極める時間が減る ex. 誤ったデータによる誤った意思決定 組織の効率と生産性を向上する ex. カスタマーサービスにかかってくる電話が減り、問い合わせを解決できるようになる\n​ 重要なデータ ​\nデータ品質管理における第一の原則は、組織とその顧客にとって最も重要なデータに改善努力を集中させること。\n​\nex. 顧客のメールアドレス欄のデータが不完全であれば、顧客にメールで商品情報を送ることができず、潜在的な売上を失う 1通のメールを送るごとに100円の収益が得られることが知られている → データ品質の改善に明確な価値があると言える\n​ 重要なデータは組織や業界によって異なるが、以下のような用途で使用されることが多い。\n​\n規制、財務、経営報告 事業運営上のニーズ 製品の品質と顧客満足度の測定 事業戦略、特に競争上の差別化への取り組み\n​ データ品質評価軸 ​\nデータ品質評価軸は、測定可能なデータの特徴または特性。\n一般的な評価軸は次のとおり。\n​\nNo. 評価軸 説明 例 1 有効性Validity データの値が定義された領域の値と一致しているかどうか。 - 数値、日付などのデータ範囲- 電話番号などの書式 2 完全性Completeness 必要なデータがすべて存在するかどうか。カラム, レコード, データセットのレベルがある。 - カラム: 必須カラムにデータが入っているか？- データセット: 都道府県マスタに47都道府県の情報はあるか？ 3 一貫性Consistency データ値が同じアプローチ、評価、価値基準を用いてコード化されていることを保証すること。レコード内、レコード間、経時的な一貫性などがある。 - すべての顧客企業の住所は本社住所となっているか？- 生徒の成績評価は時を経ても同じか？ 4 整合性Integrity データに非一貫性や破綻した関係性がないこと。 - 顧客住所の国がカナダの場合、州としてカナダの州が記載されているか？ 5 適時性Timeliness データの取得または更新後、ユーザーがデータにアクセスできるようになるまでの時間を指す。 - 電力会社は電力需要データを数秒以内に利用して需給調整する必要がある- 政府機関が四半期末の2ヶ月後に GDP 報告書を作成 6 最新性Currency データが最後に更新されてから現在までの期間と、それがまだ正しいという可能性。データセットによって期待される最新性は異なる。 - 国コードは比較的静的- 銀行口座残高は変動的 7 妥当性Reasonableness データパターンが期待に合致しているかどうか。 - 先週のクリック数と比較して今日のクリック数は普通か否か？ 8 一意性/重複排除Uniqueness/Deduplication 現実世界の実体がデータセット内に2つ以上存在しないこと。 - ユーザー ID は重複していないか？- ユーザー ID は異なるが、同一の人物を表していないか？ 9 正確性Accuracy データが「現実の」実体を正しく表している程度。 - ユーザー名は現実世界の個人の名前なのか？- 顧客は実際にそのメールアドレスを使用しているのか？ ​\nここでようやく「データ品質」が具体的なものとして見えてきた。\n測定が比較的容易なものもあれば困難なものもある。\n​\n効果的なデータ品質尺度 ​\n完全性の例\n​\n業務ルール 内容 例 業務ルール フィールドの入力は必須 郵便番号は住所テーブルに100%入力されていなければならない。 測定 データが入力されたレコードの数を数え、レコードの総数と比較する 入力数: 700,000未入力数: 300,000合計: 1,000,000 評価基準 データが入力されたレコードの割合をパーセンテージで算出 正の測定: 700,000 / 1,000,000 = 70% 判定指標 業務ルールが満たされていれば可。そうでなければ受け入れ不可。 許容できない。未入力の郵便番号がある。 ​ データ品質 業務ルール ​\n業務ルールは、組織内の有用で使用可能なデータはどうあるべきかを記述する。\nデータ品質評価軸と整合しており、データ品質要件を記述するために使用される。\n​\nex. 州や県を管理するための業務ルール 有効性: すべての州や県は、参照テーブルの値でなければならない 完全性: 国が州や県に分割されている場合、すべての住所は州や県を持たなければならない 整合性: すべての州や県の名前は、国名にリンクされた値を持たなければならない etc.\n​ 評価軸に基づき、データ品質をより具体的に規定するのが業務ルール。\n​\nデータ品質問題のよくある原因 ​\n監督の欠如による問題 指導者やスタッフの認識不足 優先順位の欠如 業務ガバナンスの欠如 etc. データ入力プロセスに起因する問題 ユーザーエンゲージメントの不足 ユーザートレーニング 使い勝手の悪さ etc. データ処理機能に起因する問題 下流を考慮しない 一貫性のないプロセスの実行 業務プロセスの変更 etc. システム設計に起因する問題 参照整合性 一意性制約 処理エラーとギャップ etc. 問題の修正によって引き起こされる問題 不十分なリカバリ機能 参照データの頻繁な変更\n​ システムで対応できるものもあれば組織的な課題となるものもある。\nデータ処理機能に起因する問題のある程度は data contract で解決できそう。\n​\nアクティビティ ​\nデータ品質フレームワークの定義 高品質データの定義 品質評価軸と関連する業務ルールの特定 最初のデータ品質アセスメントの実施 潜在的な改善点の特定と優先順位付け データ品質改善のゴールの策定 データ品質オペレーションの開発と展開 データ品質ルールの管理 データ品質の測定と監視 データ問題の管理手順の策定 データ品質 SLA の確立 データ品質への対応 (利用者への報告)\n​ データ品質およびその他の知識領域 ​\nデータ品質と「データモデリング」 ​\n高品質なデータを得るためのデータモデリング段階のポイント\n​\nデータ品質値を保持する 業務ルールを関連付ける\n​ データ品質と「メタデータ管理」 ​\nデータ品質プロセス中にメタデータが作成される。\n​\nex. 顧客住所がプロセスによって有効とマークされた場合、住所とともにメタデータとして維持されるべき\n​ データ品質メタデータがないとユーザーはそのデータを信用せず、チェックや検証に労力を費やすことになる。\n​\nデータ品質と「マスターデータ・参照データ管理」 ​\nデータを検証するために利用される。\n​\nデータ品質低下の主な原因はマスターデータ管理の弱さにある。\n未熟なマスターデータ管理は信頼性の低いデータソースを選択することにつながり、データ品質の問題を発見することが困難になる。\n​\nデータ品質と「データ統合と相互運用性」 ​\nデータの移動がデータ処理プロセスにおけるエラーの一般的な原因となるため、データ品質に影響する。\nデータ品質をテストする重要な場所でもある。\n​\nデータ品質と「データガバナンス」 ​\nデータ品質機能がデータガバナンスプログラムの一環として行われれば、より効果的。\n逆にデータ品質の問題が企業全体のデータガバナンスを確立する動機ともなる。\n​\n所感 ​\nプロダクト開発におけるコード品質と類似点が多いと感じた。\n組織としてデータ品質に投資する合意を得るのは簡単でなさそう データ品質改善に取り組めたとして、うまくいってるときに評価されにくい問題 水道からはきれいな水が出て当たり前 データ基盤においてこれらの品質を整えていくのは長い道のりになりそう。\nだがやるしかない。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-13/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003e​\u003cbr\u003e\n\u003ca href=\"https://www.dama-japan.org/Introduction.html\"\u003eDMBOK2\u003c/a\u003e を読み進めていくシリーズ。\u003cbr\u003e\n今回は第13章「データ品質」について。\u003c/p\u003e\n\u003cp\u003eこれまで業務で「データ品質」という言葉が使われることがあったが、意味が限定的だったり人によって定義が違ったりしていた。\u003cbr\u003e\nそのあたりクリアにできるとよい。\u003cbr\u003e\n​\u003c/p\u003e\n\u003ch2 id=\"内容紹介\"\u003e内容紹介\u003c/h2\u003e\n\u003cp\u003e​\u003c/p\u003e\n\u003ch3 id=\"データ品質の定義\"\u003eデータ品質の定義\u003c/h3\u003e\n\u003cp\u003e​\u003cbr\u003e\nデータ品質の簡潔な定義は「目的に適合している」。\u003cbr\u003e\nデータ品質管理の定義は「データを収集し扱うための技法を適用し、企業や地域のデータ利用者の、ニーズや利用に適したデータとすることを保証する活動を計画し、実施し、管理する」。\u003cbr\u003e\n​\u003cbr\u003e\nデータ基盤担当の仕事柄、データ品質というとどちらかというと上流であるデータソース側の定義が重要だと思っていたが、そうではなく下流であるデータ利用においての観点が起点になるというのが気づきだった。(でも考えてみれば当たり前)\u003cbr\u003e\n​\u003c/p\u003e\n\u003ch3 id=\"ビジネス上の意義\"\u003eビジネス上の意義\u003c/h3\u003e\n\u003cp\u003e​\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eステークホルダーの体験と組織の評判を高める\n\u003cul\u003e\n\u003cli\u003eex. データが正しいことを顧客が信頼し、組織との取引に自信を持てる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e組織がより有効な成果を出せるようにする\n\u003cul\u003e\n\u003cli\u003eex. ビジネスチャンスの特定と効果的な請求により売上を獲得できる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e低品質なデータによるリスクとコストを削減する\n\u003cul\u003e\n\u003cli\u003eex. データが正しいかどうかをスタッフが見極める時間が減る\u003c/li\u003e\n\u003cli\u003eex. 誤ったデータによる誤った意思決定\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e組織の効率と生産性を向上する\n\u003cul\u003e\n\u003cli\u003eex. カスタマーサービスにかかってくる電話が減り、問い合わせを解決できるようになる\u003cbr\u003e\n​\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"重要なデータ\"\u003e重要なデータ\u003c/h3\u003e\n\u003cp\u003e​\u003cbr\u003e\nデータ品質管理における第一の原則は、組織とその顧客にとって最も重要なデータに改善努力を集中させること。\u003cbr\u003e\n​\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eex.\n\u003cul\u003e\n\u003cli\u003e顧客のメールアドレス欄のデータが不完全であれば、顧客にメールで商品情報を送ることができず、潜在的な売上を失う\u003c/li\u003e\n\u003cli\u003e1通のメールを送るごとに100円の収益が得られることが知られている\u003c/li\u003e\n\u003cli\u003e→ データ品質の改善に明確な価値があると言える\u003cbr\u003e\n​\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e重要なデータは組織や業界によって異なるが、以下のような用途で使用されることが多い。\u003cbr\u003e\n​\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e規制、財務、経営報告\u003c/li\u003e\n\u003cli\u003e事業運営上のニーズ\u003c/li\u003e\n\u003cli\u003e製品の品質と顧客満足度の測定\u003c/li\u003e\n\u003cli\u003e事業戦略、特に競争上の差別化への取り組み\u003cbr\u003e\n​\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"データ品質評価軸\"\u003eデータ品質評価軸\u003c/h3\u003e\n\u003cp\u003e​\u003cbr\u003e\nデータ品質評価軸は、測定可能なデータの特徴または特性。\u003cbr\u003e\n一般的な評価軸は次のとおり。\u003cbr\u003e\n​\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eNo.\u003c/th\u003e\n          \u003cth\u003e評価軸\u003c/th\u003e\n          \u003cth\u003e説明\u003c/th\u003e\n          \u003cth\u003e例\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e有効性\u003c!-- raw HTML omitted --\u003eValidity\u003c/td\u003e\n          \u003ctd\u003eデータの値が定義された領域の値と一致しているかどうか。\u003c/td\u003e\n          \u003ctd\u003e- 数値、日付などのデータ範囲\u003c!-- raw HTML omitted --\u003e- 電話番号などの書式\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003e完全性\u003c!-- raw HTML omitted --\u003eCompleteness\u003c/td\u003e\n          \u003ctd\u003e必要なデータがすべて存在するかどうか。\u003c!-- raw HTML omitted --\u003eカラム, レコード, データセットのレベルがある。\u003c/td\u003e\n          \u003ctd\u003e- カラム: 必須カラムにデータが入っているか？\u003c!-- raw HTML omitted --\u003e- データセット: 都道府県マスタに47都道府県の情報はあるか？\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003e一貫性\u003c!-- raw HTML omitted --\u003eConsistency\u003c/td\u003e\n          \u003ctd\u003eデータ値が同じアプローチ、評価、価値基準を用いてコード化されていることを保証すること。\u003c!-- raw HTML omitted --\u003eレコード内、レコード間、経時的な一貫性などがある。\u003c/td\u003e\n          \u003ctd\u003e- すべての顧客企業の住所は本社住所となっているか？\u003c!-- raw HTML omitted --\u003e- 生徒の成績評価は時を経ても同じか？\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n          \u003ctd\u003e整合性\u003c!-- raw HTML omitted --\u003eIntegrity\u003c/td\u003e\n          \u003ctd\u003eデータに非一貫性や破綻した関係性がないこと。\u003c/td\u003e\n          \u003ctd\u003e- 顧客住所の国がカナダの場合、州としてカナダの州が記載されているか？\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e5\u003c/td\u003e\n          \u003ctd\u003e適時性\u003c!-- raw HTML omitted --\u003eTimeliness\u003c/td\u003e\n          \u003ctd\u003eデータの取得または更新後、ユーザーがデータにアクセスできるようになるまでの時間を指す。\u003c/td\u003e\n          \u003ctd\u003e- 電力会社は電力需要データを数秒以内に利用して需給調整する必要がある\u003c!-- raw HTML omitted --\u003e- 政府機関が四半期末の2ヶ月後に GDP 報告書を作成\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e6\u003c/td\u003e\n          \u003ctd\u003e最新性\u003c!-- raw HTML omitted --\u003eCurrency\u003c/td\u003e\n          \u003ctd\u003eデータが最後に更新されてから現在までの期間と、それがまだ正しいという可能性。\u003c!-- raw HTML omitted --\u003eデータセットによって期待される最新性は異なる。\u003c/td\u003e\n          \u003ctd\u003e- 国コードは比較的静的\u003c!-- raw HTML omitted --\u003e- 銀行口座残高は変動的\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e7\u003c/td\u003e\n          \u003ctd\u003e妥当性\u003c!-- raw HTML omitted --\u003eReasonableness\u003c/td\u003e\n          \u003ctd\u003eデータパターンが期待に合致しているかどうか。\u003c/td\u003e\n          \u003ctd\u003e- 先週のクリック数と比較して今日のクリック数は普通か否か？\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e8\u003c/td\u003e\n          \u003ctd\u003e一意性/重複排除\u003c!-- raw HTML omitted --\u003eUniqueness/Deduplication\u003c/td\u003e\n          \u003ctd\u003e現実世界の実体がデータセット内に2つ以上存在しないこと。\u003c/td\u003e\n          \u003ctd\u003e- ユーザー ID は重複していないか？\u003c!-- raw HTML omitted --\u003e- ユーザー ID は異なるが、同一の人物を表していないか？\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e9\u003c/td\u003e\n          \u003ctd\u003e正確性\u003c!-- raw HTML omitted --\u003eAccuracy\u003c/td\u003e\n          \u003ctd\u003eデータが「現実の」実体を正しく表している程度。\u003c/td\u003e\n          \u003ctd\u003e- ユーザー名は現実世界の個人の名前なのか？\u003c!-- raw HTML omitted --\u003e- 顧客は実際にそのメールアドレスを使用しているのか？\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e​\u003cbr\u003e\nここでようやく「データ品質」が具体的なものとして見えてきた。\u003cbr\u003e\n測定が比較的容易なものもあれば困難なものもある。\u003cbr\u003e\n​\u003c/p\u003e","title":"読書メモ: DMBOK2 第13章 データ品質"},{"content":"このポストについて Data Contract CLI を触ってみたところ、面白かったのとこれからのデータパイプライン開発について思うところがあったので書いてみる。\nData Contract CLI とは？ datacontract/datacontract-cli\nData Contract CLI は data contracts を運用するためのオープンソースのコマンドラインツールである。\ndata contracts の概念については以前の記事で詳しく書いているのでそちらをご参考いただければと。\nただしこちらの記事は1年前のものであり、今回取り上げる Data Contract CLI の登場などを含めて現在では data contracts を取り巻く状況も変わっている可能性があることに注意。\nData Contract CLI は Python で開発されており、pip でインストールすることができる。\nこの記事を書いている時点では v0.10.3 が最新であり、この記事の内容はこのバージョンに基づいている。\nData Contract CLI で扱う data contracts は YAML で定義される前提となっており、その仕様は datacontract/datacontract-specification で決められている。\nこの data contracts に対して Data Contract CLI では次のようなことが行える。\nlint によるフォーマットチェック データソースに接続した上での schema やデータ品質のテスト data contracts の破壊的な変更の検出 JSON Schema や dbt など、他の形式からの／へのインポートとエクスポート 以下の図がイメージしやすい。\ndatacontract/datacontract-cli より\nData Contract CLI の開発者が何を考えているか、この図から推測できる部分があるので後ほど考察したい。\nData Contract CLI を使ってみる 前提 手元に個人開発用の BigQuery の table があったので、これについて data contract を用意して Data Contract CLI を使ってみることにした。\n個人の MoneyForward の情報を取り込んでいる table であり、収支詳細のカテゴリを扱う table である。\ndata contract (YAML ファイル) の作成は基本的には best practice に従う。\ndata contract の作成後は export や変更の検出などを試してみる。\n準備 今回は Poetry で Python 環境を用意した。\ndatacontract-cli および BigQuery を扱うのに必要となる google-cloud-bigquery-storage を install しておく。\n[tool.poetry.dependencies] python = \u0026#34;^3.10\u0026#34; datacontract-cli = \u0026#34;^0.10.3\u0026#34; google-cloud-bigquery-storage = \u0026#34;^2.24.0\u0026#34; バージョンを確認。\n$ datacontract --version 0.10.3 DDL からの import して data contract を作成 まず data contract を用意する必要がある。\ndata contract は YAML ファイルなので仕様を見ながら一から書くこともできるが、既存の table などがある場合は import を使うことができる。\nちなみに v0.10.3 の時点で対応している import のソースは sql, avro, glue の3つ。\nBigQuery では INFORMATION_SCHEMA.TABLES から既存 table の DDL を得ることができるため、それを利用して money_forward_main_group.sql を用意した。\nCREATE TABLE `\u0026lt;PROJECT_ID\u0026gt;.\u0026lt;DATASET_ID\u0026gt;.money_forward_main_group` ( mf_main_group_id INT64, name STRING, is_income BOOL ); この DDL ファイルを使って import サブコマンドにより data contract を作成する。\n$ datacontract import --format sql --source money_forward_main_group.sql \u0026gt; datacontract_v1.yml 作成された datacontract_v1.yml の中は次のようになっている。\ndataContractSpecification: 0.9.3 id: my-data-contract-id info: title: My Data Contract version: 0.0.1 models: money_forward_main_group`: type: table fields: mf_main_group_id: type: integer name: type: string is_income: type: boolean この時点で一度 lint サブコマンドでフォーマットチェックしておく。\ndatacontract lint datacontract_v1.yml WARNING:root:Data Contract YAML is invalid. Validation error: data.models must be named by propertyName definition ERROR:root:Run operation failed: [lint] Check that data contract YAML is valid - None - failed - data.models must be named by propertyName definition - datacontract ╭────────┬────────────────────────────────────────┬───────┬────────────────────────────────────────────────────╮ │ Result │ Check │ Field │ Details │ ├────────┼────────────────────────────────────────┼───────┼────────────────────────────────────────────────────┤ │ failed │ Check that data contract YAML is valid │ │ data.models must be named by propertyName │ │ │ │ │ definition │ ╰────────┴────────────────────────────────────────┴───────┴────────────────────────────────────────────────────╯ 🔴 data contract is invalid, found the following errors: 1) data.models must be named by propertyName definition エラーになってしまった。\n実はこの時点で datacontract_v1.yml にいくつか問題があり、修正することに。\n不要な \u0026ldquo;`\u0026rdquo; が含まれている servers ブロックがない (確かに DDL を与えるだけだと BigQuery なのか Snowflake なのかわからんな…) 次のように修正して datacontract_v2.yml を作った。\n@@ -3,8 +3,13 @@ info: title: My Data Contract version: 0.0.1 +servers: + dev: + type: bigquery + project: \u0026lt;PROJECT_ID\u0026gt; + dataset: \u0026lt;DATASET_ID\u0026gt; models: - money_forward_main_group`: + money_forward_main_group: type: table fields: mf_main_group_id: 再度 lint サブコマンドを実行すると成功した。\nただし description がない旨の warning が出ている。\ntable や column の description や制約事項を追加して datacontract_v3.yml とする。\n@@ -3,6 +3,9 @@ info: title: My Data Contract version: 0.0.1 + description: | + MoneyForward の収支における分類 (項目)。 + ダウンロードできる収支詳細に記載される情報を元に作成されている。 servers: dev: type: bigquery @@ -11,10 +14,18 @@ models: money_forward_main_group: type: table + description: MoneyForward の収支における大項目 fields: mf_main_group_id: + description: 大項目 ID type: integer + required: true + primary: true + unique: true name: + description: 大項目の項目名 type: string + required: true is_income: + description: 大項目が収入の場合は true, 支出の場合は false type: boolean lint サブコマンドが成功するようになった。\n$ datacontract lint datacontract_v3.yml ╭────────┬──────────────────────────────────────────┬───────┬─────────╮ │ Result │ Check │ Field │ Details │ ├────────┼──────────────────────────────────────────┼───────┼─────────┤ │ passed │ Data contract is syntactically valid │ │ │ │ passed │ Linter \u0026#39;Field pattern is correct regex\u0026#39; │ │ │ │ passed │ Linter \u0026#39;Objects have descriptions\u0026#39; │ │ │ │ passed │ Linter \u0026#39;Field references existing field\u0026#39; │ │ │ │ passed │ Linter \u0026#39;Example(s) match model\u0026#39; │ │ │ │ passed │ Linter \u0026#39;Fields use valid constraints\u0026#39; │ │ │ │ passed │ Linter \u0026#39;Quality check(s) use model\u0026#39; │ │ │ │ passed │ Linter \u0026#39;noticePeriod in ISO8601 format\u0026#39; │ │ │ ╰────────┴──────────────────────────────────────────┴───────┴─────────╯ 🟢 data contract is valid. Run 8 checks. Took 0.304237 seconds. 初回テスト 次に test サブコマンドでテストを実行する。\nちなみに test では BiqQuery にアクセスするため、環境変数 DATACONTRACT_BIGQUERY_ACCOUNT_INFO_JSON_PATH に認証情報の JSON ファイルを指定しておく必要がある。\n$ datacontract test datacontract_v3.yml Testing datacontract_v3.yml Column,Event,Details is_income,:icon-fail: Type Mismatch, Expected Type: boolean; Actual Type: BOOL Type Mismatch, Expected Type: boolean; Actual Type: BOOL ╭────────┬──────────────────────────────────────────────────────────────────┬──────────────────┬────────────────────────────────────────────────────╮ │ Result │ Check │ Field │ Details │ ├────────┼──────────────────────────────────────────────────────────────────┼──────────────────┼────────────────────────────────────────────────────┤ │ passed │ Check that field mf_main_group_id is present │ │ │ │ passed │ Check that field mf_main_group_id has type integer │ │ │ │ passed │ Check that field name is present │ │ │ │ passed │ Check that field name has type string │ │ │ │ passed │ Check that field is_income is present │ │ │ │ failed │ Check that field is_income has type boolean │ │ Type Mismatch, Expected Type: boolean; Actual │ │ │ │ │ Type: BOOL │ │ passed │ Check that required field mf_main_group_id has no null values │ mf_main_group_id │ │ │ passed │ Check that unique field mf_main_group_id has no duplicate values │ mf_main_group_id │ │ │ passed │ Check that required field name has no null values │ name │ │ ╰────────┴──────────────────────────────────────────────────────────────────┴──────────────────┴────────────────────────────────────────────────────╯ 🔴 data contract is invalid, found the following errors: 1) Type Mismatch, Expected Type: boolean; Actual Type: BOOL エラーになってしまった。\ncolumn is_income の型が boolean ではなく BOOL だと言われている。\n確かに BiqQuery としての型は BOOL であるが、一方で data contracts の仕様としては boolean しか指定できない。\nバグっぽいのでいったん is_income の型指定をはずしておく…\n出力された内容を見るとどういったことがチェックされているかがわかる。\nこの時点では column の存在および型がチェックされていて、さらに制約事項を記載した column についてはその制約を満たしているかがチェックされている。\nデータ例の追加 次に data contract に examples ブロックを追加する。\n名前のとおりでデータの例を記載することができる。\nまず data contract を宣言して開発することを考えると、例が示されているのはとても助かる。\n次のように修正して datacontract_v4.yml を作った。\n@@ -28,4 +28,14 @@ required: true is_income: description: 大項目が収入の場合は true, 支出の場合は false - type: boolean + # type mismatch になるバグ？があるため boolean 型のチェックは除外 + # type: boolean +examples: + - type: csv + description: money_forward_main_group のレコードの例。 + model: money_forward_main_group + data: | + mf_main_group_id,name,is_income + 1,\u0026#34;収入\u0026#34;,true + 2,\u0026#34;食費\u0026#34;,false + 3,\u0026#34;日用品\u0026#34;,false オプション --examples をつけて test サブコマンドを実行すると、examples ブロックに追加したデータに対してテストを行うことができる。\n$ datacontract test --examples datacontract_v4.yml Testing datacontract_v4.yml ╭────────┬──────────────────────────────────────────────────────────────────┬──────────────────┬─────────╮ │ Result │ Check │ Field │ Details │ ├────────┼──────────────────────────────────────────────────────────────────┼──────────────────┼─────────┤ │ passed │ Check that field mf_main_group_id is present │ │ │ │ passed │ Check that field name is present │ │ │ │ passed │ Check that field is_income is present │ │ │ │ passed │ Check that required field mf_main_group_id has no null values │ mf_main_group_id │ │ │ passed │ Check that unique field mf_main_group_id has no duplicate values │ mf_main_group_id │ │ │ passed │ Check that required field name has no null values │ name │ │ ╰────────┴──────────────────────────────────────────────────────────────────┴──────────────────┴─────────╯ 🟢 data contract is valid. Run 6 checks. Took 0.232013 seconds. データ品質チェックの追加 quality ブロックを追加してデータ品質チェックを行うことができる。\nv0.10.3 の時点では SodaCL, Monte Carlo, greate expectation の記法で品質チェックを書くことができる。\nここでは SodaCL の記法で table の行数が1件以上あることを確認する簡単なチェックを追加して datacontract_v5.yml とする。\n@@ -39,3 +39,8 @@ 1,\u0026#34;収入\u0026#34;,true 2,\u0026#34;食費\u0026#34;,false 3,\u0026#34;日用品\u0026#34;,false +quality: + type: SodaCL + specification: + checks for money_forward_main_group: + - row_count \u0026gt; 0 examples と BigQuery それぞれに対して test サブコマンドを実行。\n$ datacontract test --examples datacontract_v5.yml Testing datacontract_v5.yml ╭────────┬──────────────────────────────────────────────────────────────────┬──────────────────┬─────────╮ │ Result │ Check │ Field │ Details │ ├────────┼──────────────────────────────────────────────────────────────────┼──────────────────┼─────────┤ │ passed │ Check that field mf_main_group_id is present │ │ │ │ passed │ Check that field name is present │ │ │ │ passed │ Check that field is_income is present │ │ │ │ passed │ row_count \u0026gt; 0 │ │ │ │ passed │ Check that required field mf_main_group_id has no null values │ mf_main_group_id │ │ │ passed │ Check that unique field mf_main_group_id has no duplicate values │ mf_main_group_id │ │ │ passed │ Check that required field name has no null values │ name │ │ ╰────────┴──────────────────────────────────────────────────────────────────┴──────────────────┴─────────╯ 🟢 data contract is valid. Run 7 checks. Took 0.48302 seconds. $ datacontract test datacontract_v5.yml Testing datacontract_v5.yml ╭────────┬──────────────────────────────────────────────────────────────────┬──────────────────┬─────────╮ │ Result │ Check │ Field │ Details │ ├────────┼──────────────────────────────────────────────────────────────────┼──────────────────┼─────────┤ │ passed │ Check that field mf_main_group_id is present │ │ │ │ passed │ Check that field mf_main_group_id has type integer │ │ │ │ passed │ Check that field name is present │ │ │ │ passed │ Check that field name has type string │ │ │ │ passed │ Check that field is_income is present │ │ │ │ passed │ row_count \u0026gt; 0 │ │ │ │ passed │ Check that required field mf_main_group_id has no null values │ mf_main_group_id │ │ │ passed │ Check that unique field mf_main_group_id has no duplicate values │ mf_main_group_id │ │ │ passed │ Check that required field name has no null values │ name │ │ ╰────────┴──────────────────────────────────────────────────────────────────┴──────────────────┴─────────╯ 🟢 data contract is valid. Run 9 checks. Took 5.690055 seconds. 行数 (row_count) のチェックが増えていることが確認できる。\nここまでのところで data contract (YAML) はいったん完成とする。\nこの時点での全体像を記載しておく。\ndataContractSpecification: 0.9.3 id: my-data-contract-id info: title: My Data Contract version: 0.0.1 description: | MoneyForward の収支における分類 (項目)。 ダウンロードできる収支詳細に記載される情報を元に作成されている。 servers: dev: type: bigquery project: \u0026lt;PROJECT_ID\u0026gt; dataset: \u0026lt;DATASET_ID\u0026gt; models: money_forward_main_group: type: table description: MoneyForward の収支における大項目 fields: mf_main_group_id: description: 大項目 ID type: integer required: true primary: true unique: true name: description: 大項目の項目名 type: string required: true is_income: description: 大項目が収入の場合は true, 支出の場合は false # type mismatch になるバグ？があるため boolean 型のチェックは除外 # type: boolean examples: - type: csv description: money_forward_main_group のレコードの例。 model: money_forward_main_group data: | mf_main_group_id,name,is_income 1,\u0026#34;収入\u0026#34;,true 2,\u0026#34;食費\u0026#34;,false 3,\u0026#34;日用品\u0026#34;,false quality: type: SodaCL specification: checks for money_forward_main_group: - row_count \u0026gt; 0 ここで作った data contract はフルの記載ではない。\n例えば servicelevels ブロックを追加して SLA を記載することもできる。\ndata contract のすべての仕様を知りたい場合は仕様ドキュメントを参照のこと。\nexport を試す JSON Schema data contract ができたところで export を試していきたい。\nv0.10.3 では14種類のフォーマットに対しての export がサポートされており、ここではそのうちのいくつかを使ってみる。\nまずは JSON Schema で出力してみる。\n$ datacontract export --format jsonschema datacontract_v5.yml { \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;mf_main_group_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;unique\u0026#34;: true }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;is_income\u0026#34;: {} }, \u0026#34;required\u0026#34;: [ \u0026#34;mf_main_group_id\u0026#34;, \u0026#34;name\u0026#34; ] } JSON Schema の記法でモデルが表現されたものが標準出力された。\n同様に Avro, Protobuf などの形式でも export することができる。\nデータ基盤的な話をすると、取り込みの対象であるアプリケーション (data provider) から送られてくるログは JSON 形式になっていることがよくある。\ndata contract をアプリケーション側とデータ基盤側で合意しておけば、アプリケーション側でログ生成時に JSON Schema でログに validation をかけることができる。\nこれによりログの schema の意図しない変更を防ぐ運用が可能になるだろう。\ndbt dbt の形式で export することもできる。\ndbt の YAML ドキュメントを出力してみる。\n$ datacontract export --format dbt --server bigquery datacontract_v5.yml version: 2 models: - name: money_forward_main_group config: meta: data_contract: my-data-contract-id materialized: table contract: enforced: true description: MoneyForward の収支における大項目 columns: - data_type: NUMBER description: 大項目 ID constraints: - type: not_null - type: unique name: mf_main_group_id - data_type: STRING description: 大項目の項目名 constraints: - type: not_null name: name - description: 大項目が収入の場合は true, 支出の場合は false name: is_income data_type が微妙で BigQuery の型定義になっていない。\n--server bigquery を与えているので配慮してほしいところではある。\nsources 用のドキュメントの形式でも export できる。\n$ datacontract export --format dbt-sources --server bigquery datacontract_v5.yml version: 2 sources: - name: my-data-contract-id tables: - name: money_forward_main_group description: MoneyForward の収支における大項目 columns: - tests: - dbt_expectations.dbt_expectations.expect_column_values_to_be_of_type: column_type: NUMBER - not_null - unique description: 大項目 ID name: mf_main_group_id - tests: - dbt_expectations.dbt_expectations.expect_column_values_to_be_of_type: column_type: STRING - not_null description: 大項目の項目名 name: name - description: 大項目が収入の場合は true, 支出の場合は false name: is_income description: \u0026#39;MoneyForward の収支における分類 (項目)。 ダウンロードできる収支詳細に記載される情報を元に作成されている。 \u0026#39; staging 用の SQL の形式の export もある。\n$ datacontract export --format dbt-staging-sql --server bigquery datacontract_v5.yml select mf_main_group_id, name, is_income from {{ source(\u0026#39;my-data-contract-id\u0026#39;, \u0026#39;money_forward_main_group\u0026#39;) }} このようにいくつかの dbt の形式で出力できるが、微妙な部分があるのでこのまま data contract を SSoT として dbt のファイルを CI で自動生成するという運用は現時点では難しいだろう。\nまたリッチなテストなども表現できないため、dbt をがっつり使っているチームには物足りなさがあると思われる。\ndata contract をまず作り、それをもとに dbt モデルを構築していくときのベースを作る用途なら現時点でもありかもしれない。\nHTML HTML 形式で export することもできる。\nホスティングして data contract を web で見せられるようにすることもできるだろう。\n$ datacontract export --format html datacontract_v5.yml \u0026gt; money_forward_main_group.html HTML での export 例\n頑張ればこれで data catalog を作ることもできそう。\nまた今回の data contract には含まれていないが、前述の servicelevels ブロックがあれば web 上で SLA を示せたりできて良さそうだ。\n考察 Data Contract CLI で何ができるかがわかったところで最初の図を再掲。\ndatacontract/datacontract-cli より\nこの図から Data Contract CLI としては data contracts ファーストな運用を考えていることがわかる。\nまず data provider と data consumer の合意のもとに宣言的に data contracts を作成する。\nそれに基づき、それぞれ data provider と data consumer で使われるツール用に data contracts から必要なファイルを自動生成していく。\n例えば data provider である application 向けにはログの validation 用に JSON Schema のファイルを作成する。\ndata consumer である DWH 向けには dbt のファイルを用意する、など。\nまた、data contract に記載の情報からデータに対してテストを実行することができる。\n簡単な column の制約から、SodaCL などを使ったより高度な品質チェックまで。\nデータについてのドキュメントも data contract から生成される。\nこのようにまず data contract ありきで、モデルのメタ情報としてはそれを SSoT とし、data provider / consumer で必要なファイルを CI で自動生成するという未来が見えてくるような気がする。\nただしその未来を実現するための課題も現状では多い。\nData Contract CLI の相互運用性 (interoperatbility) が最も大きなボトルネックになるだろう。\n例えば想定する dbt ドキュメントが一発で作れるか、package を使ったリッチなテストが書けるか、etc.\nそもそも前述したようにバグっぽい挙動や微妙な挙動もあり、このツール自体がまだ発展途上という感がある。\nこれから普及していくかというのは今のところ何とも言えない。\nちなみに作者の1人は Data Mesh Manager というサービスの制作者であり、(ここまでのところでは触れていなかったが) Data Contract CLI にはこの Data Mesh Manager との連携機能が組み込まれている。\ndata contract ファーストの世界のビジョンはとても面白いと思ったので、いい方向でツールが発展していくとうれしい。\n","permalink":"https://soonraah.github.io/posts/data-contract-cli/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003eData Contract CLI を触ってみたところ、面白かったのとこれからのデータパイプライン開発について思うところがあったので書いてみる。\u003c/p\u003e\n\u003ch2 id=\"data-contract-cli-とは\"\u003eData Contract CLI とは？\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/datacontract/datacontract-cli\"\u003edatacontract/datacontract-cli\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eData Contract CLI は data contracts を運用するためのオープンソースのコマンドラインツールである。\u003c/p\u003e\n\u003cp\u003edata contracts の概念については\u003ca href=\"/posts/looked-into-data-contracts/\"\u003e以前の記事\u003c/a\u003eで詳しく書いているのでそちらをご参考いただければと。\u003cbr\u003e\nただしこちらの記事は1年前のものであり、今回取り上げる Data Contract CLI の登場などを含めて現在では data contracts を取り巻く状況も変わっている可能性があることに注意。\u003c/p\u003e\n\u003cp\u003eData Contract CLI は Python で開発されており、pip でインストールすることができる。\u003cbr\u003e\nこの記事を書いている時点では v0.10.3 が最新であり、この記事の内容はこのバージョンに基づいている。\u003c/p\u003e\n\u003cp\u003eData Contract CLI で扱う data contracts は YAML で定義される前提となっており、その仕様は \u003ca href=\"https://github.com/datacontract/datacontract-specification\"\u003edatacontract/datacontract-specification\u003c/a\u003e で決められている。\u003cbr\u003e\nこの data contracts に対して Data Contract CLI では次のようなことが行える。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elint によるフォーマットチェック\u003c/li\u003e\n\u003cli\u003eデータソースに接続した上での schema やデータ品質のテスト\u003c/li\u003e\n\u003cli\u003edata contracts の破壊的な変更の検出\u003c/li\u003e\n\u003cli\u003eJSON Schema や dbt など、他の形式からの／へのインポートとエクスポート\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e以下の図がイメージしやすい。\u003c/p\u003e","title":"Data Contract CLI から考える Data Contracts ファーストのデータパイプラインの未来"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第8章「データ統合と相互運用性」について。\n業務で扱っているデータ基盤はデータ統合が不完全であるため、なんとかしたいと考えている。\n以降、特に注釈のない引用は DMBOK2 第8章からの引用とする。\nデータストレージと相互運用性とは データ統合と相互運用性 (DII: Data Integration and Interoperability) は次のように定義されている。\nアプリケーションや組織内および相互間におけるデータの移動と統合を管理する\nデータの移動を効率的に管理することがそのビジネス上の意義となる。\nほとんどの組織には数多くのデータストアがあり、組織内・組織間でデータを移動させることは IT 組織の重要な任務となっている。\n複雑さとそれに伴うコストを管理するために、全社的な視点からデータ統合を設計しなければならない。\nデータウェアハウスなどのデータハブによりアプリケーション間のインターフェースの数を削減することができる。\nDII のゴールは以下\n法令を遵守しながら、必要とするフォーマットと時間枠でデータを安全に提供する。 共有のモデルとインターフェースを開発することでソリューションを管理するコストと複雑さを軽減する。 重要なイベントを特定し、アラートとアクションを自動的に起動する。 ビジネスインテリジェンス、アナリティクス、マスターデータ管理、業務効率化の取り組みをサポートする。 概念・用語など 抽出、変換、取込 DII の中心にあるのが、抽出 (Extract)、変換 (Transform)、取込 (Load) のいわゆる ETL という基本プロセス。\n抽出 ソースから必要なデータを選択し、抽出する 抽出されたデータはディスク上やメモリ上にステージングされる 業務システムで実行される場合は、少ないリソースを利用するように設計する 変換 ソースデータを変換してターゲットデータストアの構造と互換性を持つようにする フォーマット変更、構造の変更、意味的変換、重複排除、並べ替えなどがある 取込 ターゲットシステムに物理的に格納されるか、提供される ELT ターゲットシステムにより多くの変化機能がある場合は、プロセスの順序を ELT にすることができる データレイクへの取込を行うビッグデータ環境では一般的 レイテンシ ソースシステムでデータが生成されてから、ターゲットシステムでそのデータが利用可能になるまでの時間差。\nアプローチによってレイテンシの高低が異なる。\nバッチ 利用者や自動的な要求に応えて、定期的にアプリケーションや組織間を一定量まとまって移動させる レイテンシは高いが大量データを処理するときのパフォーマンスがいい 低レイテンシを実現するためのマイクロバッチもある 変更データキャプチャ データの変更 (挿入・変更・削除) のデータセットを監視し、その差分をターゲットのシステムに渡す DBMS のアクティビティログをコピーし、処理する形で行われることもある 準リアルタイムとイベント駆動 設定された予定により1日を通して分割された少量のデータセットで処理されたり、データ更新などのイベントが発生したときに処理されたりする 一般的にエンタープライズ・サービス・バスを利用して実装される 非同期 データ提供側は受信側の更新確認を待たずに処理を続行する リアルタイム、同期 次のトランザクションを実行する前に、他のアプリケーションからの確認を受け取るまで実行プロセスが待機する 非同期と比べて状態管理の負担が少ないが、他のトランザクションをブロックしたり遅延させたりすることもある 低レイテンシまたはストリーミング イベントが発生したときにシステムからリアルタイムで連続して流れる リプリケーション 分析やクエリによるパフォーマンス低下を防ぐために、トランザクション処理環境にリプリケーション (複製) を使用することがある。\n多くの DBMS にはリプリケーションを作るためのユーティリティ機能がある。\nアーカイブ 利用頻度が低いデータは、よりコストがかからない代替データ構造やストレージに移行できる。\nアーカイブ技術が変わっても古いデータにアクセスできるようにしておくことが重要。\nエンタープライズ・メッセージフォーマット／カノニカルモデル 組織やグループでデータを共有するための標準化されたフォーマット、共通モデルがカノニカルモデル。\n組織内で合意するのは大仕事だが、データ相互運用性の複雑さとサポートコストが大幅に削減される。\nデータ連携モデル データ転送のためのシステム間を接続する方法。\nポイント・ツー・ポイント データを共有するシステム間で互いに直接データを渡す 多数のシステムが同じソースから同じデータを必要とする場合は効率が下がる ハブ＆スポーク 多くのアプリケーションが利用できる中央データハブへ共有データを集約 データウェアハウス、データマート、オペレーショナル・データストア、マスターデータ管理のハブなどがその例 ソースシステムへのリソースへの影響やインターフェース構築のコストを最小にできる パブリッシュ・サブスクライブ データを提供するシステムはデータサービスのカタログにリストされ、データを利用するシステムはそれらのサービスをサブスクライブする DII アーキテクチャの概念 アプリケーションカップリング 2つのシステムが関連し合っている結合度合い サービス、API、メッセージキューなどの技法を用いて疎結合にするのが望ましい オーケストレーションとプロセスコントロール オーケストレーションとは復数のプロセスがシステム内でどのように構成され、実行されるかを説明するために使われる プロセスコントロールはデータの発信、配信、取込が正確かつ完全であることを保証するために必要な要素 バッチジョブのログ アラート ジョブ依存チャート etc. エンタープライズアプリケーション統合 ソフトウェアモジュールは明確に定義されたインタフェース呼び出し (API) だけを介して連携するというモデル エンタープライズ・サービスバス システム間の仲介役として機能し、システム間でメッセージをやり取りするシステム 疎結合の例ではアプリケーション間のサービスとして機能する サービス指向アーキテクチャ (SOA) アプリケーション間で明確に定義されたサービス呼び出しを利用することでデータを提供したり、データを更新したりする SOA によりアプリケーションの独立性が担保され、システムの置き換えが容易になる データについてのサービスはサービスカタログ上で定義される 複合イベント処理 (CEP) 復数のソースから得られるデータを結合して重要なイベントを識別すること 振る舞いや行動を予測し、リアルタイム応答を自動的に起動できるようになる データフェデレーションと仮想化 データフェデレーションでは構造に関係なく、個々のデータストアの組み合わせにアクセスできる 仮想化では復数の異種データストアを単一のデータベースとしてアクセスできる Data as a Service SaaS (Software as a Service) と同様の概念 DaaS ではベンダーがライセンスを提供し、必要に応じてデータを提供する クラウドベースの統合 データ、プロセス、SOA、アプリケーション統合などのユースケースに対応する、クラウドサービスとして提供されるシステム統合の一形態 SaaS の登場により、クラウドベースの統合を経て、組織が持つデータセンターの外にあるデータを統合するための新しい種類の需要が生まれた データ交換標準 データエレメントの構造に関する正式なルール。\nISO により制定されている。\nシステム間で交換書式やデータレイアウトを合意すれば、起業のデータ相互運用性を大幅に簡素化し、運用コストを下げることができる。\n所感 概ね業務などを通じて知っている内容だった。\nデータ統合は難しい。\nApache Flink には CEP の機能があるけど、よく使われているのだろうか？\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-8/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.dama-japan.org/Introduction.html\"\u003eDMBOK2\u003c/a\u003e を読み進めていくシリーズ。\u003cbr\u003e\n今回は第8章「データ統合と相互運用性」について。\u003cbr\u003e\n業務で扱っているデータ基盤はデータ統合が不完全であるため、なんとかしたいと考えている。\u003c/p\u003e\n\u003cp\u003e以降、特に注釈のない引用は DMBOK2 第8章からの引用とする。\u003c/p\u003e\n\u003ch2 id=\"データストレージと相互運用性とは\"\u003eデータストレージと相互運用性とは\u003c/h2\u003e\n\u003cp\u003eデータ統合と相互運用性 (DII: Data Integration and Interoperability) は次のように定義されている。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eアプリケーションや組織内および相互間におけるデータの移動と統合を管理する\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eデータの移動を効率的に管理することがそのビジネス上の意義となる。\u003cbr\u003e\nほとんどの組織には数多くのデータストアがあり、組織内・組織間でデータを移動させることは IT 組織の重要な任務となっている。\u003cbr\u003e\n複雑さとそれに伴うコストを管理するために、全社的な視点からデータ統合を設計しなければならない。\u003cbr\u003e\nデータウェアハウスなどのデータハブによりアプリケーション間のインターフェースの数を削減することができる。\u003c/p\u003e\n\u003cp\u003eDII のゴールは以下\u003c/p\u003e\n\u003cblockquote\u003e\n\u003col\u003e\n\u003cli\u003e法令を遵守しながら、必要とするフォーマットと時間枠でデータを安全に提供する。\u003c/li\u003e\n\u003cli\u003e共有のモデルとインターフェースを開発することでソリューションを管理するコストと複雑さを軽減する。\u003c/li\u003e\n\u003cli\u003e重要なイベントを特定し、アラートとアクションを自動的に起動する。\u003c/li\u003e\n\u003cli\u003eビジネスインテリジェンス、アナリティクス、マスターデータ管理、業務効率化の取り組みをサポートする。\u003c/li\u003e\n\u003c/ol\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"概念用語など\"\u003e概念・用語など\u003c/h2\u003e\n\u003ch3 id=\"抽出変換取込\"\u003e抽出、変換、取込\u003c/h3\u003e\n\u003cp\u003eDII の中心にあるのが、抽出 (Extract)、変換 (Transform)、取込 (Load) のいわゆる ETL という基本プロセス。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e抽出\n\u003cul\u003e\n\u003cli\u003eソースから必要なデータを選択し、抽出する\u003c/li\u003e\n\u003cli\u003e抽出されたデータはディスク上やメモリ上にステージングされる\u003c/li\u003e\n\u003cli\u003e業務システムで実行される場合は、少ないリソースを利用するように設計する\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e変換\n\u003cul\u003e\n\u003cli\u003eソースデータを変換してターゲットデータストアの構造と互換性を持つようにする\u003c/li\u003e\n\u003cli\u003eフォーマット変更、構造の変更、意味的変換、重複排除、並べ替えなどがある\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e取込\n\u003cul\u003e\n\u003cli\u003eターゲットシステムに物理的に格納されるか、提供される\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eELT\n\u003cul\u003e\n\u003cli\u003eターゲットシステムにより多くの変化機能がある場合は、プロセスの順序を ELT にすることができる\u003c/li\u003e\n\u003cli\u003eデータレイクへの取込を行うビッグデータ環境では一般的\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"レイテンシ\"\u003eレイテンシ\u003c/h3\u003e\n\u003cp\u003eソースシステムでデータが生成されてから、ターゲットシステムでそのデータが利用可能になるまでの時間差。\u003cbr\u003e\nアプローチによってレイテンシの高低が異なる。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eバッチ\n\u003cul\u003e\n\u003cli\u003e利用者や自動的な要求に応えて、定期的にアプリケーションや組織間を一定量まとまって移動させる\u003c/li\u003e\n\u003cli\u003eレイテンシは高いが大量データを処理するときのパフォーマンスがいい\u003c/li\u003e\n\u003cli\u003e低レイテンシを実現するためのマイクロバッチもある\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e変更データキャプチャ\n\u003cul\u003e\n\u003cli\u003eデータの変更 (挿入・変更・削除) のデータセットを監視し、その差分をターゲットのシステムに渡す\u003c/li\u003e\n\u003cli\u003eDBMS のアクティビティログをコピーし、処理する形で行われることもある\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e準リアルタイムとイベント駆動\n\u003cul\u003e\n\u003cli\u003e設定された予定により1日を通して分割された少量のデータセットで処理されたり、データ更新などのイベントが発生したときに処理されたりする\u003c/li\u003e\n\u003cli\u003e一般的にエンタープライズ・サービス・バスを利用して実装される\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e非同期\n\u003cul\u003e\n\u003cli\u003eデータ提供側は受信側の更新確認を待たずに処理を続行する\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eリアルタイム、同期\n\u003cul\u003e\n\u003cli\u003e次のトランザクションを実行する前に、他のアプリケーションからの確認を受け取るまで実行プロセスが待機する\u003c/li\u003e\n\u003cli\u003e非同期と比べて状態管理の負担が少ないが、他のトランザクションをブロックしたり遅延させたりすることもある\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e低レイテンシまたはストリーミング\n\u003cul\u003e\n\u003cli\u003eイベントが発生したときにシステムからリアルタイムで連続して流れる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"リプリケーション\"\u003eリプリケーション\u003c/h3\u003e\n\u003cp\u003e分析やクエリによるパフォーマンス低下を防ぐために、トランザクション処理環境にリプリケーション (複製) を使用することがある。\u003cbr\u003e\n多くの DBMS にはリプリケーションを作るためのユーティリティ機能がある。\u003c/p\u003e","title":"読書メモ: DMBOK2 第8章 データ統合と相互運用性"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第5章「データストレージとオペレーション」について。\n主にデータベースの運用に関する内容となっており、いわゆるデータベースエンジニアの人には当たり前の話？が書かれている。\n以降、特に注釈のない引用は DMBOK2 第6章からの引用とする。\nデータストレージとオペレーションとは 以下のように定義されている。\nデータの価値を最大化するために、永続化されるデータを設計し、実装し、サポートすること\n主にデータベース管理者 (DBA: Database Administrators) が行うことになる。\n次の2つのアクティビティが含まれる。\nデータベースサポート: データベース環境の初期実装からデータの取得、バックアップ、廃棄までのデータライフサイクル関連アクティビティ データベース技術サポート: 技術要件を決め、技術的なアーキテクチャを定義し、技術を実装・管理する 事業がデータに依存する企業においてはデータストレージとオペレーションのアクティビティは事業の継続性のために必要不可欠である。\nゴールは次のとおり\nデータライフサイクル全体にわたるデータの可用性を管理する データ資産の完全性を保証する データ処理の性能を管理する 概念・用語など データベースアーキテクチャの種類 集中型データベース: 単一システム内で使うデータを一箇所にまとている 分散型データベース: 多数のノードにデータが配置される 連邦型データベース: 自立した複数のデータベースシステムを単一の連邦型データベースに割り当てる 仮想化／クラウドプラットフォーム: クラウド上のデータベースを実装 データベース処理のタイプ ACID: トランザクションの信頼性のための制約 原子性 (Atomicity): 操作はすべて実行されるかまったく実行されないかのどちらか 一貫性 (Consistency): トランザクションはシステムが定義するすべてのルールを常に満たさなければならない 独立性 (Isolation): 各トランザクションは実行中の他のトランザクションの影響を受けない 永続性 (Durability): トランザクションは完了すると元に戻せない BASE: データの量と多様性を受けた、ACID とは異なる考え 基本的に利用可能 (Basically Available): ノードの障害発生時もあるレベル以上の可用性を保証する ソフトステート (Soft State): データは一定の変動状態にある 最終的な一貫性の確保 (Eventual Consistency): データは最終的にすべてのノードで一貫性を保つが、各トランザクションの一貫性が常に確保されているわけではない CAP: 分散システムでは以下のどれか2つしか満たせない 一貫性 (Consistency): システムは常に想定どおり動作できなければならない 可用性 (Availability): システムは要求時に利用可能でなければならに 分断耐性 (Partition Tolerance): システムは部分的な障害の発生時に運用を続行できなければならない データベース構成 上から順により制御された構造であり、かつ古くからあるものとなっている。\n階層型 tree schema データは親子関係が必要な木構造として整理されている リレーショナル型 集合理論や関係代数にもとづく schema on write 書き込み時に構造を知っておく必要がある 非リレーショナル型 データを単純な文字列や完全なファイルとして保存することができる schema on read 様々な方法で読み取ることができる 一般的なデータベースプロセス すべてのデータベースは何らかの形で以下のプロセスを実現する。\nアーカイブ 最大容量と増加の予測 変更データキャプチャ (CDC: Change Data Capture) 廃棄 レプリケーション 耐障害性と復旧 保持 シャーディング (区画化) アクティビティ データストレージとオペレーションでは主にデータベース技術の管理とデータベースの管理の2つのアクティビティがある。\nデータベース技術の管理 データベース技術の特徴に対する理解 技術選定において候補となるデータベース技術の特徴を理解しておく必要がある 1つのデータベースアーキテクチャや DBMS があらゆるニーズに対応しているわけではない データベース技術の評価 戦略的に DBMS ソフトウェアを選定することが重要 小規模なパイロットプロジェクトや PoC の実施を推奨 データベース技術の管理と監視 データベース技術の実装、サポート、利用に関わる人にトレーニングが必要 定期的なバックアップとリカバリテスト データベースの管理 要件の理解 容量見積などを含むストレージ要件 トランザクション型、大規模な書き込み \u0026amp; 検索型などの使用パターン データアクセスのための言語、手法などのアクセス要件 事業継続性の計画 災害や有害事象への備え バックアップの作成 データのリカバリ データベースインスタンスの実装 物理ストレージ環境の管理 データベース・アクセス統制管理 ストレージコンテナの作成 物理データモデルの実装 データの取り込み データのレプリケーション管理 データベース性能の管理 SLA の設定 データベースの可用性の管理 データベースの稼働管理 サービスレベルの維持 トランザクション性能とバッチ性能 性能の問題の修復 代替環境の維持 テスト用データセットの管理 効率的なテストのための高品質なデータ データ移行の管理 まとめ この章はいわゆるオペレーショナルな RDBMS について書かれているようにも見えるが、抽象度が高いので組織レベルのデータ基盤や DWH にも当てはまる。\nDMBOK2 の内容は抽象的に書かれていることも多く、それゆえ現場エンジニアにとってぱっと理解しにくいものとなっていることもある。\nACID は大事だと思うが、データレイクの運用であまり考慮されていないケースを見かけることがある。\nCAP 定理は書籍「データ指向アプリケーションデザイン」で疑問を投げかけられてたな。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-6/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.dama-japan.org/Introduction.html\"\u003eDMBOK2\u003c/a\u003e を読み進めていくシリーズ。\u003cbr\u003e\n今回は第5章「データストレージとオペレーション」について。\u003cbr\u003e\n主にデータベースの運用に関する内容となっており、いわゆるデータベースエンジニアの人には当たり前の話？が書かれている。\u003c/p\u003e\n\u003cp\u003e以降、特に注釈のない引用は DMBOK2 第6章からの引用とする。\u003c/p\u003e\n\u003ch2 id=\"データストレージとオペレーションとは\"\u003eデータストレージとオペレーションとは\u003c/h2\u003e\n\u003cp\u003e以下のように定義されている。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eデータの価値を最大化するために、永続化されるデータを設計し、実装し、サポートすること\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e主にデータベース管理者 (DBA: Database Administrators) が行うことになる。\u003cbr\u003e\n次の2つのアクティビティが含まれる。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eデータベースサポート: データベース環境の初期実装からデータの取得、バックアップ、廃棄までのデータライフサイクル関連アクティビティ\u003c/li\u003e\n\u003cli\u003eデータベース技術サポート: 技術要件を決め、技術的なアーキテクチャを定義し、技術を実装・管理する\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e事業がデータに依存する企業においてはデータストレージとオペレーションのアクティビティは事業の継続性のために必要不可欠である。\u003cbr\u003e\nゴールは次のとおり\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cul\u003e\n\u003cli\u003eデータライフサイクル全体にわたるデータの可用性を管理する\u003c/li\u003e\n\u003cli\u003eデータ資産の完全性を保証する\u003c/li\u003e\n\u003cli\u003eデータ処理の性能を管理する\u003c/li\u003e\n\u003c/ul\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"概念用語など\"\u003e概念・用語など\u003c/h2\u003e\n\u003ch3 id=\"データベースアーキテクチャの種類\"\u003eデータベースアーキテクチャの種類\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e集中型データベース: 単一システム内で使うデータを一箇所にまとている\u003c/li\u003e\n\u003cli\u003e分散型データベース: 多数のノードにデータが配置される\u003c/li\u003e\n\u003cli\u003e連邦型データベース: 自立した複数のデータベースシステムを単一の連邦型データベースに割り当てる\u003c/li\u003e\n\u003cli\u003e仮想化／クラウドプラットフォーム: クラウド上のデータベースを実装\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"データベース処理のタイプ\"\u003eデータベース処理のタイプ\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eACID: トランザクションの信頼性のための制約\n\u003cul\u003e\n\u003cli\u003e原子性 (Atomicity): 操作はすべて実行されるかまったく実行されないかのどちらか\u003c/li\u003e\n\u003cli\u003e一貫性 (Consistency): トランザクションはシステムが定義するすべてのルールを常に満たさなければならない\u003c/li\u003e\n\u003cli\u003e独立性 (Isolation): 各トランザクションは実行中の他のトランザクションの影響を受けない\u003c/li\u003e\n\u003cli\u003e永続性 (Durability): トランザクションは完了すると元に戻せない\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBASE: データの量と多様性を受けた、ACID とは異なる考え\n\u003cul\u003e\n\u003cli\u003e基本的に利用可能 (Basically Available): ノードの障害発生時もあるレベル以上の可用性を保証する\u003c/li\u003e\n\u003cli\u003eソフトステート (Soft State): データは一定の変動状態にある\u003c/li\u003e\n\u003cli\u003e最終的な一貫性の確保 (Eventual Consistency): データは最終的にすべてのノードで一貫性を保つが、各トランザクションの一貫性が常に確保されているわけではない\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCAP: 分散システムでは以下のどれか2つしか満たせない\n\u003cul\u003e\n\u003cli\u003e一貫性 (Consistency): システムは常に想定どおり動作できなければならない\u003c/li\u003e\n\u003cli\u003e可用性 (Availability): システムは要求時に利用可能でなければならに\u003c/li\u003e\n\u003cli\u003e分断耐性 (Partition Tolerance): システムは部分的な障害の発生時に運用を続行できなければならない\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"データベース構成\"\u003eデータベース構成\u003c/h3\u003e\n\u003cp\u003e上から順により制御された構造であり、かつ古くからあるものとなっている。\u003c/p\u003e","title":"読書メモ: DMBOK2 第6章 データストレージとオペレーション"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第5章「データモデリングとデザイン」について。\n第4章でデータモデリングについて言及されていたが、ここで詳しく見ていく。\n以降、特に注釈のない引用は DMBOK2 第5章からの引用とする。\nデータモデリングとは データモデリングとは、データ要件を洗い出し、分析し、取扱スコープを決めるプロセスであり、データ要件を記述し伝えるために、明確に定義されたデータモデルと呼ばれる様式が用いられる。このプロセスは反復的であり、概念、論理、物理モデルが含まれる。\n第4章 データアーキテクチャ ではエンタープライズ・データモデルという言葉が登場したが、この第5章ではそのデータモデルがより具体的に解説されている。\nデータモデルは効果的なデータマネジメントを行うにあたり必要なものとされている。\n次のような意義がある。\nデータに関する共通語彙を提供する 組織のデータや情報システムに関しての明示的な知識を捉え文書化する プロジェクトにおいて主なコミュニケーションツールとして使われる アプリケーションをカスタマイズ、統合、リプレースする際の出発点となる データモデルは組織が把握した現状、またはあるべき姿として組織のデータを記述する。\nカテゴリー情報、リソース情報、業務イベント情報、詳細取引情報がその対象となる。\nデータモデルの構成要素 ほとんどのデータモデルはエンティティ、リレーションシップ、属性、ドメインにより構成される。\nエンティティ エンティティとはある組織が情報を収集する対象のこと。\n組織が使う名詞。\n以下は Marmaid の Entity Relationship Diagrams でエンティティを表現した例。\n学校を例にしており、学生、コース、インストラクタがエンティティとして表されている。\n--- title: Entity --- erDiagram Student Course Instructor リレーションシップ リレーションシップはエンティティ間の関連性を表す。\n以下の例は学生はコースを \u0026ldquo;履修する\u0026rdquo;、インストラクタはコースを \u0026ldquo;教える\u0026rdquo; ことを表している。\n--- title: Relationship --- erDiagram Student }|--|{ Course : take Instructor }|--|{ Course : teach 線の両端の形はリレーションシップの cardinality (多重度) を表している。\nこの場合ではそれぞれカラスの足状になっているが、それぞれ「1またはそれ以上」という意味で、「学生は1つ以上のコースを履修する」「コースは1人以上の学生に履修される」ということを表している。\nリレーションシップの arity (項数) は一般的には二項型 (バイナリリレーションシップ) であることが多く、2つのエンティティ間の関係を表す。\n一方で線の両端が同じエンティティにつながっている単項型、3のエンティティが関連する三項型もある。\n以下は単項型の例で、前提となるコースの履修が必要であることを示す。\n--- title: Unary Relationship --- erDiagram Course }|--|| Course : \"prerequisite course is required\" 2つのエンティティ間でリレーションシップがある場合、外部キーが暗黙的に作成されることがある。\n属性 属性はエンティティを識別、記述、評価するプロパティ。\n属性はドメインを持つことができる。(後述)\n以下の例では学生のエンティティに属性を付与した。\n--- title: Attribute --- erDiagram Student { int student_id PK string first_name string last_name date birthday } ここで学生番号 student_id は単体で PK (primary key) に指定されており、インスタンスを一意に識別する1つの属性、つまりシンプルキーであると言える。\nドメイン ドメイン (定義域) は属性が取りうる値一式のこと。\n例えばコース開始日という属性があった場合は、現実に存在する日付として有効な値がドメインに含まれることになる。\nコース開始日は開校日以降とする、のように範囲を限定することもできる。\n余談だが…\nエンティティ、リレーションシップ、属性などのデータモデルの構成要素は概念データモデルのレベルで説明した方がよかったかもしれない。\nしかし Mermaid の ER 図はおそらく物理データモデル用のものとなっており、物理っぽい例になってしまった。\nデータモデリング・スキーム スキームとはデータの表現方法のようなもので、DMBOK2 では以下に示す6種類のスキームが紹介されている。\n各スキームそれぞれに表記法がいくつか存在する。\nスキームごとにどの (物理的な) データベースで使用できるか、またどの詳細レベルで使用できるかが決まっている。\n例えばリレーショナルスキームでは3つの詳細レベルすべてで RDBMS 状にモデルを構築できるが、他のタイプのデータベースでは概念モデル (CDM) と論理モデル (LDM) のみを構築できる。\nスキーム RDBMS MDBMS オブジェクト型 ドキュメント型 カラム型 グラフ型 キーバリュー型 リレーショナル CDM LDM PDM CDM LDM CDM LDM CDM LDM CDM LDM CDM LDM CDM LDM ディメンショナル CDM LDM PDM CDM LDM PDM オブジェクト指向 CDM LDM PDM CDM LDM PDM ファクトベース CDM LDM PDM CDM LDM CDM LDM CDM LDM CDM LDM CDM LDM CDM LDM タイムベース PDM NoSQL PDM PDM PDM PDM PDM DMBOK2 表10 スキームとデータベースの相互参照表\nこのポストではこのうち2つだけ触れておく。\nリレーショナルスキーム 1970年、Edward Codd により提唱された。\n業務データを正確に表現し、冗長性を除去することが設計目標である。\nIE 表記法というのがもっとも一般的とのことで、Mermaid の ER 図も IE 表記法に近い。\nerDiagram Student }|--|{ Course : take ディメンショナルスキーム ディメンショナルモデルでは大量データに対する問い合わせと分析を最適化しようとしている。\n特定の業務プロセスに的を絞った業務上の設問に対応する。\n以下の例は入学申請というプロセスを表したものであり、例えば学生が所属するゾーンごとに分析できる。\nゾーンから地域、国へと分析を拡大できる。\nDMBOK2 図40 ディメンショナルモデルの分析軸表記\nこの分析軸表記法は DMBOK2 では有用とされているがネットで検索してもほとんど出てこない…\nただしデータウェアハウスの設計でよく目にする Kimball のディメンショナルモデリングは正にこれなので重要である。\nファクトテーブルやディメンションテーブルという概念もここで出てくる。\nちなみにフラットで単一なディメンショナル構造を正規化し、階層構造やネットワーク構造のコンポーネントを作ることをスノーフレーク化というとのこと。\nあの Snowflake の製品名はここから来ているのだろうか？\n詳細レベル データモデリングには概念・論理・物理の3つの詳細レベルがあり、新しいアプリケーションを開発するときなどはこの順にモデリングを進めることになる。\n概念モデル 概念データモデルには、関連する概念の集合体としてデータ要件の概要が取り込まれる。ここには、特定の領域や業務機能に関する基本的で重要なビジネスエンティティのみが含まれ、各エンティティの説明とエンティティ間のリレーションシップが含まれる。\nここは実装によらずあくまで業務を説明するものであり、したがって table 名や column 名ではなく業務の言葉で表現されることになる。\n論理モデル 論理データモデルは、詳細なデータ要件が表現されたものであり、通常、アプリケーション要件のような特定の使用シナリオに適応する。論理データモデルも技術や具体的な実装上の制約から独立している。論理データモデルは概念データモデルの拡張として始まることが多い。\nリレーショナルスキームの場合、概念モデルに属性を追加して拡張し、正規化を適用する。\n物理モデル 物理データモデル (PDM) は詳細な技術的ソリューションを表す。このモデルは論理データモデルを出発点として作成されることが多く、そこからハードウェア、ソフトウェア、ネットワークツールを組み合わせた環境に適応するように設計される。\nこのモデルは実装方法に依存しており、具体的な table 名や column 名で記述される。\nパフォーマンスのために非正規化されることがある。\nビューにするかやパーティションなどについても考慮する。\nデータモデリングの導入・推進 ビジネスアナリスト、データモデラーによって以下のように進められる。\nデータモデリング計画 データモデルの構築 データモデルのレビュー データモデルの維持と更新管理 データモデルの構築において、新しいアプリケーションを開発する際に要件定義から始めるプロセスをフォワードエンジニアリングと呼ぶ。\nフォワードエンジニアリングでは概念 -\u0026gt; 論理 -\u0026gt; 物理の順にデータモデリングを行い、具体的にしていく。\nその逆順で行われるのがリバースエンジニアリングであり、既存のデータベースを文書化するプロセスとなる。\nデータモデルのレビューにおいては Data Model Scorecard (Hoberman, 2015) というものが使われる。\nまとめ 曖昧な理解だったデータモデリングについて把握できた。\n例えば dbt はデータモデリングツールだと言われることがあるが、あればリレーショナル物理モデリングの一部を行っている、という言い方が正しそうだ。\nモダンなデータ基盤では概念・論理モデルはどのやって構築するのがベストプラクティスなのだろうか？\nそれともあまり概念・論理のモデリングは行われていないのだろうか？\n詳細レベルの話は結構大事だと思っていて、3つのレベルを分けずにまとめてやることが多かったのであまり良くなかったと反省した。\nまとめてやると物理を殴れるエンジニアがやるしかないが、レベルを分ければ概念モデルなどはデータアナリスト、営業などが参加しやすくなる。\n今の仕事のデータ追加のフローを改善できそうな気がしてきた。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-5/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.dama-japan.org/Introduction.html\"\u003eDMBOK2\u003c/a\u003e を読み進めていくシリーズ。\u003cbr\u003e\n今回は第5章「データモデリングとデザイン」について。\u003cbr\u003e\n第4章でデータモデリングについて言及されていたが、ここで詳しく見ていく。\u003c/p\u003e\n\u003cp\u003e以降、特に注釈のない引用は DMBOK2 第5章からの引用とする。\u003c/p\u003e\n\u003ch2 id=\"データモデリングとは\"\u003eデータモデリングとは\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eデータモデリングとは、データ要件を洗い出し、分析し、取扱スコープを決めるプロセスであり、データ要件を記述し伝えるために、明確に定義されたデータモデルと呼ばれる様式が用いられる。このプロセスは反復的であり、概念、論理、物理モデルが含まれる。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"../dmbok-chapter-4/\"\u003e第4章 データアーキテクチャ\u003c/a\u003e ではエンタープライズ・データモデルという言葉が登場したが、この第5章ではそのデータモデルがより具体的に解説されている。\u003cbr\u003e\nデータモデルは効果的なデータマネジメントを行うにあたり必要なものとされている。\u003cbr\u003e\n次のような意義がある。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cul\u003e\n\u003cli\u003eデータに関する共通語彙を提供する\u003c/li\u003e\n\u003cli\u003e組織のデータや情報システムに関しての明示的な知識を捉え文書化する\u003c/li\u003e\n\u003cli\u003eプロジェクトにおいて主なコミュニケーションツールとして使われる\u003c/li\u003e\n\u003cli\u003eアプリケーションをカスタマイズ、統合、リプレースする際の出発点となる\u003c/li\u003e\n\u003c/ul\u003e\u003c/blockquote\u003e\n\u003cp\u003eデータモデルは組織が把握した現状、またはあるべき姿として組織のデータを記述する。\u003cbr\u003e\nカテゴリー情報、リソース情報、業務イベント情報、詳細取引情報がその対象となる。\u003c/p\u003e\n\u003ch2 id=\"データモデルの構成要素\"\u003eデータモデルの構成要素\u003c/h2\u003e\n\u003cp\u003eほとんどのデータモデルはエンティティ、リレーションシップ、属性、ドメインにより構成される。\u003c/p\u003e\n\u003ch3 id=\"エンティティ\"\u003eエンティティ\u003c/h3\u003e\n\u003cp\u003eエンティティとはある組織が情報を収集する対象のこと。\u003cbr\u003e\n組織が使う名詞。\u003c/p\u003e\n\u003cp\u003e以下は Marmaid の \u003ca href=\"https://mermaid.js.org/syntax/entityRelationshipDiagram.html\"\u003eEntity Relationship Diagrams\u003c/a\u003e でエンティティを表現した例。\u003cbr\u003e\n学校を例にしており、学生、コース、インストラクタがエンティティとして表されている。\u003c/p\u003e\n\u003cpre class=\"mermaid\"\u003e---\ntitle: Entity\n---\nerDiagram\n    Student\n    Course\n    Instructor\n\u003c/pre\u003e\n\n\u003ch3 id=\"リレーションシップ\"\u003eリレーションシップ\u003c/h3\u003e\n\u003cp\u003eリレーションシップはエンティティ間の関連性を表す。\u003cbr\u003e\n以下の例は学生はコースを \u0026ldquo;履修する\u0026rdquo;、インストラクタはコースを \u0026ldquo;教える\u0026rdquo; ことを表している。\u003c/p\u003e\n\u003cpre class=\"mermaid\"\u003e---\ntitle: Relationship\n---\nerDiagram\n    Student }|--|{ Course : take\n    Instructor }|--|{ Course : teach\n\u003c/pre\u003e\n\n\u003cp\u003e線の両端の形はリレーションシップの cardinality (多重度) を表している。\u003cbr\u003e\nこの場合ではそれぞれカラスの足状になっているが、それぞれ「1またはそれ以上」という意味で、「学生は1つ以上のコースを履修する」「コースは1人以上の学生に履修される」ということを表している。\u003c/p\u003e\n\u003cp\u003eリレーションシップの arity (項数) は一般的には二項型 (バイナリリレーションシップ) であることが多く、2つのエンティティ間の関係を表す。\u003cbr\u003e\n一方で線の両端が同じエンティティにつながっている単項型、3のエンティティが関連する三項型もある。\u003cbr\u003e\n以下は単項型の例で、前提となるコースの履修が必要であることを示す。\u003c/p\u003e","title":"読書メモ: DMBOK2 第5章 データモデリングとデザイン"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第4章「データアーキテクチャ」について。\nやや抽象度が高い内容となっており、理解が難しいと感じた。\n以降、特に注釈のない引用は DMBOK2 第4章からの引用とする。\nデータアーキテクチャとは データアーキテクチャの定義は以下のとおり。\n企業の (組織構造に関係なく) データニーズを明確にし、ニーズに合うマスターとなる青写真を設計し、維持する。マスターとなる青写真を使ってデータ統合を手引し、データ資産をコントロールし、ビジネス戦略に合わせてデータへの投資を行う。\n業務戦略と技術実装の間を橋渡しすることがデータアーキテクチャの目的。\n以下が成果物、つまり前述の定義の「マスターとなる青写真」となる。\nデータの保存と処理の要件 企業の現在のデータ要件と長期のデータ要件を満たす構造や計画の立案 これらを記述するためにエンタープライズ・データモデル (EDM) とデータフロー設計が必要となる。(後述)\nエンタープライズアーキテクチャ データアーキテクチャとビジネス、アプリケーション、テクニカルなど他の領域のアーキテクチャとの違いは下表のようになる。\nそれぞれが影響を及ぼし合う関係となっており、データアーキテクチャはビジネスアーキテクチャによって決められる。\nドメイン エンタープライズ・ビジネスアーキテクチャ エンタープライズ・データアーキテクチャ エンタープライズ・アプリケーションアーキテクチャ エンタープライズ・テクニカルアーキテクチャ 目的 企業が顧客や他のステークホルダに どのように価値提供しているかを 明らかにする データがどのように整理・管理されるべきか記述する 企業内アプリケーションの 構造と機能を記述する システムを稼働して価値を 提供するために必要な 物理実装技術を記述する 要素 ビジネスモデル、業務プロセス、業務能力、サービス、イベント、戦略、語彙集 データモデル、データ定義、データマッピング仕様、データフロー、構造化データの API ビジネスシステム、ソフトウェアパッケージ、データベース テクニカルプラットフォーム、ネットワーク、セキュリティ、統合ツール 依存関係 他のドメインに対する要件を設定する ビジネスアーキテクチャによって作られ、要求されるデータを管理する ビジネス要件に基づいて特定されたデータに対応する アプリケーションアーキテクチャを提供し実行する 役割 ビジネスアーキテクトとアナリスト、ビジネス・データスチュワード データアーキテクトとモデラー、データスチュワード アプリケーションアーキテクト インフラストラクチャアーキテクト DMBOK2 表6 アーキテクチャ領域\nweb 系エンジニア的には「アーキテクチャ」という言葉だけ聞くとクラウドサービスのアイコンを使ってアーキテクチャ図をお絵描きするよくあるアレを想像しがちだが、データアーキテクチャはそれではないということ。\n(DMBOK2 の分類ではそれはテクニカルアーキテクチャの領域)\nエンタープライズ・データモデル (EDM) EDM とは\n全体的でエンタープライズレベルの実装に依存しない概念または論理モデルであり、企業全体にわたるデータに関して一貫した共通のビューを提供する。\n次の図は概念・論理モデルと物理的なデータモデルとの関係を示す。\n垂直方向には対象領域ごとに対応する異なるレベルのモデルが関係している。\n水平方向には同じレベルのモデルがあり、対象領域をまたいでエンティティが関係し合う。\nDMBOK2 図23 エンタープライズ・データモデル\nこれらはトップダウンまたはボトムアップのアプローチで構築される。\nレベル別に段階的・反復的に構築するのがセオリーとのこと。\nいきなりすべてを記述しようとするのは良くない。\nデータモデリングについて詳しくは第5章で見ていくことになる。\nデータフロー設計 データフロー設計はデータベース、アプリケーション、その他にまたがるデータの処理と格納の要件。\nデータリネージのドキュメントであり、データがビジネスプロセスやシステムをどのように移動するかを示したデータフロー図により記述される。\n以下はデータフロー図の例。\nDMBOK2 図26 データフロー図例\nデータと以下の関係をマッピングする。\nアプリケーション データベース ネットワークセグメント ビジネス上の役割 地域的祭が生じている拠点 データアーキテクチャの導入・推進 データアーキテクチャの導入には導入失敗のリスクアセスメント、および (他の多くの DMBOK 知識領域と同様) 組織と文化の変革が必要になる。\n次のようにしてデータアーキテクチャの慣行を確立する。\n既存のデータアーキテクチャ設計書の評価 ロードマップの開発 各プロジェクトにおける全社要件の管理 開発プロジェクトはデータアーキテクチャによって確立された業務要件と標準に基づいて、データを取得・格納・配布するためのソリューションを実装するもの データアーキテクチャは各プロジェクトのスコープ境界を決めるのに影響する。\nプロジェクトのデータ要件定義 プロジェクトのデータ設計レビュー データリネージの影響判定 データアプリケーションの制御 データアーキテクチャ標準の徹底 データテクノロジーと更新の意思決定ガイド プロジェクトは一般的にプロジェクト固有のアーキテクチャの優先事項を重視するが、全社的なデータアーキテクチャには積極的に取り組むべきとされる。\nまとめ データアーキテクチャの概念は抽象度が高く、データ基盤を単に IT の一貫として捉えている組織では理解されにくいと感じた。\n私自身、本章を読むまではデータ基盤まわりのシステムアーキテクチャのことだと思っていたが、異なるレベルの話だった。\n逆に言うとデータマネジメント系の勉強会で発表するようなデータマネジメントが推進されている会社ではデータアーキテクチャについての考えが明確になっているような気がしている。\n複数社で働いてきた自分の経験からだが、マネジメント層でも抽象度が高い仕事ができる人はそれほど多くない。\nそのへんのケイパビリティが組織のデータマネジメントの成否に強く影響してくるのだろう。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-4/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.dama-japan.org/Introduction.html\"\u003eDMBOK2\u003c/a\u003e を読み進めていくシリーズ。\u003cbr\u003e\n今回は第4章「データアーキテクチャ」について。\u003cbr\u003e\nやや抽象度が高い内容となっており、理解が難しいと感じた。\u003c/p\u003e\n\u003cp\u003e以降、特に注釈のない引用は DMBOK2 第4章からの引用とする。\u003c/p\u003e\n\u003ch2 id=\"データアーキテクチャとは\"\u003eデータアーキテクチャとは\u003c/h2\u003e\n\u003cp\u003eデータアーキテクチャの定義は以下のとおり。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e企業の (組織構造に関係なく) データニーズを明確にし、ニーズに合うマスターとなる青写真を設計し、維持する。マスターとなる青写真を使ってデータ統合を手引し、データ資産をコントロールし、ビジネス戦略に合わせてデータへの投資を行う。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e業務戦略と技術実装の間を橋渡しすることがデータアーキテクチャの目的。\u003cbr\u003e\n以下が成果物、つまり前述の定義の「マスターとなる青写真」となる。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cul\u003e\n\u003cli\u003eデータの保存と処理の要件\u003c/li\u003e\n\u003cli\u003e企業の現在のデータ要件と長期のデータ要件を満たす構造や計画の立案\u003c/li\u003e\n\u003c/ul\u003e\u003c/blockquote\u003e\n\u003cp\u003eこれらを記述するためにエンタープライズ・データモデル (EDM) とデータフロー設計が必要となる。(後述)\u003c/p\u003e\n\u003ch2 id=\"エンタープライズアーキテクチャ\"\u003eエンタープライズアーキテクチャ\u003c/h2\u003e\n\u003cp\u003eデータアーキテクチャとビジネス、アプリケーション、テクニカルなど他の領域のアーキテクチャとの違いは下表のようになる。\u003cbr\u003e\nそれぞれが影響を及ぼし合う関係となっており、データアーキテクチャはビジネスアーキテクチャによって決められる。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003cstrong\u003eドメイン\u003c/strong\u003e\u003c/th\u003e\n          \u003cth\u003e\u003cstrong\u003eエンタープライズ・ビジネスアーキテクチャ\u003c/strong\u003e\u003c/th\u003e\n          \u003cth\u003e\u003cstrong\u003eエンタープライズ・データアーキテクチャ\u003c/strong\u003e\u003c/th\u003e\n          \u003cth\u003e\u003cstrong\u003eエンタープライズ・アプリケーションアーキテクチャ\u003c/strong\u003e\u003c/th\u003e\n          \u003cth\u003e\u003cstrong\u003eエンタープライズ・テクニカルアーキテクチャ\u003c/strong\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e目的\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e企業が顧客や他のステークホルダに \u003c!-- raw HTML omitted --\u003e どのように価値提供しているかを \u003c!-- raw HTML omitted --\u003e 明らかにする\u003c/td\u003e\n          \u003ctd\u003eデータがどのように整理・管理されるべきか記述する\u003c/td\u003e\n          \u003ctd\u003e企業内アプリケーションの \u003c!-- raw HTML omitted --\u003e 構造と機能を記述する\u003c/td\u003e\n          \u003ctd\u003eシステムを稼働して価値を \u003c!-- raw HTML omitted --\u003e 提供するために必要な \u003c!-- raw HTML omitted --\u003e 物理実装技術を記述する\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e要素\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eビジネスモデル、業務プロセス、業務能力、サービス、イベント、戦略、語彙集\u003c/td\u003e\n          \u003ctd\u003eデータモデル、データ定義、データマッピング仕様、データフロー、構造化データの API\u003c/td\u003e\n          \u003ctd\u003eビジネスシステム、ソフトウェアパッケージ、データベース\u003c/td\u003e\n          \u003ctd\u003eテクニカルプラットフォーム、ネットワーク、セキュリティ、統合ツール\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e依存関係\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e他のドメインに対する要件を設定する\u003c/td\u003e\n          \u003ctd\u003eビジネスアーキテクチャによって作られ、要求されるデータを管理する\u003c/td\u003e\n          \u003ctd\u003eビジネス要件に基づいて特定されたデータに対応する\u003c/td\u003e\n          \u003ctd\u003eアプリケーションアーキテクチャを提供し実行する\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e役割\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eビジネスアーキテクトとアナリスト、ビジネス・データスチュワード\u003c/td\u003e\n          \u003ctd\u003eデータアーキテクトとモデラー、データスチュワード\u003c/td\u003e\n          \u003ctd\u003eアプリケーションアーキテクト\u003c/td\u003e\n          \u003ctd\u003eインフラストラクチャアーキテクト\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cem\u003eDMBOK2 表6 アーキテクチャ領域\u003c/em\u003e\u003c/p\u003e","title":"読書メモ: DMBOK2 第4章 データアーキテクチャ"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第15章「データマネジメント成熟度アセスメント」について。\nデータマネジメントの導入において重要な役割を果たしそうなので早めに確認しておきたかった。\n以降、特に注釈のない引用は DMBOK2 第15章からの引用とする。\nデータマネジメント成熟度アセスメントとは データマネジメント成熟度アセスメント (DMMA: Data Management Maturity Assessment) はその名の通り、組織のデータマネジメントのレベルの評価に基づくプロセス改善の取り組みのこと。\n能力成熟度アセスメント (CMA: Capability Maturity Assessment) というものがあり、それのデータマネジメント版が DMMA。\nCMA では\n成熟度モデルは進化の観点から定義され、それにはプロセスの特性を表すレベルが使用される。組織がプロセスの特性を理解すると、組織は成熟度を測り、その能力を向上させるための計画を立てることができる。(中略) 新しいレベルに上がる度にプロセスの実行はより一貫性を増し、予測可能な状態となり、信頼性が高くなる。\nレベルは通常0~5の6段階で表される。\nDMMA は\n全体的なデータマネジメントを評価するために使用したり、単一の知識領域や、単一のプロセスに焦点を当てて使用したりできる。どのような点に焦点を当てたとしても、DMMA はデータマネジメント業務の健全性と有効性について、業務と IT の視点のギャップを埋めるために役立つ。\n組織が DMMA を実施する理由は、規制への対応、データガバナンス、プロセス改善、etc.\nDMMA の第一のゴールはデータマネジメント活動の現状を評価することであり、それにより改善計画を立てることができるようになる。\nアセスメントレベル 以下はアセスメントレベルの概要。\nデータマネジメントの各知識領域 (ex. データガバナンス、メタデータ管理、etc.) ごとにレベルが評価される。\nlevel 0: 能力が欠如した状態 データマネジメントの取り組みがない level 1: 初期／場当たり的な状態 限られたツールセットを用いた一般的なデータマネジメント ガバナンスは低レベル データ処理は一部の専門家に依存し、役割や責任は部門別に定義されている level 2: 反復可能な状態 組織は一元化された共通ツールを使い始める 役割は明確化されており、一部の専門家のみに依存しない level 3: 定義された状態 拡張可能なプロセスの導入と制度化 組織全体である程度統制されたデータの複製 データ品質全体の総体的な向上 組織的なポリシー定義と統制 level 4: 管理された状態 新しいプロジェクトやタスクから得られる結果が予測され、リスクの管理が始まる データマネジメントに成果に対する評価尺度が含まれる データマネジメント用の標準ツールが集中管理計画とガバナンス機能に組み合わされている level 5: 最適化された状態 活動の成果は十分予測可能に 組織は継続的な改善に重点を置く 十分理解された評価尺度を使ってデータ品質とプロセスが管理・測定される 次のように各知識領域ごとに可視化することができる。\n現状ランクと求められるランクの乖離が大きいところが組織にとってのリスクとなる。\nDMBOK2 図105 データマネジメント成熟度アセスメントを視覚化した例\n既存のフレームワーク DMMA のフレームワークとして5つのフレームワークが挙げられている。\n多くのベンダーが独自のモデルを開発しているため、ベンダーを選ぶ前、または独自のフレームワークを開発する前に複数のフレームワークを比較検討する必要がある。\nCMMI データマネジメント成熟度モデル (DMM) EDM 協議会 DCAM IBM データガバナンス評議会成熟度モデル スタンフォード・データガバナンス成熟度モデル ガートナーのエンタープライズ・インフォメーションマネジメント成熟度モデル ※DMBOK2 の出版が2018年で情報が古くなっているものもあり、リンク先が公式ではないものも含む。\nデータマネジメント成熟度アセスメントの実施 次のサイクルで実施する。\nアセスメントアクティビティの計画 経営幹部や業務部門が理解できるようアセスメントの目的を形式化する 組織全体をスコープとするのは難しいこともあるため、最初のアセスメントでは単一の業務領域などスコープをしぼる 情報収集のアプローチ (ワークショップ、インタビュー、アンケート、成果物レビューなど) を定義する 成熟度アセスメントを実行する 情報収集の実施 評価付を行い、合意された見解に達する 評価結果を吟味する 目標とする次の状態にいたるステップを明確化 アセスメント結果を報告 改善を達成するためのターゲットプログラムを作成する 特定のデータマネジメント機能を改善するためのアクションを特定 そのスケジュール、および実施された場合に期待される DMMA の改善を示す 成熟度の再評価 定期的に再評価を行うべき 定期的に改善が測定できると組織全体のコミットメントと熱意が維持される まとめ DMMA はデータマネジメントの導入時に政治的な意味でも重要になりそうだと思った。\n「データマネジメント？なにそれおいしいの？」というレベルの経営層に現実を見せて動機づけにできそう。\nまあデータマネジメントにちゃんと取り組むならどっちにしろ最初にやるべき。\n関係ないけどかなーり昔に働いていた職場で上司が「CMMI やることになった」とバタバタしていたのを思い出した。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-15/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.dama-japan.org/Introduction.html\"\u003eDMBOK2\u003c/a\u003e を読み進めていくシリーズ。\u003cbr\u003e\n今回は第15章「データマネジメント成熟度アセスメント」について。\u003cbr\u003e\nデータマネジメントの導入において重要な役割を果たしそうなので早めに確認しておきたかった。\u003c/p\u003e\n\u003cp\u003e以降、特に注釈のない引用は DMBOK2 第15章からの引用とする。\u003c/p\u003e\n\u003ch2 id=\"データマネジメント成熟度アセスメントとは\"\u003eデータマネジメント成熟度アセスメントとは\u003c/h2\u003e\n\u003cp\u003eデータマネジメント成熟度アセスメント (DMMA: Data Management Maturity Assessment) はその名の通り、組織のデータマネジメントのレベルの評価に基づくプロセス改善の取り組みのこと。\u003cbr\u003e\n能力成熟度アセスメント (CMA: Capability Maturity Assessment) というものがあり、それのデータマネジメント版が DMMA。\u003c/p\u003e\n\u003cp\u003eCMA では\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e成熟度モデルは進化の観点から定義され、それにはプロセスの特性を表すレベルが使用される。組織がプロセスの特性を理解すると、組織は成熟度を測り、その能力を向上させるための計画を立てることができる。(中略) 新しいレベルに上がる度にプロセスの実行はより一貫性を増し、予測可能な状態となり、信頼性が高くなる。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eレベルは通常0~5の6段階で表される。\u003cbr\u003e\nDMMA は\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e全体的なデータマネジメントを評価するために使用したり、単一の知識領域や、単一のプロセスに焦点を当てて使用したりできる。どのような点に焦点を当てたとしても、DMMA はデータマネジメント業務の健全性と有効性について、業務と IT の視点のギャップを埋めるために役立つ。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e組織が DMMA を実施する理由は、規制への対応、データガバナンス、プロセス改善、etc.\u003cbr\u003e\nDMMA の第一のゴールはデータマネジメント活動の現状を評価することであり、それにより改善計画を立てることができるようになる。\u003c/p\u003e\n\u003ch2 id=\"アセスメントレベル\"\u003eアセスメントレベル\u003c/h2\u003e\n\u003cp\u003e以下はアセスメントレベルの概要。\u003cbr\u003e\nデータマネジメントの各知識領域 (ex. データガバナンス、メタデータ管理、etc.) ごとにレベルが評価される。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elevel 0: \u003cstrong\u003e能力が欠如した状態\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eデータマネジメントの取り組みがない\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003elevel 1: \u003cstrong\u003e初期／場当たり的な状態\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e限られたツールセットを用いた一般的なデータマネジメント\u003c/li\u003e\n\u003cli\u003eガバナンスは低レベル\u003c/li\u003e\n\u003cli\u003eデータ処理は一部の専門家に依存し、役割や責任は部門別に定義されている\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003elevel 2: \u003cstrong\u003e反復可能な状態\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e組織は一元化された共通ツールを使い始める\u003c/li\u003e\n\u003cli\u003e役割は明確化されており、一部の専門家のみに依存しない\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003elevel 3: \u003cstrong\u003e定義された状態\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e拡張可能なプロセスの導入と制度化\u003c/li\u003e\n\u003cli\u003e組織全体である程度統制されたデータの複製\u003c/li\u003e\n\u003cli\u003eデータ品質全体の総体的な向上\u003c/li\u003e\n\u003cli\u003e組織的なポリシー定義と統制\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003elevel 4: \u003cstrong\u003e管理された状態\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e新しいプロジェクトやタスクから得られる結果が予測され、リスクの管理が始まる\u003c/li\u003e\n\u003cli\u003eデータマネジメントに成果に対する評価尺度が含まれる\u003c/li\u003e\n\u003cli\u003eデータマネジメント用の標準ツールが集中管理計画とガバナンス機能に組み合わされている\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003elevel 5: \u003cstrong\u003e最適化された状態\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e活動の成果は十分予測可能に\u003c/li\u003e\n\u003cli\u003e組織は継続的な改善に重点を置く\u003c/li\u003e\n\u003cli\u003e十分理解された評価尺度を使ってデータ品質とプロセスが管理・測定される\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e次のように各知識領域ごとに可視化することができる。\u003cbr\u003e\n現状ランクと求められるランクの乖離が大きいところが組織にとってのリスクとなる。\u003c/p\u003e","title":"読書メモ: DMBOK2 第15章 データマネジメント成熟度アセスメント"},{"content":"このポストについて 前回の記事 dlt 入門 - ELT の Extract と Load を担う data load tool では dlt の概要を説明した。\nこの記事ではそれを踏まえ、dlt を使って CSV ファイルを BigQuery に load するという一連の開発作業をやってみる。\n現実の CSV はそのまま使えなかったりするので、BigQuery に入れるまでに泥臭い前処理のような作業があることが多い。\nそのへんもまとめて dlt でやってみるとこんな感じ、というのが示せるとよい。\nやりたいこと 個人で管理しているお金の情報を個人用の BigQuery に置きたい、という要件を想定。\nデータ概要 具体的には MoneyForward のデータを load していく。\n個人では API を利用できないので、web UI から export できる CSV のデータで収入・支出詳細と資産推移月次を対象とする。\nCSV の export 方法は以下を参照。\n入出金履歴はダウンロードできますか – マネーフォワード MEサポートサイト データの内容は次のようになっている。\n収入・支出詳細_2023-11-01_2023-11-30.csv \u0026#34;計算対象\u0026#34;,\u0026#34;日付\u0026#34;,\u0026#34;内容\u0026#34;,\u0026#34;金額（円）\u0026#34;,\u0026#34;保有金融機関\u0026#34;,\u0026#34;大項目\u0026#34;,\u0026#34;中項目\u0026#34;,\u0026#34;メモ\u0026#34;,\u0026#34;振替\u0026#34;,\u0026#34;ID\u0026#34; \u0026#34;1\u0026#34;,\u0026#34;2023/11/30\u0026#34;,\u0026#34;AMAZON.CO.JP\u0026#34;,\u0026#34;-2830\u0026#34;,\u0026#34;楽天カード\u0026#34;,\u0026#34;食費\u0026#34;,\u0026#34;食料品\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;EPv92ZjQcOxgWQx_cLbhD1\u0026#34; \u0026#34;1\u0026#34;,\u0026#34;2023/11/24\u0026#34;,\u0026#34;東京ガス\u0026#34;,\u0026#34;-4321\u0026#34;,\u0026#34;楽天カード\u0026#34;,\u0026#34;水道・光熱費\u0026#34;,\u0026#34;ガス・灯油代\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;r6wuQPfrIRS6aFpNYZE5Eh\u0026#34; \u0026#34;1\u0026#34;,\u0026#34;2023/11/24\u0026#34;,\u0026#34;給与 カ) フロッグログ\u0026#34;,\u0026#34;700000\u0026#34;,\u0026#34;みずほ銀行\u0026#34;,\u0026#34;収入\u0026#34;,\u0026#34;給与\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;doettKpYyNp0Tml9KQQXm1\u0026#34; ヘッダーがあり、各列に名前が付いている。\nencoding が CP932 であることに注意。\nID の列があるので、行の識別に使えそう。\n資産推移月次.csv \u0026#34;日付\u0026#34;,\u0026#34;合計（円）\u0026#34;,\u0026#34;預金・現金・暗号資産（円）\u0026#34;,\u0026#34;株式(現物)（円）\u0026#34;,\u0026#34;投資信託（円）\u0026#34;,\u0026#34;債券（円）\u0026#34;,\u0026#34;年金（円）\u0026#34;,\u0026#34;ポイント（円）\u0026#34; \u0026#34;2023/12/17\u0026#34;,\u0026#34;11505000\u0026#34;,\u0026#34;5000000\u0026#34;,\u0026#34;300000\u0026#34;,\u0026#34;5000000\u0026#34;,\u0026#34;200000\u0026#34;,\u0026#34;1000000\u0026#34;,\u0026#34;5000\u0026#34; \u0026#34;2023/12/16\u0026#34;,\u0026#34;11505000\u0026#34;,\u0026#34;5000000\u0026#34;,\u0026#34;300000\u0026#34;,\u0026#34;5000000\u0026#34;,\u0026#34;200000\u0026#34;,\u0026#34;1000000\u0026#34;,\u0026#34;5000\u0026#34; \u0026#34;2023/12/15\u0026#34;,\u0026#34;11505000\u0026#34;,\u0026#34;5000000\u0026#34;,\u0026#34;300000\u0026#34;,\u0026#34;5000000\u0026#34;,\u0026#34;200000\u0026#34;,\u0026#34;1000000\u0026#34;,\u0026#34;5000\u0026#34; encoding などについては同じ。\n当月についてはすべての 日付 がある一方、それ以前については 2023/11/30, 2023/10/31, \u0026hellip; のように月末だけが含まれている。\n日付 で行の識別ができる。\nこれらはまず最初に GCS bucket に手動で置くものとする。\n開発手順 CSV そのままだと schema まわりで問題が起こりそうなので、そのあたり try \u0026amp; error で解決したい。\nというのをいきなり BigQuery 上でやると手間もかかるし汚くなるので、最初は試験的にローカル環境の DuckDB に load するようにする。\nDuckDB でうまくいったら BigQuery へと移行する。\n全体の流れとしては次のようになる。\ndlt のインストール DuckDB 用の pipeline project を作成 pipeline の実装 configuration pipeline の実行 Streamlit app による結果の確認 schema の調整 不要レコードのフィルタリング incremental loading への対応 BigQuery への移行 BigQuery への load の確認 開発作業 1. dlt のインストール destination としては DuckDB, BigQuery を使うので、以下のようにして一緒にインストールする。\n(Poetry の例)\npoetry add \u0026#39;dlt[duckdb,bigquery]\u0026#39; インストールされたバージョンを確認。\n$ dlt --version dlt 0.3.25 また、gsfs, pandas, streamlit, google-cloud-bigquery-storage も必要になるのでインストールしておく。\n2. pipeline project を作成 次のコマンドで pipeline project を用意する。\n$ dlt init filesystem duckdb これは verified source として Filesystem、destination として DuckDB を指定して pipeline project を作るという意味。\nFilesystem はローカルのファイルシステムや S3, GCS のようなクラウドストレージからファイルを読むことが可能。\nこのコマンドが成功すると次のようなディレクトリ構造が作られる。\n. ├── .dlt │ ├── .sources │ ├── config.toml │ └── secrets.toml ├── .gitignore ├── filesystem │ ├── README.md │ ├── __init__.py │ ├── helpers.py │ ├── readers.py │ └── settings.py └── filesystem_pipeline.py filesystem/ 以下には Filesystem を使うための関数定義や README などが生成されている。\nfilesystem_pipeline.py には pipeline の実装例がある。\nこれを元に修正していってもいいが、今回は新しく money_forward_pipeline.py を用意して実装していく。\n3. pipeline の実装 money_forward_pipeline.py は次のように実装した。\nimport dlt from filesystem import filesystem, read_csv, readers class MFFileSpec: def __init__(self, file_name: str, table_name: str): self.file_name = file_name self.table_name = table_name def run_pipeline() -\u0026gt; None: pipeline = dlt.pipeline( pipeline_name=\u0026#34;money_forward\u0026#34;, destination=\u0026#34;duckdb\u0026#34;, dataset_name=\u0026#34;raw_money_forward\u0026#34;, full_refresh=True, ) load_info = pipeline.run(money_forward()) print(load_info) @dlt.source def money_forward() -\u0026gt; dlt.extract.source.DltSource: file_specs = [ MFFileSpec(\u0026#34;収入・支出詳細_*.csv\u0026#34;, \u0026#34;income_expence_details\u0026#34;), MFFileSpec(\u0026#34;資産推移月次.csv\u0026#34;, \u0026#34;monthly_assets\u0026#34;), ] for file_spec in file_specs: records = filesystem( file_glob=f\u0026#34;money_forward/{file_spec.file_name}\u0026#34;, ) | read_csv(encoding=\u0026#34;cp932\u0026#34;) yield records.with_name(file_spec.table_name) if __name__ == \u0026#34;__main__\u0026#34;: run_pipeline() pipeline を作り、run() するときに source として money_forward() を与えている。\n試験的に何度か table を作り直すので full_refresh=True を指定している。\nちなみに source は resource をグルーピングしたものであり、resource ごとに table が作られると思っていい。\nmoney_forward() の中では収入・支出詳細と資産推移月次のそれぞれに対して resource が作られる。\nfilesystem() でファイルをリストアップし、それを read_csv() に渡してデータを読んでいる。\nここで GCS の bucket や認証情報が指定されていないことに気づくかもしれない。\n関数 filesystem() の定義は次のようになっている。\n@dlt.resource( primary_key=\u0026#34;file_url\u0026#34;, spec=FilesystemConfigurationResource, standalone=True ) def filesystem( bucket_url: str = dlt.secrets.value, credentials: Union[FileSystemCredentials, AbstractFileSystem] = dlt.secrets.value, file_glob: Optional[str] = \u0026#34;*\u0026#34;, files_per_page: int = DEFAULT_CHUNK_SIZE, extract_content: bool = False, ) -\u0026gt; Iterator[List[FileItem]]: ... bucket_url, credentials のデフォルト値は dlt.sercrets.value になっている。\nこれは設定ファイルや環境変数などから取得された設定値を使うことを意味する。\n4. configuration 認証情報などの設定は dlt の設定ファイルや環境変数で与えることができる。\nここでは設定ファイルを使うことにする。\nbucket や認証情報を次のように記載した。\n.dlt/config.toml [sources.money_forward_pipeline] bucket_url = \u0026#34;gs://\u0026lt;BUCKET_NAME\u0026gt;\u0026#34; .dlt/secrets.toml [sources.credentials] client_email = \u0026#34;\u0026lt;CLIENT_EMAIL\u0026gt;\u0026#34; private_key = \u0026#34;\u0026lt;PRIVATE_KEY\u0026gt;\u0026#34; project_id = \u0026#34;\u0026lt;PROJECT_ID\u0026gt;\u0026#34; .dlt/secrets.toml の情報から GcpServiceAccountCredentials の object が自動で生成され、それが filesystem() に渡されるようになる。\n認証情報の各値を得るには GCP で service account を作り、key を用意してやる必要がある。\nservice account には必要な権限を付与しておく。\n当然だが .dlt/secrets.toml は GitHub 等に push してはいけない。\nちなみに設定周りの便利な機能として、足りない設定があったときに設定できる場所の一覧を教えてくれるというものがある。\n例えば bucket_url を書き忘れて pipeline を実行した場合、次のようなエラーメッセージが出力される。\nFollowing fields are missing: [\u0026#39;bucket_url\u0026#39;] in configuration with spec FilesystemConfigurationResource for field \u0026#34;bucket_url\u0026#34; config providers and keys were tried in following order: In Environment Variables key MONEY_FORWARD__SOURCES__MONEY_FORWARD_PIPELINE__FILESYSTEM__BUCKET_URL was not found. In Environment Variables key MONEY_FORWARD__SOURCES__MONEY_FORWARD_PIPELINE__BUCKET_URL was not found. In Environment Variables key MONEY_FORWARD__SOURCES__BUCKET_URL was not found. In Environment Variables key MONEY_FORWARD__BUCKET_URL was not found. In secrets.toml key money_forward.sources.money_forward_pipeline.filesystem.bucket_url was not found. In secrets.toml key money_forward.sources.money_forward_pipeline.bucket_url was not found. In secrets.toml key money_forward.sources.bucket_url was not found. In secrets.toml key money_forward.bucket_url was not found. In config.toml key money_forward.sources.money_forward_pipeline.filesystem.bucket_url was not found. In config.toml key money_forward.sources.money_forward_pipeline.bucket_url was not found. In config.toml key money_forward.sources.bucket_url was not found. In config.toml key money_forward.bucket_url was not found. In Environment Variables key SOURCES__MONEY_FORWARD_PIPELINE__FILESYSTEM__BUCKET_URL was not found. In Environment Variables key SOURCES__MONEY_FORWARD_PIPELINE__BUCKET_URL was not found. In Environment Variables key SOURCES__BUCKET_URL was not found. In Environment Variables key BUCKET_URL was not found. In secrets.toml key sources.money_forward_pipeline.filesystem.bucket_url was not found. In secrets.toml key sources.money_forward_pipeline.bucket_url was not found. In secrets.toml key sources.bucket_url was not found. In secrets.toml key bucket_url was not found. In config.toml key sources.money_forward_pipeline.filesystem.bucket_url was not found. In config.toml key sources.money_forward_pipeline.bucket_url was not found. In config.toml key sources.bucket_url was not found. In config.toml key bucket_url was not found. bucket_url が設定される可能性のある環境変数や設定ファイル中のパスが優先度順で一覧となって表示されている。\nこれにより何か設定が間違っているときにどこに書き足せばいいかがわかるようになっている。\nちなみにパスの違いはスコープの違いを表していて、\nIn secrets.toml key sources.money_forward_pipeline.filesystem.bucket_url was not found. In secrets.toml key sources.money_forward_pipeline.bucket_url was not found. の2つを比べると前者は pipeline money_forward_pipeline の filesystem という resource のための bucket_url だが、後者はそれより広く filesystem 以外でも参照できるものとなっている。\n設定まわりの挙動について、詳しくは Configuration | dlt Docs を参照のこと。\n5. pipeline の実行 ここまでで pipeline を実行する準備が整ったので実行してみる。\n$ python money_forward_pipeline.py 成功していれば次のようなログが出力される。\nPipeline money_forward completed in 2.96 seconds 1 load package(s) were loaded to destination duckdb and into dataset raw_money_forward_20231218112802 The duckdb destination used duckdb:////Users/sonoyou/Dev/python/dlt_sample/money_forward.duckdb location to store data Load package 1702898884.890725 is LOADED and contains no failed jobs 6. Streamlit app による結果の確認 意図したとおりにデータが load されたのか結果を確認する方法はいくつかあるが、ここでは Streamlit を使ってみる。\n$ dlt pipeline money_forward show このコマンドを実行すると web ブラウザが立ち上がり、Streamlit app が表示される。\n左のサイドバーで \u0026ldquo;Explore data\u0026rdquo; を選択すると DuckDB と接続し、pipeline により作られた table の schema やデータを見ることができる。\nさらにクエリを実行することも可能。便利。\nStreamlit app の画面: Exprole data\n\u0026ldquo;Load info\u0026rdquo; を選択すると load 時の統計情報などを見ることができる。\nStreamlit app の画面: Load info\nさて、前者の画面で table income_expence_detals (収入・支出詳細) の schema を見ると明らかにおかしいことがわかる。\nこちらを修正していく必要がある。\n日本語 column 名の修正 _dlt の prefix がついた column は dlt の管理用なのでいったん無視するとして、それ以外だと x, id のみしかない。\n元の CSV のヘッダーに記載されていた column 名は日本語表記になっており、1つだけ ID という英字表記の column があった。\nどうやら日本語表記の column 名に問題がありそうだ。\ncolumn 名を指定する方法はいくつかあるが、read_csv() の引数 names を使うことにした。\n関数 money_forward() を次のように修正した。\n@dlt.source def money_forward() -\u0026gt; dlt.extract.source.DltSource: file_specs = [ MFFileSpec( file_name=\u0026#34;収入・支出詳細_*.csv\u0026#34;, table_name=\u0026#34;income_expence_details\u0026#34;, columns=[\u0026#34;is_calc_target\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;details\u0026#34;, \u0026#34;amount_yen\u0026#34;, \u0026#34;financial_institution\u0026#34;, \u0026#34;main_group\u0026#34;, \u0026#34;mid_group\u0026#34;, \u0026#34;memo\u0026#34;, \u0026#34;transfer\u0026#34;, \u0026#34;id\u0026#34;], ), MFFileSpec( file_name=\u0026#34;資産推移月次.csv\u0026#34;, table_name=\u0026#34;monthly_assets\u0026#34;, columns=[\u0026#34;date\u0026#34;, \u0026#34;total_yen\u0026#34;, \u0026#34;deposit_cash_crypto_yen\u0026#34;,\u0026#34;stock_yen\u0026#34;,\u0026#34;investment_trust_yen\u0026#34;,\u0026#34;bond_yen\u0026#34;,\u0026#34;pension_yen\u0026#34;,\u0026#34;point_yen\u0026#34;], ), ] for file_spec in file_specs: records = filesystem( file_glob=f\u0026#34;money_forward/{file_spec.file_name}\u0026#34;, ) | read_csv(encoding=\u0026#34;cp932\u0026#34;, names=file_spec.columns, header=0) yield records.with_name(file_spec.table_name) class MFFileSpec に field columns を追加し、read_csv() に names として渡すようにした。\ncolumns には CSV のヘッダー情報を英訳したものをハードコードしている。\nこちらの read_csv() では内部的に pandas が使われており、pandas.read_csv() の引数を取ることができる。\nnames で明示的に column 名を指定した形になる。\n再度 pipeline を実行し、できあがった table income_expence_details の schema を確認してみる。\nname data_type nullable 0 is_calc_target bigint true 1 date text true 2 details text true 3 amount_yen bigint true 4 financial_institution text true 5 main_group text true 6 mid_group text true 7 transfer bigint true 8 id text true 9 _dlt_load_id text false 10 _dlt_id text false 11 memo text true (なぜか memo が末尾になっているのが気になるが)\n大丈夫そうに見えるけど、もう少し手直しする。\n7. schema の調整 上記の column date に注目したい。\nこれは日付を表す column だが、型が text になっている。\ndate 型として扱えるようにしたい。\nここでは add_map() により resource に対して変換をかませる対応を行った。\nmoney_forward_pipeline.py に次の変換用関数を追加する。\ndef convert_date(d: dict) -\u0026gt; dict: if \u0026#34;date\u0026#34; in d: d[\u0026#34;date\u0026#34;] = datetime.datetime.strptime(d[\u0026#34;date\u0026#34;], \u0026#34;%Y/%m/%d\u0026#34;).date() return d 元の値としては \u0026quot;2023/11/30\u0026quot; のような文字列が入っているため、これを Python の datetime.date に変換する。\n関数 money_forward() の中で resource を yield しているところに ad_map() でこれを適用。\nfor file_spec in file_specs: records = filesystem( file_glob=f\u0026#34;money_forward/{file_spec.file_name}\u0026#34;, ) | read_csv(encoding=\u0026#34;cp932\u0026#34;, names=file_spec.column_names, header=0) yield records.add_map(convert_date).with_name(file_spec.table_name) 再度 pipeline を実行すると column date の型が date になることが確認できた。\nただし、本来はこのような schema 調整は import_schema_path, export_schema_path による YAML の編集で対応する方が好ましい。\n今回はそれがうまくいかなかったので add_map() による変換を使うやり方にした。\n詳しくは Adjust a schema | dlt Docs を参照。\nちなみに同じ要領で add_map() により個人情報をマスキングしたりもできる。\n8. 不要レコードのフィルタリング ここで資産推移月次の CSV ファイルについて思い出してみる。\n当月についてはすべての 日付 がある一方、それ以前については 2023/11/30, 2023/10/31, \u0026hellip; のように月末だけが含まれている。\n月次である以上、月ごとのデータのみが含まれていることが好ましい。\nつまり当月の 2023/12/17, 2023/12/17 のような途中のレコードは不要であり、2023/11/30, 2023/10/31 のような月末のレコードのみを残したい。\nそれには add_filter() を使う。\n次のようなフィルタリング用の関数を用意する。\ndate の値の日付が月末だった場合のみ True を返す。\ndef is_last_day_of_month(d: dict) -\u0026gt; bool: if \u0026#34;date\u0026#34; in d: date = d[\u0026#34;date\u0026#34;] return date.day == calendar.monthrange(date.year, date.month)[1] return True やはりこれを add_filter() により resource に加えるわけだが、今回は table monthly_assets (資産推移月次) にのみ適用したい。\n次のようにした。\nfor file_spec in file_specs: records = filesystem( file_glob=f\u0026#34;money_forward/{file_spec.file_name}\u0026#34;, ) | read_csv(encoding=\u0026#34;cp932\u0026#34;, names=file_spec.column_names, header=0) records = records.add_map(convert_date) if file_spec.table_name == \u0026#34;monthly_assets\u0026#34;: records = records.add_filter(is_last_day_of_month) yield records.with_name(file_spec.table_name) これで pipeline を再実行したところ、table monthly_assets に月末以外のレコードが含まれないようになった。\n9. incremental loading への対応 DWH への load を運用するにあたり、load を incremental に行えるかについても考えないといけない。\n例えば 収入・支出詳細_2023-11-01_2023-11-30.csv にレコードの追加や変更があった場合、同じファイルを再度 load したい。\n資産推移月次.csv については export 時に毎回全期間書き出しなのでこちらも同じファイルを load することになる。\nこのとき、単に新しくレコードが追加されるだけだと、例えば同じ買い物が2回計上されてしまうなどの問題が生じる。\ndlt では incremental loading をサポートしており、primary_key (または merge_key) により load 後のレコードの重複を避けることができる。\n前述のとおり収入・支出詳細は ID、資産推移月次は 日付 でレコードを一意にできる。\n@dlt.source def money_forward() -\u0026gt; dlt.extract.source.DltSource: file_specs = [ MFFileSpec( file_name=\u0026#34;収入・支出詳細_*.csv\u0026#34;, table_name=\u0026#34;income_expence_details\u0026#34;, columns=[\u0026#34;is_calc_target\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;details\u0026#34;, \u0026#34;amount_yen\u0026#34;, \u0026#34;financial_institution\u0026#34;, \u0026#34;main_group\u0026#34;, \u0026#34;mid_group\u0026#34;, \u0026#34;memo\u0026#34;, \u0026#34;transfer\u0026#34;, \u0026#34;id\u0026#34;], primary_keys=[\u0026#34;id\u0026#34;], ), MFFileSpec( file_name=\u0026#34;資産推移月次.csv\u0026#34;, table_name=\u0026#34;monthly_assets\u0026#34;, columns=[\u0026#34;date\u0026#34;, \u0026#34;total_yen\u0026#34;, \u0026#34;deposit_cash_crypto_yen\u0026#34;,\u0026#34;stock_yen\u0026#34;,\u0026#34;investment_trust_yen\u0026#34;,\u0026#34;bond_yen\u0026#34;,\u0026#34;pension_yen\u0026#34;,\u0026#34;point_yen\u0026#34;], primary_keys=[\u0026#34;date\u0026#34;], ), ] for file_spec in file_specs: records = filesystem( file_glob=f\u0026#34;money_forward/{file_spec.file_name}\u0026#34;, ) | read_csv(encoding=\u0026#34;cp932\u0026#34;, names=file_spec.columns, header=0) records = records.add_map(convert_date) if file_spec.table_name == \u0026#34;monthly_assets\u0026#34;: records = records.add_filter(is_last_day_of_month) @dlt.transformer(name=file_spec.table_name, primary_key=file_spec.primary_keys, write_disposition=\u0026#34;merge\u0026#34;) def dummy(items): return items yield records | dummy class MFFileSpec に primary_keys を追加した。\n生成される resource に primary_keys を指定するために @dlt.transformer として関数 dummy() を定義した。\ndummy() は transform の処理自体は何もしないが、primary_key および write_disposition を指定している。\nwrite_disposition は incremental loading の挙動を決めるパラメータであり、\u0026quot;replace\u0026quot;, \u0026quot;append\u0026quot;, \u0026quot;merge\u0026quot; の3つが指定できる。\nここでは primary_keys を使って追加・変更されたレコードを一意にしたかったので \u0026quot;merge\u0026quot; を指定した。\n詳しくは Pipeline Tutorial | dlt Docs を参照。\nDuckDB, BigQuery ともにすべての write_disposition をサポートしているが、destination によってはサポートされないものもあるので注意。\n(dummy() なんか使わずにもっときれいにできる方法があるかもしれないが…)\n10. BigQuery への移行 ローカルの DuckDB でやりたいことができるようになってきたのでいよいよ BigQuery へと移行する。\n2点変更すればよい。\nまずは pipeline の destination 指定を \u0026quot;duckdb\u0026quot; から \u0026quot;bigquery\u0026quot; に変更する。\npipeline = dlt.pipeline( pipeline_name=\u0026#34;money_forward\u0026#34;, destination=\u0026#34;bigquery\u0026#34;, dataset_name=\u0026#34;raw_money_forward\u0026#34;, full_refresh=True, ) 加えて .dlt/secrets.toml に destination 用の設定を追加する。\n[destination.bigquery] location = \u0026#34;asia-northeast1\u0026#34; [destination.bigquery.credentials] client_email = \u0026#34;\u0026lt;CLIENT_EMAIL\u0026gt;\u0026#34; private_key = \u0026#34;\u0026lt;PRIVATE_KEY\u0026gt;\u0026#34; project_id = \u0026#34;\u0026lt;PROJECT_ID\u0026gt;\u0026#34; 以上で destination 変更が完了。\npipeline を再実行すると成功した。\n11. BigQuery への load の確認 BigQuery にデータが load されているかを確認する。\nもちろん前述の Streamlit app から見る方法でも確認できるが、Cloud Shell 上から bq コマンドで確認する。\nまずは dataset を確認。\n$ bq ls datasetId ------------------------------------------ raw_money_forward_20231219112438 raw_money_forward_20231219112438_staging dlt.pipeline() で指定した dataset 名に timestamp っぽい文字列がついた名前で dataset が作成されている。\nこの timestamp は邪魔だと思ったが、除外する方法が分からなかった。\nload ごとに dataset を分けるべきという思想なのだろうか…？\n[追記]\ndlt.pipeline() の引数で full_refresh=True になっていたのが dataset 名に timestamp がついてしまう原因だった。\nfull_refresh=True は名前のとおり完全に作り直す挙動になっており、try \u0026amp; error で何度も作り直すためにつけていた。\nTrue のときに dataset 名が一意になるよう、timestamp をつけるという挙動は理解できる。\nこれを False にすると timestamp のない raw_money_forward という名前で dataset が作成されることを確認した。\n末尾に _staging がついているものは incremental loading のための一時的なデータ置き場のようなもの。\ndadtaset raw_money_forward_20231219112438 の table 一覧を見てみる。\n$ bq query --nouse_legacy_sql \u0026#39;select table_name from raw_money_forward_20231219112438.INFORMATION_SCHEMA.TABLES\u0026#39; +------------------------+ | table_name | +------------------------+ | _dlt_loads | | _dlt_version | | monthly_assets | | _dlt_pipeline_state | | income_expence_details | +------------------------+ monthly_assets, income_expence_details の2つの table が作成されていることを確認した。\nデータもちゃんと入っている模様。\n_dlt の prefix を持つ table には dlt 関連の管理情報が含まれている。\n意図して見ることは少ないと思うが、何か問題が起こったときには参照することになるだろう。\nというわけで BiqQuery への load まで成功した。\nいろいろ編集したが、最終的な money_forwared_pipeline.py のコードを貼っておく。\nimport calendar import datetime import typing import dlt from filesystem import filesystem, read_csv, readers class MFFileSpec: def __init__(self, file_name: str, table_name: str, columns: typing.List[str], primary_keys: typing.List[str]): self.file_name = file_name self.table_name = table_name self.columns = columns self.primary_keys = primary_keys def run_pipeline() -\u0026gt; None: pipeline = dlt.pipeline( pipeline_name=\u0026#34;money_forward\u0026#34;, destination=\u0026#34;bigquery\u0026#34;, dataset_name=\u0026#34;raw_money_forward\u0026#34;, full_refresh=False, ) load_info = pipeline.run(money_forward()) print(load_info) @dlt.source def money_forward() -\u0026gt; dlt.extract.source.DltSource: file_specs = [ MFFileSpec( file_name=\u0026#34;収入・支出詳細_*.csv\u0026#34;, table_name=\u0026#34;income_expence_details\u0026#34;, columns=[\u0026#34;is_calc_target\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;details\u0026#34;, \u0026#34;amount_yen\u0026#34;, \u0026#34;financial_institution\u0026#34;, \u0026#34;main_group\u0026#34;, \u0026#34;mid_group\u0026#34;, \u0026#34;memo\u0026#34;, \u0026#34;transfer\u0026#34;, \u0026#34;id\u0026#34;], primary_keys=[\u0026#34;id\u0026#34;], ), MFFileSpec( file_name=\u0026#34;資産推移月次.csv\u0026#34;, table_name=\u0026#34;monthly_assets\u0026#34;, columns=[\u0026#34;date\u0026#34;, \u0026#34;total_yen\u0026#34;, \u0026#34;deposit_cash_crypto_yen\u0026#34;,\u0026#34;stock_yen\u0026#34;,\u0026#34;investment_trust_yen\u0026#34;,\u0026#34;bond_yen\u0026#34;,\u0026#34;pension_yen\u0026#34;,\u0026#34;point_yen\u0026#34;], primary_keys=[\u0026#34;date\u0026#34;], ), ] for file_spec in file_specs: records = filesystem( file_glob=f\u0026#34;money_forward/{file_spec.file_name}\u0026#34;, ) | read_csv(encoding=\u0026#34;cp932\u0026#34;, names=file_spec.columns, header=0) records = records.add_map(convert_date) if file_spec.table_name == \u0026#34;monthly_assets\u0026#34;: records = records.add_filter(is_last_day_of_month) @dlt.transformer(name=file_spec.table_name, primary_key=file_spec.primary_keys, write_disposition=\u0026#34;merge\u0026#34;) def dummy(items): return items yield records | dummy def convert_date(d: dict) -\u0026gt; dict: if \u0026#34;date\u0026#34; in d: d[\u0026#34;date\u0026#34;] = datetime.datetime.strptime(d[\u0026#34;date\u0026#34;], \u0026#34;%Y/%m/%d\u0026#34;).date() return d def is_last_day_of_month(d: dict) -\u0026gt; bool: if \u0026#34;date\u0026#34; in d: date = d[\u0026#34;date\u0026#34;] return date.day == calendar.monthrange(date.year, date.month)[1] return True if __name__ == \u0026#34;__main__\u0026#34;: run_pipeline() まとめ ドキュメントを見ながら一通りのことができるものを実装することができた。\nコードに関しては慣れればもう少しきれいに書くことができそうな気がする。\n最後に destination を DuckDB から BiqQuery に変更する作業はとても簡単で体験が良かった。\n現在業務で DWH の移行を考えているが、こういう機能があると移行がとても楽だし、DuckDB のようなローカルで気楽に検証できる環境に切り替えられるのもすごくいい。\nDDD みを感じる。\nもちろん GCS 上の CSV ファイルを BiqQuery に読み込む方法は公式で提供されているので、dlt は必須ではない。\nしかし上記のように容易に destination が変えられたり verified source が提供されていたりというところや、前処理のようなごちゃごちゃしたこと pipeline の定義と一緒に Python でどうとでも書けるところにメリットがある。\nまた今回は触れていないが schema evolution についての配慮もある。\n仕事で使ってみてもいいと思った。\nただし dlt が動くマシンにデータを載せることになるので、基本的にはあまり大きなデータの移動には向かない。\nライトなユースケースがマッチするだろう。\n","permalink":"https://soonraah.github.io/posts/load-csv-data-into-bq-by-dlt/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003e前回の記事 \u003ca href=\"../what-is-dlt/\"\u003edlt 入門 - ELT の Extract と Load を担う data load tool\u003c/a\u003e では dlt の概要を説明した。\u003cbr\u003e\nこの記事ではそれを踏まえ、dlt を使って CSV ファイルを BigQuery に load するという一連の開発作業をやってみる。\u003c/p\u003e\n\u003cp\u003e現実の CSV はそのまま使えなかったりするので、BigQuery に入れるまでに泥臭い前処理のような作業があることが多い。\u003cbr\u003e\nそのへんもまとめて dlt でやってみるとこんな感じ、というのが示せるとよい。\u003c/p\u003e\n\u003ch2 id=\"やりたいこと\"\u003eやりたいこと\u003c/h2\u003e\n\u003cp\u003e個人で管理しているお金の情報を個人用の BigQuery に置きたい、という要件を想定。\u003c/p\u003e\n\u003ch3 id=\"データ概要\"\u003eデータ概要\u003c/h3\u003e\n\u003cp\u003e具体的には MoneyForward のデータを load していく。\u003cbr\u003e\n個人では API を利用できないので、web UI から export できる CSV のデータで収入・支出詳細と資産推移月次を対象とする。\u003cbr\u003e\nCSV の export 方法は以下を参照。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://support.me.moneyforward.com/hc/ja/articles/900004382483-%E5%85%A5%E5%87%BA%E9%87%91%E5%B1%A5%E6%AD%B4%E3%81%AF%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%81%8B\"\u003e入出金履歴はダウンロードできますか – マネーフォワード MEサポートサイト\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eデータの内容は次のようになっている。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e収入・支出詳細_2023-11-01_2023-11-30.csv\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u0026#34;計算対象\u0026#34;,\u0026#34;日付\u0026#34;,\u0026#34;内容\u0026#34;,\u0026#34;金額（円）\u0026#34;,\u0026#34;保有金融機関\u0026#34;,\u0026#34;大項目\u0026#34;,\u0026#34;中項目\u0026#34;,\u0026#34;メモ\u0026#34;,\u0026#34;振替\u0026#34;,\u0026#34;ID\u0026#34;\n\u0026#34;1\u0026#34;,\u0026#34;2023/11/30\u0026#34;,\u0026#34;AMAZON.CO.JP\u0026#34;,\u0026#34;-2830\u0026#34;,\u0026#34;楽天カード\u0026#34;,\u0026#34;食費\u0026#34;,\u0026#34;食料品\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;EPv92ZjQcOxgWQx_cLbhD1\u0026#34;\n\u0026#34;1\u0026#34;,\u0026#34;2023/11/24\u0026#34;,\u0026#34;東京ガス\u0026#34;,\u0026#34;-4321\u0026#34;,\u0026#34;楽天カード\u0026#34;,\u0026#34;水道・光熱費\u0026#34;,\u0026#34;ガス・灯油代\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;r6wuQPfrIRS6aFpNYZE5Eh\u0026#34;\n\u0026#34;1\u0026#34;,\u0026#34;2023/11/24\u0026#34;,\u0026#34;給与 カ) フロッグログ\u0026#34;,\u0026#34;700000\u0026#34;,\u0026#34;みずほ銀行\u0026#34;,\u0026#34;収入\u0026#34;,\u0026#34;給与\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;doettKpYyNp0Tml9KQQXm1\u0026#34;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eヘッダーがあり、各列に名前が付いている。\u003cbr\u003e\nencoding が CP932 であることに注意。\u003cbr\u003e\n\u003ccode\u003eID\u003c/code\u003e の列があるので、行の識別に使えそう。\u003c/p\u003e","title":"現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる"},{"content":"このポストについて このポストは datatech-jp Advent Calendar 2023 の18日目の投稿です。\nweb の記事で見かけた dlt というツールが気になったので調べてみた。\ndlt の概要について書いていく。\nWhat is dlt? https://dlthub.com/\ndlt とは \u0026ldquo;data load tool\u0026rdquo; の略。\n雑に言うとデータパイプラインにおける ELT の Extract と Load を行う ものとなっている。\n主にベルリンとニューヨークに拠点を持つ dltHub 社によって開発されており、OSS の Python ライブラリとして提供されている。\n次のような特徴を持つ。\nプラットフォームではなくあくまでライブラリであることが強調されている つまり Airflow, GitHub Actions, Google Cloud Functions, ローカル環境などどこでも動かすことができる スケールアウト可能な分散処理ではない extract と load にまつわる反復的で平凡な作業をなくすことを目指している schema 推論や schema evolution をサポート 宣言的なコードでメンテナンスを楽にする incremental loading をサポート 豊富な source GA, Salesforce, Kinesis などいろいろ例が挙げられている 要は API からの取得も含めて JSON-like な形式で Python で読めるものなら何でも 豊富な destination BigQuery, Snowflake など主要なクラウド DWH DuckDB はローカルでの動作確認に便利 Airflow, dbt などとの連携がある CLI の提供もある その他、Glossary を見ておくとドキュメントが読みやすくなる。\ndlt の仕組み dlt は内部的には extract, normalize, load の3つのステージを実行する。\nextract API その他から得られた JSON データを parse する normalize 正規化エンジンにより JSON 入れ子構造をリレーショナルテーブルに展開 schema 推論なども含む load destination へと冪等で atomic なロードを行う 荒れがちな取り込みのコードをこの型にはめられるというのがいいところかなと思う。\narchitecutre diagram from How dlt works | dlt Docs\nコード例 以下は Getting Started | dlt Docs に記載のもっともシンプルなコード例。\nimport dlt data = [ {\u0026#39;id\u0026#39;: 1, \u0026#39;name\u0026#39;: \u0026#39;Alice\u0026#39;}, {\u0026#39;id\u0026#39;: 2, \u0026#39;name\u0026#39;: \u0026#39;Bob\u0026#39;} ] pipeline = dlt.pipeline( pipeline_name=\u0026#39;quick_start\u0026#39;, destination=\u0026#39;duckdb\u0026#39;, dataset_name=\u0026#39;mydata\u0026#39; ) load_info = pipeline.run(data, table_name=\u0026#34;users\u0026#34;) print(load_info) これを quick_start_pipeline.py に保存して、\npython quick_start_pipeline.py のように普通に Python スクリプトとして実行すると DuckDB の mydata.users にデータが load される。\nまとめ ざっとだが dlt がどういったものかを解説した。\nFivetran や Airbyte を用意するほどでもない、データ量もそれほど多くといったライトなケースに dlt の選択肢が出てくるのだろうか。\n次のポストでは実際に dlt でパイプラインを構築する開発手順を追ってみたい。\n最後に dlthub に記載の Suraj Rajan 氏 (Snowflake の偉い人) の言葉を引用しておく。\nDLT + DBT can be a killer combination for customers.\nよさそう。\ndlt という命名も dbt を意識してる感じがする。\n","permalink":"https://soonraah.github.io/posts/what-is-dlt/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003eこのポストは \u003ca href=\"https://qiita.com/advent-calendar/2023/datatech-jp\"\u003edatatech-jp Advent Calendar 2023\u003c/a\u003e の18日目の投稿です。\u003c/p\u003e\n\u003cp\u003eweb の記事で見かけた dlt というツールが気になったので調べてみた。\u003cbr\u003e\ndlt の概要について書いていく。\u003c/p\u003e\n\u003ch2 id=\"what-is-dlt\"\u003eWhat is dlt?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://dlthub.com/\"\u003ehttps://dlthub.com/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003edlt とは \u0026ldquo;data load tool\u0026rdquo; の略。\u003cbr\u003e\n雑に言うとデータパイプラインにおける \u003cstrong\u003eELT の Extract と Load を行う\u003c/strong\u003e ものとなっている。\u003cbr\u003e\n主にベルリンとニューヨークに拠点を持つ dltHub 社によって開発されており、OSS の Python ライブラリとして提供されている。\u003c/p\u003e\n\u003cp\u003e次のような特徴を持つ。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプラットフォームではなくあくまでライブラリであることが強調されている\n\u003cul\u003e\n\u003cli\u003eつまり Airflow, GitHub Actions, Google Cloud Functions, ローカル環境などどこでも動かすことができる\u003c/li\u003e\n\u003cli\u003eスケールアウト可能な分散処理ではない\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eextract と load にまつわる反復的で平凡な作業をなくすことを目指している\n\u003cul\u003e\n\u003cli\u003eschema 推論や schema evolution をサポート\u003c/li\u003e\n\u003cli\u003e宣言的なコードでメンテナンスを楽にする\u003c/li\u003e\n\u003cli\u003eincremental loading をサポート\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e豊富な \u003ca href=\"https://dlthub.com/docs/dlt-ecosystem/verified-sources/\"\u003esource\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eGA, Salesforce, Kinesis などいろいろ例が挙げられている\u003c/li\u003e\n\u003cli\u003e要は API からの取得も含めて JSON-like な形式で Python で読めるものなら何でも\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e豊富な \u003ca href=\"https://dlthub.com/docs/dlt-ecosystem/destinations/\"\u003edestination\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eBigQuery, Snowflake など主要なクラウド DWH\u003c/li\u003e\n\u003cli\u003eDuckDB はローカルでの動作確認に便利\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eAirflow, dbt などとの連携がある\u003c/li\u003e\n\u003cli\u003eCLI の提供もある\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eその他、\u003ca href=\"https://dlthub.com/docs/general-usage/glossary\"\u003eGlossary\u003c/a\u003e を見ておくとドキュメントが読みやすくなる。\u003c/p\u003e","title":"dlt 入門 - ELT の Extract と Load を担う data load tool"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第12章「メタデータ管理」について。\n仕事でメタデータを扱い始めたので読んでおきたかった。\n以降、特に注釈のない引用は DMBOK2 第12章からの引用とする。\nメタデータとは 一般的な説明としては「データに関するデータ」とよく言われている。\nデータに関するデータはすべてメタデータなので、メタデータはとても幅広い内容となっている。\nDMBOK2 ではメタデータの説明として図書館の例を挙げている。\nそこには数十万の書籍と雑誌があるのに、図書目録がない。図書目録がなければ、読者は特定の本や特定のトピックの検索を開始する方法さえ分からないかもしれない。図書目録は、必要な情報 (図書館が所有する本と資料、保管場所) を提供するだけでなく、利用者が様々な着眼点 (対象分野、著者、タイトル) から資料を見つけることを可能にする。 (中略) メタデータを持たない組織は、図書目録のない図書館のようなものである。\nデータという資産を管理するためにも、データを利用するためにも、リスクマネジメントのためにもメタデータは必要となる。\nメタデータの種類 メタデータはビジネス、テクニカル、オペレーショナルの3つに分類することができる。\nビジネスメタデータ 主にデータの内容と状態に重点を置く。\nIT からは独立している。\ndataset, table, column の定義と説明 業務ルール、変換ルール、計算方法、導出方法 データモデル etc. テクニカルメタデータ 技術的詳細やシステムに関する情報。\n主に IT に関連している。\n物理 database の table, column の名称 column のプロパティ アクセス権 etc. オペレーショナルメタデータ データの処理とアクセスの詳細を示す。\n運用で得られる情報とも言える。\nバッチプログラムのジョブ実行ログ データの抽出とその結果などの履歴 運用スケジュールの以上 etc. 以上、各種のメタデータで例に挙げたのはあくまで一部であり、現実にはもっと多くのメタデータが存在する。\nメタデータを管理する意義 図書館の例からもわかるとおり、メタデータなしではデータを管理することはできない。\n信頼性が高く管理されたメタデータにより、次のようなことができるようになる。\nデータのコンテキストを提供し、それによりデータ品質を測定可能にして信頼性を向上させる 業務効率の向上、および古いデータや誤ったデータの利用防止 データ利用者とエンジニアの間のコミュニケーションの改善 法令遵守の支援 etc. メタデータの管理が不十分だと次のようなことが起こる。\n一貫性のないデータ利用と誤った定義によるリスク メタデータは複製されて保管されることによる冗長性 利用者の信頼性低下 etc. メタデータアーキテクチャ メタデータの内容は幅広いがしたがってその取得元も幅広く、ビジネス用語集、BI ツール、モデリングツール、等々が挙げられる。\nこれらを何らかの方法で集約し、一箇所のメタデータポータルで閲覧できるようにする必要がある。\nつまり「ここに来ればデータについてのことがわかる」という入り口を設けることになる。\nそのためのアーキテクチャの構成が4つ挙げられている。\n集中型メタデータアーキテクチャ 様々なソースから得られたメタデータを保持する1つの中央のメタデータリポジトリを含む。\nDMBOK2 図85 集中型メタデータアーキテクチャ\n分散型メタデータアーキテクチャ 単一のメタデータリポジトリは持たず、メタデータポータルでの検索時に各種ソースのメタデータを直接参照する。\n集中型は可用性や検索性など、分散型は鮮度やメンテナンス性などに重きをおいているという違いがある。\nDMBOK2 図86 分散型メタデータアーキテクチャ\nハイブリッド型メタデータアーキテクチャ 集中型と分散型のハイブリッド。\n手動で作ったメタデータなどは中央のメタデータリポジトリに起き、それ以外は各種ソースを参照する。\nDMBOK2 図87 ハイブリッド型メタデータアーキテクチャ\n双方向メタデータアーキテクチャ メタデータリポジトリまたはソース側のどこでもメタデータの編集を行うことができ、双方向に反映される。\n図はなし。\n…なんだけど、どうも図のキャプションが各アーキテクチャの説明とずれている気がしてならない。\n図85 -\u0026gt; 双方向メタデータアーキテクチャ 図86 -\u0026gt; 分散型メタデータアーキテクチャ 図87 -\u0026gt; 集中型メタデータアーキテクチャ なのでは？\nメタデータ管理の導入 メタデータ管理の活動は次のような流れになる。\nメタデータ戦略の策定 メタデータ要件の把握 メタデータアーキテクチャの定義 メタデータの作成と維持 メタデータのクエリ、レポート、分析 また\n組織へのリスクを最小限に抑え容易に受け入れてもらえるように、管理されたメタデータ環境を段階的に導入することを薦める。\nとある。\nまずはメタデータリポジトリを実装し、その後必要に応じてインタフェースなどを追加していく。\n(とはいえ最近のメタデータツールならリポジトリと UI が一緒になっていたりする)\nメタデータを管理しないことによるリスクのリスクアセスメント、メタデータを維持管理していくための組織と文化の変革も必要。\n所感 データマネジメントに関心がない人にメタデータの意義を説明するのは難しいと感じる。\nこのへん上手く話せるようになりたい。\nそもそもメタデータって？というところからになるが、図書館の例はわかりやすいので使っていこうと思う。\n「信頼性」という言葉がたびたび出てくるがデータを提供するにあたって信頼性はとても重要で、データ資産を利用してもらえるか、つまりデータ資産の価値に大きく影響を与えるとの認識。\n信頼性が大事だということを理解してもらうのも意外と難しい。\n現在業務で OpenMetadata を使ったメタデータ管理の導入を進めているので、これについても気が向いたら何か書きたい。\n私はデータエンジニアなので技術スタックやツールから考えがちだが、実際はメタデータリポジトリに何を入れるかやそれをどうメンテするかが肝だということがわかった。\nメタデータの付与や更新は中央のデータチームだけではまかなえず、データプロバイダーである組織内の各チームの協力は必須。\n付与だけでなくメンテナンスも含めて組織として体制をつくる必要があり、ガバナンスの力が効いてくるのだろうなと。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-12/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.dama-japan.org/Introduction.html\"\u003eDMBOK2\u003c/a\u003e を読み進めていくシリーズ。\u003cbr\u003e\n今回は第12章「メタデータ管理」について。\u003cbr\u003e\n仕事でメタデータを扱い始めたので読んでおきたかった。\u003c/p\u003e\n\u003cp\u003e以降、特に注釈のない引用は DMBOK2 第12章からの引用とする。\u003c/p\u003e\n\u003ch2 id=\"メタデータとは\"\u003eメタデータとは\u003c/h2\u003e\n\u003cp\u003e一般的な説明としては「データに関するデータ」とよく言われている。\u003cbr\u003e\nデータに関するデータはすべてメタデータなので、メタデータはとても幅広い内容となっている。\u003cbr\u003e\nDMBOK2 ではメタデータの説明として図書館の例を挙げている。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eそこには数十万の書籍と雑誌があるのに、図書目録がない。図書目録がなければ、読者は特定の本や特定のトピックの検索を開始する方法さえ分からないかもしれない。図書目録は、必要な情報 (図書館が所有する本と資料、保管場所) を提供するだけでなく、利用者が様々な着眼点 (対象分野、著者、タイトル) から資料を見つけることを可能にする。 \u003cem\u003e(中略)\u003c/em\u003e メタデータを持たない組織は、図書目録のない図書館のようなものである。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eデータという資産を管理するためにも、データを利用するためにも、リスクマネジメントのためにもメタデータは必要となる。\u003c/p\u003e\n\u003ch2 id=\"メタデータの種類\"\u003eメタデータの種類\u003c/h2\u003e\n\u003cp\u003eメタデータはビジネス、テクニカル、オペレーショナルの3つに分類することができる。\u003c/p\u003e\n\u003ch3 id=\"ビジネスメタデータ\"\u003eビジネスメタデータ\u003c/h3\u003e\n\u003cp\u003e主にデータの内容と状態に重点を置く。\u003cbr\u003e\nIT からは独立している。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edataset, table, column の定義と説明\u003c/li\u003e\n\u003cli\u003e業務ルール、変換ルール、計算方法、導出方法\u003c/li\u003e\n\u003cli\u003eデータモデル\u003c/li\u003e\n\u003cli\u003eetc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"テクニカルメタデータ\"\u003eテクニカルメタデータ\u003c/h3\u003e\n\u003cp\u003e技術的詳細やシステムに関する情報。\u003cbr\u003e\n主に IT に関連している。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e物理 database の table, column の名称\u003c/li\u003e\n\u003cli\u003ecolumn のプロパティ\u003c/li\u003e\n\u003cli\u003eアクセス権\u003c/li\u003e\n\u003cli\u003eetc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"オペレーショナルメタデータ\"\u003eオペレーショナルメタデータ\u003c/h3\u003e\n\u003cp\u003eデータの処理とアクセスの詳細を示す。\u003cbr\u003e\n運用で得られる情報とも言える。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eバッチプログラムのジョブ実行ログ\u003c/li\u003e\n\u003cli\u003eデータの抽出とその結果などの履歴\u003c/li\u003e\n\u003cli\u003e運用スケジュールの以上\u003c/li\u003e\n\u003cli\u003eetc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e以上、各種のメタデータで例に挙げたのはあくまで一部であり、現実にはもっと多くのメタデータが存在する。\u003c/p\u003e\n\u003ch2 id=\"メタデータを管理する意義\"\u003eメタデータを管理する意義\u003c/h2\u003e\n\u003cp\u003e図書館の例からもわかるとおり、メタデータなしではデータを管理することはできない。\u003cbr\u003e\n信頼性が高く管理されたメタデータにより、次のようなことができるようになる。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eデータのコンテキストを提供し、それによりデータ品質を測定可能にして信頼性を向上させる\u003c/li\u003e\n\u003cli\u003e業務効率の向上、および古いデータや誤ったデータの利用防止\u003c/li\u003e\n\u003cli\u003eデータ利用者とエンジニアの間のコミュニケーションの改善\u003c/li\u003e\n\u003cli\u003e法令遵守の支援\u003c/li\u003e\n\u003cli\u003eetc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eメタデータの管理が不十分だと次のようなことが起こる。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e一貫性のないデータ利用と誤った定義によるリスク\u003c/li\u003e\n\u003cli\u003eメタデータは複製されて保管されることによる冗長性\u003c/li\u003e\n\u003cli\u003e利用者の信頼性低下\u003c/li\u003e\n\u003cli\u003eetc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"メタデータアーキテクチャ\"\u003eメタデータアーキテクチャ\u003c/h2\u003e\n\u003cp\u003eメタデータの内容は幅広いがしたがってその取得元も幅広く、ビジネス用語集、BI ツール、モデリングツール、等々が挙げられる。\u003cbr\u003e\nこれらを何らかの方法で集約し、一箇所のメタデータポータルで閲覧できるようにする必要がある。\u003cbr\u003e\nつまり「ここに来ればデータについてのことがわかる」という入り口を設けることになる。\u003cbr\u003e\nそのためのアーキテクチャの構成が4つ挙げられている。\u003c/p\u003e","title":"読書メモ: DMBOK2 第12章 メタデータ管理"},{"content":"このポストについて このポストは Distributed computing Advent Calendar 2023 の3日目の記事になります。\n1日目、2日目に続いて Apache Iceberg について書きますが、このポストでは Iceberg の実用例を書きます。\nAWS DMS による CDC の結果を Apache Iceberg 形式にして Amazon Athena でクエリできるようにするという内容になります。\nやっていることとしては Perform upserts in a data lake using Amazon Athena and Apache Iceberg | AWS Big Data Blog で紹介されている内容と近いですが、実務としての背景や工夫したところなどを書いていきます。\n背景 私の所属する事業会社では日々プロダクトから様々なデータが発生しており、プロダクトの分析やレポーティング、ML など様々な用途で利用されている。\nそれを支える基盤としてデータ基盤が存在している。\nデータ基盤ではクエリエンジンとして Amazon Athena を使っている。\nストレージとしては S3 を使用しており、主に分析用として Parquet 形式でデータが置かれる。\nここに業務用の operational な database から日次でデータを取り込んでいる。\nデータソースは RDS (Aurora MySQL) であり、比較的大きなデータとなっている。\nこれまではこの RDS -\u0026gt; S3 のデータ取り込みには RDS の S3 snapshot export という機能を利用していた。\nこの機能では比較的簡単な設定により、バックアップ用のスナップショットの内容を S3 に export することができる。\nちなみに対象 database のスナップショットのサイズは数十 TB ある。\n課題 RDS の S3 snapshot export には次のような課題があった。\n料金が高い 💸 料金はデータ送信の量で決まるとのこと 具体的な額は伏せるが、思った以上にかかっていた export に時間がかかる 🕓 数時間程度 まれに通常より遅延することがあり、1日以上かかることもあった export の頻度を高くできない 🐌 コストも時間もかかるので1日より短くするのは無理 データソース側で更新されたレコードをデータ基盤上で分析に利用できるまで、最大で1日+数時間のラグがある もっと早く分析できるようにしたいという要望もあった 対象 database の特にデータ量の多い table についてレコードの更新タイミングを確認したところ、日によって波はあるが1日の中で更新されているレコードはおおよそ1~4割だった。\nスナップショットをまるまる export する場合は更新されていないデータも対象となるため、本来は不要であるはずのデータ移動が発生してしまっている。\nソリューション概要 スナップショット全体のコピーには無駄が多い。\nCDC (Change Data Capture) で差分のみ送るようにすればよりデータの移動が少なくなり、上記の問題が解決できると考えた。\nCDC とは RDB のデータを外部にレプリケーションする方法の一つで、レコードの追加・更新・削除などのイベントをログとして送信するというやり方。\nこのようなイベントログがあれば送信先にて table の状態を復元することができる。\nLog-Based Change Data Capture, Change Data Capture (CDC): What it is and How it Works - Striim\nCDC については以下も参照。\nLog-Based Change Data Capture in Change Data Capture (CDC): What it is and How it Works - Striim Track Data Changes - SQL Server | Microsoft Learn CDC を実現する方法はいくつかある。\nmodern data stack 的には Debezium が最も有名だろう。\n今回は元々クラウドサービスとして AWS を利用していたため、比較的導入が容易な AWS DMS (Database Migration Service) を使うことにした。\nsource として RDS、target として S3 を指定して DMS の replication task を動かすことになる。\nちなみに CDC を始める前に現在の table 全体をコピーする full load という処理を実行する必要があるが、このポストでは割愛。\nDMS の CDC で S3 へと送信されたイベントログはそのままでは分析に使えない。\n元の table と同じ形に組み上げる必要がある。\nここでは Apache Iceberg という table 形式を使ってそれを行うことにした。\nIceberg については当アドベントカレンダーの他のポストや本ブログの Iceberg についてのポストも参考にしてほしい。\nIceberg を採用した理由は以下のとおり。\nAthena でサポートされている table 形式である SQL の merge into 文 (後述) により比較的容易にイベントログを元の table の形に復元できる MOR (Merge on Read、後述) により、コストの低い table 更新を実現できる Athena および Glue (後述) で使いたいので Iceberg catalog としては Glue Data Catalog を使うことになる。\n他に Athena で使えて似たことができる table 形式としては Apache Hudi がある。\nしかし以前技術検証したときに Athena のエンジンが Hudi の MOR に対応していなかったため、採用しなかった。\n(今はできるかもしれない、未確認)\nアーキテクチャは以下のようになる。\nsolution architecture\nソリューション詳細 主に Iceberg まわりについて詳しく見ていく。\nmerge into 文による Iceberg table の更新 アーキテクチャ図の 2. にあたる処理。\nDMS の CDC では near real time で S3 にイベントログが出力される。\nただ、今回はデータ基盤については near real time の更新の要件はなかったため、1時間ごとの更新を試みることにした。\nソースである RDS 上の table が 2023-11-11 09:59:59.999999 の時点で次のような状態 (A) だったとする。\nid name phone_number 1 Alice 123-456 2 Bob 456-789 これが1時間後には次のようになっていたとする。(B)\nid name phone_number 1 Alice 123-000 3 Charlie 456-789 このとき、DMS による1時間分の CDC 結果の出力は例えば次のようになっている。(C)\nOp timestamp id name phone_number I 2023-11-11 10:05:00.000000 3 Charlie 333-333 U 2023-11-11 10:10:00.000000 1 Alice 123-000 D 2023-11-11 10:15:00.000000 2 Bob 456-789 column Op は DMS によって付与される column であり、イベントの種別を表す。\nI: 挿入 (Insert) U: 更新 (Update) D: 削除 (Delete) column timestamp も DMS によって付与された column であり、ソース側でその変更が commit された時刻を表す。 (column 名は設定で指定可能)\nソースの table が (A) の状態から (C) に記載されているような変更を経て (B) の状態になった、ということである。\n前回までの変更を Iceberg table が追従できているのであれば (A) の状態になっているはずであり、ここで (C) の差分を適用して (B) の状態にする必要がある。\nこれを自分で実装するとなると結構めんどうだが、幸いに SQL の merge into 文を使って Iceberg table を更新することができる。\nWrites - Apache Iceberg この例だと例えば次のような SQL で挿入・更新・削除を一度に適用できる。\nmerge into my_catalog.my_database.iceberg_table as iceberg using ( select * from cdc_table as cdc -- CDC 結果を1時間分だけ読み込んだ DataFrame ) on -- この column の一致で同一レコードとみなす (複数指定も可) iceberg.id = cdc.id -- 一致する id があり Op=D の場合は削除 when matched and cdc.Op = \u0026#39;D\u0026#39; then delete -- 一致する id があり Op=I or U の場合は更新 when matched and cdc.Op != \u0026#39;D\u0026#39; then update set id = cdc.id, name = cdc.name, phone_number = cdc.phone_number -- 一致する id がなく Op=I or U の場合は挿入 when not matched and cdc.Op != \u0026#39;D\u0026#39; then insert(id, name, phone_number) values(cdc.id, cdc.name, cdc.phone_number) 今回の実装では column 名取得の都合などから Glue Job の Spark SQL でこの SQL を実行するものとした。\nちなみに merge into 文自体は Athena でもサポートしているので、Athena でも実行することができる。\nIceberg table は MOR (Merge on Read) 形式にしており、table 更新時は列指向形式データをすべて書き換えるのではなく差分のみが追加されるようになっている。\nこのため大きな table であっても比較的低コストでの table 更新が可能となる。\n一方で読み取りは COW (Copy on Write) の場合と比べて遅くなるが、基本的には列指向なので分析クエリに対してはそこそこのパフォーマンスとなる。\nMOR, COW については以下を参考。\nRow-Level Changes on the Lakehouse: Copy-On-Write vs. Merge-On-Read in Apache Iceberg | Dremio (Dremio さんの Iceberg 解説記事にはいつもお世話になっています！)\ndaily snapshot の作成 アーキテクチャ図の 4. にあたる処理。\nIceberg table は hourly で更新する一方、Iceberg table から daily でスナップショットとして Parquet 形式に書き出すという処理も追加した。\nIceberg には time travel の機能があり、Iceberg table 自身の中でスナップショットを持つことも可能である。\nしかしわざわざ daily スナップショットを外に書き出しているのは、チームとして Iceberg table の運用経験がなく、長期的に安定運用できるかわからなかったため。\n例えばもし Iceberg table の metadata がぶっ壊れたら…などと考えると、Iceberg の外にデータを別で持っておきたい気持ちがわかっていただけるのではないだろうか。\n元々が daily の更新だったため、最低限 daily のスナップショットが残っていれば少なくとも現在の業務は継続することができる。\n分析者視点だと\n直近1週間 (後述) の鮮度の高いデータの分析 -\u0026gt; hourly 更新の table (Iceberg) 長期間におけるデータの分析 -\u0026gt; daily 更新の table (Parquet) と使い分けてもらうことになる。\n後々 Iceberg table が安定運用できることが分かれば Iceberg 1本にしてもいい。\nIceberg table のメンテナンス 前後してアーキテクチャ図の 3. にあたる処理。\nここでは Iceberg table の compaction と古いスナップショットの削除を行う。\nこれらも daily で実行するものとする。\nMOR の場合、更新が何回も行われ差分の世代が多くなってくるとそれらを merge して table としてのデータを読み取る速度が遅くなってくる。\nこれを解消するためには差分を統合して1つのファイルにまとめる必要があり、これを compaction と言う。\nこの compaction の処理を追加している。\nFine-Tuning Apache Iceberg Tables - Dremio Blog | Dremio Iceberg table は内部にスナップショットという概念があり、table の状態を複数バージョン持っている。\nこれにより time travel query が実行できたり、ACID transction を提供できたりと大きなメリットとなっている。\n前述のとおり daily でデータを Parquet として書き出しているので、Iceberg 内で過去の古いスナップショットを持っているとデータの重複になってしまう。\nよって定期的に Iceberg 内の古いスナップショットを削除する処理を追加している。\nここでは1週間分残すようにし、それより古いスナップショットは削除するようにした。\ncompaction、古い snashot の削除はともに Spark SQL から実行することができる。\nそれぞれ rewrite_data_files(), expire_snapshots() という procedure が対応している。\nMaintenance - Apache Iceberg 運用上の工夫 DMS mirgration task の監視 ソースである RDS 側で手動または自動でフェイルオーバーが実行されたり、エンジンバージョンアップが行われたりすると、CDC を継続的に行っている DMS の replication task が Failed で死んでしまうことがある。\nしたがって replication task の実行状況を監視して通知する仕組みが必須となる。\nreplication task が失敗した場合は速やかに再度実行することになる。\nこのあたりの運用はちょっと面倒なので、再実行の自動化を検討している。\n遅延チェック 上記とも関係しているが、何らかの理由で CDC の結果が S3 に届くのが遅れることも想定される。\n遅れているのに気づかずに「HH 時台の CDC イベントログは全部届いたよね！」とみなして Iceberg table に更新をかけると table が意図しない状態になってしまうかもしれない。\nこれを防ぐために各時間の merge into 文を実行する前に過去 N 時間分の CDC の出力をチェックして、イベントログが遅れて届いていることがないことを確認するようにしている。\n遅れて届いているイベントログがあった場合、procedure rollback_to_snapshot() などで table の状態を rollback することになる。\nちなみにこういった処理の依存関係はおなじみ Airflow (Amazon MWAA) で管理している。\nまた、合わせて DMS の CloudWatch Mterics の CDCLatencySource や CDCLatencyTarget を見ておくとよい。\nそれぞれソースと replication instance, replication instance とターゲット間の遅延を表している。\nschema evolution column 追加など、ソース側の table の schema が変更されることは当然起こりうる。\nRDS の table を管理しているチームには schema を変更するときは前もって教えてもらえる一応伝えてある。\n我々のチームでは通常 Athena というか Glue Data Catalog 上の table の定義は CDK で IaC として管理している。(dbt は未導入)\nただし Iceberg table の schema 変更は metadata として管理される。\n当然 CDK というかその中身の CloudFormation では Iceberg の metadata は扱えない。\nIceberg table の schema については IaC に乗せることはできず、DDL で管理することになっている。\nmerge into 文における単一レコードの複数回操作への配慮 前述の merge into 文のサンプル SQL を見て、例えば同じ id のレコードに対して1時間の中で\n複数回更新が行われた 挿入の後、削除が行われた など複数回の操作があった場合はどうなるんだ？と思った方はごもっとも。\n実際上記のサンプル SQL では複数回の操作を考慮できていない。\nなので cdc のレコードをどの順で読み取るかによる、未定義な挙動になると考えられる。\nではどうすればいいかというと、ある id に対して1時間の中で最後に行われた操作だけを見ればよい。\n最後の操作が削除であればそのレコードは最終的に存在しないし、最後の操作が挿入または更新であれば存在する。\n具体的には using (...) の中の select 文を id ごとに最後の操作だけ見るように書き換えればよい。\nここで「最後」を定義するために column timestamp が必要になってくる。\ntimestamp と window 関数を使ってごにょごにょすると…？\n適用の結果 (まだない) この記事において、適用の結果 AWS コストが◯◯%削減できました！と言えればよかったのだが、まだこの仕組みは運用を始めたばかりであり実績値は出せない。\nプロジェクト着手前の試算では結構がっつり減らせる想定ではあった。\n前述のとおり処理が多段的になり運用の手間も増えているが、ペイできる程度のコスト削減を見込んでいる。\n気が向いたら結果を追記するかもしれない。\nまとめ 実際に Apache Iceberg を使っているプロジェクトの実務としての背景や運用について書いてみた。\nそんなに新しい内容はないが、ノウハウ的なところはあまり見かけない気がするのでまあ許していただきたい。\n今回は Iceberg に注目したので DMS について詳しく書かなかったが、こっちはこっちでいろいろあったりする。\nDistributed computing Advent Calendar 2023、明日は Delta Lake についてのお話のようですね。\nこちらも興味深いです。\n弊チームではこういったお仕事や modern data stack の導入などを一緒にやってくれるデータエンジニアを募集しています。\nご興味のある方は X にて DM ください。\nカジュアル面談しましょう。\n","permalink":"https://soonraah.github.io/posts/ingest-data-into-athena-by-cdc-and-iceberg/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003eこのポストは \u003ca href=\"https://qiita.com/advent-calendar/2023/distributed-computing\"\u003eDistributed computing Advent Calendar 2023\u003c/a\u003e の3日目の記事になります。\u003cbr\u003e\n1日目、2日目に続いて Apache Iceberg について書きますが、このポストでは Iceberg の実用例を書きます。\u003c/p\u003e\n\u003cp\u003eAWS DMS による CDC の結果を Apache Iceberg 形式にして Amazon Athena でクエリできるようにするという内容になります。\u003cbr\u003e\nやっていることとしては \u003ca href=\"https://aws.amazon.com/jp/blogs/big-data/perform-upserts-in-a-data-lake-using-amazon-athena-and-apache-iceberg/\"\u003ePerform upserts in a data lake using Amazon Athena and Apache Iceberg | AWS Big Data Blog\u003c/a\u003e で紹介されている内容と近いですが、実務としての背景や工夫したところなどを書いていきます。\u003c/p\u003e\n\u003ch2 id=\"背景\"\u003e背景\u003c/h2\u003e\n\u003cp\u003e私の所属する事業会社では日々プロダクトから様々なデータが発生しており、プロダクトの分析やレポーティング、ML など様々な用途で利用されている。\u003cbr\u003e\nそれを支える基盤としてデータ基盤が存在している。\u003c/p\u003e\n\u003cp\u003eデータ基盤ではクエリエンジンとして \u003ca href=\"https://docs.aws.amazon.com/athena/latest/ug/what-is.html\"\u003eAmazon Athena\u003c/a\u003e を使っている。\u003cbr\u003e\nストレージとしては S3 を使用しており、主に分析用として Parquet 形式でデータが置かれる。\u003c/p\u003e\n\u003cp\u003eここに業務用の operational な database から日次でデータを取り込んでいる。\u003cbr\u003e\nデータソースは RDS (Aurora MySQL) であり、比較的大きなデータとなっている。\u003c/p\u003e\n\u003cp\u003eこれまではこの RDS -\u0026gt; S3 のデータ取り込みには RDS の \u003ca href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ExportSnapshot.html\"\u003eS3 snapshot export\u003c/a\u003e という機能を利用していた。\u003cbr\u003e\nこの機能では比較的簡単な設定により、バックアップ用のスナップショットの内容を S3 に export することができる。\u003cbr\u003e\nちなみに対象 database のスナップショットのサイズは数十 TB ある。\u003c/p\u003e","title":"CDC + Apache Iceberg で Amazon Athena にデータを取り込む"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第3章「データガバナンス」について。\nDAMA ホイール図において中心に置かれているので先に読んでおこうと思った次第。\n以降、特に注釈のない引用は DMBOK2 第3章からの引用とする。\nデータガバナンスとは データガバナンス (DG) の定義は、データ資産の管理 (マネジメント) に対して職務権限を通し統制 (コントロール) することである。統制とは計画を立て、実行を監視し、徹底させることを指す。\nこの定義からデータガバナンスがなぜ DAMA ホイール図の中心に位置しているかがわかる。\nデータガバナンスにより周囲の各知識領域の計画や実施を統制するという建付けになる。\nデータガバナンス (DG) のゴールは組織がデータを資産として管理できるようにすることである。DG はデータを資産として管理するための原則、ポリシー、プロセス、フレームワーク、評価指標を提供し、組織の各階層レベルでデータマネジメントアクティビティを牽引する。\nこれを可能にするためにデータガバナンスは持続可能であり、業務プロセスに組み込まれており、測定可能になっている必要がある。\nデータガバナンス組織 次の組織構成が一般的なデータガバナンスモデルであるとのこと。\nDMBOK2 データガバナンス組織構成\n右側がデータマネジメントを実施するロールになっており、左側がデータガバナンスによりデータマネジメントさせるロールになっている。\nデータガバナンス運営委員会が組織のデータガバナンスの頂点となっており、最も権限が強い。\n各部門にはデータガバナンス評議会 (DGC) が置かれており、これらがポリシー・評価指標の開発などのデータガバナンスの取り組みや課題、報告を管理する。\nデータガバナンスオフィス (DGO) はデータスチュワード (後述) などで構成され、企業レベルのデータ定義とデータマネジメントにフォーカスする。\n大規模、かつデータマネジメントの意識が高い組織でないとこういった組織構成はなかなか作れないのではと思った。\nライト版の図も欲しいところ。\nDMBOK2 データ問題の報告経路\nポリシーやステークホルダー利害の不一致、権限、データ品質などなど、データに関する問題は上記のような報告経路をたどる。\nデータスチュワード制 データスチュワード制はデータとプロセスの実行責任と結果責任を表す最も一般的な呼び名であり、データ資産の効率的な統制と利用を確かなものとする。\nちょっとこの説明ではイメージしにくいかもしれない。\nデータガバナンスの文脈において、データスチュワードは現場を含む組織の各レベルでデータガバナンスを効かせるための活動を行う実務者だと理解した。\nデータスチュワードという職務名があってもいいが、そうでなくてもよいらしい。\n次のようなことをやる。\n核となるメタデータの作成と管理 ルールと標準の文書化 データ品質の問題管理 データガバナンス運営アクティビティの実施 データスチュワードについては以下も参考。\n参考: データスチュワードとは？役割やメリット、育成方法、選定基準について解説！ | trocco®(トロッコ) データポリシー データポリシーとは、データとインフォーメーションを生成し、取得し、健全性を保ち、セキュリティを守り、品質を維持し、利用することを統制する基本的なルールに、原則と管理糸を盛り込む指示である。\nポリシーはデータガバナンスの \u0026ldquo;What\u0026rdquo; を説明する。\n通常はデータマネジメント・プロフェッショナルか業務ポリシー担当者が起草し、最終的に DGC により承認される。\n組織に対して効果的に伝達、実施される必要があり、そのためには簡潔で直感的な表現であるべき。\n社内ポータルなどオンラインで閲覧できるようになっているのがよい。\nポリシー大事。\nデータガバナンスの導入 効果的で長期に渡るデータガバナンス・プログラムにはデータに関する組織的思考や行動に関する文化の変革に加え、データについて将来のあるべき行動を実現するための新しい考えて方、行動、ピリシー、プロセスをサポートする継続的なチェンジマネジメント・プログラムが必要である。\nデータガバナンスを効かせるということは組織の文化の変革をともなうため、組織内で変化や学習への抵抗が起こる。\nデータガバナンスがいくら正しくても組織特有の文化に配慮しないと組織に浸透させることはできない。\nこのためチェンジマネジメントが重要になってくる。\n恥ずかしながら個人的にはチェンジマネジメントという言葉を知らなかったが、コンサル用語でそういうのがあるらしい。\n参考: チェンジマネジメントとは何か？成功事例と共に徹底解説！ このチェンジマネジメントという言葉は第3章で何度も登場する。\nチェンジマネジメントではチェンジマネジメントの計画、トレーニング、システム開発への組み込み、ポリシー導入、コミュニケーションなどを行う。\nその他、データガバナンスの導入には調整とコミュニケーションが必要となる。\n業務用語集 意外かもしれないが第3章では業務用語集についても何度か触れられている。\n業務用語集の目的は共通理解によるコミュニケーションの推進、データ誤用リスクの低減、IT資産と業務組織との連携の改善など。\n業務用語集の作成・管理にはデータスチュワードが責任を負う。\n所感 自社の置かれている状況から遠いというのと自分がマネジメント側でないのとで、具体的にデータガバナンスをどう効かせるか正直なところイメージしにくいと感じる部分が多かった。\nガバナンスには権威・権限が必要である一方、あまり大きくない会社だとトップ層がプロダクトマネジメントに関心がないことも多く、その辺どうやって巻き込んでいくかが難しいところだと思う。\nどうでもいいけど「データガバナンス評議会」っていう響きかっこよくないですか？\nメガネを光らせながら「データガバナンス評議会の者ですが」って言ってみたい。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-3/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.dama-japan.org/Introduction.html\"\u003eDMBOK2\u003c/a\u003e を読み進めていくシリーズ。\u003cbr\u003e\n今回は第3章「データガバナンス」について。\u003cbr\u003e\n\u003ca href=\"../dmbok-chapter-1/#dama-dmbok-%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%83%AF%E3%83%BC%E3%82%AF\"\u003eDAMA ホイール図\u003c/a\u003eにおいて中心に置かれているので先に読んでおこうと思った次第。\u003c/p\u003e\n\u003cp\u003e以降、特に注釈のない引用は DMBOK2 第3章からの引用とする。\u003c/p\u003e\n\u003ch2 id=\"データガバナンスとは\"\u003eデータガバナンスとは\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eデータガバナンス (DG) の定義は、データ資産の管理 (マネジメント) に対して職務権限を通し統制 (コントロール) することである。統制とは計画を立て、実行を監視し、徹底させることを指す。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eこの定義からデータガバナンスがなぜ DAMA ホイール図の中心に位置しているかがわかる。\u003cbr\u003e\nデータガバナンスにより周囲の各知識領域の計画や実施を統制するという建付けになる。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eデータガバナンス (DG) のゴールは組織がデータを資産として管理できるようにすることである。DG はデータを資産として管理するための原則、ポリシー、プロセス、フレームワーク、評価指標を提供し、組織の各階層レベルでデータマネジメントアクティビティを牽引する。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eこれを可能にするためにデータガバナンスは持続可能であり、業務プロセスに組み込まれており、測定可能になっている必要がある。\u003c/p\u003e\n\u003ch2 id=\"データガバナンス組織\"\u003eデータガバナンス組織\u003c/h2\u003e\n\u003cp\u003e次の組織構成が一般的なデータガバナンスモデルであるとのこと。\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\n    \u003cimg loading=\"lazy\" src=\"/image/dmbok/data_governance_org_structure.png\"\n         alt=\"DMBOK2 データガバナンス組織構成\"/\u003e \u003cfigcaption\u003e\n            \u003cp\u003eDMBOK2 データガバナンス組織構成\u003c/p\u003e\n        \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003e右側がデータマネジメントを実施するロールになっており、左側がデータガバナンスによりデータマネジメントさせるロールになっている。\u003cbr\u003e\nデータガバナンス運営委員会が組織のデータガバナンスの頂点となっており、最も権限が強い。\u003cbr\u003e\n各部門にはデータガバナンス評議会 (DGC) が置かれており、これらがポリシー・評価指標の開発などのデータガバナンスの取り組みや課題、報告を管理する。\u003cbr\u003e\nデータガバナンスオフィス (DGO) はデータスチュワード (後述) などで構成され、企業レベルのデータ定義とデータマネジメントにフォーカスする。\u003c/p\u003e\n\u003cp\u003e大規模、かつデータマネジメントの意識が高い組織でないとこういった組織構成はなかなか作れないのではと思った。\u003cbr\u003e\nライト版の図も欲しいところ。\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\n    \u003cimg loading=\"lazy\" src=\"/image/dmbok/data_problem_reporting_path.png\"\n         alt=\"DMBOK2 データ問題の報告経路\"/\u003e \u003cfigcaption\u003e\n            \u003cp\u003eDMBOK2 データ問題の報告経路\u003c/p\u003e\n        \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eポリシーやステークホルダー利害の不一致、権限、データ品質などなど、データに関する問題は上記のような報告経路をたどる。\u003c/p\u003e\n\u003ch2 id=\"データスチュワード制\"\u003eデータスチュワード制\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eデータスチュワード制はデータとプロセスの実行責任と結果責任を表す最も一般的な呼び名であり、データ資産の効率的な統制と利用を確かなものとする。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eちょっとこの説明ではイメージしにくいかもしれない。\u003cbr\u003e\nデータガバナンスの文脈において、データスチュワードは現場を含む組織の各レベルでデータガバナンスを効かせるための活動を行う実務者だと理解した。\u003cbr\u003e\nデータスチュワードという職務名があってもいいが、そうでなくてもよいらしい。\u003cbr\u003e\n次のようなことをやる。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e核となるメタデータの作成と管理\u003c/li\u003e\n\u003cli\u003eルールと標準の文書化\u003c/li\u003e\n\u003cli\u003eデータ品質の問題管理\u003c/li\u003e\n\u003cli\u003eデータガバナンス運営アクティビティの実施\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eデータスチュワードについては以下も参考。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e参考: \u003ca href=\"https://blog.trocco.io/glossary/data-steward\"\u003eデータスチュワードとは？役割やメリット、育成方法、選定基準について解説！ | trocco®(トロッコ)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"データポリシー\"\u003eデータポリシー\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eデータポリシーとは、データとインフォーメーションを生成し、取得し、健全性を保ち、セキュリティを守り、品質を維持し、利用することを統制する基本的なルールに、原則と管理糸を盛り込む指示である。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eポリシーはデータガバナンスの \u0026ldquo;What\u0026rdquo; を説明する。\u003cbr\u003e\n通常はデータマネジメント・プロフェッショナルか業務ポリシー担当者が起草し、最終的に DGC により承認される。\u003c/p\u003e","title":"読書メモ: DMBOK2 第3章 データガバナンス"},{"content":"このポストについて なんか個人的にデータマネジメントの機運が高まってきたので、ずっと積ん読していた DAMA-DMBOK を読んでいこうかなと。\nで、せっかくなのでデータ関係の皆さんがよくやっているように自分としても読書メモをまとめてみようと思った。\n内容を網羅するのではなく、現場のデータエンジニアとして活動した経験を踏まえて自分なりの観点でまとめてみたい。\n今回は第1章「データマネジメント」で、それ以降は関心がある章をつまみ食い的に読んでいく。\nDAMA-DMBOK とは DAMA とは DAta Management Association の略であり、\n世界各地に80の支部を持ち、8,000名を越える会員を擁する全世界のデータ専門家のための国際的な非営利団体です。 特定のベンダーや技術、手法に依存しないことを前提として、データや情報、知識をエンタープライズの重要な資産として管理する必要性の理解を促し、この分野の成長を推進しております。\n\u0026ndash; 一般社団法人 データマネジメント協会 日本支部(DAMA Japan)\nとのこと。\nこの DAMA が刊行しているのがデータマネジメント知識体系ガイド (The DAMA Guide to Data Management Body of Knowledge) であり、その略称が DMBOK である。\nＤＡＭＡ ＤＭＢＯＫは、データマネジメントプロフェッショナルにとって有益な資料かつ指針となることを目指し、データ管理のもっとも信頼できる入門書となるよう編集されています\n\u0026ndash; 一般社団法人 データマネジメント協会 日本支部(DAMA Japan)\nIT 業界歴の長い人なら PMBOK というプロジェクトマネジメントについて書かれた本をご存知かもしれないが、あれのデータマネジメント版だと思っていい。\n私としてはデータマネジメントの教科書的なものだと考えている。\n2023年現在における最新版は2018年の第2版となっている。\n以降では DMBOK2 とする。\n5年前なのでやや古いと思うかもしれないが、内容的には特定技術について書かれているわけではなく、ある程度抽象度が高い話になっているので陳腐化はしにくい。\nデータマネジメントとは 以降、特に注釈のない引用は DMBOK2 第1章からの引用とする。\nデータマネジメントとは、データとインフォメーションという資産の価値を提供し、管理し、守り、高めるために、それらのライフサイクルを通して計画、方針、スケジュール、手順などを開発、実施、監督することである。\nこの一文にいろいろと集約されているので見ていこう。\nデータとインフォーメーション ここでいう「データ」と「インフォメーション」は DIKW モデルにもとづく解釈でよい。\nデータはインフォメーションの原材料であり、インフォメーションは意味と目的、つまりコンテキストを付与されたデータと言える。\nex. 前四半期の売上レポートは「インフォメーション」、それの元になっている DWH のデータが「データ」 データマネジメントにおいては特にこのデータとインフォーメションを扱う。\n資産 資産とは経済的資源であり、所有可能、管理可能で、それ自体に価値があるか、価値を生み出すことができるものである。\nこの定義からデータは資産であるとみなされ、したがって資産として組織で管理される必要がある。\n金融資産などと同じ面もあるが異なっている面も多くあり、例えば使用しても失われない。\nこのような特性が管理方法に影響する。\n資産としてのデータは本来はデータにかかるコスト、データから得る利益を測定して経済的観点から評価されるべき。\nしかしまだそのための基準はないとのこと。\nなので実質的に経済的観点からの評価を実行するのはとてもむずかしいと思われる。\nデータ基盤がコストセンターだと認識されているような組織だとこのあたり必要になってくるのかもしれない。\n提供し データライフサイクル (後述) においてはデータの生成と利用が最も重要である。\nデータは利用されて初めて価値を生む。\nデータにはそれぞれ生産者と利用者がいる。\n社内の一つの部門内でデータ生産・利用されることもあるが、一方で生産と利用が別部門であることもある。\nよってデータライフサイクルを考えるためには組織横断の全社的な視点が必要になってくる。\n管理し データを資産として管理するためには質の高いメタデータが必要となる。\nメタデータとはデータについてのデータという意味。\nデータを記述するものがなければ意味のある管理はできないということ。\nメタデータもデータの一種であるためデータとして管理される必要がある。\n多くの場合、メタデータ管理がデータマネジメント全体を改善する出発点となる。\n守り データは資産であると同時にリスクでもある。\nなくなったり、盗まれたり、誤用されたりもするため、そうならないようにリスクを管理する。\n不正確・不完全・期限切れなど低品質のデータからは正しくないインフォーメーションが得られるリスクや誤用されるリスクがある。\nリスクを避けるためにデータを用いた意思決定に必要なデータ品質が必要となる。\n当然セキュリティの話もある。\n昨今は個人情報への関心も高まっており、それらは保護されなければならない。\n高める データ品質を高める、または高品質であることを保証しなければならない。\nDMBOK2 ではデータ品質に重きを置いており、\nこれがデータマネジメントの根幹である。\nとまで言い切っている。\n利用者からすると品質の問題が明らかになるまではデータの信頼性は高いものと想定されるが、一度信頼が失われると取り戻すのは簡単ではない。\nデータ基盤の同じデータに対して、5分前にクエリした結果と今クエリした結果が違っていると「これ大丈夫かな」ってなりますよね？\n(何らかの原因によるデータ生成処理の再実行など、運用上でよく起こりうる話)\nデータの品質問題にたいしょするために収益の10〜30%を費やしていると専門家は考えている。IBM は米国において低品質のデータのために費やしたコストは2016年で3.1兆ドルであったと推定している。\nとあるとおり、低品質のデータにはコストとリスクがかかる。\n一方で高品質のデータからはより高い生産性や競合に対する優位性などさまざまな恩恵が得られる。\nライフサイクル データにはその発生から破棄に至るまでのライフサイクルがある。\nDMBOK2 データライフサイクルの主要アクティビティ\n上記はその概念的な図だが、実務におけるデータライフサイクルを詳細に記述するのは困難を伴う。\n生産者から利用者までの経路、すなわちデータリネージを表現する必要がある。\nデータ品質、メタデータ、セキュリティなどはライフサイクル全体を通して管理していかなければなない。\n計画、方針、スケジュール データから偶然に価値が生まれることはなく、様々な側面から計画が必要である。\nより高品質なデータを目指す計画においては、アーキテクチャ、モデリング、その他設計機能に対して、戦略的なアプローチが必要である。業務と IT のリーダーが戦略的に連携することも欠かせない。\n組織横断であるため、(理想的には) CDO がリーダーシップを発揮、ビジョンや目的を示し、自らそれにコミットメントしないと効果的なデータマネジメントにならない。\nCDO がいる組織はまだ多くないため、その状況でこの役割を誰が担うかというのがデータマネジメントの成果に大きな影響を与えるはず。\nデータマネジメント・フレームワーク データマネジメントには様々な側面がある。\nデータマネジメントを総合的に理解し、その構成要素間の関係を理解するためにいくつかのフレームワークが提案されている。\nここでは DMBOK2 で紹介されたフレームワークのうちのいくつかを紹介する。\nDAMA-DMBOK フレームワーク データマネジメント全体を構成する知識領域についてまとめたもの。\nDAMA ホイール図, DAMA のご紹介 - DAMA Japan\nこの DAMA ホイール図はおそらく一番有名なもので、見たことある人も多いのでは。\nデータマネジメントにどういった知識領域があるのかが一覧できる。\nただデータガバナンスが中心にあるというだけで、それ以外の各領域の関係性はわかりにくいかもしれない。\n知識領域コンテキスト図, DMBOKv2 Image Downloads - DAMA International\nこちらの知識領域コンテキスト図も DAMA-DMBOK フレームワークの一部である。\nDMBOK1 のときに最も人気のあった図とのこと。\n最初ぱっとこれを見たときはピンとこなかったが、ある事業ドメインにおけるデータの流れやコンテキストを表しているというのに気づきとても有用だと思った。\nこの内容を各事業ドメインで整理すると、組織のデータについての理解が大きく捗りそう。(めっちゃたいへんだけど)\n上記の解釈は誤り。\nこの図は事業ドメインごとではなく、知識領域、つまり DAMA ホイール図に記載の「データガバナンス」「アーキテクチャ」などの各要素ごとに作られる。\nこの図により各知識領域を簡潔に定義する。\nDMBOK2 では各章でそれぞれの知識領域について書かれており、章の冒頭にこの知識領域コンテキスト図が記載されている。\nある知識領域の概略をつかむにはその図を見ればよい。\nDMBOK ピラミッド ほとんどの組織はデータマネジメントの戦略を決めてからデータ管理を始めるということができず、走りながらデータマネジメントの改善に取り組むことになる。\nその中でどういったステップをたどるかを示してくれるのが Peter Aiken\u0026rsquo;s Framework である。\nPeter Aiken\u0026rsquo;s Framework, Why HR Data Management Strategy is important in your HR Transformation | SAP Blogs (元ネタは DMBOK2)\nPhase 1 (青): データの保存、統合 Phase 2 (黃): データ品質、アーキテクチャ、メタデータ Phase 3 (緑): ガバナンスと利用推進 Phase 4 (赤): 分析などの高度な利用 組織のデータマネジメントは Phase 1 -\u0026gt; 4 の順で進めていく。\nPhase の順序は上下関係にはなっておらず、上下関係は知識領域間の依存関係を示している。\nこの図は実務でデータマネジメントを進めるにあたり、良い指針となる。\n所感 第1章はタイトルのとおりデータマネジメントについて俯瞰するのに良かった。\n品質、ライフサイクル、リスク管理、メタデータなどの各要素がそれぞれに影響しあっていることが理解できた。\nこれからその各要素を詳しく見ていくことになる。\n組織横断的なデータの扱いを始めてからそんなに経っていない組織だと、CDO はおろかデータマネジメントに関心があるリーダー層がいないというケースが多いのではないだろうか。\nCDO が無理なら CTO あたりに関心を持ってコミットしてもらいたい。\nそれもできない場合は現場のデータエンジニアやアナリストなどが進めていくしかないが、それは厳しい状況でのデータマネジメントになりそう。\nエライ人を巻き込む政治力のようなものが問われる。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-1/","summary":"\u003ch2 id=\"このポストについて\"\u003eこのポストについて\u003c/h2\u003e\n\u003cp\u003eなんか個人的にデータマネジメントの機運が高まってきたので、ずっと積ん読していた DAMA-DMBOK を読んでいこうかなと。\u003cbr\u003e\nで、せっかくなのでデータ関係の皆さんがよくやっているように自分としても読書メモをまとめてみようと思った。\u003cbr\u003e\n内容を網羅するのではなく、現場のデータエンジニアとして活動した経験を踏まえて自分なりの観点でまとめてみたい。\u003cbr\u003e\n今回は第1章「データマネジメント」で、それ以降は関心がある章をつまみ食い的に読んでいく。\u003c/p\u003e\n\u003ch2 id=\"dama-dmbok-とは\"\u003eDAMA-DMBOK とは\u003c/h2\u003e\n\u003cp\u003eDAMA とは DAta Management Association の略であり、\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e世界各地に80の支部を持ち、8,000名を越える会員を擁する全世界のデータ専門家のための国際的な非営利団体です。 特定のベンダーや技術、手法に依存しないことを前提として、データや情報、知識をエンタープライズの重要な資産として管理する必要性の理解を促し、この分野の成長を推進しております。\u003c!-- raw HTML omitted --\u003e\u003cbr\u003e\n\u0026ndash; \u003c!-- raw HTML omitted --\u003e\u003ca href=\"https://www.dama-japan.org/Introduction.html\"\u003e一般社団法人 データマネジメント協会 日本支部(DAMA Japan)\u003c/a\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eとのこと。\u003cbr\u003e\nこの DAMA が刊行しているのがデータマネジメント知識体系ガイド (The DAMA Guide to Data Management Body of Knowledge) であり、その略称が DMBOK である。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eＤＡＭＡ ＤＭＢＯＫは、データマネジメントプロフェッショナルにとって有益な資料かつ指針となることを目指し、データ管理のもっとも信頼できる入門書となるよう編集されています\u003cbr\u003e\n\u0026ndash; \u003c!-- raw HTML omitted --\u003e\u003ca href=\"https://www.dama-japan.org/Introduction.html\"\u003e一般社団法人 データマネジメント協会 日本支部(DAMA Japan)\u003c/a\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eIT 業界歴の長い人なら PMBOK というプロジェクトマネジメントについて書かれた本をご存知かもしれないが、あれのデータマネジメント版だと思っていい。\u003cbr\u003e\n私としてはデータマネジメントの教科書的なものだと考えている。\u003c/p\u003e","title":"読書メモ: DMBOK2 第1章 データマネジメント"},{"content":"前回のポストでは merge on read で Apache Iceberg の table を near real time で更新するということを行った。\nこのポストではそのメンテナンスについて触れて、かつそれを実行してみる。\nmerge on read の課題 merge on read で table を更新する場合、copy on write の場合と違い table 全体を洗い替えする必要はなく差分のみを追記することになる。\nしたがって更新にかかる時間は copy on write よりも短くなる。\n一方で merge on read の名のとおり読み出し時に積み重なった差分とベースを merge して最新の snapshot とするため、読み出しの速度は copy on write より遅くなる。\n長時間更新され差分がたくさん存在しているとなおさら遅い。\nなので\n更新頻度が低く、参照頻度が高いユースケース -\u0026gt; copy on write 更新頻度が高く、参照頻度が低いユースケース -\u0026gt; merge on write という使い分けがよいとされている。\n前回ポストの例では一晩更新を続けた後の merge on read の table に対して簡単な select 文を実行したところ、6分程度かかってしまった。\nレコード数はたかだか128件程度であることを考えるとかなり遅いと言える。\nこのままでは使い物にならない。\nしかし更新頻度が高く、参照もよく行われる場合はどうすればいいか？\nというところで compaction が必要になってくる。\nCompaction compaction は追加された差分ファイルをベースファイルと merge して新たなベースファイルを作るという処理である。\ncompaction 後の select クエリは compaction 以前の差分ファイルを読む必要がなくなるため、読み込みが速くなる。\nしたがって長期的に運用される merge on read の table では定期的に compaction が行われることが望ましい。\nIceberg の公式ドキュメントに compaction の記載があるが、Dremio の記事の方が図もあってわかりやすい。\nCompact data files | Apache Iceberg Compaction in Apache Iceberg: Fine-Tuning Your Iceberg Table’s Data Files | Dremio Iceberg で Spark から compaction を行う方法は2つある。\n1つは SparkActions.rewriteDataFiles() を使う方法、もう1つは SparkSQL 内で procedure rewrite_data_files を呼び出す方法だ。\n今回は主に SparkSQL ベースで実装しているということもあり後者にした。\nちなみに compaction は Iceberg 固有の機能ではなく、Hudi や Delta Lake などでも存在している。\nmerge on read をサポートする table format においては一般的なトピックだと思われる。\nその他のメンテナンス compaction 以外でも以下2点も対応する。\n古い snapshot の削除 長期間 table の更新を続けると snapshot が蓄積していく。\nデータが大きくなり続けるため、定期的に snapshot を削除していくことが推奨されている。\nprocedure expire_snapshots により指定の時刻より古い snapshot を削除することができる。\n古い metadata file の削除 データ参照の入り口である metadata file も同様に増え続ける。\nこれも定期的に削除するが、頻繁に更新が行われる table ではこれも削除した方がよい。\nmetadata file の削除は create table 時に table property として write.metadata.delete-after-commit.enabled および write.metadata.previous-versions-max を指定することで自動で行われる。\nこのように Iceberg の table 構造は物理的・論理的に多層的になっているので色々なレベルで配慮が必要という印象。\nメンテナンスの実装 以上を踏まえて、前回ポスト時の実装に対して変更を追加した。\nまず前回作った更新用の app 中の create table を変更。\ntblproperties として古い metadata file を削除するための設定を追加した。(末尾の2つ)\n// Create table spark .sql( \u0026#34;\u0026#34;\u0026#34;create table | my_catalog.my_db.device_temperature |( | device_id int, | operation string, | temperature double, | ts timestamp |) |using iceberg |tblproperties ( | \u0026#39;format-version\u0026#39; = \u0026#39;2\u0026#39;, | \u0026#39;write.delete.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.update.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.merge.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.metadata.delete-after-commit.enabled\u0026#39; = \u0026#39;true\u0026#39;, | \u0026#39;write.metadata.previous-versions-max\u0026#39; = \u0026#39;100\u0026#39; |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) 次に更新処理とは別プロセスとしてメンテナンスを実施する app を追加した。\n通常の更新処理の mini batch の中にメンテナンスを組み込んでもよかったのだが、そうすると mini batch が遅れる可能性がある。\n運用を考えてもメンテナンス用の処理は別で実行できるようになっていた方がいいだろう。\nIceberg は後述の1点を気をつければ並列書き込みが可能であるため、別プロセスで実施する方針とした。\nwhile (true) { println(\u0026#34;Execute compaction\u0026#34;) retry(3) { () =\u0026gt; spark .sql( \u0026#34;\u0026#34;\u0026#34;call my_catalog.system.rewrite_data_files( | table =\u0026gt; \u0026#39;my_db.device_temperature\u0026#39;, | strategy =\u0026gt; \u0026#39;binpack\u0026#39; |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) .show(truncate = false) } val ts = Timestamp.from(Instant.now().minusSeconds(20 * 60)) println(s\u0026#34;Expire snapshots older than ${ts.toString}\u0026#34;) retry(3) { () =\u0026gt; spark .sql( s\u0026#34;\u0026#34;\u0026#34;call my_catalog.system.expire_snapshots( | table =\u0026gt; \u0026#39;my_db.device_temperature\u0026#39;, | older_than =\u0026gt; timestamp \u0026#39;${ts.toString}\u0026#39;, | retain_last =\u0026gt; 20 |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) .show(truncate = false) } Thread.sleep(10 * 60 * 1000) } 無限ループによりおよそ10分に1回、それぞれ procedure 呼び出しにより compaction と snapshot 削除が実行されるようになっている。\nここで retry() は自前の実装だが、名前のとおり失敗しても指定回数まで retry するというものになっている。\nなぜ retry が必要かというと、Iceberg は並列した書き込みができるが lock などは取らず、楽観的な実行となっている。(optimistic concurrency)\n一連の書き込み処理の準備が終わって最後に commit するときに、その table が他のプロセスにより更新されたことがわかると失敗となる。\n実際に retry なしの場合は次のようなエラーが出ることがあった。\n[error] java.lang.RuntimeException: Cannot commit rewrite because of a ValidationException or CommitFailedException. This usually means that this rewrite has conflicted with another concurrent Iceberg operation. To reduce the likelihood of conflicts, set partial-progress.enabled which will break up the rewrite into multiple smaller commits controlled by partial-progress.max-commits. Separate smaller rewrite commits can succeed independently while any commits that conflict with another Iceberg operation will be ignored. This mode will create additional snapshots in the table history, one for each commit. このメッセージでは partial-progress.enabled の設定が推奨されているが、今回の問題設定はそもそも細かい更新だったのでちょっと違うかなというところで設定していない。\nこれを設定したところで retry 的な配慮は結局必要になるというのもある。\nメンテナンスの実行結果 以上のように実装した2つの処理を並列実行した。\n30秒に1回、table を更新 約10分に1回、table をメンテナンス 2日以上これをまわしっぱなしにした。\nCompaction の効果 select 文を実行したところ、数秒で完了した。\ncompaction を実施する前は一晩更新を続けた後の select で6分かかっていたので、かなり速くなったと言える。\ncompaction により差分の merge のコストが小さくなったからだ。\nOLAP ならこれぐらいで十分だろう。\nSnapshot の削除 metadata file は JSON 形式になっており、次のような形で利用可能な snapshot の情報が記載されている。\n{ ..., \u0026#34;snapshots\u0026#34;: [ { \u0026#34;sequence-number\u0026#34;: 25, \u0026#34;snapshot-id\u0026#34;: 8639168923204820422, \u0026#34;parent-snapshot-id\u0026#34;: 5556833698284258695, \u0026#34;timestamp-ms\u0026#34;: 1684722810993, \u0026#34;summary\u0026#34;: { \u0026#34;operation\u0026#34;: \u0026#34;overwrite\u0026#34;, \u0026#34;spark.app.id\u0026#34;: \u0026#34;local-1684722133108\u0026#34;, \u0026#34;added-data-files\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;added-position-delete-files\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;added-delete-files\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;added-records\u0026#34;: \u0026#34;26\u0026#34;, \u0026#34;added-files-size\u0026#34;: \u0026#34;13020\u0026#34;, \u0026#34;added-position-deletes\u0026#34;: \u0026#34;27\u0026#34;, \u0026#34;changed-partition-count\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;total-records\u0026#34;: \u0026#34;141\u0026#34;, \u0026#34;total-files-size\u0026#34;: \u0026#34;52397\u0026#34;, \u0026#34;total-data-files\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;total-delete-files\u0026#34;: \u0026#34;25\u0026#34;, \u0026#34;total-position-deletes\u0026#34;: \u0026#34;51\u0026#34;, \u0026#34;total-equality-deletes\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;manifest-list\u0026#34;: \u0026#34;data/warehouse/my_db/device_temperature/metadata/snap-8639168923204820422-1-7ea46920-6ac3-444d-aed1-8dbc8d0c24fd.avro\u0026#34;, \u0026#34;schema-id\u0026#34;: 0 }, ... ], ... } 最新の metadata file に含まれる snapshot の数をカウントしてみる。\n$ cat data/warehouse/my_db/device_temperature/metadata/v6133.metadata.json | jq \u0026#39;.snapshots | length\u0026#39; 59 table は30秒に1回更新されて新しい snapshot が作られている。\nそれと並行して10分に1回、その時点より20分以上前の snapshot を削除しているため、snapshot の数はおおよそ40〜60ぐらいになる。(実際は compaction で作られた snapshot も入ってくるためこれより少し多くなる)\nsnapshot の削除が効いていることがわかった。\n他の metadata file の shapshot 数も確認したが、だいたい上記の範囲におさまっていた。\nmetadata file の削除 metadata file の削除が効いているかも確認。\n$ ls -l data/warehouse/my_db/device_temperature/metadata/v*.metadata.json | wc -l 101 create table では 'write.metadata.previous-versions-max' = '100' を指定していた。\n現行 version 1 件 + 過去の version 100 件ということで期待どおりのファイル数にコントロールされていることがわかった。\n雑感 compaction などのメンテナンスが期待どおりに実行されていることが確認できた。\nmerge on read の table を長期的に運用する場合、こういったメンテナンスの処理や設定を導入することは必須となるだろう。\nIceberg で merge on read の table を高頻度で更新するにあたり、課題だなと思ったことを3点挙げておく。\nファイル形式 Apache Hudi や Delta Lake で merge on read の table を更新する場合、ベースファイルは Parquet 等の列指向、差分ファイルは Avro などの行指向がデフォルトになっている。\n直感的に少量の差分は行指向になっているのが効率がいいように思うが、一方で Iceberg でこういった形での更新ができるのかがわからなかった。\n複数のファイル形式がまざっていても manifest で吸収できるので読み込みはできるはず、しかし Spark からこういった書き込みができるという記述をドキュメントで見つけることができなかった。\n更新にかかる時間 以下は merge on read で table 更新を行う Spark app において、Spark web UI 上で表示される各 mini batch の実行時間を表すグラフである。\n約10分周期で mini batch の実行時間が増加していき、ストンと下がるということが確認される。\nこの下がっている部分は compaction のタイミングであり、逆に言うと compaction しないと書き込みの時間が増大するということになる。\n\u0026ldquo;merge on read\u0026rdquo; の名のとおり、本来は読み込み時に差分を merge するのであって書き込み時はそうならないはず。\nしかし差分が増えると書き込み時間が増えていくというのはこの仕様に合わない。\n(Iceberg のソースコードを読めばわかりそうだが、今回はそこまでやってない)\n求められる技術力 Iceberg の table は ACID transaction, time travel, schema/partition evolution など運用上便利な機能がサポートされている。\n一方でこれを運用していくには table format についての知見や Spark などの分散処理の知見を持つデータエンジニアのリソースが必要となってくる。\nBigQuery や Snowflake などの DWH を中心としたアーキテクチャと比べて人材をそろえるハードルが高いと考えられる。\nこれは Iceberg というよりは Lakehouse Architecture と DWH の差だろう。\n…などと課題を挙げてはみたが、総じて Iceberg はいいものだと感じた。\nうまく抽象化されたレイヤーを挟むことでデータレイクのいろいろな課題を解決しており、かつ動作も比較的わかりやすい。\nエンジニアリングが強い組織だと Iceberg を使って Lakehouse Architecture のも悪くなさそう。\n","permalink":"https://soonraah.github.io/posts/maintain-iceberg-table-updated-in-near-real-time/","summary":"\u003cp\u003e\u003ca href=\"../update-iceberg-table-in-near-real-time\"\u003e前回のポスト\u003c/a\u003eでは merge on read で Apache Iceberg の table を near real time で更新するということを行った。\u003cbr\u003e\nこのポストではそのメンテナンスについて触れて、かつそれを実行してみる。\u003c/p\u003e\n\u003ch2 id=\"merge-on-read-の課題\"\u003emerge on read の課題\u003c/h2\u003e\n\u003cp\u003emerge on read で table を更新する場合、copy on write の場合と違い table 全体を洗い替えする必要はなく差分のみを追記することになる。\u003cbr\u003e\nしたがって更新にかかる時間は copy on write よりも短くなる。\u003cbr\u003e\n一方で merge on read の名のとおり読み出し時に積み重なった差分とベースを merge して最新の snapshot とするため、読み出しの速度は copy on write より遅くなる。\u003cbr\u003e\n長時間更新され差分がたくさん存在しているとなおさら遅い。\u003cbr\u003e\nなので\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e更新頻度が低く、参照頻度が高いユースケース -\u0026gt; copy on write\u003c/li\u003e\n\u003cli\u003e更新頻度が高く、参照頻度が低いユースケース -\u0026gt; merge on write\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eという使い分けがよいとされている。\u003c/p\u003e\n\u003cp\u003e前回ポストの例では一晩更新を続けた後の merge on read の table に対して簡単な \u003ccode\u003eselect\u003c/code\u003e 文を実行したところ、6分程度かかってしまった。\u003cbr\u003e\nレコード数はたかだか128件程度であることを考えるとかなり遅いと言える。\u003cbr\u003e\nこのままでは使い物にならない。\u003c/p\u003e","title":"near real time で更新される Apache Iceberg の table のメンテナンス"},{"content":"Apache Iceberg の table を near real time に、つまり高頻度で更新するということをやってみた。\nApache Iceberg とは Apache Iceberg (以下 Iceberg) は分散ファイルシステムやクラウドストレージ上の table format であり、Apache Hudi や Delta Lake と並んで data lake や lakehouse architecture で用いられる。\n特徴的なのは table とデータ実体 (Parquet, Avro など) の間に metadata file, manifest list, manifest file の抽象的なレイヤーがあり、ファイル単位で table の状態を track できること。\nこれにより強い isolation level、パフォーマンス、schema evolution など様々な機能・性能を実現できるようになっている。\nApache Iceberg Iceberg Table Spec\n詳しくは公式ドキュメントを参照のこと。\n最近では SmartNews 社が Iceberg で data lake を構築したことを記事にしていた。\nFlink-based Iceberg Real-Time Data Lake in SmartNews (Part I) | by SmartNews | SmartNews, Inc | Apr, 2023 | Medium ベンダー提供の DWH 中心ではなく Lakehouse Architecture を目指すのであれば最有力の table format の1つだと言えそう。\nこのポストでやりたいこと このポストでは Iceberg の table を near real time 更新することを想定して実験を行う。\n多数の IoT デバイスで測定された温度情報が逐次的に送られていることを想定し、定期的にそれをデバイス ID ごとの最新の温度情報を持つ table へと書き出すものとする。\n次のような要件を仮定する。\n低レイテンシが要求されており、near real time での table 更新を行う 次のような schema のレコードが IoT デバイスから送信され得られる状態になっているものとする device_id: デバイスに対して一意に振られている ID operation: upsert or delete (後述) temperature: 測定された温度の値 ts: 測定時の timestamp IoT デバイスは登録削除されることがあり、その場合は table 上の当該デバイス ID のレコードを削除する 削除の場合は operation = 'delete' となっている。それ以外は 'upsert' 送られてくるレコードは timestamp (ts) の順になっているとは限らない (out of order) リアルタイム処理のフレームワークとしては Spark Structured Streaming を使用するものとする。\ntable 更新の実装 上記実験のための実装を行った。\n各言語・フレームワーク等は以下のバージョンを使っている。\nScala 2.13.10 Java 11 Spark 3.3.2 Iceberg 1.2.1 実行可能な sbt project の全体を GitHub repository soonraah/streaming_iceberg に置いているのでご参考まで。\n実装の主要な部分を次に示す。\n// Create SparkSession instance val spark = SparkSession .builder() .appName(\u0026#34;StreamingIcebergExperiment\u0026#34;) .master(\u0026#34;local[2]\u0026#34;) .config(\u0026#34;spark.sql.extensions\u0026#34;, \u0026#34;org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog\u0026#34;, \u0026#34;org.apache.iceberg.spark.SparkCatalog\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog.type\u0026#34;, \u0026#34;hadoop\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog.warehouse\u0026#34;, \u0026#34;data/warehouse\u0026#34;) .getOrCreate() spark.sparkContext.setLogLevel(\u0026#34;WARN\u0026#34;) import spark.implicits._ // Drop table spark .sql( \u0026#34;\u0026#34;\u0026#34;drop table if exists | my_catalog.my_db.device_temperature |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) // Create table spark .sql( \u0026#34;\u0026#34;\u0026#34;create table | my_catalog.my_db.device_temperature |( | device_id int, | operation string, | temperature double, | ts timestamp |) |using iceberg |tblproperties ( | \u0026#39;format-version\u0026#39; = \u0026#39;2\u0026#39;, | \u0026#39;write.delete.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.update.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.merge.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39; |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) val random = new Random() val addOutOfOrderness = udf { (timestamp: Timestamp) =\u0026gt; // add time randomly in [-5, +5) sec val ms = timestamp.getTime + random.nextInt(10000) - 5000 new Timestamp(ms) } // Prepare input data val dfInput = spark .readStream .format(\u0026#34;rate\u0026#34;) .option(\u0026#34;rowsPerSecond\u0026#34;, 1) .load() .select( // Generate device_id in 0~127 randomly (rand() * 128).cast(IntegerType).as(\u0026#34;device_id\u0026#34;), // 90% of operation is upsert and 10% is delete when(rand() \u0026gt; 0.1, \u0026#34;upsert\u0026#34;).otherwise(\u0026#34;delete\u0026#34;).as(\u0026#34;operation\u0026#34;), // Generate temperature randomly based on standard normal distribution (randn() * 5.0 + 20.0).as(\u0026#34;temperature\u0026#34;), // timestamp is out of order addOutOfOrderness($\u0026#34;timestamp\u0026#34;).as(\u0026#34;ts\u0026#34;) ) // Update table for each mini batch dfInput .writeStream .trigger(Trigger.ProcessingTime(30.seconds)) .foreachBatch { (dfBatch: DataFrame, batchId: Long) =\u0026gt; println(s\u0026#34;Processing batchId=$batchId\u0026#34;) // Eliminate duplicated device_id val dfDedup = dfBatch .withColumn( \u0026#34;row_num\u0026#34;, row_number() .over(Window.partitionBy($\u0026#34;device_id\u0026#34;).orderBy($\u0026#34;ts\u0026#34;.desc)) ) .where($\u0026#34;row_num\u0026#34; === 1) .drop($\u0026#34;row_num\u0026#34;) // createOrReplaceTempView() doesn\u0026#39;t work dfDedup.createOrReplaceGlobalTempView(\u0026#34;input\u0026#34;) // Insert, update and delete records by \u0026#39;merge into\u0026#39; spark .sql( \u0026#34;\u0026#34;\u0026#34;merge into | my_catalog.my_db.device_temperature |using( | select | * | from | global_temp.input |) as input |on | device_temperature.device_id = input.device_id | and device_temperature.ts \u0026lt; input.ts |when matched and input.operation = \u0026#39;upsert\u0026#39; then update set * |when matched and input.operation = \u0026#39;delete\u0026#39; then delete |when not matched then insert * |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) () } .start .awaitTermination() spark.stop() この中の各処理を以下で説明していく。\n// Create SparkSession instance val spark = SparkSession .builder() .appName(\u0026#34;StreamingIcebergExperiment\u0026#34;) .master(\u0026#34;local[2]\u0026#34;) .config(\u0026#34;spark.sql.extensions\u0026#34;, \u0026#34;org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog\u0026#34;, \u0026#34;org.apache.iceberg.spark.SparkCatalog\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog.type\u0026#34;, \u0026#34;hadoop\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog.warehouse\u0026#34;, \u0026#34;data/warehouse\u0026#34;) .getOrCreate() Spark から Iceberg を使うためには extensions の設定、および catalog の設定が必要となる。\nこれらは設定ファイルやコマンドラインからも指定できるが、今回は実験のためソースコード上で簡易に設定した。\n// Create table spark .sql( \u0026#34;\u0026#34;\u0026#34;create table | my_catalog.my_db.device_temperature |( | device_id int, | operation string, | temperature double, | ts timestamp |) |using iceberg |tblproperties ( | \u0026#39;format-version\u0026#39; = \u0026#39;2\u0026#39;, | \u0026#39;write.delete.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.update.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.merge.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39; |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) table 作成のクエリを実行している。\n頻度の高い更新・削除を想定しているため、ここでは各操作に対して copy-on-write ではなく merge-on-read を指定している。\nこれにより書き込み時にデータファイル全体をコピーするのではなく、差分のみ追加するという形となり書き込みのコストが小さくなる。\nmerge-on-read は format version 2 でしか指定できないため、その設定もしている。\n// Prepare input data val dfInput = spark .readStream .format(\u0026#34;rate\u0026#34;) .option(\u0026#34;rowsPerSecond\u0026#34;, 1) .load() .select( // Generate device_id in 0~127 randomly (rand() * 128).cast(IntegerType).as(\u0026#34;device_id\u0026#34;), // 90% of operation is upsert and 10% is delete when(rand() \u0026gt; 0.1, \u0026#34;upsert\u0026#34;).otherwise(\u0026#34;delete\u0026#34;).as(\u0026#34;operation\u0026#34;), // Generate temperature randomly based on standard normal distribution (randn() * 5.0 + 20.0).as(\u0026#34;temperature\u0026#34;), // timestamp is out of order addOutOfOrderness($\u0026#34;timestamp\u0026#34;).as(\u0026#34;ts\u0026#34;) ) ここでは IoT デバイスから送信されたとするストリームデータを作成している。\nまずは .readStream.foramt(\u0026quot;rate\u0026quot;) によりストリームを発生させ、select() の部分で想定する schema にして値を入れている。\n詳細はコメントを参照。\n1点だけ補足すると、addOutOfOrderness() の部分では timestamp に対してわざと±5秒以内のランダムなゆらぎを与えおり、レコードが timestamp どおりに流れてこない状況を作っている。\ndfInput .writeStream .trigger(Trigger.ProcessingTime(30.seconds)) .foreachBatch { ... } .start .awaitTermination() ストリームデータは foreachBatch により、30秒ごとの mini batch の単位で処理される。\nこの mini batch の中で table 更新を行う。\n// Eliminate duplicated device_id val dfDedup = dfBatch .withColumn( \u0026#34;row_num\u0026#34;, row_number() .over(Window.partitionBy($\u0026#34;device_id\u0026#34;).orderBy($\u0026#34;ts\u0026#34;.desc)) ) .where($\u0026#34;row_num\u0026#34; === 1) .drop($\u0026#34;row_num\u0026#34;) Iceberg の merge into (後述) では更新をかける側の table において結合のキーとなる column の重複は許されず、重複があった場合は次のようなエラーになってしまう。\norg.apache.spark.SparkException: The ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table. This could result in the target row being operated on more than once with an update or delete operation and is not allowed. これを避けるため結合のキーとなる device_id ごとに一意となるよう、最新の行だけ残して重複を排除している。\n// Insert, update and delete records by \u0026#39;merge into\u0026#39; spark .sql( \u0026#34;\u0026#34;\u0026#34;merge into | my_catalog.my_db.device_temperature |using( | select | * | from | global_temp.input |) as input |on | device_temperature.device_id = input.device_id | and device_temperature.ts \u0026lt; input.ts |when matched and input.operation = \u0026#39;upsert\u0026#39; then update set * |when matched and input.operation = \u0026#39;delete\u0026#39; then delete |when not matched then insert * |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) はい、ここが update, insert, delete の最もキモとなるところ。\n個人的には merge into という SQL 構文は知らなかったのだが、どうやら Iceberg 特有のものでもないらしい。\ntable device_temperature に対して update, delete, insert を行う SQL となっている。\nこのクエリが実行されると Iceberg の table が更新され、snapshot が1つ前に進むことになる。\nDDL のところで merge-on-read を指定したため、差分のみのファイルが作成される。\ntable 更新の確認 上記のコードを実行すると、table の更新処理が始まる。\nしばらく流した後、SparkSQL で\nspark .sql( \u0026#34;\u0026#34;\u0026#34;select | * |from | my_catalog.my_db.device_temperature |order by | device_id |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) .show(128, truncate = false) のようなクエリを実行すると、\n+---------+---------+------------------+-----------------------+ |device_id|operation|temperature |ts | +---------+---------+------------------+-----------------------+ |0 |upsert |15.879340359832794|2023-05-08 07:39:56.637| |1 |upsert |20.303530210621492|2023-05-08 07:40:26.157| |2 |upsert |21.07822922592327 |2023-05-08 07:41:10.937| |3 |upsert |20.81228214216027 |2023-05-08 07:37:09.756| |6 |upsert |20.571664434275807|2023-05-08 07:38:43.124| |7 |upsert |19.44731196983606 |2023-05-08 07:39:56.1 | |8 |upsert |27.964942467735458|2023-05-08 07:39:39.623| |9 |upsert |23.319385015293673|2023-05-08 07:40:59.377| |10 |upsert |22.392313247902365|2023-05-08 07:40:40.946| ... のような結果が得られる。\n少し間をおいて実行すると中身が変わっており、更新されていることがわかる。\nまた、ある程度時間を経た後においても欠番があり (ex. 上記の device_id = 4, 5)、削除も行われていることが確認できる。\n雑感 Apache Hudi に比べて動作が素直でわかりやすいように感じた。\n一方で Hudi で言うところの PRECOMBINE_FIELD のようなものがなく、自分で重複排除する必要がありちょっと面倒。\n必然的にタイムトラベルできる粒度は mini batch の粒度となる。\nmerge-on-read の table では差分ファイルが大きくなるにつれ、読み取りのコストも大きくなっていく。\n上記の更新コードを一晩実行した後だと、単純な select に6分程度かかってしまった。\n最新 snapshot としてはレコード数はたかだか128程度なのでこれはかなり遅い。\nというところで compaction などの table のメンテナンス操作が必要となってくる。\nこれについては次のポストで扱いたい。\n","permalink":"https://soonraah.github.io/posts/update-iceberg-table-in-near-real-time/","summary":"\u003cp\u003eApache Iceberg の table を near real time に、つまり高頻度で更新するということをやってみた。\u003c/p\u003e\n\u003ch2 id=\"apache-iceberg-とは\"\u003eApache Iceberg とは\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://iceberg.apache.org/\"\u003eApache Iceberg\u003c/a\u003e (以下 Iceberg) は分散ファイルシステムやクラウドストレージ上の table format であり、Apache Hudi や Delta Lake と並んで data lake や lakehouse architecture で用いられる。\u003cbr\u003e\n特徴的なのは table とデータ実体 (Parquet, Avro など) の間に metadata file, manifest list, manifest file の抽象的なレイヤーがあり、ファイル単位で table の状態を track できること。\u003cbr\u003e\nこれにより強い isolation level、パフォーマンス、schema evolution など様々な機能・性能を実現できるようになっている。\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\n    \u003cimg loading=\"lazy\" src=\"/image/iceberg/iceberg_metadata.png\"\n         alt=\"Apache Iceberg\"/\u003e \u003cfigcaption\u003e\n            \u003cp\u003eApache Iceberg\n                    \u003ca href=\"https://iceberg.apache.org/spec/\"\u003eIceberg Table Spec\u003c/a\u003e\u003c/p\u003e\n        \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003e詳しくは公式ドキュメントを参照のこと。\u003cbr\u003e\n最近では SmartNews 社が Iceberg で data lake を構築したことを記事にしていた。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://medium.com/smartnews-inc/flink-based-iceberg-real-time-data-lake-in-smartnews-part-i-19a7628ce110\"\u003eFlink-based Iceberg Real-Time Data Lake in SmartNews (Part I) | by SmartNews | SmartNews, Inc | Apr, 2023 | Medium\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eベンダー提供の DWH 中心ではなく Lakehouse Architecture を目指すのであれば最有力の table format の1つだと言えそう。\u003c/p\u003e","title":"Apache Iceberg の table を near real time で更新する"},{"content":"データエンジニアリングの領域で少し前から目にするようになった \u0026ldquo;data contract\u0026rdquo; という言葉。\nなんとなく今の業務で困っている課題の解決になりそうな気がしつつもよくわかっていなかったので調べてみた。\ndata contract について語られているいくつかのブログ記事などを参考にしている。\nData Contract とは データの schema というのはナマモノで、いろいろな理由で変更されることがある。\nschema を変更する場合、その schema のデータ (table や log) が所属する単一のビジネス機能や application のドメインで行われることになる。\nそのドメインの閉じた世界で考える分にはこれで問題ないのだが、DWH や data lake など組織レベルのデータ基盤でデータを流通していた場合はその先のことも考えないといけなくなる。\nこのようにチームを超える影響というのは、ビジネス機能に責任を持っているチームからは見えにくくなっていることが多い。\n上流の application 側で schema を変更したら下流のデータ基盤の ETL 処理がぶっ壊れてしまった、というのはデータ基盤運用あるあるではないだろうか。\nというところを解決して平和に過ごせるようにすることが data contract の主なモチベーションだと思われる。\n\u0026ldquo;contract\u0026rdquo; は日本語で言うところの「契約」。\n組織におけるデータ流通において、データの送り手である producer 側と受け手である consumer 側との間で合意した契約を遵守することにより、前述のような問題を避けることができるというのが data contract である。\n組織内のデータの見通しがよくなったり、パイプラインを宣言的に開発することができるようになるというメリットもある。\nエンジニアにとっては Datafold のブログ記事の例を読むとイメージしやすいかもしれない。\nTo provide another analogy, data contracts are what API is for the web services. Say we want to get data from Twitter. One way is to scrape it by downloading and parsing the HTML of Twitter’s webpage. This may work, but our scraper will likely break occasionally, if Twitter, for instance, changes a name of a CSS class or HTML structure. There is no contract between Twitter’s web page and our scraper. However, if we access the same data via Twitter’s API, we know exactly the structure of the response we’re going to get. An API has required inputs, predictable outputs, error codes, SLAs (service level agreements – e.g. uptime), and terms of use, and other important properties. Importantly, API is also versioned which helps ensure that changes to the API won’t break end user’s applications, and to take advantage of those changes users would graciously migrate to the new version.\nThe Best Data Contract is the Pull Request - datafold.com\nTwitter から情報を取ることを考えたとき、scraping する場合は UI の変更などで処理が壊れてしまうがちゃんと定義された API を使う場合は処理が壊れない。\nこの違いは約束事があるかどうかであり、data contract がない場合は約束事がない scraping のようなものだよね、という話。\n最初の例ではデータ基盤に流れてくるデータの schema が変わることを示したが consumer は中央集権的なデータ基盤である必要はなく、どちらかというと data mesh のような非中央集権的なコンテキストで話題に上がる方が多い。\nまた契約の対象となるのは schema だけではなく、次のような様々な情報を含んでよい。\nschema 所有者 利用者 セマンティクス 更新頻度 etc. 扱っている情報としては data catalog に近かったりもするが、data catalog はどうなっているかを示しているのに対して data contract はどうなっているべきかを示すという違いがある。\nData Contract の概況 data contract についてはいろいろな人が語っているが、なんとなく現時点でコミュニティにおいてある程度合意がありそうなことをいくつか挙げる。\n議論はまだまだこれからという感じなので、これらは時間が経つと変わっていく可能性が高いことに注意。\nスタンダードになるようなサービスや製品はまだない まるっと data contract をやってくれるようなサービスや製品はまだないので、data contract をやりたければある程度自社で作り込んだりルール整備したりする必要がある。\nこのようなサービスや製品がまだない理由については The Case for Data Contracts - uncomfortablyidiosyncratic.substack.com で議論があったのでご興味のある方はご確認ください。\nSchemata という OSS の schema modeling のフレームワークがあるがそんなに広まっていない？\nschema metadata についてはわかるけど schema score がどう使われるのかがイメージつかない。\nschema は IDL で表現して data contract に含める data contract の対象の中ではデータの schema は重要視されている。\nschema は IDL (Interface Description Language) で記述される。\nIDL として代表的なものとして Protocol Buffer と Apache Avro が挙げられる。\nschema を記述するための規定の方法が open source で提供されており、これを使ってレコード単位のチェックを行うことができる。\n他にも JSON Schema もあるが、DB の世界との間で型の扱いに差があるので個人的にはおすすめできない。\n契約の形は様々 契約 (contract) の形は data contract を語る人によっていろいろあってこれが議論になっている模様。\nproducer 側が主体的な場合もあれば consumer 側が主体的な場合もある。\n比較的よく挙げられれているのが pull-request のレビューをもって契約の合意とするという方式。\ndata contract が規定の形式のドキュメントになっていれば、その追加や変更についての二者間の合意はエンジニアにとって慣れ親しんだ pull-request という形で行うことができる。\nThe Best Data Contract is the Pull Request - datafold.com の記事が正にこれについて書かれたものとなっている。\n後述の実装例でもいくつか紹介する。\nData Contract の実装例 そうは言うてもどないして data contract を実現するんや？ってなりますよね。\nというところでその例をいくつか以下に記載してみる。\n詳細はリンク先を参照。\n例 1 An Engineer\u0026rsquo;s Guide to Data Contracts - Pt. 1 - dataproducts.substac.com\nentity、つまり DB の table の変更を流通させる場合の例 Pt. 2 の記事では event log の流通について書かれている data contract は Protocol Buffers で記載 data contract は CI/CD でテストや互換性チェックが行われ、Schema Registry に登録される production 環境では table の変更が CDC で Kafka の topic に送られている Kafka に送られたレコードを KSQL や Flink のフレームワーク + Schema Registry でチェックして新たな topic に transactional outbox pattern についても配慮 例 2 Aurimas Griciūnas’ Post - LinkedIn\n別の例だが例 1 に近い やはり Kafka や Flink を利用 例 3 Implementing Data Contracts at GoCardless - Medium\nPub/Sub や BigQuery に入る前に data contract による validation が行われる データ所有者が Jsonnet で定義された data contract を Git で merge すると、必要なリソースやサービスが生成される 宣言的でかっこいい 契約の主体が producer 側、つまりデータ所有者に寄っている印象 所有権を意識させる 例 4 Fine, let\u0026rsquo;s talk about data contracts - benn.substack.com\ndbt を使った例 なのでバッチ処理的なチェック 契約の主体が consumer 側、つまりデータ基盤に寄っている印象 (data contract というよりもデータ品質テストに近い気が) 所感 私がお仕事で運用しているデータ基盤では、最初に挙げたあるあるのような上流のデータの問題で ETL が影響を受けるということが度々あり課題となっていた。\nなので data contract という概念には期待がある。\n前回の記事 では Glue Schema Registry の導入を諦めた旨を書いたが、data contract という文脈で再度 Glue Schema Registry の採用を検討してみてもいいかもしれない。\n一方、国内ではあまり data contract の事例を聞いたことがなく、この辺不思議に思っている。\n理由としては\nこのような課題が存在しない -\u0026gt; んなわけない 課題はあるが認識されていない -\u0026gt; あるかもしれない 認識されて data contract 的なことをしているが data contract という言葉が使われていない -\u0026gt; 某社の事例を噂で聞いたことがある などが考えられる。\n国内企業の事例も聞いてみたいところだし、自分のところで事例を出せるといいなとも思っている。\nあと CDC とかのリアルタイム処理はもう当たり前になってるんだな。\n参考 Data Contracts — From Zero To Hero The Best Data Contract is the Pull Request The Case for Data Contracts An Engineer\u0026rsquo;s Guide to Data Contracts - Pt. 1 Aurimas Griciūnas Post - LinkedIn Implementing Data Contracts at GoCardless Fine, let\u0026rsquo;s talk about data contracts ","permalink":"https://soonraah.github.io/posts/looked-into-data-contracts/","summary":"\u003cp\u003eデータエンジニアリングの領域で少し前から目にするようになった \u0026ldquo;data contract\u0026rdquo; という言葉。\u003cbr\u003e\nなんとなく今の業務で困っている課題の解決になりそうな気がしつつもよくわかっていなかったので調べてみた。\u003cbr\u003e\ndata contract について語られているいくつかのブログ記事などを参考にしている。\u003c/p\u003e\n\u003ch2 id=\"data-contract-とは\"\u003eData Contract とは\u003c/h2\u003e\n\u003cp\u003eデータの schema というのはナマモノで、いろいろな理由で変更されることがある。\u003cbr\u003e\nschema を変更する場合、その schema のデータ (table や log) が所属する単一のビジネス機能や application のドメインで行われることになる。\u003cbr\u003e\nそのドメインの閉じた世界で考える分にはこれで問題ないのだが、DWH や data lake など組織レベルのデータ基盤でデータを流通していた場合はその先のことも考えないといけなくなる。\u003cbr\u003e\nこのようにチームを超える影響というのは、ビジネス機能に責任を持っているチームからは見えにくくなっていることが多い。\u003cbr\u003e\n上流の application 側で schema を変更したら下流のデータ基盤の ETL 処理がぶっ壊れてしまった、というのはデータ基盤運用あるあるではないだろうか。\u003c/p\u003e\n\u003cp\u003eというところを解決して平和に過ごせるようにすることが data contract の主なモチベーションだと思われる。\u003cbr\u003e\n\u0026ldquo;contract\u0026rdquo; は日本語で言うところの「契約」。\u003cbr\u003e\n組織におけるデータ流通において、データの送り手である producer 側と受け手である consumer 側との間で合意した契約を遵守することにより、前述のような問題を避けることができるというのが data contract である。\u003cbr\u003e\n組織内のデータの見通しがよくなったり、パイプラインを宣言的に開発することができるようになるというメリットもある。\u003c/p\u003e\n\u003cp\u003eエンジニアにとっては Datafold のブログ記事の例を読むとイメージしやすいかもしれない。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTo provide another analogy, data contracts are what API is for the web services. Say we want to get data from Twitter. One way is to scrape it by downloading and parsing the HTML of Twitter’s webpage. This may work, but our scraper will likely break occasionally, if Twitter, for instance, changes a name of a CSS class or HTML structure. There is no contract between Twitter’s web page and our scraper. However, if we access the same data via Twitter’s API, we know exactly the structure of the response we’re going to get. An API has required inputs, predictable outputs, error codes, SLAs (service level agreements – e.g. uptime), and terms of use, and other important properties. Importantly, API is also versioned which helps ensure that changes to the API won’t break end user’s applications, and to take advantage of those changes users would graciously migrate to the new version.\u003c/p\u003e","title":"Data Contract について調べた"},{"content":"業務で AWS Glue Schema Registry を使おうとしたけど、やっぱりやめたというお話。\nGlue Schema Registry What\u0026rsquo;s Schema Registry? AWS Glue Schema Registry は2020年に発表された AWS の機能だ。\nControl the evolution of data streams using the AWS Glue Schema Registry 一方、私が最初に schema registry 的なものを見たのは Confluent の例。\nSchema Registry の概要 - Confluent AWS の Glue Schema Registry はこれより後のリリースであり、同等のものの AWS マネージド版といったところだろうか。\nschema registry で何ができるかは Confluent のリンク先の図がとてもわかりやすいので参考にしていただきたい。\nGlue Schema Registry もだいたい同じで、ストリーム処理のための機能である。\nGlue Schema Registry で解決したい課題とその機能 データ基盤上のストリーム処理における schema 管理はバッチ処理のそれとは異なる難しさがある。\nこれは schema evolution と呼ばれる問題で以前のポストでも述べている。\nバッチ処理おじさんがストリーム処理のシステムを開発するにあたって調べたこと 難しい点として以下のようなことが挙げられる。\n処理は常に行われており、producer の application 等をしばらく停止させない限りはオンラインで schema 変更をするしかない ただしデータ基盤起因で producer を止めることはなるべくしたくない オンラインの場合、out of orderness に配慮する必要がある deploy 順序、ネットワーク遅延など様々な要因から、新しい schema のレコードが古い schema のレコードより後に届くとはかぎらない consumer 側で新旧 schema に対する互換性 (compatibility) に配慮する必要がある Glue Schema Registry はこの課題に対応する。\nざっと説明すると次のようにして利用される。\nbroker (Apache Kafka, Amazon Kinesis Data Stremas, etc.) を挟んで producer と consumer の両方から参照できる場所にバージョン管理された schema 定義を配置する (これが Glue Schema Registry) producer からレコードを serialize して送信。レコードには schema のバージョン情報を含む consumer はレコードに含まれるバージョン情報と schema registry を参照してレコードの deserialize・解釈を行う これを実現するために Glue Schema Registry では次のような機能が提供されている。\nschema の登録と互換性ルールにもとづく schema のバージョン管理 producer, consumer で利用される SerDe ライブラリの提供 producer 側でのレコード検証、serialize、配信 consumer 側での deserialize これによりストリーム処理における producer と consumer の間で schema 変更を運用することが可能になる。\nやりたかったこと …という Glue Schema Registry が自分の業務でも使えるのではないか、そんなふうに考えていた時期が私にもありました。\n現在私は B to B to C のビジネスをやっている企業でデータ基盤の開発・運用を行うチームに所属している。\nその中で、ある application から発生する JSON 形式のログを near real time でデータ基盤に取り込み、分析したいという案件があった。\n以下のようなアーキテクチャを考えた。\napplication 側から SerDe ライブラリを使い、Kinesis Data Streams (KDS) に逐次的にレコードを配信 Spark Structured Streaming で Glue streaming ETL の job を実装し、KDS を購読してSerDe ライブラリ経由でデータ取得 Spark Structured Streaming から Apache Hudi の形式 (Merge on Read) で S3 へと保存 schema の変更がある場合は Glue Data Catalog 上の Hudi table の schema を変更 Athena でクエリ (ちなみに Glue Schema Registry のアイコンは AWS では提供されていない)\napplication はビジネスロジックに近いシステムを担当しているチームの管理であり、それ以降がデータ基盤チームの管理となっている。\nnear real time 以外でもデータソース側とデータ基盤側の間での schema 管理は以前から課題があり、この Glue Schema Registry を使えばうまくやる前例を作れるのではという期待があった。\n4 の schema 変更は自動で行われ、したがって application ログの schema と Hudi table の schema を別で管理・メンテする必要がない。\napplication 側とデータ基盤側の中間で schema を定義・管理できるのが素晴らしい、そんなふうに考えていた時期が私にもありました。\n断念した理由 が、結局は Glue Schema Registry の採用は断念するという結論になった。\n次のような理由にもとづく。\nSerDe 導入の壁 JSON Schema の折り合い 互換性の運用の難しさ 以下、それぞれについて述べる。\nSerDe 導入の壁 前述のようなアーキテクチャを実現するには、application 側のチームに SerDe ライブラリを導入してもらう必要がある。\nSerDe は Java のライブラリとして提供されているため、application 側が Java 系の言語で開発されている必要がある。\nここに大きなハードルがあると感じた。\nちなみに KDS まわりであれば SDK や KPL 経由で使うことも可能である。\n今回は application は Scala だったので、頼み込めば今回の案件については導入してもらうこともできたかもしれない。\nただ、うまくいけば今回の案件に限らず横展開したいと考えていたので、それを考えるとかなりハードルが高いという判断となった。\nJSON Schema の折り合い application とデータ基盤の間の schema 管理については data contract という考え方が出てきている。\nそれについて書かれたブログ記事 7 Lessons From GoCardless’ Implementation of Data Contracts で\nFor example, most teams didn’t want to use AVRO so we decided to use JSON as the interchange format for the contracts because it’s extensible.\nとあるように、Avro 形式を各 application チームに使ってもらうというのはまあまあハードルが高い。\nGlue Schema Registry ではレコードの形式として Avro, Protobuf, JSON をサポートしている。\n本来はできれば Avro や Protobuf のような、型がしっかりしている形式を使いたいところだが、このようなハードルがあり我々も JSON の仕様を前提に考えていた。\nJSON の場合は JSON Schema の仕様に従うことになる。\nこの JSON Schema、当たり前だが JSON の世界を表現するためのものとなっている。\n何が言いたいかというと JSON の世界の型の表現と Hudi table を含めた DB 的な世界の型の表現とが一致しないのだ。\n例えば数値表現を見てみると、JSON Schema においては integer, number, Multiples, Range の4つの型が定義されている。\n一般的な整数型 short, int, long 等の違いを表現することができない。\nGlue Schema Registry に登録された JSON Schema により Hudi の table の schema を自動更新したかったので、これは困ったぞ、となってしまった。\n互換性の運用の難しさ これはちょっと言葉にしにくいのだが、前方／後方互換性まわりの仕様が直感的に理解しにくかった。\nどういったときにフィールド追加が許容されるのか、互換性の起点となる checkpoint はどういったときに更新されるのか、etc.\nこのあたりいろいろ調査すれば理解できると思うのだが、ここまでの課題もあり気持ちが折れてしまった。\nまた、チームとしてこれを踏まえた schema 更新を運用するのはハードルが高いと感じた。\n結局どうしたか Glue Schema Registry は使わず、では結局どうしたかというと application 側とデータ基盤側の間でシステム的に schema を共有することなく連携する形にした。\nシステム的に共有しないので、人が共有してやるしかない。\nschema 変更する場合は次のようにチーム間で協調して進める。\nチーム間で認識合わせ データ基盤チームによる Glue Data Catalog の schema の変更作業 consumer に前方互換性があるという前提 application チームによる application ログの schema の変更作業 このあたりもっとうまくできる方法があるのではと考えている。\nとりあえず data contract の発展に期待しておく。\nまとめ ストリーム処理における Glue Schema Registry の思想、考え方は正しいものであると感じる。\nただし、扱いが難しい面があり、今回のユースケースやそれを踏まえた横展開には合わないという判断となった。\nチームやシステムをまたいだ schema 管理は簡単ではない。\n","permalink":"https://soonraah.github.io/posts/give-up-on-schema-registry/","summary":"\u003cp\u003e業務で \u003ca href=\"https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html\"\u003eAWS Glue Schema Registry\u003c/a\u003e を使おうとしたけど、やっぱりやめたというお話。\u003c/p\u003e\n\u003ch2 id=\"glue-schema-registry\"\u003eGlue Schema Registry\u003c/h2\u003e\n\u003ch3 id=\"whats-schema-registry\"\u003eWhat\u0026rsquo;s Schema Registry?\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html\"\u003eAWS Glue Schema Registry\u003c/a\u003e は2020年に発表された AWS の機能だ。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://aws.amazon.com/about-aws/whats-new/2020/11/control-evolution-data-streams-using-aws-glue-schema-registry/\"\u003eControl the evolution of data streams using the AWS Glue Schema Registry\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e一方、私が最初に schema registry 的なものを見たのは Confluent の例。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://docs.confluent.io/ja-jp/platform/7.1.1/schema-registry/index.html\"\u003eSchema Registry の概要 - Confluent\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAWS の Glue Schema Registry はこれより後のリリースであり、同等のものの AWS マネージド版といったところだろうか。\u003cbr\u003e\nschema registry で何ができるかは Confluent のリンク先の図がとてもわかりやすいので参考にしていただきたい。\u003cbr\u003e\nGlue Schema Registry もだいたい同じで、ストリーム処理のための機能である。\u003c/p\u003e\n\u003ch3 id=\"glue-schema-registry-で解決したい課題とその機能\"\u003eGlue Schema Registry で解決したい課題とその機能\u003c/h3\u003e\n\u003cp\u003eデータ基盤上のストリーム処理における schema 管理はバッチ処理のそれとは異なる難しさがある。\u003cbr\u003e\nこれは schema evolution と呼ばれる問題で以前のポストでも述べている。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"../study-streaming-system/#schema-evolution\"\u003eバッチ処理おじさんがストリーム処理のシステムを開発するにあたって調べたこと\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e難しい点として以下のようなことが挙げられる。\u003c/p\u003e","title":"Glue Schema Registry の導入を断念した話"},{"content":"「データレイク」という言葉は使う人によって異なった意味があるように感じており、気になっていた。\nこのポストではアーキテクチャ目線でのデータレイクと内容物目線でのデータレイクの違いについて書いてみる。\n便宜上前者を「データレイク」、後者を「データレイク層」と呼ぶことにする。\nアーキテクチャ目線の「データレイク」 「データレイク」については以前こちらのポストで書いたのでここでは詳しく触れない。\n詳細はリンク先を見ていただきたい。\nここでキーとなるのが、\n加工前データや非構造化データを含むあらゆるデータを保存 一元的なデータ管理 という部分だ。\nあらゆるデータを一元的に管理するという思想であり、これができるアーキテクチャがデータレイクということだ。\n例えば AWS や Azure のドキュメントを見るとデータレイクの中が zone に分けられており、生データを保持する raw zone や加工されたデータを置いておく curated zone などがある。\n(zone の命名にもいくつかの流派があるようだ…)\nReference architecture - Data Analytics Lens Data lake zones and containers - Cloud Adoption Framework | Microsoft Docs 次の Robinhood 社の例でもデータレイク中に生データとその派生データが存在している。\nFresher Data Lake on AWS S3 | by Balaji Varadarajan | Robinhood 内容物目線の「データレイク層」 一方でデータレイクには生データのみを置くべき、という考えもある。\n本書におけるデータレイク（DataLake）層とは、元のデータをコピーして、1つのシステムに集約したものを指します。 データソース（＝水源）から流れてきたデータを蓄える場所なのでレイク（湖）と呼びます。\nECサイトの注文履歴データを、分析用DBにコピーしている場合、それがデータレイクと言えます。データレイクのデータは、データソースと一対一の関係にあります。何も加工していない、ただのコピーだからです。\n何も加工していない、ただのコピーであることが重要です。仮にデータの中身に誤りがあったとしても、修正や加工をせず、そのまま集約しましょう。\n\u0026ndash; ゆずたそ,渡部 徹太郎,伊藤 徹郎. 実践的データ基盤への処方箋〜 ビジネス価値創出のためのデータ・システム・ヒトのノウハウ (Japanese Edition) (pp.57-58). Kindle 版.\nこちらの書籍にならってこのポストではこれを「データレイク層」と呼ぶことにする。\nこの考え方では生データの「データレイク層」の他に加工されたデータを置くための「DWH 層」「データマート層」がある。\n本書においてはアーキテクチャというよりは中に何を入れるかで「データレイク層」が規定される。\nこの「データレイク層」の考えは日本企業でよく見かける気がしている。(が、私が知らないだけで海外事例もあるかもしれない)\n以下はその例。\n分析者から見た使いにくいデータ基盤の話 | リクルート　メンバーズブログ データの民主化を目指して ~ データ基盤ができるまで ~ - LIFULL Creators Blog 2024-02-04 追記 海外でも「データレイク層」の意味で data lake という言葉が使われているケースがある模様。\nSnowflake data zones : r/snowflake 「データレイク」と「データレイク層」の比較 「データレイク」も「データレイク層」もやっていることは同じで、ただ何を「データレイク (層)」と呼んでいるかが違っている。\nどちらも生データ、加工データ等を zone や層として分けて管理する。\n以下のようなニュアンスの違いがあると認識している。\nデータレイク データレイク層 内容物 生データ、加工データ 生データのみ アーキテクチャ オブジェクトストレージ等で一元的 データレイク層と DWH 層は別でもよい 使われている場所 海外 国内？ 個人的には「データレイク層」は「raw data 層」というような命名の方が混乱を避けつつ実を表しておりいいのではという感じがする。\n(つまり raw zone ですね…)\n一方で「データレイク層」と呼びたい気持ちもわかる。\nまとめ 誰かが「データレイク」についてしゃべっているときはどちらの「データレイク」のことを言っているのか、気をつけた方がよい。\nもっと言うと世の中にはまた別の定義もあるかもしれない。\n私がこれまで一緒に仕事をしたエンジニアの中でも優秀な人たちは言葉の定義に敏感な人が多かった。\nこういったところ気をつけていきたい。\n","permalink":"https://soonraah.github.io/posts/data-lake-and-data-lake-layer/","summary":"\u003cp\u003e「データレイク」という言葉は使う人によって異なった意味があるように感じており、気になっていた。\u003cbr\u003e\nこのポストではアーキテクチャ目線でのデータレイクと内容物目線でのデータレイクの違いについて書いてみる。\u003cbr\u003e\n便宜上前者を「データレイク」、後者を「データレイク層」と呼ぶことにする。\u003c/p\u003e\n\u003ch2 id=\"アーキテクチャ目線のデータレイク\"\u003eアーキテクチャ目線の「データレイク」\u003c/h2\u003e\n\u003cp\u003e「データレイク」については以前\u003ca href=\"../what-is-a-data-lake\"\u003eこちらのポスト\u003c/a\u003eで書いたのでここでは詳しく触れない。\u003cbr\u003e\n詳細はリンク先を見ていただきたい。\u003cbr\u003e\nここでキーとなるのが、\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cul\u003e\n\u003cli\u003e加工前データや非構造化データを含むあらゆるデータを保存\u003c/li\u003e\n\u003c/ul\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cul\u003e\n\u003cli\u003e一元的なデータ管理\u003c/li\u003e\n\u003c/ul\u003e\u003c/blockquote\u003e\n\u003cp\u003eという部分だ。\u003cbr\u003e\nあらゆるデータを一元的に管理するという思想であり、これができるアーキテクチャがデータレイクということだ。\u003c/p\u003e\n\u003cp\u003e例えば AWS や Azure のドキュメントを見るとデータレイクの中が zone に分けられており、生データを保持する raw zone や加工されたデータを置いておく curated zone などがある。\u003cbr\u003e\n(zone の命名にもいくつかの流派があるようだ…)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/modern-data-architecture-reference-architecture.html\"\u003eReference architecture - Data Analytics Lens\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/scenarios/cloud-scale-analytics/best-practices/data-lake-zones\"\u003eData lake zones and containers - Cloud Adoption Framework | Microsoft Docs\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e次の Robinhood 社の例でもデータレイク中に生データとその派生データが存在している。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://robinhood.engineering/author-balaji-varadarajan-e3f496815ebf\"\u003eFresher Data Lake on AWS S3 | by Balaji Varadarajan | Robinhood\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"内容物目線のデータレイク層\"\u003e内容物目線の「データレイク層」\u003c/h2\u003e\n\u003cp\u003e一方でデータレイクには生データのみを置くべき、という考えもある。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e　本書におけるデータレイク（DataLake）層とは、元のデータをコピーして、1つのシステムに集約したものを指します。 データソース（＝水源）から流れてきたデータを蓄える場所なのでレイク（湖）と呼びます。\u003cbr\u003e\n　ECサイトの注文履歴データを、分析用DBにコピーしている場合、それがデータレイクと言えます。データレイクのデータは、データソースと一対一の関係にあります。何も加工していない、ただのコピーだからです。\u003cbr\u003e\n　\u003cstrong\u003e何も加工していない、ただのコピーであることが重要です。仮にデータの中身に誤りがあったとしても、修正や加工をせず、そのまま集約しましょう。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u0026ndash; \u003c!-- raw HTML omitted --\u003e\u003ca href=\"\"\u003eゆずたそ,渡部 徹太郎,伊藤 徹郎. 実践的データ基盤への処方箋〜 ビジネス価値創出のためのデータ・システム・ヒトのノウハウ (Japanese Edition) (pp.57-58). Kindle 版.\u003c/a\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e","title":"「データレイク」と「データレイク層」"},{"content":"ポエムです。\n事業フェーズごとのデータサイエンティストの役割 まずはこちらの発表。\n事業立ち上げにデータサイエンティストは必要なのか？ | CA BASE NEXT とても納得できる内容だった。\n一部抜き出して要約すると\n事業の立ち上げフェーズ データがまだなかったり、整備されていない状態 データサイエンスによる改善がしにくい 事業のグロースフェーズ 大規模なデータが使える状態 データサイエンスによる改善がやりやすい とのこと。異論はない。\nでは事業が立ち上がり、グロースが落ち着いたその後の成熟フェーズではどうなのだろうかという話。\n成熟フェーズにおける改善の難しさ 端的に言うと成熟フェーズでは ML によるさらなる改善は困難になってくると思う。\nここで言う成熟フェーズにおいてはプロダクトの進化とともに機械学習もそれなりに適用されてきたものとする。\n成熟フェーズということで既存の ML モデル、特にビジネスインパクトが大きい箇所はこれまでいろいろな改善が重ねられてきている。\nそのモデルの精度をさらに上げるとなると、より高度なアルゴリズム、より複雑なデータ等を扱う必要がある。\nしかし技術的によっぽど大きなブレークスルーがない限りは精度の改善幅はグロースフェーズよりもかなり小さいものとなるだろう。\n精度が上がれば上がるほど、次の1%を上げるためのコストは大きくなっていく。\n改善が進むほどに次の改善業務は困難になっていく。\n(蛇足だがある程度大きな組織でなければ高度で state-of-the-art な ML アルゴリズムは運用しない方がいいと考えている)\nでは既存ではない新しい適用箇所に ML を使えばいいのではとなるかもしれない。\nしかしやはりそれも難しい。\nビジネスインパクトが大きく、かつわかりやすい適用箇所にはおそらくすでに ML が適用されているからだ。\nその状態から更によい適用箇所を見つけるには深いドメイン知識が必要になったりする。\nという感じでいわゆるキラキラした「ML でビジネスをドライブ！」みたいなことは成熟フェーズでは難しいことが多いのではないか。\nしかしデータサイエンティストにやることがないわけではない。\n成熟フェーズで何ができるか ぱっと思いつくのは次のような仕事。\nデータドリブンな施策の立案・評価 これは事業フェーズ問わずあるべき ドメイン知識が必要 ML エンジニアリング パイプラインの改善や属人性をなくすお仕事 ML モデルの受動的なメンテナンス 精度が変化したときの調査 内部的・外部的要因によるデータの変化への対応 やっぱり ML モデルの精度改善 成熟フェーズということでビジネスもスケールしていれば 0.1% の精度改善でも売上的なインパクトは大きいかもしれない いわゆる狭義のデータサイエンスではなく、ドメイン知識であったりアナリストやエンジニア的な視点が絡んだ仕事が増えてくる。\nよくある「ML だけじゃなく◯◯もできると強いよね」みたいな話になってしまった。\nおわりに …という話が少し前に Twitter で知人との話題に上がった。\n若者が歴史的にいろんな人が改善に取り組んできた ML モデルの改善にアサインされている、というのが近いところで観測されたのでたいへんそうだなあと思いつつこの件を思い出したので書いてみた。\nここまで書いて思ったけど、成熟フェーズでキラキラしたことがやりづらくなるのはデータサイエンティストだけじゃないよな。\nでも狭義のデータサイエンスのスキルは特に、事業の存続期間と比べて大きく貢献できる期間が短いのかもしれない、と個人的には考えている。\n成熟フェーズでは高度なスキル、高い賃金に見合ったプロダクトへの貢献が得にくくなっていくのではないだろうろうか。\nでも反論もいっぱいありそうな気はします。\n「ML でビジネスインパクトどっかんどっかんやで！！」みたいな仕事をしたい人はそれなりの頻度で事業部を移る or 転職するというのが賢い動きになるんでしょうかね。\n","permalink":"https://soonraah.github.io/posts/ds-in-maturation-phase/","summary":"\u003cp\u003eポエムです。\u003c/p\u003e\n\u003ch2 id=\"事業フェーズごとのデータサイエンティストの役割\"\u003e事業フェーズごとのデータサイエンティストの役割\u003c/h2\u003e\n\u003cp\u003eまずはこちらの発表。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://ca-base-next.cyberagent.co.jp/sessions/start-up-and-data-science/\"\u003e事業立ち上げにデータサイエンティストは必要なのか？ | CA BASE NEXT\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eとても納得できる内容だった。\u003cbr\u003e\n一部抜き出して要約すると\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e事業の立ち上げフェーズ\n\u003cul\u003e\n\u003cli\u003eデータがまだなかったり、整備されていない状態\u003c/li\u003e\n\u003cli\u003eデータサイエンスによる改善がしにくい\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e事業のグロースフェーズ\n\u003cul\u003e\n\u003cli\u003e大規模なデータが使える状態\u003c/li\u003e\n\u003cli\u003eデータサイエンスによる改善がやりやすい\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eとのこと。異論はない。\u003cbr\u003e\nでは事業が立ち上がり、グロースが落ち着いたその後の成熟フェーズではどうなのだろうかという話。\u003c/p\u003e\n\u003ch2 id=\"成熟フェーズにおける改善の難しさ\"\u003e成熟フェーズにおける改善の難しさ\u003c/h2\u003e\n\u003cp\u003e端的に言うと成熟フェーズでは ML によるさらなる改善は困難になってくると思う。\u003cbr\u003e\nここで言う成熟フェーズにおいてはプロダクトの進化とともに機械学習もそれなりに適用されてきたものとする。\u003c/p\u003e\n\u003cp\u003e成熟フェーズということで既存の ML モデル、特にビジネスインパクトが大きい箇所はこれまでいろいろな改善が重ねられてきている。\u003cbr\u003e\nそのモデルの精度をさらに上げるとなると、より高度なアルゴリズム、より複雑なデータ等を扱う必要がある。\u003cbr\u003e\nしかし技術的によっぽど大きなブレークスルーがない限りは精度の改善幅はグロースフェーズよりもかなり小さいものとなるだろう。\u003cbr\u003e\n精度が上がれば上がるほど、次の1%を上げるためのコストは大きくなっていく。\u003cbr\u003e\n改善が進むほどに次の改善業務は困難になっていく。\u003c/p\u003e\n\u003cp\u003e(蛇足だがある程度大きな組織でなければ高度で state-of-the-art な ML アルゴリズムは運用しない方がいいと考えている)\u003c/p\u003e\n\u003cp\u003eでは既存ではない新しい適用箇所に ML を使えばいいのではとなるかもしれない。\u003cbr\u003e\nしかしやはりそれも難しい。\u003cbr\u003e\nビジネスインパクトが大きく、かつわかりやすい適用箇所にはおそらくすでに ML が適用されているからだ。\u003cbr\u003e\nその状態から更によい適用箇所を見つけるには深いドメイン知識が必要になったりする。\u003c/p\u003e\n\u003cp\u003eという感じでいわゆるキラキラした「ML でビジネスをドライブ！」みたいなことは成熟フェーズでは難しいことが多いのではないか。\u003cbr\u003e\nしかしデータサイエンティストにやることがないわけではない。\u003c/p\u003e\n\u003ch2 id=\"成熟フェーズで何ができるか\"\u003e成熟フェーズで何ができるか\u003c/h2\u003e\n\u003cp\u003eぱっと思いつくのは次のような仕事。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eデータドリブンな施策の立案・評価\n\u003cul\u003e\n\u003cli\u003eこれは事業フェーズ問わずあるべき\u003c/li\u003e\n\u003cli\u003eドメイン知識が必要\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eML エンジニアリング\n\u003cul\u003e\n\u003cli\u003eパイプラインの改善や属人性をなくすお仕事\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eML モデルの受動的なメンテナンス\n\u003cul\u003e\n\u003cli\u003e精度が変化したときの調査\u003c/li\u003e\n\u003cli\u003e内部的・外部的要因によるデータの変化への対応\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eやっぱり ML モデルの精度改善\n\u003cul\u003e\n\u003cli\u003e成熟フェーズということでビジネスもスケールしていれば 0.1% の精度改善でも売上的なインパクトは大きいかもしれない\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eいわゆる狭義のデータサイエンスではなく、ドメイン知識であったりアナリストやエンジニア的な視点が絡んだ仕事が増えてくる。\u003cbr\u003e\nよくある「ML だけじゃなく◯◯もできると強いよね」みたいな話になってしまった。\u003c/p\u003e\n\u003ch2 id=\"おわりに\"\u003eおわりに\u003c/h2\u003e\n\u003cp\u003e…という話が少し前に Twitter で知人との話題に上がった。\u003cbr\u003e\n若者が歴史的にいろんな人が改善に取り組んできた ML モデルの改善にアサインされている、というのが近いところで観測されたのでたいへんそうだなあと思いつつこの件を思い出したので書いてみた。\u003c/p\u003e","title":"成熟フェーズの事業におけるデータサイエンティスト"},{"content":"ストリーム処理のフレームワークが備える backpressure という機能がある。\nこのポストでは Apache Flink の backpressure について調べたことを記載する。\nBackpressure の目的 backpressure はストリーム処理システムにおける負荷管理の仕組みの一つ。\n一時的な入力データ量の増大に対応する。\nインターネットユーザの行動履歴やセンサーデータなどは常に一定量のデータが流れているわけではなく、単位時間あたりのデータ量は常に変動している。\n一時的にスパイクしてデータ量が増大するようなことも起こりうる。\n複数の operator からなる dataflow graph により構成されるストリーム処理システムにおいては、処理スピードのボトルネックとなる operator が存在する。\n一時的に入力データ量が増えてボトルネックの operator の処理速度を上回ってしまった場合に、データの取りこぼしが発生するのを防ぐのが backpressure の目的となる。\nBackpressure の仕組み Buffer-based ここでは以前のブログでも紹介した、ストリーム処理で必要とされる機能について書かれた Fragkoulis et al. 1 を引用して一般論としての backpressure について述べたい。\n上流／下流の operator をそれぞれ producer, consumer とする。\nproducer, consumer (それらの subtask と言ってもいいかも) がそれぞれ異なる物理マシンに deploy されているケースが Figure 12b となる。\n各 subtask は input と output の buffer を持っており、\nproducer は処理結果を output buffer に書き出す TCP 等の物理的な接続でデータを送信 consumer 側の output buffer にデータを格納 consumer がそれを読み込んで処理する というような流れになる。\nbuffer はマシンごとの buffer pool で管理されており、input/output で buffer が必要となった場合はこの buffer pool に buffer が要求される。\nここで赤い丸で示されている subtask の処理速度が入力データの速度よりも遅かったとする。\nconsumer 側の input buffer の待機列が長くなり、さらにこの状況が続くとやがて buffer pool の buffer を使い果たす。\nすると producer 側から新しいデータを送信することができなくなり、producer 側の output buffer を使い始める。\n同様に producer 側でも output buffer を追加できなくなると producer は処理を待たざるを得なくなる。\nこのようにボトルネックとなる operator から上流に向かって buffer が埋まっていくことになる。\nこれが backpressure だ。\ndataflow graph を構成する operator 全体、物理的にはそれらのマシンのメモリにより一時的なデータ量の増加を buffer するという形になる。\nFigure 2a は producer と consumer が同じマシンにある場合の例であり、この場合はネットワークを介さずに buffer 上でやりとりができる。\nボトルネックがあれば同様に buffer を使い切り、それが上流に向かって伝播していくことになる。\nCredit-based 上記のような buffer-based な流量制御の場合、複数の channel が同じ下流のマシンにデータを送信する場合、同じ TCP socket を使うことになる。\n下流のある一部の channel が遅延して backpressure が働くと (data skew) 上流のすべての channel が影響を受けるという問題がある。\nこれを解決するのが credit-based な流量制御であり、Figure 13 はそれを示したものである。\nデータ送信を試みる前に credit という形で consumer 側から producer 側に buffer 状況を送信する。\nある channel で consumer の buffer がなくなると credit=0 となり、producer 側でその channel に送信できなるなり backpressure が発生する。\n一方、並列する他の channel には backpressure はかからず、TCP socket は利用可能となっている。\nFlink の Backpressure 残念ながら Flink 公式のドキュメントには backpressure についてあまり詳しく説明されていない。\nモニタリングについて書かれているのみである。\nMonitoring Back Pressure backpressure が起こっているかどうかは web UI 上から確認できるとのことだ。\n一方で Flink のブログや Alibaba のブログ等では内部的な挙動が詳しく書かれている。\nAnalysis of Network Flow Control and Back Pressure: Flink Advanced Tutorials A Deep-Dive into Flink\u0026rsquo;s Network Stack 前述のように buffer-based な仕組みの上に credit-based な挙動が採用されていることがわかる。\nここで Flink の設定の中で backpressure に影響がありそうなものを見ておく。\ntaskmanager.network.memory.buffers-per-channel こちらは channel 単位の排他的な buffer の数を指定する。\nskew 発生時において、この値が大きすぎると遅延している channel 以外で buffer が遊ぶことになり、逆に小さすぎると遅延していない channel でも処理が滞りやすくなると考えられる。\ntaskmanager.network.memory.floating-buffers-per-gate すべての input channel で共有される floating buffer の数。\nこの部分で skew をある程度吸収しようとするのだろう。\ntaskmanager.network.memory.max-buffers-per-channel channel ごとの最大 buffer 数。\n最大 buffer 数を制限することにより skew 時に backpressure が起こりやすくなり、結果として checkpoint のアラインメントを速くする効果があるとのこと。\n最大 buffer 数の制限がゆるいとボトルネックの operator で長い待ち行列を待つ必要があり、checkpoint barrier が移動するのに時間がかかってしまうということだろうか。\nweb.backpressure.cleanup-interval web.backpressure.delay-between-samples web.backpressure.num-samples web.backpressure.refresh-interval 上の4つは web UI によるモニタリング関連の設定値であり、backpressure 関連の挙動に直接影響を与えるものではない。\n…と書いてみたものの通常はこれらの値をチューニングすることはあまりないのではという印象。\nまとめ Flink の backpressure がどのように働くかがだいたい概観できた。\nそもそもなぜ backpressure を調べたかというと今開発している Flink アプリケーションで checkpoint size が増大し続ける問題があって backpressure の影響を疑っていた。\n結局 backpressure は関係なさそうだな…\nFragkoulis, M., Carbone, P., Kalavri, V., \u0026amp; Katsifodimos, A. (2020). A Survey on the Evolution of Stream Processing Systems.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://soonraah.github.io/posts/backpressure-for-flink/","summary":"\u003cp\u003eストリーム処理のフレームワークが備える backpressure という機能がある。\u003cbr\u003e\nこのポストでは Apache Flink の backpressure について調べたことを記載する。\u003c/p\u003e\n\u003ch2 id=\"backpressure-の目的\"\u003eBackpressure の目的\u003c/h2\u003e\n\u003cp\u003ebackpressure はストリーム処理システムにおける負荷管理の仕組みの一つ。\u003cbr\u003e\n一時的な入力データ量の増大に対応する。\u003c/p\u003e\n\u003cp\u003eインターネットユーザの行動履歴やセンサーデータなどは常に一定量のデータが流れているわけではなく、単位時間あたりのデータ量は常に変動している。\u003cbr\u003e\n一時的にスパイクしてデータ量が増大するようなことも起こりうる。\u003c/p\u003e\n\u003cp\u003e複数の operator からなる dataflow graph により構成されるストリーム処理システムにおいては、処理スピードのボトルネックとなる operator が存在する。\u003cbr\u003e\n一時的に入力データ量が増えてボトルネックの operator の処理速度を上回ってしまった場合に、データの取りこぼしが発生するのを防ぐのが backpressure の目的となる。\u003c/p\u003e\n\u003ch2 id=\"backpressure-の仕組み\"\u003eBackpressure の仕組み\u003c/h2\u003e\n\u003ch3 id=\"buffer-based\"\u003eBuffer-based\u003c/h3\u003e\n\u003cp\u003eここでは\u003ca href=\"https://soonraah.github.io/posts/functionality-of-streaming-system/\"\u003e以前のブログ\u003c/a\u003eでも紹介した、ストリーム処理で必要とされる機能について書かれた Fragkoulis et al. \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e を引用して一般論としての backpressure について述べたい。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Figure 12\" loading=\"lazy\" src=\"/image/paper/fragkoulis_et_al_2020/fig12.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上流／下流の operator をそれぞれ producer, consumer とする。\u003cbr\u003e\nproducer, consumer (それらの subtask と言ってもいいかも) がそれぞれ異なる物理マシンに deploy されているケースが Figure 12b となる。\u003cbr\u003e\n各 subtask は input と output の buffer を持っており、\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eproducer は処理結果を output buffer に書き出す\u003c/li\u003e\n\u003cli\u003eTCP 等の物理的な接続でデータを送信\u003c/li\u003e\n\u003cli\u003econsumer 側の output buffer にデータを格納\u003c/li\u003e\n\u003cli\u003econsumer がそれを読み込んで処理する\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eというような流れになる。\u003cbr\u003e\nbuffer はマシンごとの buffer pool で管理されており、input/output で buffer が必要となった場合はこの buffer pool に buffer が要求される。\u003c/p\u003e","title":"Apache Flink の Backpressure の仕組みについて調べた"},{"content":"はじめに 前回のポストではデータレイクとはどういうものかというのを調べた。\n今回はデータレイクの文脈でどのような OSS が注目されているのかを見ていきたい。\n以下は NTT データさんによる講演資料であり、その中で「近年登場してきた、リアルタイム分析に利用可能なOSSストレージレイヤソフト」というのが3つ挙げられている。\n大規模データ活用向けストレージレイヤソフトのこれまでとこれから（NTTデータ テクノロジーカンファレンス 2019 講演資料、2019/09/05） from NTT DATA Technology \u0026amp; Innovation Delta Lake Apache Hudi Apache Kudu これらはすべて論理的なストレージレイヤーを担う。\nこちらの講演資料に付け足すようなこともないかもしれないが、このポストではデータレイクという文脈から自分で調べて理解した内容をまとめるということを目的にする。\n当然 Hadoop, Hive, Spark 等もデータレイクの文脈において超重要だが、「データレイク」という言葉がよく聞かれるようになる前から普及していたのでこのポストでは触れないことにする。\nDelta Lake https://delta.io/\nDelta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.\nDelta Lake\nDelta Lake は Apache Spark の読み書きに ACID な transaction を提供するストレージレイヤーの OSS である。\nDatabricks が作り、2019年4月に v0.1.0 がリリースされたのが最初だ。\n使い方はめちゃ簡単で、dependency を設定した上で Spark で\ndataframe .write .format(\u0026#34;delta\u0026#34;) .save(\u0026#34;/data\u0026#34;) のように読み書きすればよい。\nデータレイク化が進むと多種多様なデータが一元的に管理され、それらデータに対して横断的なクエリを実行できるようになる。\n各データの更新タイミングも様々であり、そのような状況では ACID 特性の中でも特に Isolation (独立性) が問題となってくる。\nSpark を処理エンジンとして使う場合、データソース・読み方によっては isolation level が弱くなってしまうことがあるというのは過去のポストでも述べた。\nおそらくこのことが Delta Lake の開発の強い動機となっているのではないだろうか。\nDelta Lake は最も強い isolation level である \u0026ldquo;serializability\u0026rdquo; を提供する。\nACID transaction の他には schema に合わないデータを弾いたり過去のデータのスナップショットにアクセスしたりなどの機能がある。\nどれもデータレイクの治安を守る方向であり、データスワンプ化に抵抗したいようだ。\nこれらを実現しているのが transaction log という仕組みとのこと。\nDelta Lake は table に対する変更の transaction を atomic な commit という単位に分け、commit ごとに操作を JSON file に書き出していく。\nJSON file には 000000.json, 000001.json のように連番が振られており、2つの application が同時に table を更新するような場合は各 application が次の番号の JSON file を作れるかどうかで衝突を制御している。\nJSON file はずっと蓄積していくため再計算のコストが大きくなっていくが、その時点での table の完全な状態を parquet にした checkpoint file というものを時折出力するという、compaction のようなことも行われる。\n(Delta Lake は parquet 形式のデータ保存を前提としている)\n詳しくは Databricks の blog を参照。\nDiving Into Delta Lake: Unpacking The Transaction Log Apache Hudi https://hudi.apache.org/\nHudi brings stream processing to big data, providing fresh data while being an order of magnitude efficient over traditional batch processing.\nApache Hudi\nApache Hudi で何ができるかを一言で説明するのは難しい。\n簡単にまとめると HDFS や S3 等にある table にリアルタイムに近いデータ取り込みと、処理速度とデータの新鮮さのトレードオフに配慮した読み込みを提供する。\n2016年から Uber が開発をしており、2020年に Apache Software Fundation の top-level project となった。\nデータレイクではあらゆる種類のデータを一元管理するが、その中には当然リアルタイム性の高いデータも含むことになる。\nデータレイク中のリアルタイム性の高いデータについては次のような要求が出てくる。\ntable へのデータの取り込みはリアルタイムに近いスピードで細かくやりたい 分析クエリ等の table の読み込みにおけるクエリの速度は速くしたい (例えば列指向形式で保存されたデータの様に) 取り込まれたデータをすぐに読めるようにしたい Hudi はこれに応えるものとなっている。\nこれらを実現するのが Merge On Read のデータ構造だ。\nHudi の table には Copy On Write と Merge On Read の2種類がある。\nここでは Hudi の肝である後者について触れておきたい。\nApache Hudi Merge On Read TablePermalink\ntable に加えられた変更についての情報は timeline に追加される。\n時系列になった変更についてのメタデータのようなものだろうか。\nこの timeline によって snapshot isolation が保証される。\n最初にデータが追加されたときは parquet 等の列指向のフォーマットで保存される。\nその一方でその後のデータの追加・更新については Avro 等の行指向のフォーマットの delta log に記載される。\nここがミソであり、列指向のフォーマットでは1レコードずつなどの細かい追加・更新が高コストになるのでその部分を行指向の delta log にまかせている。\n追加・更新が増えてくると delta log がどんどん長くなってしまうので、あるタイミングで compaction を行って直近までの delta log の変更内容を反映した列指向ファイルを作成する。\nこれを読む方法は2通りある。\nSnapshot Queries では読み取り時に列指向ファイルと行指向の delta log をどちらも読んで merge して最新の結果を返す。\n(これが \u0026ldquo;Merge On Read\u0026rdquo; ということ)\n一方で Read Optimized Queries では直近で compaction が行われた時点での列指向ファイルだけを読み、その時点の結果を返す。\nつまりデータの新鮮さが重要な場合は Snapshot Queries, データの新鮮さよりもクエリの速度を優先したい場合は Read Optimized Queries が有利ということだ。\nこれらはトレードオフとなるので状況に応じて使いわけることになる。\n2通りと述べたが実際はもう1つ、 Incremental Queries というものもある。\nこれはある時点からの差分のみを読み出して処理するというものとなっている。\nevent time と processing time の差があるものを DFS 上に書き出すのに適している。\nちなみに Merge On Read ではないもう1つの table type である Copy On Write は、Merge On Read の構造から delta log を消したものとなっている。\nすなわち書き込み時に常に列指向ファイルの更新がおこり、新しく作り直される。(という意味で \u0026ldquo;Copy On Write\u0026rdquo;)\n書き込みのたびに常に compaction が発生していると言ってもよい。\n更新頻度が低いデータならこちらの table type を使うのが適しているだろう。\nHudi の table は Spark, Hive, Presto 等からクエリすることができる。\n公式ドキュメント的には Spark 推しの感がある。\nApache Kudu https://kudu.apache.org/\nApache Kudu is an open source distributed data storage engine that makes fast analytics on fast and changing data easy.\nApache Kudu はストリーム処理などの追加・更新の速いデータをすぐに分析できるようにすることを目的としている。\nCloudera 社の内部プロジェクトとして始まり、2016年から Apache Software Foundation の top-level project となった。\nKudu の目指すところは Hudi とよく似ているが、次の点で異なっている。\nKudu は OLTP つまり小さなデータアクセスを大量にさばくのにも向いている Kudu は Hudi の incremental queries のようなことはできない Kudu は HDFS や S3 のような cloud storage 上にデータを持つのでははく、Raft の合意で制御された独自のサーバー群を要する Kudu では各 table のデータが tablet という単位により構成される。\ntablet はいわゆる partition によく似た概念となっており、key の範囲による分割、ハッシュ値による分割、またはその組み合わせにより分割される。\n1つの tablet は複数の tablet server に replication されており、そのうちの1つが leader として振る舞い書き込みを受け付ける。\nleader と follower の関係は Raft 合意アルゴリズム により管理される。\n一方で master server では tablet のメタデータ等が管理されており、client はまず master と通信することになる。\nApache Kudu Architectural Overview\n読み書きに関する内部的な振る舞いについては Cloudera 社のブログ記事 (日本語訳) が参考になる。\nCLOUDERA Blog Apache Kudu Read \u0026amp; Write Paths\nclient 側からはおそらく見えないが、内部的には\nメモリ上の MemRowSet, DeltaMemStore 列指向の base data file 差分を表す delta file (UNDO/REDO records) の3段の構成になっている。 (それと WAL も)\ndelta file を使うのは Hudi 等と同じだが一度メモリ上で変更を受けるという段があるのが特徴的だ。\n挿入はある tablet のメモリ上の MemRowSet にまず追加される。\nまた任意の timestamp の snapshot を得るために、MemRowSet 上のデータへの更新・削除はの差分は REDO records へと保存される。\nMemRowSet がいっぱいになると最新の状態が列指向の base data file へ書き出され、更新・削除前の状態は UNDO records へと書き出される。\n読み取りのときは MemRowSet とディスク上の base data file + delta file をスキャンすることになる。\nしたがって delta file の数やサイズが大きくなると遅くなる。\nやはりここでも compaction が必要となってくる。\nこのように memory を使うため server が必要であり、データレイクでよく言われるコンピューティングとストレージの分離が完全にはできない。\nCloudera 的と言えるかもしれない。\n読み取りには2つのモードがある。\nデフォルトは READ_LATEST であり、名前のとおり snapshot をとってすぐにデータを読むとのこと。\nREAD_LATEST は比較的弱い read committed の isolation level を示す。\nこれはおそらく Raft や WAL を経て変更が可視になるまでに時間を要するためだ。\nread committed は実用では問題が起こることもある。(ex. Spark クエリの分離レベル)\nもう一つは READ_AT_SNAPSHOT であり、明示的 (推奨) または暗黙的に読み取る対象の timestamp を指定する。\n書き込みの operation が完了し、その timestamp までの変更が安全に読めるようになるまで待って結果を返すことになる。\nisolation level はおそらく最も強い serializable となっている。\nしたがって2つのモードはデータの新鮮さと isolation level (consistency も？) のトレードオフとなっている。\nまとめ Delta Lake, Apache Hudi, Apache Kudu の3つを見比べて見てとても面白いのは、課題感は少しずつ違っているのにどれも列指向ファイル + 差分ファイル (delta file) というアーキテクチャを中心に置いているということだ。\nDelta Lake では transaction を重視している一方で Hudi ではリアルタイムデータをすぐに分析することを目指し、かつ Kudu ではさらに OLTP もサポートする。\nおそらく導入は Delta Lake が最も簡単であり、Kudu に至っては server を用意する必要があるのでハードルが1段高い。\n同じアーキテクチャということもあり、例えば time travel の機能などは共通して提供されている。\nバランス的には Apache Hudi がよさそうだが、どれを使うべきかは work load 次第だろう。\nhourly のデータ更新に慣れすぎていて fast data - fast analysis の需要に気づけないといこともよくありそう。\n","permalink":"https://soonraah.github.io/posts/oss-for-data-lake/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://soonraah.github.io/posts/what-is-a-data-lake/\"\u003e前回のポスト\u003c/a\u003eではデータレイクとはどういうものかというのを調べた。\u003cbr\u003e\n今回はデータレイクの文脈でどのような OSS が注目されているのかを見ていきたい。\u003c/p\u003e\n\u003cp\u003e以下は NTT データさんによる講演資料であり、その中で「近年登場してきた、リアルタイム分析に利用可能なOSSストレージレイヤソフト」というのが3つ挙げられている。\u003c/p\u003e\n\n\u003cdiv style=\"text-align: center\"\u003e\n\u003ciframe src=\"//www.slideshare.net/slideshow/embed_code/key/5sVeEqby7oL0AE?startSlide=37\" width=\"510\" height=\"420\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen\u003e \u003c/iframe\u003e \u003cdiv style=\"margin-bottom:5px\"\u003e \u003cstrong\u003e \u003ca href=\"//www.slideshare.net/nttdata-tech/bigdata-storage-layer-software-nttdata\" title=\"大規模データ活用向けストレージレイヤソフトのこれまでとこれから（NTTデータ テクノロジーカンファレンス 2019 講演資料、2019/09/05）\" target=\"_blank\"\u003e大規模データ活用向けストレージレイヤソフトのこれまでとこれから（NTTデータ テクノロジーカンファレンス 2019 講演資料、2019/09/05）\u003c/a\u003e \u003c/strong\u003e from \u003cstrong\u003e\u003ca href=\"//www.slideshare.net/nttdata-tech\" target=\"_blank\"\u003eNTT DATA Technology \u0026amp; Innovation\u003c/a\u003e\u003c/strong\u003e \u003c/div\u003e\n\u003c/div\u003e\n\u003cbr\u003e\n\n\u003cul\u003e\n\u003cli\u003eDelta Lake\u003c/li\u003e\n\u003cli\u003eApache Hudi\u003c/li\u003e\n\u003cli\u003eApache Kudu\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこれらはすべて論理的なストレージレイヤーを担う。\u003cbr\u003e\nこちらの講演資料に付け足すようなこともないかもしれないが、このポストではデータレイクという文脈から自分で調べて理解した内容をまとめるということを目的にする。\u003c/p\u003e\n\u003cp\u003e当然 Hadoop, Hive, Spark 等もデータレイクの文脈において超重要だが、「データレイク」という言葉がよく聞かれるようになる前から普及していたのでこのポストでは触れないことにする。\u003c/p\u003e\n\u003ch2 id=\"delta-lake\"\u003eDelta Lake\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://delta.io/\"\u003ehttps://delta.io/\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eDelta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cfigure\u003e\n    \u003cimg loading=\"lazy\" src=\"/image/deltalake/Delta-Lake-marketecture-0423c.png\"/\u003e \u003cfigcaption\u003e\n            \u003cp\u003e\n                    \u003ca href=\"https://delta.io/\"\u003eDelta Lake\u003c/a\u003e\u003c/p\u003e\n        \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eDelta Lake は Apache Spark の読み書きに ACID な transaction を提供するストレージレイヤーの OSS である。\u003cbr\u003e\nDatabricks が作り、2019年4月に v0.1.0 がリリースされたのが最初だ。\u003cbr\u003e\n使い方はめちゃ簡単で、dependency を設定した上で Spark で\u003c/p\u003e","title":"データレイク関連の OSS - Delta Lake, Apache Hudi, Apache Kudu"},{"content":"最近よく聞かれるようになった「データレイク」という概念にあまりついていけていなかったため、いまさらながらざっと調べてみた。\nデータレイクとは Wikipedia によると最初にこの言葉を使ったのは Pentaho 社の CTO である James Dixon らしい。\nその時の彼のブログ (10年前…) を読むと、既にあったデータマートに対して\nOnly a subset of the attributes are examined, so only pre-determined questions can be answered. The data is aggregated so visibility into the lowest levels is lost\n\u0026ndash;Pentaho, Hadoop, and Data Lakes - James Dixon’s Blog というような問題意識からデータレイクというコンセプトを提案したようだ。\n最近？のデータレイクについてはベンダー等の記事が参考になる。\nデータレイクとは - AWS データレイクとは？ - talend データレイクとは？データレイクの落とし穴と効果 - Informatica 書籍だと『AWSではじめるデータレイク: クラウドによる統合型データリポジトリ構築入門』がいいだろうか。\nデータレイクの概要と AWS が考えている構築・運用がざっとわかる。\nAmazon で検索した限りだと現時点でタイトルに「データレイク」を含む和書はこれのみだった。\n上に挙げたような文献ではデータレイクはデータウェアハウスとの対比という形でよく語られている。\n共通している内容は概ね以下のとおり。\n加工前データや非構造化データを含むあらゆるデータを保存 データウェアハウスでは加工され構造化されたデータのみを含む データレイクでは加工前の半構造化、非構造化データも含む ex. ログ、画像、音声 Scheme on Read 書き込み時にデータの構造を決める (Scheme on Write) のではなく、使用時に決める なので詳細なスキーマ設計なしに様々なデータを置いていくことができる 一元的なデータ管理 データがサイロ化しないよう、組織全体のデータを一元的に管理 なのでデータへのアクセス権の管理が重要になる 多様な分析用途に対応 データウェアハウスはビジネスアナリストが決まったレポートを出すために使われる データレイクでは機械学習など更に高度な分析をデータサイエンティストが行う 機械学習の普及がデータレイクを強く後押ししているように思う。\n機械学習をやっていれば様々な特徴量を扱いたいというのは自然な欲求であり、データレイクはそれを実現する。\nまた、クラウドベンダーは以下のような点も強調している。\n従量課金のクラウドストレージによるメリット 運用開始前の時点でどんな生データがどれだけ来るか、見積もるのはとても難しい 従量課金のクラウドストレージ (Amazon S3, Google Cloud Storage, etc.) なら必要なときに必要なサイズだけ追加できる 安価なクラウドストレージの普及がデータレイクを後押し ストレージとコンピューティングの分離 処理側のリソースを処理内容に応じて確保できる 処理側のバージョンアップや変更が容易 (オンプレ Hadoop クラスタのバージョンアップの辛さを思い出してください) なるほどです。\n沼にはまらないために データレイクとの対比でデータスワンプ (沼) という言葉がある。\n以下の説明がわかりやすいだろうか。\nデータスワンプ（Data Swamp）とは、データの沼地（Swamp）という意味です。これの対比語としてデータレイク（Data Lake：データの湖）があります。沼には、いろんな魚が住んでいるかもしれませんが、水が濁っているため、どこにどんな魚がいるか全く見えません。また、全く見えないため「魚が住んでいないんじゃないか」とも思い、魚を捕るのも諦めてしまいがちです。その一方で、湖は、水が澄んでいるため、魚を見ることができ「おっ！魚がいるな。何とか捕まえてみよう」と思えます。\nこの沼と湖にいる魚を、データの例えとして使っているのが、データスワンプと、データレイクという言葉です。\n\u0026ndash; データスワンプ - Realize\nデータスワンプにならないようにということについては次の資料が参考になる。\nThe difference between a data swamp and a data lake? 5 signs Metadata Separates Data Lakes From Data Swamps From Data-Swamp to Data-Lake on AWS (Part 1) すごくざっくりまとめるとデータレイクの構築・運用に当たって次の点に配慮するとよい。\nメタデータ、カタログの整備 どんなに優れたデータレイクを構築したとしても、利用者がどこに何のデータがあるかわからないと意味がない 多種のデータ資産を一元的に管理するデータレイクにおいて目的のデータを見つけるために必要な機能 データガバナンス 誰がどのデータに対して何ができるのか 統制をきつくしすぎると自由度が減るため、これらのバランスを考慮しないといけない その他、常識的な運用 クラウドベンダーが推進するデータレイク クラウドベンダーがどのようなデータレイクを考えているかというのも見ておく。\nGCP Google Cloud データレイクとしての Cloud Storage\nデータレイクとしての Cloud Storage というドキュメントに GCP が考えるデータレイクの構成が記載されている。\n構成としては以下のとおり。\n保存: Cloud Storage 変換: Cloud Dataproc, Cloud Dataprep, Cloud Dataflow 分析: BigQuery, Cloud ML Engine, Cloud Datalab, etc. データ保存のストレージとして、みんな大好きな BigQuery ではなく Cloud Storage を想定しているのは非構造化データを扱うためだろう。\nいわゆるリレーショナル的なものより Cloud Storage や S3 のようなオブジェクトストレージの方がこの要件に適している。\nもちろん Cloud Storage に保存された生データを処理してから BigQuery のデータウェアハウスに再びつっこむのも悪くない。\n変換や分析には様々なサービスが使えるし、今後も幅は広がっていくと考えられる。\nクラウドにおけるストレージとコンピューティングの分離の恩恵と言えるだろう。\n現時点ではこのドキュメントにはカタログ化についての記述はないが、GCP には Data Catalog というサービスがあり Cloud Storage のメタデータを扱えるようだ。\nAWS AWS Sample AWS data lake platform\n前述の書籍『AWSではじめるデータレイク: クラウドによる統合型データリポジトリ構築入門』に加え、Building Big Data Storage Solutions (Data Lakes) for Maximum Flexibility も読んでおくとよい。\n保存: Amazon S3 (, AWS Glue Data Catalog) 変換: AWS Glue ETL, Amazon EMR, AWS Lambda, etc. 分析: Amazon Athena, Amazon Redshift Spectrum, Amazon ML, etc. やはり中心となるのは S3 だ。\nGCP も AWS もストレージのコストメリットを推している。\nGlue Data Catalog がメタデータを管理する。\nGCP 同様、変換・分析には様々なオプションがある。\nBuilding Big Data Storage Solutions (Data Lakes) for Maximum Flexibility には記載がないのだが、AWS Lake Formation というサービスもある。\n上に挙げたようなサービスの上に一枚被せて、データレイクとして運用しやすくするといったようなものらしい。\n特に権限管理的な意味合いが強いように思う。\n組織のデータを一元的に管理するデータレイクには様々な部署・役割の人からのアクセスがある。\nどのデータを誰が扱えるのか、複数のサービスを横断して IAM で権限を管理のはかなりきつそうな印象でそういうのを楽にしてくれるのだろう。\nまとめ データレイクの概観やベンダーが考えていることがわかった。\n普段業務で使っているということもあり、全体的に AWS 寄りになってしまったように思うのでその点をご留意していただきたい。\nこの後はデータレイク関連の OSS について調べておきたいところ。\n","permalink":"https://soonraah.github.io/posts/what-is-a-data-lake/","summary":"\u003cp\u003e最近よく聞かれるようになった「データレイク」という概念にあまりついていけていなかったため、いまさらながらざっと調べてみた。\u003c/p\u003e\n\u003ch2 id=\"データレイクとは\"\u003eデータレイクとは\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Data_lake\"\u003eWikipedia\u003c/a\u003e によると最初にこの言葉を使ったのは Pentaho 社の CTO である James Dixon らしい。\u003cbr\u003e\nその時の\u003ca href=\"https://jamesdixon.wordpress.com/2010/10/14/pentaho-hadoop-and-data-lakes/\"\u003e彼のブログ\u003c/a\u003e (10年前…) を読むと、既にあったデータマートに対して\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cul\u003e\n\u003cli\u003eOnly a subset of the attributes are examined, so only pre-determined questions can be answered.\u003c/li\u003e\n\u003cli\u003eThe data is aggregated so visibility into the lowest levels is lost\u003cbr\u003e\n\u0026ndash;\u003c!-- raw HTML omitted --\u003e\u003ca href=\"https://jamesdixon.wordpress.com/2010/10/14/pentaho-hadoop-and-data-lakes/\"\u003ePentaho, Hadoop, and Data Lakes - James Dixon’s Blog\u003c/a\u003e\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003c/ul\u003e\u003c/blockquote\u003e\n\u003cp\u003eというような問題意識からデータレイクというコンセプトを提案したようだ。\u003c/p\u003e\n\u003cp\u003e最近？のデータレイクについてはベンダー等の記事が参考になる。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://aws.amazon.com/jp/big-data/datalakes-and-analytics/what-is-a-data-lake/\"\u003eデータレイクとは - AWS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.talend.com/jp/resources/what-is-data-lake/\"\u003eデータレイクとは？ - talend\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://blogs.informatica.com/jp/2019/09/30/data-lake/\"\u003eデータレイクとは？データレイクの落とし穴と効果 - Informatica\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e書籍だと\u003ca href=\"https://www.amazon.co.jp/gp/product/491031301X/ref=as_li_tl?ie=UTF8\u0026amp;camp=247\u0026amp;creative=1211\u0026amp;creativeASIN=491031301X\u0026amp;linkCode=as2\u0026amp;tag=froglog02-22\u0026amp;linkId=4e6220e28ad8d55e37e1884c69938ab0\"\u003e『AWSではじめるデータレイク: クラウドによる統合型データリポジトリ構築入門』\u003c/a\u003eがいいだろうか。\u003cbr\u003e\nデータレイクの概要と AWS が考えている構築・運用がざっとわかる。\u003cbr\u003e\nAmazon で検索した限りだと現時点でタイトルに「データレイク」を含む和書はこれのみだった。\u003c/p\u003e","title":"いまさらながらのデータレイク"},{"content":"ストリーム処理における CSV ファイルの読み込み Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。\nしかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。\nstar schema における dimension table 的な情報をストリームに結合したい場合 等が考えられる。\nこのポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。\nFlink は現時点の stable である v1.11 を想定。\nCSV ファイルを読む方法 DataStream API ベースの実装で CSV ファイルを読むには StreamExecutionEnvironment のメソッドである readFile() を使う。\noverload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。\nまず1つめは FileInputFormat\u0026lt;OUT\u0026gt; inputFormat であり、こちらは data stream の生成に用いる入力フォーマットを指定する。\nおそらく最も一般的なのが TextInputFormat だと思われる。\nもちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。\nPojoCsvInputFormat RowCsvInputFormat TupleCsvInputFormat なんとなく名前でわかると思うが、それぞれ readFile() の結果として返される DataStreamSource が内包する型が異なる。\nこれについては後述の実験にて確認する。\n次に FileProcessingMode watchType も見ておきたい。\nこの引数ではデータソースの監視についてのモードを指定する。\nモードは2つある。\nFileProcessingMode.PROCESS_CONTINUOUSLY 対象のファイルが更新され、その更新に追随する必要がある場合に利用 指定のインターバルでファイルの更新をチェック 更新があった場合はファイル全体を読む FileProcessingMode.PROCESS_ONCE 対象のファイルの更新がない、更新について考えない場合に利用 最初に一度だけファイルを読む おそらく多くの場合は前者が必要になるのではないだろうか。\n利用にあたっては更新があった場合にファイル全体が読まれるということに注意が必要だ。\n例えばファイル末尾にレコードを1件追加するような更新であったとしても、全レコードが再度ストリームに流されるということである。\n詳しくは ドキュメント を参照。\nこれはファイル全体で1つの atomic な単位だとみなされているものと思われる。\nレコード単位で処理していくストリーム処理にファイルというバルクな単位のデータを流そうとしているのでこうなってしまう。\nそう考えるとやはり static なファイルのデータは dimension table として情報を付加するような、ストリームの本川に合流する支川のような使い方が想定されているのだろう。\nちなみに CsvReader というものもあるが、こちらは DataSet API、つまりバッチ処理向けのようなので今回は扱わない。\n実験 実際にコードを書いて readFile() で CSV を読んでみる。\nここでは PojoCsvInputFormat と TupleCsvInputFormat を切り替えられるようにした。\nコード package com.example.entry import org.apache.flink.api.common.typeinfo.BasicTypeInfo import org.apache.flink.api.java.io.{PojoCsvInputFormat, TupleCsvInputFormat} import org.apache.flink.api.java.tuple.Tuple3 import org.apache.flink.api.java.typeutils.{PojoField, PojoTypeInfo, TupleTypeInfo} import org.apache.flink.core.fs.Path import org.apache.flink.streaming.api.functions.source.FileProcessingMode import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, createTypeInformation} import scala.collection.JavaConverters._ import scala.concurrent.duration._ /** * The experiment to read CSV file by Flink. * It reads CSV file as POJOs or tuples and just prints on console. */ object ReadCsvFileExperimentRunner { /** POJO */ case class Company(name: String, ticker: String, numEmployees: Int) /** Tuple */ type CompanyTuple = Tuple3[String, String, Int] def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val companiesFilePath = \u0026#34;data/companies.csv\u0026#34; val interval = 10.seconds args.headOption match { case None | Some(\u0026#34;pojo\u0026#34;) =\u0026gt; val inputFormat = createPojoCsvInputFormat(companiesFilePath) env .readFile(inputFormat, companiesFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, interval.toMillis) .map(_.toString) .print() case Some(\u0026#34;tuple\u0026#34;) =\u0026gt; val inputFormat = createTupleCsvInputFormat(companiesFilePath) env .readFile(inputFormat, companiesFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, interval.toMillis) .map(_.toString) .print() case _ =\u0026gt; throw new RuntimeException(s\u0026#34;Unsupported input format: ${args(0)}\u0026#34;) } env.execute() } private def createPojoCsvInputFormat(csvFilePath: String): PojoCsvInputFormat[Company] = { val clazz = classOf[Company] val pojoFields = Seq( new PojoField(clazz.getDeclaredField(\u0026#34;name\u0026#34;), BasicTypeInfo.STRING_TYPE_INFO), new PojoField(clazz.getDeclaredField(\u0026#34;ticker\u0026#34;), BasicTypeInfo.STRING_TYPE_INFO), new PojoField(clazz.getDeclaredField(\u0026#34;numEmployees\u0026#34;), BasicTypeInfo.INT_TYPE_INFO) ).asJava val pojoTypeInfo = new PojoTypeInfo[Company](clazz, pojoFields) val fieldNames = Array(\u0026#34;name\u0026#34;, \u0026#34;ticker\u0026#34;, \u0026#34;numEmployees\u0026#34;) val inputFormat = new PojoCsvInputFormat[Company](new Path(csvFilePath), pojoTypeInfo, fieldNames) inputFormat.setSkipFirstLineAsHeader(true) inputFormat } private def createTupleCsvInputFormat(csvFilePath: String): TupleCsvInputFormat[CompanyTuple] = { val types = Seq( BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.INT_TYPE_INFO ) val tupleTypeInfo = new TupleTypeInfo[CompanyTuple](classOf[CompanyTuple], types: _*) val inputFormat = new TupleCsvInputFormat[CompanyTuple](new Path(csvFilePath), tupleTypeInfo) inputFormat.setSkipFirstLineAsHeader(true) inputFormat } } ポイントは POJO 版も tuple 版も型情報を作ってやる必要があるということだ。\nそれぞれ PojoTypeInfo, TupleTypeInfo を用意してやる必要があり、これがやや癖があって面倒。\nあるフィールドを数値として読むことは可能だが、日付の parse のようなことはできないようである。\nというのを考えると TextInputFormat で読んで自分で parse するのと比べてあまりうれしくないような…\nデータ 実験用のデータとして会社情報を示す簡単な CSV ファイルを適当に作って data/companies.csv に配置。\nname,ticker,num_employees Alphabet Inc,GOOG,98771 Apple Inc,AAPL,147000 Facebook Inc,FB,49942 Amazon.com Inc,AMZN,798000 実行 まずは POJO 版を実行してみた。\nプログラムが起動するとすぐに以下が出力された。\n[info] running com.example.entry.ReadCsvFileExperimentRunner pojo 2\u0026gt; Company(Alphabet Inc,GOOG,98771) 1\u0026gt; Company(Facebook Inc,FB,49942) 2\u0026gt; Company(Apple Inc,AAPL,147000) 1\u0026gt; Company(Amazon.com Inc,AMZN,798000) Company インスタンスとして CSV ファイルの内容を取得できている。\nプログラムは止まっていないが CSV ファイルの内容を一通り吐き出したところで出力は止まった。\nここで CSV ファイルに次の1行を追加してみる。\nMicrosoft Corporation,MSFT,163000 すると出力は\n[info] running com.example.entry.ReadCsvFileExperimentRunner pojo 2\u0026gt; Company(Alphabet Inc,GOOG,98771) 1\u0026gt; Company(Facebook Inc,FB,49942) 2\u0026gt; Company(Apple Inc,AAPL,147000) 1\u0026gt; Company(Amazon.com Inc,AMZN,798000) 2\u0026gt; Company(Alphabet Inc,GOOG,98771) 1\u0026gt; Company(Amazon.com Inc,AMZN,798000) 2\u0026gt; Company(Apple Inc,AAPL,147000) 1\u0026gt; Company(Microsoft Corporation,MSFT,163000) 2\u0026gt; Company(Facebook Inc,FB,49942) となり、最初に出力された4行に加えて新たに5行追加された。\nCSV ファイルには1行追加しただけだが、既存の行も含む CSV ファイル全体が再度出力された。\nドキュメントに記載されているとおりの仕様となっている。\ntuple 版で実行すると出力は次のようになった。\n[info] running com.example.entry.ReadCsvFileExperimentRunner tuple 1\u0026gt; (Facebook Inc,FB,49942) 1\u0026gt; (Amazon.com Inc,AMZN,798000) 2\u0026gt; (Alphabet Inc,GOOG,98771) 2\u0026gt; (Apple Inc,AAPL,147000) tuple として読めているようだ。\nwatchType が同じなので CSV ファイルの更新についての挙動は同様だった。\nちなみに実行可能なプロジェクトは GitHub に置いている。\nsbt 'runMain com.example.entry.ReadCsvFileExperimentRunner pojo' または sbt 'runMain com.example.entry.ReadCsvFileExperimentRunner tuple' で実行できる。\n(Ctrl + C で終了)\nまとめ TextInputFormat で読んで自分で parse するのと比べ、*CsvInputFormat を使う方法はコーディングとしてはあまりメリットが感じられなかった。\nまた、ストリーム処理においてやはりファイルというデータソースは傍流なんだなという感じ。\nちなみに Table API で CSV を読むこともおそらく可能。\n気が向いたら書く。\n","permalink":"https://soonraah.github.io/posts/read-csv-by-flink-datastream-api/","summary":"\u003ch2 id=\"ストリーム処理における-csv-ファイルの読み込み\"\u003eストリーム処理における CSV ファイルの読み込み\u003c/h2\u003e\n\u003cp\u003eApache Flink は unbounded なストリームデータを処理するためのフレームワークだ。\u003cbr\u003e\nしかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。\u003cbr\u003e\n\u003ca href=\"https://soonraah.github.io/posts/flink-join-by-broadcast-state-pattern/\"\u003estar schema における dimension table 的な情報をストリームに結合したい場合\u003c/a\u003e 等が考えられる。\u003c/p\u003e\n\u003cp\u003eこのポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。\u003cbr\u003e\nFlink は現時点の stable である v1.11 を想定。\u003c/p\u003e\n\u003ch2 id=\"csv-ファイルを読む方法\"\u003eCSV ファイルを読む方法\u003c/h2\u003e\n\u003cp\u003eDataStream API ベースの実装で CSV ファイルを読むには \u003ccode\u003eStreamExecutionEnvironment\u003c/code\u003e のメソッドである \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/api/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.html#readFile-org.apache.flink.api.common.io.FileInputFormat-java.lang.String-org.apache.flink.streaming.api.functions.source.FileProcessingMode-long-\"\u003e\u003ccode\u003ereadFile()\u003c/code\u003e\u003c/a\u003e を使う。\u003cbr\u003e\noverload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。\u003c/p\u003e\n\u003cp\u003eまず1つめは \u003ccode\u003eFileInputFormat\u0026lt;OUT\u0026gt; inputFormat\u003c/code\u003e であり、こちらは data stream の生成に用いる入力フォーマットを指定する。\u003cbr\u003e\nおそらく最も一般的なのが \u003ccode\u003eTextInputFormat\u003c/code\u003e だと思われる。\u003cbr\u003e\nもちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ePojoCsvInputFormat\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eRowCsvInputFormat\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eTupleCsvInputFormat\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eなんとなく名前でわかると思うが、それぞれ \u003ccode\u003ereadFile()\u003c/code\u003e の結果として返される \u003ccode\u003eDataStreamSource\u003c/code\u003e が内包する型が異なる。\u003cbr\u003e\nこれについては後述の実験にて確認する。\u003c/p\u003e","title":"Apache Flink の DataStream API 利用時の CSV ファイル読み込み"},{"content":"ちょっと昔話 かつて参画したプロジェクトの話。\nそのプロジェクトでは他社から受注した受託開発として機械学習系のシステムを開発していた。\n当時としては新しいフレームワークを使い、かなり頑張ってなんとか納期内で完成させた。\nその中の1つの機能として A/B テストができるようにしていた。\nパラメータチューニングによりパフォーマンスを改善することを想定していた。\nしかし結局その機能は使われることがなかった。\nなぜか。\nA/B テストを実施するためのクライアントの追加の予算がつかなかったためである。\n受託なのでなおさらなのだが、売上にならなければ工数をかけるこはできない。\n工数を使ってパフォーマンス改善することはできなかった。\n手はあるのに。\n機械学習の精度は必ずしも利益に結びつかない この昔話で何が言いたいかというと、機械学習の精度改善は必ずしも利益に結びつかないということである。\nそのことを示しているとても素晴らしい資料がこちら。\n機械学習の精度と売上の関係 from Tokoroten Nakayama 前述の昔話の例はこの資料で言うところの③ロジスティック型 (=外注) となる。\nいったん売上が立った後、追加予算がつかなかったので精度改善では売上は増えなかったのだ。\n倫理感による精度改善 受託開発を主としている組織であれば工数にはシビアなので、売上の立たない工数をかけることはあまりないだろう。\n(よっぽどの炎上鎮火とかでなければ)\nしかし自社で製品やサービスを作って提供しているような組織の場合、利益にならない精度改善をしているのを時折見かける。\nなぜそのようなことが起こるかと言うと多くの場合はデータサイエンティスト／機械学習エンジニアとしての倫理感からなのではないだろうか。\n「◯◯予測という機能なのでできるだけ良い予測精度を示すべきだ」\n「ユーザには気づかれない部分だが精度が悪いので改善したい」\n倫理感や興味が先行してしまっているのだ。\nしかしその精度を上げた先に利益があるとは限らない。\n機械学習で職を得ている人間は自分の仕事を機械学習の精度を上げるゲームだとみなす傾向があるように思う。\n例えばインターネット広告の CTR 予測。\nこれは予測精度が高いほど利益は改善するし、広告主に価値も提供できる。\n精度改善に倫理と利益が伴っている、とても機械学習がハマる例だと思う。\n本来はこれらを兼ね備えているのが良い適用先であるはずだ。\nイシューは行き渡っているのか 利益に結びつかない、または間接的にしか結びつかないような精度改善をやることが許されるというのは組織に余裕があるということで悪いことではないのかもしれない。\nしかし単によいイシューの設定ができてないだけという可能性もある。\n自社で製品やサービスを作って提供しているような組織において、単純なロジスティック回帰でコアなところのビジネスを大きく加速させることができた時期を過ぎると機械学習で解くのに適したよい問題を恒常的に見つけ出すのは実は難しいのではないだろうかと最近考えるようになった。\nビジネスの領域拡大よりも既存領域への機械学習の適用の方が速いということは十分ありうる。\nもちろんチームの規模にもよる。\n機械学習チームの人的リソースの規模に対して機械学習で解くべきよいイシューを見つけ出せているのか、ということだ。\n少し前にちょっと話題になったこちらの件もイシューが大事だと言っている。\n全ての機械学習の論文は新しいアルゴリズムを提案しているのですか？ - Quora キャリアの行く末 事業会社においてビジネスの領域拡大よりも既存領域への機械学習の適用の方が速く、よいイシューを提供しにくいということがよく起こるのであれば、機械学習チームのリソースは余剰気味になりやすいということになる。\nこれが続くと今後機械学習しかやらない人材の市場価値は下がっていくのかもしれない。\nもしくは自社で製品やサービスを持っている組織ではなく、受託開発やコンサルが主戦場になっていくのかもしれない。\n何にせよ特定のプロダクトに commit したいのであれば機械学習エンジニアは機械学習以外のスキルも磨いていく必要があるように思う。\nおわりに 見える範囲にいる人が利益にならない精度改善をしているのを横目で見てこのようなことを考えていた。\n難しいけどできるだけ金を生んでいきたい。\n","permalink":"https://soonraah.github.io/posts/ml-accuracy-profit-ethic-issue/","summary":"\u003ch2 id=\"ちょっと昔話\"\u003eちょっと昔話\u003c/h2\u003e\n\u003cp\u003eかつて参画したプロジェクトの話。\u003cbr\u003e\nそのプロジェクトでは他社から受注した受託開発として機械学習系のシステムを開発していた。\u003cbr\u003e\n当時としては新しいフレームワークを使い、かなり頑張ってなんとか納期内で完成させた。\u003c/p\u003e\n\u003cp\u003eその中の1つの機能として A/B テストができるようにしていた。\u003cbr\u003e\nパラメータチューニングによりパフォーマンスを改善することを想定していた。\u003c/p\u003e\n\u003cp\u003eしかし結局その機能は使われることがなかった。\u003cbr\u003e\nなぜか。\u003cbr\u003e\nA/B テストを実施するためのクライアントの追加の予算がつかなかったためである。\u003cbr\u003e\n受託なのでなおさらなのだが、売上にならなければ工数をかけるこはできない。\u003cbr\u003e\n工数を使ってパフォーマンス改善することはできなかった。\u003cbr\u003e\n手はあるのに。\u003c/p\u003e\n\u003ch2 id=\"機械学習の精度は必ずしも利益に結びつかない\"\u003e機械学習の精度は必ずしも利益に結びつかない\u003c/h2\u003e\n\u003cp\u003eこの昔話で何が言いたいかというと、機械学習の精度改善は必ずしも利益に結びつかないということである。\u003cbr\u003e\nそのことを示しているとても素晴らしい資料がこちら。\u003c/p\u003e\n\n\u003ciframe src=\"//www.slideshare.net/slideshow/embed_code/key/pIXmCy2QsvkYxs\" width=\"510\" height=\"420\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen\u003e \u003c/iframe\u003e \u003cdiv style=\"margin-bottom:5px\"\u003e \u003cstrong\u003e \u003ca href=\"//www.slideshare.net/TokorotenNakayama/ss-93185418\" title=\"機械学習の精度と売上の関係\" target=\"_blank\"\u003e機械学習の精度と売上の関係\u003c/a\u003e \u003c/strong\u003e from \u003cstrong\u003e\u003ca href=\"//www.slideshare.net/TokorotenNakayama\" target=\"_blank\"\u003eTokoroten Nakayama\u003c/a\u003e\u003c/strong\u003e \u003c/div\u003e\n\n\u003cp\u003e前述の昔話の例はこの資料で言うところの③ロジスティック型 (=外注) となる。\u003cbr\u003e\nいったん売上が立った後、追加予算がつかなかったので精度改善では売上は増えなかったのだ。\u003c/p\u003e\n\u003ch2 id=\"倫理感による精度改善\"\u003e倫理感による精度改善\u003c/h2\u003e\n\u003cp\u003e受託開発を主としている組織であれば工数にはシビアなので、売上の立たない工数をかけることはあまりないだろう。\u003cbr\u003e\n(よっぽどの炎上鎮火とかでなければ)\u003c/p\u003e\n\u003cp\u003eしかし自社で製品やサービスを作って提供しているような組織の場合、利益にならない精度改善をしているのを時折見かける。\u003cbr\u003e\nなぜそのようなことが起こるかと言うと多くの場合はデータサイエンティスト／機械学習エンジニアとしての倫理感からなのではないだろうか。\u003c/p\u003e\n\u003cp\u003e「◯◯予測という機能なのでできるだけ良い予測精度を示すべきだ」\u003cbr\u003e\n「ユーザには気づかれない部分だが精度が悪いので改善したい」\u003c/p\u003e\n\u003cp\u003e倫理感や興味が先行してしまっているのだ。\u003cbr\u003e\nしかしその精度を上げた先に利益があるとは限らない。\u003cbr\u003e\n機械学習で職を得ている人間は自分の仕事を機械学習の精度を上げるゲームだとみなす傾向があるように思う。\u003c/p\u003e\n\u003cp\u003e例えばインターネット広告の CTR 予測。\u003cbr\u003e\nこれは予測精度が高いほど利益は改善するし、広告主に価値も提供できる。\u003cbr\u003e\n精度改善に倫理と利益が伴っている、とても機械学習がハマる例だと思う。\u003cbr\u003e\n本来はこれらを兼ね備えているのが良い適用先であるはずだ。\u003c/p\u003e\n\u003ch2 id=\"イシューは行き渡っているのか\"\u003eイシューは行き渡っているのか\u003c/h2\u003e\n\u003cp\u003e利益に結びつかない、または間接的にしか結びつかないような精度改善をやることが許されるというのは組織に余裕があるということで悪いことではないのかもしれない。\u003c/p\u003e\n\u003cp\u003eしかし単によいイシューの設定ができてないだけという可能性もある。\u003cbr\u003e\n自社で製品やサービスを作って提供しているような組織において、単純なロジスティック回帰でコアなところのビジネスを大きく加速させることができた時期を過ぎると機械学習で解くのに適したよい問題を恒常的に見つけ出すのは実は難しいのではないだろうかと最近考えるようになった。\u003cbr\u003e\nビジネスの領域拡大よりも既存領域への機械学習の適用の方が速いということは十分ありうる。\u003cbr\u003e\nもちろんチームの規模にもよる。\u003c/p\u003e\n\u003cp\u003e機械学習チームの人的リソースの規模に対して機械学習で解くべきよいイシューを見つけ出せているのか、ということだ。\u003cbr\u003e\n少し前にちょっと話題になったこちらの件もイシューが大事だと言っている。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://jp.quora.com/%E5%85%A8%E3%81%A6%E3%81%AE%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E8%AB%96%E6%96%87%E3%81%AF%E6%96%B0%E3%81%97%E3%81%84%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E3%82%92%E6%8F%90%E6%A1%88\"\u003e全ての機械学習の論文は新しいアルゴリズムを提案しているのですか？ - Quora\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"キャリアの行く末\"\u003eキャリアの行く末\u003c/h2\u003e\n\u003cp\u003e事業会社においてビジネスの領域拡大よりも既存領域への機械学習の適用の方が速く、よいイシューを提供しにくいということがよく起こるのであれば、機械学習チームのリソースは余剰気味になりやすいということになる。\u003cbr\u003e\nこれが続くと今後機械学習しかやらない人材の市場価値は下がっていくのかもしれない。\u003c/p\u003e\n\u003cp\u003eもしくは自社で製品やサービスを持っている組織ではなく、受託開発やコンサルが主戦場になっていくのかもしれない。\u003cbr\u003e\n何にせよ特定のプロダクトに commit したいのであれば機械学習エンジニアは機械学習以外のスキルも磨いていく必要があるように思う。\u003c/p\u003e\n\u003ch2 id=\"おわりに\"\u003eおわりに\u003c/h2\u003e\n\u003cp\u003e見える範囲にいる人が利益にならない精度改善をしているのを横目で見てこのようなことを考えていた。\u003cbr\u003e\n難しいけどできるだけ金を生んでいきたい。\u003c/p\u003e","title":"機械学習の精度と利益と倫理とイシューと"},{"content":"はじめに このポストではストリーム処理の survay 論文の話題に対して Apache Flink における例を挙げて紹介する。\n論文概要 Fragkoulis, M., Carbone, P., Kalavri, V., \u0026amp; Katsifodimos, A. (2020). A Survey on the Evolution of Stream Processing Systems.\n2020年の論文。\n過去30年ぐらいのストリーム処理のフレームワークを調査し、その発展を論じている。\nストリーム処理に特徴的に求められるいくつかの機能性 (functionality) についてその実現方法をいくつか挙げ、比較的古いフレームワークと最近のフレームワークでの対比を行っている。\nこのポストのスコープ このポストでは前述のストリーム処理システムに求められる機能性とそれがなぜ必要となるかについて簡単にまとめる。\n論文ではそこからさらにその実現方法がいくつか挙げられるが、ここでは個人的に興味がある Apache Flink ではどのように対処しているかを見ていく。\nちなみに論文中では Apache Flink はモダンなフレームワークの1つとしてちょいちょい引き合いに出されている。\nここでは Flink v1.11 をターゲットとする。\n以下では論文で挙げられている機能性に沿って記載していく。\nOut-of-order Data Management Out-of-order ストリーム処理システムにやってくるデータの順序は外的・内的要因により期待される順序になっていないことがある。\n外的要因としてよくあるのはネットワークの問題。\nデータソース (producer) からストリーム処理システムに届くまでのルーティング、負荷など諸々の条件により各レコードごとに転送時間は一定にはならない。\n各 operator の処理などストリーム処理システムの内的な要因で順序が乱されることもある。\nout-of-order は処理の遅延や正しくない結果の原因となることがある。\nout-of-order を管理するためにストリーム処理システムは処理の進捗を検出する必要がある。\n\u0026ldquo;進捗\u0026rdquo; とはある時間経過でレコードの処理がどれだけ進んだかというもので、レコードの順序を表す属性 A (ex. event time) により定量化される。\nある期間で処理された最古の A を進捗の尺度とみなすことができる。\nApache Flink の場合 Flink ではこの進捗を測るのに watermark という概念が使われている。\nEvent Time and Watermarks Apache Flink Event Time and Watermarks\n(図が見にくい場合はページ上部の太陽みたいなマークをクリックして light mode にしてください)\nこちらの図でストリーム中の破線で描かれているのが watermark であり、W(11) の wartermark は「timestamp が11以下の event はこの後もう来ないものとみなす」ということを下流の operator に伝えるものである。\nwatermark は metadata 的なものだが、通常の event と同じようにストリーム中を流れている (これを panctuation という)。\n下流の operator が window 処理をしていた場合、W(11) が届いた時点で timestamp が11までのところの window 処理を完結してさらに下流に output することができる。\nwatermark がいつ・どのような値で発生するかについては Flink application の開発者の実装次第ということになる。\nしかし現実的には Writing a Periodic WatermarkGenerator の例にある BoundedOutOfOrdernessGenerator のように、 WatermarkGenerator にやってきた event の event time を元に決めることが多いと思われる。\nState Management ストリーム処理における状態 \u0026ldquo;状態\u0026rdquo; とは継続的なストリーム処理の中で内部的な副作用をとらえたもの。\nアクティブな window、レコードのかたまり、aggregation の進捗など。\nユーザ定義のものも含まれる。\n状態については以下のようなトピックがある。\nProgrammability プログラミングモデルにおいて状態がどのように定義・管理されるか 定義と管理についてそれぞれシステムとユーザの場合がある Scalability and Persistency 最近のストリーム処理は scalable の時流を汲んでおり、scale out するときに状態をどのように扱うか 内外の記憶装置に状態を永続化するという方法がよく取られる Consistency transaction level の保証について Apache Flink の場合 ドキュメントの TOP ページ における Flink を表す一文\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams.\nにおいても \u0026ldquo;stateful\u0026rdquo; という言葉が使われているとおり、状態の扱いは Flink の設計思想の中でもかなり重要な部分となっている。\nFlink における状態の扱いについてはこちらを参照。\nStateful Stream Processing Programmability Flink では application 開発者が任意の状態を定義することができる。\nWorking with State 一方で状態の管理はフレームワーク側でやってくれるので、開発者は checkpoint や restore 等のことは特に配慮する必要はない。\n論文中ではこれを \u0026ldquo;User-Declared System-Managed State\u0026rdquo; と呼んでおり、最近のストリーム処理システムの傾向となっている。\nScalability Flink では keyBy() により key-level の状態を持つことができる。\nkey ごとに並列 task 内での partitioning し、分散することが可能ということである。\nPersistency 論文では永続化については scalability と絡めて述べられていたが、Flink のドキュメントでは fault tolerance の文脈で永続化について書かれている。\nFlink の fault tolerance の肝は stream replay と checkpointing である。\ncheckpointing とはストリームと operator の状態の一貫性のあるスナップショットをとることである。\nApache Flink Snapshotting Operator State\nこの checkpoint を作成する過程で各 operator の状態が state backend へと永続化される。\nstate backend では RocksDB の key/value store に各 checkpoint, 各 operator の状態が保存される。\n(RocksDB 以外にもメモリやファイルシステムなどもある)\nConsistency Flink では checkpoint のインターバルの期間の単位 (epoch という) で一貫性のある状態を永続化する。\n上図の barriers がその単位を決めている。\nChandy Lamport algorithm という分散スナップショットの手法がインスパイアされており、unaligned/aligned で各 operator の状態のスナップショットを取るようになっている。\nFault Tolerance \u0026amp; High Availability Fault Tolerance ストリーム処理システムにとって fault tolerance は2つの理由から重要である。\nストリーム処理システムは stateful な計算を終わりのないデータに対して行っている fault tolerance がなければ、障害があったときに最初から状態を計算しなおさなければならない 一方で、多くの場合過去に処理されたデータは既に失われている 最近のストリーム処理システムは分散アーキテクチャを採用している 物理マシンの数だけ問題が起こりやすくなる output commit problem についても考慮する必要がある。\nこれは出力が公開された位置から状態を復元できることが確かな場合のみ、システムは外界に出力を公開するというもの。\n言い換えると、障害からの復旧時などに同じ出力を2回してしまわない、出力を exactly-once にできるかというものである。\nHigh Availability 過去の研究においてストリーム処理システムの可用性は recovery time, performance overhead (throughput \u0026amp; latency), resource utilization により定量化されてきた。\nこの論文では\nA streaming system is available when it can provide output based on the processing of its current input.\nを可用性の定義として提案する。\n時間ごとの processing time と event time の差により定量化される。\nApache Flink の場合 Fault Tolerance 論文中では Flink は output commit problem については Kafka などの出力先の外部システムの責任とするスタンスだとしている。\nKafka には idempotent producer という機能があり、たぶんこれのことを言っている。\nまた一方で TwoPhaseCommitSinkFunction の2相コミットによって exectly-once semantics を提供するという方法も示されている。\nAn Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!) checkpointing における JobManager を2相コミットの coordinator とみなし、checkpoint barrier が最後の operator に到達するまでをコミット要求相、その後の JobManager からの checkpointing 完了通知をコミット相としている。\nコミット相において外部システムへの書き出しの transaction が完了する形となる。\nfault tolerance については State Management の項も参照。\nHigh Availability Flink のプロセスには JobManager と TaskManager があり、前者は cluster に1つだけ動く。\nしたがって JobManager が SPOF になり、可用性に影響しうる。\nhigh availability (高可用性) を実現するためには JobManager が SPOF となることを避けることができる。\nstandalone または YARN の cluster として deploy した場合は JobManager が SPOF となることを避けることができる。\n以下は standalone の例。\n1つの JobManager が leader として動いているが、それが crash すると standby のインスタンスが leader を引き継ぐ。\n(論文中では passive replication として紹介)\nApache Flink Standalone Cluster High Availability\nLoad Management, Elasticity \u0026amp; Reconfiguration Load Management ストリーム処理システムは、外部のデータソースがデータを送る流速を制御することができない。\n入力データの流速がシステムのキャパより大きいことによるパフォーマンス劣化を防ぐための対応が必要となる。\n次のような手法がある。\nload shedding 多すぎる入力データを落とす方法 back-pressure 入力データを落とせないときに buffering と組み合わせて使う dataflow graph 上に速度制限が波及していく elasticity 分散アーキテクチャと cloud にもとづく方法 いわゆる scale out Apache Flink の場合 Flink では back pressure および elasticity の組み合わせとなっている。\nback pressure は一時的な入力データの増加に対応する。\n各 operator (subtask?) は入出力の buffer を持っており、これにより operator 間の処理速度の違いをある程度吸収できる。\nしかし入力データが著しく多くなると\nボトルネックとなる operator の処理が滞る その operator の入力 buffer がいっぱいになる (ボトルネックではない) 上流の operator の出力 buffer がいっぱいになる 上流の operator の処理が滞る (以降繰り返し) のように、dataflow graph の上流へ上流へと遅延が波及する。\nelasticity の面では、JobManager や TaskManager の追加や削除ができるようになっている。\nAdding JobManager/TaskManager Instances to a Cluster TaskManager の追加や削除においては状態の再配分が行われる。\n再配分される状態は key group という単位で partitioning されており、consistent hash 的な方法で各 TaskManager 配下の operator へと配分される。\nちなみに AWS が提供する Flink の managed service である Amazon Kinesis Data Analytics for Apache Flink では CPU 使用率をモニタリングして自動的に scale out が行われるようになっている。\nApplication Scaling in Kinesis Data Analytics for Apache Flink まとめ バッチ処理ではあまりクリティカルにならないような問題でもストリーム処理では重大な影響を及ぼすことがある。\nストリーム処理に求められる機能性を実現するに当たり、Apache Flink では checkpoint の仕組みが中心的な役割を果たしているということが理解できた。\n","permalink":"https://soonraah.github.io/posts/functionality-of-streaming-system/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003eこのポストではストリーム処理の survay 論文の話題に対して Apache Flink における例を挙げて紹介する。\u003c/p\u003e\n\u003ch3 id=\"論文概要\"\u003e論文概要\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2008.00842\"\u003eFragkoulis, M., Carbone, P., Kalavri, V., \u0026amp; Katsifodimos, A. (2020). A Survey on the Evolution of Stream Processing Systems.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e2020年の論文。\u003cbr\u003e\n過去30年ぐらいのストリーム処理のフレームワークを調査し、その発展を論じている。\u003cbr\u003e\nストリーム処理に特徴的に求められるいくつかの機能性 (functionality) についてその実現方法をいくつか挙げ、比較的古いフレームワークと最近のフレームワークでの対比を行っている。\u003c/p\u003e\n\u003ch3 id=\"このポストのスコープ\"\u003eこのポストのスコープ\u003c/h3\u003e\n\u003cp\u003eこのポストでは前述のストリーム処理システムに求められる機能性とそれがなぜ必要となるかについて簡単にまとめる。\u003cbr\u003e\n論文ではそこからさらにその実現方法がいくつか挙げられるが、ここでは個人的に興味がある Apache Flink ではどのように対処しているかを見ていく。\u003cbr\u003e\nちなみに論文中では Apache Flink はモダンなフレームワークの1つとしてちょいちょい引き合いに出されている。\u003cbr\u003e\nここでは Flink v1.11 をターゲットとする。\u003c/p\u003e\n\u003cp\u003e以下では論文で挙げられている機能性に沿って記載していく。\u003c/p\u003e\n\u003ch2 id=\"out-of-order-data-management\"\u003eOut-of-order Data Management\u003c/h2\u003e\n\u003ch3 id=\"out-of-order\"\u003eOut-of-order\u003c/h3\u003e\n\u003cp\u003eストリーム処理システムにやってくるデータの順序は外的・内的要因により期待される順序になっていないことがある。\u003cbr\u003e\n外的要因としてよくあるのはネットワークの問題。\u003cbr\u003e\nデータソース (producer) からストリーム処理システムに届くまでのルーティング、負荷など諸々の条件により各レコードごとに転送時間は一定にはならない。\u003cbr\u003e\n各 operator の処理などストリーム処理システムの内的な要因で順序が乱されることもある。\u003cbr\u003e\nout-of-order は処理の遅延や正しくない結果の原因となることがある。\u003c/p\u003e\n\u003cp\u003eout-of-order を管理するためにストリーム処理システムは処理の進捗を検出する必要がある。\u003cbr\u003e\n\u0026ldquo;進捗\u0026rdquo; とはある時間経過でレコードの処理がどれだけ進んだかというもので、レコードの順序を表す属性 \u003cem\u003eA\u003c/em\u003e (ex. event time) により定量化される。\u003cbr\u003e\nある期間で処理された最古の \u003cem\u003eA\u003c/em\u003e を進捗の尺度とみなすことができる。\u003c/p\u003e","title":"ストリーム処理システムに求められる機能性、および Apache Flink におけるその対応"},{"content":"ほとんどバッチ処理しか書いたことのない者だがストリーム処理のシステムを開発することになった。\nそれにあたって独学で調べたことなどまとめておく。\nストリーム処理とは そもそも \u0026ldquo;ストリーム処理\u0026rdquo; とは何を指しているのか。\n以下の引用が簡潔に示している。\na type of data processing engine that is designed with infinite data sets in mind. Nothing more.\n\u0026ndash; Streaming 101: The world beyond batch\nこちらは \u0026ldquo;streaming system\u0026rdquo; について述べたものだが、つまり終わりのないデータを扱うのがストリーム処理ということである。\n例えば web サービスから生まれ続けるユーザ行動ログを逐次的に処理するというのがストリーム処理。\nweb サービスが終了しないかぎりはユーザ行動ログの生成には終わりがない。\nこれに対して \u0026ldquo;1日分のユーザ行動ログ\u0026rdquo; 等のように有限の量のデータを切り出して処理する場合、これはバッチ処理となる。\nストリーム処理とバッチ処理の違いは扱うデータが無限なのか有限なのかということだ。\nこの後触れていくが、この終わりのないデータを継続的に処理し続けるというところにバッチ処理にはない難しさがある。\nなぜストリーム処理なのか なぜストリーム処理なのか。\nひとえに逐次的な入力データに対する迅速なフィードバックが求められているからと言えるだろう。\n迅速なフィードバックがビジネス上のメリットとなることは自明だ。\nSNS の配信 カーシェアリングにおける配車や料金設定 クレジットカードや広告クリックなどの不正検知 もしこれらの application が例えば hourly のバッチ処理で実装されていたらどうだろうか。\nまあ待っていられない。\n一般的なストリーム処理の構成 モダンな…と言っていいのかわからないが、ストリーム処理を行うための一般的なシステムは次の3つの要素で構成される。\nproducer broker consumer producer は最初にレコードを生成する、ストリームデータの発生源となるものである。\n例えばログを生成する web application であったり、何らかのセンサーを持つ IoT 機器であったりがこれに該当する。\nproducer は絶え間なくログを生成し、それを broker へと送る。\nbroker は producer から送られたログを格納し、任意のタイミングで取り出せるようにするものである。\n誤解を恐れずに言うとメッセージキューに近いイメージだ。\nApache Kafka クラスタや Amazon Kinesis Data Streams 等がこれに該当する。\nconsumer は broker からログを取り出し、それに対し何かしらの処理を行うものだ。\ntime window 集計であったりログからの異常検知であったり、処理した結果として何かビジネス上意味があるものを得るのである。\nこれを行うフレームワークとしては Spark Streaming や Apache Flink 等がメジャーなのだろうか。\nproducer と consumer の間に broker を挟むメリットとしては次のようなことが挙げられる。\nproducer が M 個、consumer が N 個の場合に M * N の関係になるところを broker を挟めば M + N にできる producer, consumer に多数のシステムがあったとしても各自は broker との接続だけを考えればよい 任意のタイミングでデータを読み出せる producer または consumer に問題が発生してもデータロスが起こりにくくできる その分 broker には高い可用性が求められる Kafka はクラスタで冗長構成 Kinesis Data Streams は複数 AZ でレプリケーション 時間の概念 ストリーム処理では時間の概念がいくつかあり、集計などの処理をどの時間をベースにして実行するのか、意識する必要がある。\nevent time producer 側でログイベントが発生した時間 ingestion time broker にそのログイベントのレコードが挿入された時間 processing time consumer 側でレコードを処理した時間 processing time を使うのが一番簡単なのだが、おそらく分析系の処理であれば window 集計等では event time を使うことが多いのではないだろうか。\ningestion time はおそらく実際のプロダクトではあまり使われないのではと思われる。\n(ネットワークのパフォーマンスを見るぐらい？)\nWindowing ストリーム処理の中で sum, count などを伴う集計処理を行う場合、通常は時間方向の window で切って処理するということになるのではないだろうか。\nwindow で切らずに完全なデータセットがそろうまで待つことはできないし、データが来るたびに逐次的に全体の結果を更新するしていくというのも割に合わない。\nwindow の切り方もいくつかある。\ntumbling window 固定長でオーバーラップしない sliding window 固定長でオーバーラップを含む session window いわゆる web の session のように、ある種のイベントがある期間発生しないことにより window が区切られる これらについては Flink のドキュメントが図もあってわかりやすい。\n個人的な感想だが、この time window の集計がない単なる map 的なストリーム処理であれば traditional なアーキテクチャでも難しくはない。\nしかし time window 集計が必要となった場合は Spark Streaming 等のモダンなフレームワークが威力を発揮してくる。\nWatermark 時間で window を切るときは、前述のどの時間の定義を用いるかを考えなければいけない。\nprocessing time を用いる場合は簡単だが event time はやや難しい。\nconsumer 側では event のレコードがどれくらい遅れてやってくるかわからないためだ。\nネットワークその他の影響により、event のレコードが producer -\u0026gt; broker -\u0026gt; consumer という経路で consumer に届くまでの時間というのは一定にはならない。\nまた、古い event が新しい event より後に届くというように順番が前後することも起こりうる。\nここで \u0026ldquo;watermark\u0026rdquo; という考え方が必要になってくる。\nA watermark with a value of time X makes the statement: \u0026quot;all input data with event times less than X have been observed.\u0026quot;\n\u0026ndash; Streaming 102: The world beyond batch\nある processing time において「event time X より前のレコードはすべて到着したよ」というのが watermark である。\n別の言い方をすると watermark により event のレコードがどの程度遅延してもよいかが定義される。\nevent time X より前のレコードが真の意味ですべて到着した、というのは難しい。\n実際には heuristic にどの程度遅れていいかを決め、それより遅れた場合はある event time 期間における window 処理には含めないということになる。\nwatermark の決め方はフレームワーク次第だろうか。\n例えば Spark Structured Streaming の例だと図もあって比較的わかりやすい。\nSchema Evolution 何らかの業務システムや web システム等をある程度運用したことがある人ならわかると思うが、データの schema というのはナマモノだ。\n一度決めたら終わりというわけではなくプロダクトやビジネスの変化に応じて変化していく。\nカラムが増えたり、削除されたり、名前や型が変わったり…\nこのようにデータの構造が変化していくこと、またはそれを扱うことを \u0026ldquo;schema evolution\u0026rdquo; という。\nバッチ処理において schema の変更に追従することを考えるのはそれほど難しくない。\nhourly のバッチ処理であったとしても、バッチ処理とバッチ処理の間の時間で application を更新すればいいだけだ。\n(が、実際に行うのは困難が伴うことも多い)\nではストリーム処理ではどうだろうか。\nいわゆるストリーム処理においては処理と処理の間というものがなく、application がずっと稼働しっぱなしということになる。\nバッチ処理のような更新はできない。\nもっと言うと producer で生まれた新しい schema のレコードがいつ届くかもわからない。\nおそらくこの問題には2つの対応方法がある。\n1つめは consumer 側のシステムで前方互換性を保つという方法である。\nこの場合、新しいフィールドは必ず末尾に追加される等、producer 側での schema 更新についてある程度のルールが必要となるだろう。\nproducer 側で生成されるレコードの schema の変更が必ず事前にわかるというのであれば後方互換性でもいいが、多くの場合は難しい。\nところで前方互換と後方互換、どっちがどっちなのか覚えられません。\n2つめの方法として schema 情報をレコード自体に入れ込んでしまうという方法もある。\nApach Avro のような serialization の方法を取っているとレコード自体に schema の情報を付与することができる。\nおそらく最もエレガントにこれをやるのが Confluent の Schema Registry という機能だ。\nproducer から送出されるレコードには schema ID を付与する。\nschema の実体は Schema Registry という broker とは別の場所で管理されており、consumer 側では受け取ったレコードに付与されている schema ID と Schema Registry に登録されている shcema の実体を参照してレコードを deserialize することができる。\nDeploy ストリーム処理を行うシステムは終わりのないデータを処理するためのものであり、ずっと動き続けることが期待されている。\nしかし通常システムは一度立ち上げれば終わりということではなく、運用されている中で更新していく必要がある。\nずっと動かしながらどのように deploy, release するのか。\nこの問題は主に consumer 側のシステムで配慮が必要になると思われる。\n正直これについてはちゃんと調べられていないが、2点ほど述べておきたい。\nまず1点目、application を中断・更新・再開するのにどの程度の時間がかかるのかを知っておく必要があるということ。\nアーキテクチャやフレームワーク、処理の内容や checkpoint (後述) を使うか等によりこの時間は変わってくる。\n一例だが、AWS 環境において\nAWS Glue + Spark Structured Streaming Amazon Kinesis Data Analytics + Flink の比較をしたことがある。\n前者は再開に数分かかったのに対し、後者は1分未満で再開できた。\n再開までの時間が十分に短いと判断できるのであればそのまま deploy, release してしまっていいだろう。\n一方そうでない場合はどうすべきかという話が2点目。\n再開までの時間が長く、システム要件的に許容できないというのであれば、release 時は二重で動かすというような措置が必要かもしれない。\nおそらく Blue-Green Deployment のようなことを考えることになるだろう。\nCheckpoint 前述のとおり、ストリーム処理を行うシステムはずっと動き続けることが期待されている。\nしかし予定された application の更新や不測のエラー等、何らかの理由で一時的に中断されるということが実際の運用中には起こる。\n中断されたとき速やかに復帰する仕組みとして \u0026ldquo;checkpoint\u0026rdquo; というものがいくつかの consumer 側のフレームワークで提供されている。\n雑に説明すると、処理のある時点における進捗や内部状態などをディスク等に永続化し、そこから処理を再開できるようにするものである。\nRecovering from Failures with Checkpointing - Apache Spark Checkpointing - Apache Flink 上記は Spark Structured Streaming と Flink の例だ。\ncheckpoint には次のようなメリットがあり、運用上有用だと言える。\n内部の状態を保持しているため、速やかに復帰できる 中断した位置から再開できるので出力に穴が開かない 一方で落とし穴もある。\ncheckpoint では内部の状態が永続化されるわけだが、内部の状態というのは当然 application の実装が決めているものである。\napplication のコードを変更したとき、変更の内容によっては永続化された checkpoint と application が合わなくなることがあるのだ。\n未定義の挙動となることもあるので、checkpoint の運用には十分に配慮する必要がある。\nどのような変更なら checkpoint が安全に利用できるのかはフレームワークのドキュメントに記載があるので確認しておきたい。\nRDB の世界との折り合い みんな大好きな RDB の世界では table を操作してデータの処理を行う。\n基本的には table というものはある時点における完全なデータセットを表すものである。 (ex. isolation)\n他方、ストリーム処理はやってきたデータを逐次的に処理するものである (mini-batch の場合もあるが)。\n直感的にこの2つは相性が悪そうに見える。\nしかし Spark や Flink では table ベースの操作でストリーム処理を行うための API が提供されている。\nおそらく\nストリーム処理の周辺のデータソースとして RDB が存在する RDB 的な table 操作があまりにも浸透している というところが API が必要である理由なのだろう。\nストリームデータを table 的に扱うというのが、やや直感的な理解をしにくいものとなっている。\nフレームワークのドキュメントを確認しておきたい。\n例えば Spark Structured Streaming であれば処理の出力のための3つの output mode が示されている。\nAppend mode: 追加された行だけ出力 Complete mode: table 全体を出力 Update mode: 更新された行だけ出力 どれを選ぶかにより必要とする内部メモリの大きさも影響される。\nまとめ 思ったより長文になってしまった。\n結局ストリーム処理の難しさは以下の2点に尽きるだろう。\n複数の時間の概念 常時稼働のシステム 独学なので抜け漏れがあったり、話が新しくなかったりすることもあると思われる。\n参考 Streaming 101: The world beyond batch Apache Beam PMC によるストリーム処理の解説ポスト。必読 Streaming 102: The world beyond batch 上の続きであり watermark について触れている Analytics Lens - AWS Well-Architected Framework AWS の資料。ストリーム処理のシステムの全体感がつかめる Structured Streaming Programming Guide - Apache Spark Spark Structured Streaming のドキュメント。consumer の気持ちがわかる Flink DataStream API Programming Guide - Apache Flink Flink のドキュメントの方がより詳しい。DataStream API の解説を中心に読むとよい ","permalink":"https://soonraah.github.io/posts/study-streaming-system/","summary":"\u003cp\u003eほとんどバッチ処理しか書いたことのない者だがストリーム処理のシステムを開発することになった。\u003cbr\u003e\nそれにあたって独学で調べたことなどまとめておく。\u003c/p\u003e\n\u003ch2 id=\"ストリーム処理とは\"\u003eストリーム処理とは\u003c/h2\u003e\n\u003cp\u003eそもそも \u0026ldquo;ストリーム処理\u0026rdquo; とは何を指しているのか。\u003cbr\u003e\n以下の引用が簡潔に示している。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ea type of data processing engine that is designed with infinite data sets in mind. Nothing more.\u003c!-- raw HTML omitted --\u003e\u003cbr\u003e\n\u0026ndash; \u003c!-- raw HTML omitted --\u003e\u003ca href=\"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/\"\u003eStreaming 101: The world beyond batch\u003c/a\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eこちらは \u0026ldquo;streaming system\u0026rdquo; について述べたものだが、つまり終わりのないデータを扱うのがストリーム処理ということである。\u003c/p\u003e\n\u003cp\u003e例えば web サービスから生まれ続けるユーザ行動ログを逐次的に処理するというのがストリーム処理。\u003cbr\u003e\nweb サービスが終了しないかぎりはユーザ行動ログの生成には終わりがない。\u003c/p\u003e\n\u003cp\u003eこれに対して \u0026ldquo;1日分のユーザ行動ログ\u0026rdquo; 等のように有限の量のデータを切り出して処理する場合、これはバッチ処理となる。\u003cbr\u003e\nストリーム処理とバッチ処理の違いは扱うデータが無限なのか有限なのかということだ。\u003cbr\u003e\nこの後触れていくが、この終わりのないデータを継続的に処理し続けるというところにバッチ処理にはない難しさがある。\u003c/p\u003e\n\u003ch2 id=\"なぜストリーム処理なのか\"\u003eなぜストリーム処理なのか\u003c/h2\u003e\n\u003cp\u003eなぜストリーム処理なのか。\u003cbr\u003e\nひとえに逐次的な入力データに対する迅速なフィードバックが求められているからと言えるだろう。\u003cbr\u003e\n迅速なフィードバックがビジネス上のメリットとなることは自明だ。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSNS の配信\u003c/li\u003e\n\u003cli\u003eカーシェアリングにおける配車や料金設定\u003c/li\u003e\n\u003cli\u003eクレジットカードや広告クリックなどの不正検知\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eもしこれらの application が例えば hourly のバッチ処理で実装されていたらどうだろうか。\u003cbr\u003e\nまあ待っていられない。\u003c/p\u003e","title":"バッチ処理おじさんがストリーム処理のシステムを開発するにあたって調べたこと"},{"content":"前提 ここでは web システムで使われている機械学習のモデルやアルゴリズムを改善するための online の A/B テストを考える。\n具体的に述べると web 広告における CTR 予測や EC サイトのレコメンデーション等が対象である。\nよくあるやつ。\nweb システムにおいて online の A/B テストは KPI 改善の根幹でありとても重要だ。\nそれが重くなるとつらい、という話。\nここで「重い」と言っているのは計算資源のことではなく、A/B テストを実施する担当者の運用コストについて。\nA/B テストの運用が重い場合のデメリット デメリット 1. KPI 改善が遅くなる デメリットと言えばこれが一番大きい。\n単純に A/B テストを1回まわすのに時間がかかってしまうし、それがゆえに online の A/B テストに入るまでの offline のテストが厚くなりここでも時間がかかってしまう。\nKPI 改善に時間がかかるというのはつまり売上や利益を大きくするのに時間がかかってしまうということである。\nデメリット 2. KPI 改善における offline テストの比重が大きくなる 前述のとおりだが online の A/B テストが重いとそこで失敗できなくなり、結果としてその前段の offline のテストを厚くするということになる。\noffline のテストが厚いことの何が問題だろうか。\nここで前提としている CTR 予測やレコメンデーションのようなタスクの場合、offline のデータは既存のモデルやアルゴリズムの影響を受けることになる。\n例えばレコメンデーションの場合を考えると、新しいモデルを offline で評価するための実験データの正例 (コンテンツの閲覧等) は既存モデルによって生み出される。\n既存モデルが「このコンテンツがいいよ」といってユーザに出したリスト、その中からコンテンツの閲覧が行われ正例となるからだ。\nこのような状況下での offline テストにおいては既存モデルと近い好みを持ったモデルのスコアが高くなる傾向がある。\nもしかしたら既存モデルの影響を受けないようなデータをあえて用意しているような場合もあるかもしれないが、レアケースだろう。\n既存モデルの影響を受けないということは恩恵を受けないということなのでトレードオフでもある。\nデメリット 3. 新モデル／アルゴリズムを却下しにくくなる A/B テストの運用が重いと何度も繰り返すことができない。\nなので一発の A/B テストで既存モデル／アルゴリズムを置き換える成果を出さなければならないという圧が強くかかってしまう。\n既存モデル／アルゴリズムが勝って新しいものが負けてしまったときの埋没費用が大きいからだ。\nそうなると次のような事が起こる。\n「新モデルをチューニングしてみよう！」 数ヶ月前のデータでチューニングされていない既存モデル vs. 直近のデータでチューニングされた新モデル 「新モデルが KPI で勝っている間に意思決定しよう！」 勝ったり負けたりする中で… つまりフェアなテストではなくなってしまう。\nなぜ A/B テスト運用が重くなるのか 理由 1. リリース作業のコスト A/B テストの運用が重くなる理由の1として、A/B テストを始めるときのオペレーション、つまり新しいモデル／アルゴリズムをリリースする際の作業コストが重いことが挙げられる。\nこれにはモデル／アルゴリズムが web のシステムに対してどのような形でデプロイされているかが影響する。\nmercari が次のような資料を公開しているので参考にしたい。\n機械学習システムの設計パターンを公開します。 例えば Synchronous pattern のように web サーバと推論をする場所が分離している場合に比べて Web single pattern のように一体化している場合は新しいモデル／アルゴリズムのリリースに繊細にならざるを得ないのではないだろうか。\nコードベースもそうだし、マシンリソースの管理も後者の方が難しい。\n特にリリース担当者がデータサイエンティスト的な人だった場合、リリースの心理的障壁が上がる。\n理由 2. はっきりしない指標 A/B テストするときはテストの指標として何らかの KPI を置く。\nこの KPI が複数ある場合がある。\n例えば web 広告における CTR 予測モデルであれば CTR と impression 等。\n現実のビジネスは複雑なので複数の KPI があるのは珍しくないと思われる。\n一方で KPI が複数になると評価が難しくなる。\n「KPI A は5上がったけど B は3下がった、この場合はどうなの？」ということになり \u0026ldquo;マネージャの肌感\u0026rdquo; みたいなものが必要になってしまう。\n当然機械的に判定することによる自動化なども行えない。\nA/B テストの経過における現状の良し悪しの判断にコストがかかってしまうことになる。\nまた判断基準が複雑なことによりおとり効果のようなことも発生しうるだろう。\nこれについての1つの解として OEC: Overall Evaluation Criterion という考え方がある。\nWhat does \u0026ldquo;Overall Evaluation Criterion\u0026rdquo; mean? 複数の KPI を重みをつけて組み合わせてただ1つの値として評価できるようにする、というものらしい。\n逆に考えると OEC を定義せずに複数の KPI で A/B テストをするということは KPI 間のバランスの合意なしで走ってしまっているということになる。\n理由 3. 煩雑な流量オペレーション これは実際に見たことだが、A/B テストの運用に際して組織の一部の人間しか理解していない煩雑なオペレーションを伴うことがある。\n最初は control group を 0.1% だけ流して、1日経って問題がなければ 1% に上げて、その次は5%で…などのような操作を求められるのである。\n担当者にすればまあまあのストレスだし、時間がかかってしまう。\nまとめ ソフトウェア開発の分野でも大きな変更をえいやーでリリースするのはキツイということで git-flow であったり GitHub Flow であったりが導入されてきたはず。\nA/B テストが重いということはそれの逆をいっている状態となる。つらい。\n","permalink":"https://soonraah.github.io/posts/heavy-ab-testing-operation/","summary":"\u003ch2 id=\"前提\"\u003e前提\u003c/h2\u003e\n\u003cp\u003eここでは web システムで使われている機械学習のモデルやアルゴリズムを改善するための online の A/B テストを考える。\u003cbr\u003e\n具体的に述べると web 広告における CTR 予測や EC サイトのレコメンデーション等が対象である。\u003cbr\u003e\nよくあるやつ。\u003c/p\u003e\n\u003cp\u003eweb システムにおいて online の A/B テストは KPI 改善の根幹でありとても重要だ。\u003cbr\u003e\nそれが重くなるとつらい、という話。\u003cbr\u003e\nここで「重い」と言っているのは計算資源のことではなく、A/B テストを実施する担当者の運用コストについて。\u003c/p\u003e\n\u003ch2 id=\"ab-テストの運用が重い場合のデメリット\"\u003eA/B テストの運用が重い場合のデメリット\u003c/h2\u003e\n\u003ch3 id=\"デメリット-1-kpi-改善が遅くなる\"\u003eデメリット 1. KPI 改善が遅くなる\u003c/h3\u003e\n\u003cp\u003eデメリットと言えばこれが一番大きい。\u003cbr\u003e\n単純に A/B テストを1回まわすのに時間がかかってしまうし、それがゆえに online の A/B テストに入るまでの offline のテストが厚くなりここでも時間がかかってしまう。\u003cbr\u003e\nKPI 改善に時間がかかるというのはつまり売上や利益を大きくするのに時間がかかってしまうということである。\u003c/p\u003e\n\u003ch3 id=\"デメリット-2-kpi-改善における-offline-テストの比重が大きくなる\"\u003eデメリット 2. KPI 改善における offline テストの比重が大きくなる\u003c/h3\u003e\n\u003cp\u003e前述のとおりだが online の A/B テストが重いとそこで失敗できなくなり、結果としてその前段の offline のテストを厚くするということになる。\u003cbr\u003e\noffline のテストが厚いことの何が問題だろうか。\u003c/p\u003e\n\u003cp\u003eここで前提としている CTR 予測やレコメンデーションのようなタスクの場合、offline のデータは既存のモデルやアルゴリズムの影響を受けることになる。\u003cbr\u003e\n例えばレコメンデーションの場合を考えると、新しいモデルを offline で評価するための実験データの正例 (コンテンツの閲覧等) は既存モデルによって生み出される。\u003cbr\u003e\n既存モデルが「このコンテンツがいいよ」といってユーザに出したリスト、その中からコンテンツの閲覧が行われ正例となるからだ。\u003cbr\u003e\nこのような状況下での offline テストにおいては既存モデルと近い好みを持ったモデルのスコアが高くなる傾向がある。\u003c/p\u003e","title":"A/B テストの運用が重くてつらいという話"},{"content":"前回の記事 では Apache Flink における stream data と static data の join において、DataStream API における broadcast state pattern を使う方法を示した。\n今回の記事では Table API の temporal table function を用いた実験を行う。\nTable API Table API は名前のとおりで class Table を中心として SQL-like な DSL により処理を記述するという、DataStream API より high-level な API となっている。\nこれらの関係は Apaceh Spark の RDD と DataFrame (DataSet) の関係に似ている。\nSQL-like な API で記述された処理が実行時に最適化されて low-level API の処理に翻訳されるところも同じだ。\nRDB の table の概念を元にしているものと考えられるが、本質的に table の概念とストリーム処理はあまりマッチしないと思う。\ntable はある時点のデータセット全体を表すのに対し、ストリーム処理ではやってくるレコードを逐次的に処理したい。\nここを合わせているため、ストリーム処理における Table API による処理の挙動の理解には注意が必要だ。\nStreaming Concepts 以下のドキュメントを確認しておきたい。\nTemporal Table Function star schema における、変更されうる dimension table を stream data と結合する方法として、temporal table という仕組みが提供されている。\nドキュメントでは為替レートの例が示されている。\nstream でやってくる fact table 的なレコードに対して、為替のように時々刻々と変化する dimension table をそのレコードの時刻における snap shot としてぶつけるような形となる。\nレコードの時刻としては processing time または event time を扱うことができる。\nevent time の場合であっても watermark で遅延の許容を定義できるため dimension table のすべての履歴を状態として保持する必要はなく、processing time または event time の watermark に応じて過去の履歴は捨てることが可能となっている。\nTable API において temporal table を使うには temporal table function という形を取ることになる。\n実験 実験概要 やることは 前回の記事 とまったく同じで乱数で作った株価のデータを扱う。\n前回と違うのは DataStream API ではなく Table API で処理を記述したところである。\nコード 上記を実行するためのコードを以下に示す。\n実行可能なプロジェクトは GitHub に置いておいた。\nEntry Point toTable() により入力データの DataStream を Table に変換した後、処理を記述した。\nfunc が temporal table function に当たる。\n今回は processing time を基準として join しているが、実際のシステムでは event time を基準としたいことが多いのではないだろうか。\npackage example import org.apache.flink.api.java.io.TextInputFormat import org.apache.flink.api.scala._ import org.apache.flink.core.fs.Path import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration import org.apache.flink.streaming.api.TimeCharacteristic import org.apache.flink.streaming.api.functions.source.FileProcessingMode import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment} import org.apache.flink.table.api.bridge.scala.{StreamTableEnvironment, _} import org.apache.flink.table.api.{AnyWithOperations, EnvironmentSettings, FieldExpression, call} import org.apache.flink.test.util.MiniClusterWithClientResource object FlinkTableJoinTest { // See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html#testing-flink-jobs private val FlinkCluster = new MiniClusterWithClientResource( new MiniClusterResourceConfiguration .Builder() .setNumberSlotsPerTaskManager(2) .setNumberTaskManagers(1) .build ) def main(args: Array[String]): Unit = { FlinkCluster.before() // for batch programs use ExecutionEnvironment instead of StreamExecutionEnvironment val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime) env.setParallelism(2) // create settings val setting = EnvironmentSettings .newInstance() .useBlinkPlanner() .inStreamingMode() .build() // create a TableEnvironment val tableEnv = StreamTableEnvironment.create(env, setting) // create a Table instance for Company val companiesMasterFilePath = \u0026#34;data/companies.csv\u0026#34; val companies = readCompaniesMaster(companiesMasterFilePath, env) .toTable(tableEnv, $\u0026#34;ticker\u0026#34;.as(\u0026#34;c_ticker\u0026#34;), $\u0026#34;name\u0026#34;, $\u0026#34;c_proc_time\u0026#34;.proctime) // temporal table function val func = companies.createTemporalTableFunction($\u0026#34;c_proc_time\u0026#34;, $\u0026#34;c_ticker\u0026#34;) // create a Table instance for Stock val stocks = env .fromCollection(new UnboundedStocks) .toTable(tableEnv, $\u0026#34;ticker\u0026#34;.as(\u0026#34;s_ticker\u0026#34;), $\u0026#34;price\u0026#34;, $\u0026#34;s_proc_time\u0026#34;.proctime) // join with a temporal table function val results = stocks .joinLateral(call(func, $\u0026#34;s_proc_time\u0026#34;), $\u0026#34;s_ticker\u0026#34; === $\u0026#34;c_ticker\u0026#34;) .select($\u0026#34;s_ticker\u0026#34;, $\u0026#34;name\u0026#34;, $\u0026#34;price\u0026#34;) .toAppendStream[(String, String, Double)] .print env.execute() FlinkCluster.after() } private def readCompaniesMaster(companiesMasterFilePath: String, env: StreamExecutionEnvironment): DataStream[Company] = { env .readFile( new TextInputFormat(new Path(companiesMasterFilePath)), companiesMasterFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 10 * 1000 ) .map { line =\u0026gt; val items = line.split(\u0026#34;,\u0026#34;) Company(items(0), items(1)) } } } 実行結果 上記を実行すると以下のような stream と static が結合された結果レコードが流れ続ける。\n2\u0026gt; (AMZN,Amazon,110.05826176785374) 2\u0026gt; (AMZN,Amazon,237.82717323588966) 1\u0026gt; (FB,Facebook,147.96046700184428) 1\u0026gt; (GOOGL,Google,393.58555322242086) 2\u0026gt; (AMZN,Amazon,104.18843434881401) 前回と同様に data/companies.csv の中身を更新するとその結果が反映される。\n削除が反映されないのも同じだった。\nおそらく physical な処理としてはほぼ同じようになっていると思われる。\nまとめ 前回と同様の stream data と static data の join を、Table API + temporal table function で行えることを確認した。\ntemporal table function の概念さえ把握できれば Straem API のときに比べて簡潔に処理を記述できた。\n","permalink":"https://soonraah.github.io/posts/flink-join-by-temporal-table-function/","summary":"\u003cp\u003e\u003ca href=\"https://soonraah.github.io/posts/flink-join-by-broadcast-state-pattern/\"\u003e前回の記事\u003c/a\u003e では Apache Flink における stream data と static data の join において、DataStream API における broadcast state pattern を使う方法を示した。\u003cbr\u003e\n今回の記事では Table API の temporal table function を用いた実験を行う。\u003c/p\u003e\n\u003ch2 id=\"table-api\"\u003eTable API\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/\"\u003eTable API\u003c/a\u003e は名前のとおりで class \u003ccode\u003eTable\u003c/code\u003e を中心として SQL-like な DSL により処理を記述するという、DataStream API より high-level な API となっている。\u003cbr\u003e\nこれらの関係は Apaceh Spark の \u003ccode\u003eRDD\u003c/code\u003e と \u003ccode\u003eDataFrame\u003c/code\u003e (\u003ccode\u003eDataSet\u003c/code\u003e) の関係に似ている。\u003cbr\u003e\nSQL-like な API で記述された処理が実行時に最適化されて low-level API の処理に翻訳されるところも同じだ。\u003c/p\u003e\n\u003cp\u003eRDB の table の概念を元にしているものと考えられるが、本質的に table の概念とストリーム処理はあまりマッチしないと思う。\u003cbr\u003e\ntable はある時点のデータセット全体を表すのに対し、ストリーム処理ではやってくるレコードを逐次的に処理したい。\u003cbr\u003e\nここを合わせているため、ストリーム処理における Table API による処理の挙動の理解には注意が必要だ。\u003cbr\u003e\n\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/\"\u003eStreaming Concepts\u003c/a\u003e 以下のドキュメントを確認しておきたい。\u003c/p\u003e","title":"Apache Flink の Temporary Table Function を用いた stream data と static data の join"},{"content":"star schema における fact table と dimension table の join のようなことを Apache Flink におけるストリーム処理で行いたい。\nstream data と static data の join ということになる。\nただし dimension table 側も更新されるため、完全な static というわけではない。\nこのポストでは Flink v1.11 を前提とした。\njoin の方法 今回は DataStream API でこれを実現することを考える。\nFlink のドキュメントを読むと broadcast state pattern でできそうだ。\nThe Broadcast State Pattern やり方としては次のようになる。\nstatic data のファイルを FileProcessingMode.PROCESS_CONTINUOUSLY で読み込み DataStream 化 1 を broadcast() stream data の DataStream と 2 を connect() static data を PROCESS_CONTINUOUSLY で読むのは変更を得るため。\nPROCESS_ONCE で読んでしまうとストリーム処理の開始時に1回読むだけになり、dimension table の変更を得られない。\nこのあたりの仕様については Data Sources を参照。\nその後は broadcast state pattern にそのまま従う。\nしたがって BroadcastProcessFunction or KeyedBroadcastProcessFunction を実装する必要がある。\nその中で static data を取り込んで state として持ち、stream data 側で参照すればよい。\n2つのデータの各 term に対する関係性を以下に示す。\nin star schema stream or static broadcast or not dimension table static data broadcasted fact table stream data non-broadcasted 実験 実験概要 stream data として株価のデータを考える。\n適当に乱数で作った株価が \u0026ldquo;GOOGL\u0026rdquo; 等の ticker とともに流れてくる。\n一方、会社情報が記載された dimension table 的なファイルも用意する。\n流れ続ける株価データに対して ticker を key にして会社情報を紐付ける、ということを行う。\nコード 上記を実行するためのコードを以下に示す。\n実行可能なプロジェクトを GitHub に置いたので興味があればどうぞ。\nEntry Point main() の実装。\nMiniClusterWithClientResource は本来は単体テスト用だが、簡単に local で cluster を動かすためにここで使用している。\npackage example import org.apache.flink.api.common.state.MapStateDescriptor import org.apache.flink.api.java.io.TextInputFormat import org.apache.flink.core.fs.Path import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration import org.apache.flink.streaming.api.functions.source.FileProcessingMode import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment, createTypeInformation} import org.apache.flink.test.util.MiniClusterWithClientResource object FlinkJoinTest { // See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html#testing-flink-jobs private val FlinkCluster = new MiniClusterWithClientResource( new MiniClusterResourceConfiguration .Builder() .setNumberSlotsPerTaskManager(2) .setNumberTaskManagers(1) .build ) def main(args: Array[String]): Unit = { FlinkCluster.before() val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val companiesMasterFilePath = \u0026#34;data/companies.csv\u0026#34; val companies = readCompaniesMaster(companiesMasterFilePath, env) .broadcast(new MapStateDescriptor( \u0026#34;CompanyState\u0026#34;, classOf[String], // ticker classOf[String] // name )) // the broadcast state pattern // See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html env .fromCollection(new UnboundedStocks) .connect(companies) .process(new StockBroadcastProcessFunction) .print() env.execute(\u0026#34;flink join test\u0026#34;) FlinkCluster.after() } private def readCompaniesMaster(companiesMasterFilePath: String, env: StreamExecutionEnvironment): DataStream[Company] = { env .readFile( new TextInputFormat(new Path(companiesMasterFilePath)), companiesMasterFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 10 * 1000 ) .map { line =\u0026gt; val items = line.split(\u0026#34;,\u0026#34;) Company(items(0), items(1)) } } } Records 各種レコードを表す case class。\nUnboundedStocks は一定のインターバルで Stock を無限に返す iterator であり、stream data 生成に利用する。\ncase class Company(ticker: String, name: String) case class Stock(ticker: String, price: Double) /** * Iterator to generate unbounded stock data */ class UnboundedStocks extends Iterator[Stock] with Serializable { override def hasNext: Boolean = true // unbounded override def next(): Stock = { Thread.sleep(1000) val tickers = Seq(\u0026#34;GOOGL\u0026#34;, \u0026#34;AAPL\u0026#34;, \u0026#34;FB\u0026#34;, \u0026#34;AMZN\u0026#34;) val ticker = tickers(Random.nextInt(tickers.size)) // one of GAFA val price = 100 + Random.nextDouble() * 300 // random price Stock(ticker, price) } } BroadcastProcessFunction 肝である BroadcastProcessFunction の実装。\npackage example import org.apache.flink.api.common.state.MapStateDescriptor import org.apache.flink.streaming.api.functions.co.BroadcastProcessFunction import org.apache.flink.util.Collector class StockBroadcastProcessFunction extends BroadcastProcessFunction[Stock, Company, (String, String, Double)] { private val StateDescriptor = new MapStateDescriptor( \u0026#34;CompanyState\u0026#34;, classOf[String], // ticker classOf[String] // name ) override def processElement(value: Stock, ctx: BroadcastProcessFunction[Stock, Company, (String, String, Double)]#ReadOnlyContext, out: Collector[(String, String, Double)]): Unit = { val companyName = ctx.getBroadcastState(StateDescriptor).get(value.ticker) out.collect((value.ticker, Option(companyName).getOrElse(\u0026#34;-\u0026#34;), value.price)) } override def processBroadcastElement(value: Company, ctx: BroadcastProcessFunction[Stock, Company, (String, String, Double)]#Context, out: Collector[(String, String, Double)]): Unit = { ctx.getBroadcastState(StateDescriptor).put(value.ticker, value.name) } } 実行結果 上記を実行すると以下のような stream と static が結合された結果レコードが流れ続ける。\n2\u0026gt; (FB,Facebook,158.76057838239333) 1\u0026gt; (GOOGL,Google,288.4271251901199) 2\u0026gt; (AAPL,Apple,191.00515338617706) 1\u0026gt; (FB,Facebook,121.98205452369652) 2\u0026gt; (FB,Facebook,140.05023554456997) この状態で会社情報が記載されている data/companies.csv を更新することを考える。\n例えば \u0026ldquo;GOOGL\u0026rdquo; の社名を \u0026ldquo;Google\u0026rdquo; から \u0026ldquo;Alphabet\u0026rdquo; に変更して保存してみた。\nするとしばらくしてその修正が反映された結果が流れてくるようになる。\n1\u0026gt; (GOOGL,Alphabet,288.1008081843843) 2\u0026gt; (AMZN,Amazon,137.11135563851838) 1\u0026gt; (GOOGL,Alphabet,121.78368168964735) 2\u0026gt; (FB,Facebook,236.53483047124948) 1\u0026gt; (FB,Facebook,220.44300865769645) static data の更新が反映されることが確認できた。\n今回は10秒に1回のインターバルで元ファイルを確認するようにファイルを読んでいるため、変更してからそれが反映されるまで最大10秒程度かかる。\n懸念点 join はできたが次のような懸念点がある。\nレコードの削除に対応していない state を上書きしているだけなので companies.csv から削除されたレコードは感知できない ファイルの更新時に処理が重くなる可能性がある companies.csv の更新タイミングでその中の全レコードを処理してしまう checkpoint が大きくなる state が broadcast されているため、task ごとに重複した state が保存されてしまう See Important Considerations まとめ このように broadcast state pattern によって stream data と static data との join 処理を行うことができた。\nただし、まだちゃんと調べていないが DataStream API ではなく Table API を使えばもう少しカジュアルな感じで近いことができるかもしれない。\n気が向いたらそちらも試してみる。\n追記 Table API を使った場合についての記事を追加しました。\nApache Flink の Temporary Table Function を用いた stream data と static data の join ","permalink":"https://soonraah.github.io/posts/flink-join-by-broadcast-state-pattern/","summary":"\u003cp\u003estar schema における fact table と dimension table の join のようなことを Apache Flink におけるストリーム処理で行いたい。\u003cbr\u003e\nstream data と static data の join ということになる。\u003cbr\u003e\nただし dimension table 側も更新されるため、完全な static というわけではない。\u003c/p\u003e\n\u003cp\u003eこのポストでは Flink v1.11 を前提とした。\u003c/p\u003e\n\u003ch2 id=\"join-の方法\"\u003ejoin の方法\u003c/h2\u003e\n\u003cp\u003e今回は DataStream API でこれを実現することを考える。\u003cbr\u003e\nFlink のドキュメントを読むと broadcast state pattern でできそうだ。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html\"\u003eThe Broadcast State Pattern\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eやり方としては次のようになる。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003estatic data のファイルを \u003ccode\u003eFileProcessingMode.PROCESS_CONTINUOUSLY\u003c/code\u003e で読み込み \u003ccode\u003eDataStream\u003c/code\u003e 化\u003c/li\u003e\n\u003cli\u003e1 を \u003ccode\u003ebroadcast()\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003estream data の \u003ccode\u003eDataStream\u003c/code\u003e と 2 を \u003ccode\u003econnect()\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003estatic data を \u003ccode\u003ePROCESS_CONTINUOUSLY\u003c/code\u003e で読むのは変更を得るため。\u003cbr\u003e\n\u003ccode\u003ePROCESS_ONCE\u003c/code\u003e で読んでしまうとストリーム処理の開始時に1回読むだけになり、dimension table の変更を得られない。\u003cbr\u003e\nこのあたりの仕様については \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html#data-sources\"\u003eData Sources\u003c/a\u003e を参照。\u003c/p\u003e","title":"Apache Flink の Broadcast State Pattern を用いた stream data と static data の join"},{"content":"GitHub Flow ベースの開発においてはあまり大きな pull request を作ってほしくないという話。\nなんというか今更わざわざ言わなくてもいいんだけど…\n仕事で何度か大きな pull request が投げられているのを見てしまったので、それはあまりよくないよというのを自分でも指摘しやすくするためにまとめておく。\nReference 最初に参考資料を挙げておく。\n100 Duck-Sized Pull Requests 「巨大プルリク1件vs細かいプルリク100件」問題を考える（翻訳） Optimal pull request size これらを読めば特に私から言うこともないのだが…\nこれらに書いてあるとおりだが補足しておく。\nPull Request を小分けにしたときのメリット module を適切に切り出すモチベーションが得られる 次のような話がある。\n共通化という考え方はアンチパターンを生み出すだけ説 これを理由に module を分けるべきところが分けられず、いろんなことができてしまうベタッと大きな module が生まれるのを見た。\nもちろんよく考えられていない共通化は駄目だが、上記ポストでは一方で\nただし共通化という名の下におこなわれるのは「同じロジックを持つコードをまとめる」行為であって、抽象化のようにそのコード単位の意味を捉える作業はその範疇にない。抽象化というのはロジックを意味単位ごとにひとくくりにしていく行為で、これがどういうことなのかは次以降で述べていく。\n\u0026ndash; 共通化という考え方はアンチパターンを生み出すだけ説 - タオルケット体操\nと述べており抽象化、つまりコードの意味を考慮した上で適切な単位でまとめておくことは否定していない。\npull request を小さく分けるという行為のためには module や機能を適度なまとまりで切り分けること、つまり抽象化を考えていくことが必要となる。\nしたがって何でもできてしまう大きな module が生まれるのを防ぐ方向に働く。\n(もちろんここで駄目な共通化がなされてしまうこともあるだろう)\nリリースまでの期間が短く済む 前述の参考資料においても pull request が大きいと\nDevelopers would put off reviews until they had time/energy and the development process would come to a halt.\n\u0026ndash; 100 Duck-Sized Pull Requests\ndevelopment continues on the master branch, it often results in merge conflicts, rebases, and other fun.\n\u0026ndash; 100 Duck-Sized Pull Requests\nとなると述べられている。\n仮にすぐにレビューされ、かつ conflict なども発生しなかっとしても大きな1つの pull request をレビューする場合とそれを3つに分けた場合の開発プロセスの進行を比較すると\n大きな1つの pull request 全体の開発 全体のレビュー 3つに分割された pull request part-1 の開発 part-1 のレビュー \u0026amp; part-2 の開発 part-2 のレビュー \u0026amp; part-3 の開発 part-3 のレビュー のようになり、開発とレビューを並列で進められる部分があるので全体としての開発期間を短くすることができる。\nもちろんこれはうまく噛み合ったときの例だが。\n早めにフィードバックが得られるのも大きい。\nPull Request を小分けにしたときのデメリット 小分けにすることでレビュワーが全体感をつかみにくくなるというのはあるかもしれない。\npart-1 と part-2 で同じレビュワーになるとも限らない。\nこれに対しては開発に着手する前にまず全体のざっくりとした設計についてレビューを受けることで対応できる。\nこのステップを入れることで「とりあえず手を動かそう」となりにくくなる。\n手を動かすと仕事してる感が出るのでとりあえず手を動かしたくなりがちだが、ちょっと待てよと。\nまず何を作るか、全体の工程を考えましょうと。\nWorking under these constraints causes developers to break problems down into incremental deliverables. It helps avoid the temptation of jumping into development without a clear plan.\n\u0026ndash; 100 Duck-Sized Pull Requests\nまとめ pull request を分けてくれ、頼む。\n","permalink":"https://soonraah.github.io/posts/no-more-huge-pull-request/","summary":"\u003cp\u003eGitHub Flow ベースの開発においてはあまり大きな pull request を作ってほしくないという話。\u003cbr\u003e\nなんというか今更わざわざ言わなくてもいいんだけど…\u003c/p\u003e\n\u003cp\u003e仕事で何度か大きな pull request が投げられているのを見てしまったので、それはあまりよくないよというのを自分でも指摘しやすくするためにまとめておく。\u003c/p\u003e\n\u003ch2 id=\"reference\"\u003eReference\u003c/h2\u003e\n\u003cp\u003e最初に参考資料を挙げておく。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://kurtfunai.com/2017/12/100-duck-sized-pull-requests.html\"\u003e100 Duck-Sized Pull Requests\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://techracho.bpsinc.jp/hachi8833/2018_02_07/51095\"\u003e「巨大プルリク1件vs細かいプルリク100件」問題を考える（翻訳）\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://smallbusinessprogramming.com/optimal-pull-request-size/\"\u003eOptimal pull request size\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこれらを読めば特に私から言うこともないのだが…\u003cbr\u003e\nこれらに書いてあるとおりだが補足しておく。\u003c/p\u003e\n\u003ch2 id=\"pull-request-を小分けにしたときのメリット\"\u003ePull Request を小分けにしたときのメリット\u003c/h2\u003e\n\u003ch3 id=\"module-を適切に切り出すモチベーションが得られる\"\u003emodule を適切に切り出すモチベーションが得られる\u003c/h3\u003e\n\u003cp\u003e次のような話がある。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://hachibeechan.hateblo.jp/entry/dont-think-just-make-it-abstract\"\u003e共通化という考え方はアンチパターンを生み出すだけ説\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eこれを理由に module を分けるべきところが分けられず、いろんなことができてしまうベタッと大きな module が生まれるのを見た。\u003c/p\u003e\n\u003cp\u003eもちろんよく考えられていない共通化は駄目だが、上記ポストでは一方で\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eただし共通化という名の下におこなわれるのは「同じロジックを持つコードをまとめる」行為であって、抽象化のようにそのコード単位の意味を捉える作業はその範疇にない。抽象化というのはロジックを意味単位ごとにひとくくりにしていく行為で、これがどういうことなのかは次以降で述べていく。\u003c!-- raw HTML omitted --\u003e\u003cbr\u003e\n\u0026ndash; \u003c!-- raw HTML omitted --\u003e\u003ca href=\"https://hachibeechan.hateblo.jp/entry/dont-think-just-make-it-abstract\"\u003e共通化という考え方はアンチパターンを生み出すだけ説 - タオルケット体操\u003c/a\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eと述べており抽象化、つまりコードの意味を考慮した上で適切な単位でまとめておくことは否定していない。\u003c/p\u003e\n\u003cp\u003epull request を小さく分けるという行為のためには module や機能を適度なまとまりで切り分けること、つまり抽象化を考えていくことが必要となる。\u003cbr\u003e\nしたがって何でもできてしまう大きな module が生まれるのを防ぐ方向に働く。\u003cbr\u003e\n(もちろんここで駄目な共通化がなされてしまうこともあるだろう)\u003c/p\u003e\n\u003ch3 id=\"リリースまでの期間が短く済む\"\u003eリリースまでの期間が短く済む\u003c/h3\u003e\n\u003cp\u003e前述の参考資料においても pull request が大きいと\u003c/p\u003e","title":"あまり大きな Pull Request を作ってほしくない"},{"content":"2020-07-31 にオンライン開催された Spark Meetup Tokyo #3 Online に参加した。\n感想などメモしておく。\n全体感 トピックとしては主に\nSpark 3.0 Spark + AI Summit 2020 Spark 周辺要素 といったところだろうか。\n最近のコミュニティの動向や関心を日本語で聞くことができてよかった。\n運営 \u0026amp; スピーカーの皆様、ありがとうございます。\n発表 発表資料は公開されたら追加していく。\nSPARK+AI Summit 2020 イベント概要 スピーカー: @tokyodataguy さん\nSummit は金融業界の参加者が増えているらしい Spark で最も使われている言語は Python とのことだったが、Databricks の notebook サービスの話だった プロダクションコードではまた違うのだろう Spark 3.0 の update をざっくりと Spark 周辺要素の話をざっくりと Koalas Delta Lake Redash MLflow Introducing Koalas 1.0 Introducing Koalas 1.0 (and 1.1) from Takuya UESHIN スピーカー: @ueshin さん\nWhat\u0026rsquo;s Koalas open source の pure Python library pandas の API で Spark を動かせるようにする 小さいデータも大きなデータも同じように扱えるように 最近 v1.1 をリリース (個人的には pandas の API があまり好きではなく…) SPARK+AI Summit 2020 のセッションハイライト Spark + AI Summit 2020セッションのハイライト（Spark Meetup Tokyo #3 Online発表資料） from NTT DATA Technology \u0026amp; Innovation スピーカー: @masaru_dobashi さん\nSummit のセッションから case study 的なセッションを2つピックアップ USCIS の例 Lessons Learned from Modernizing USCIS Data Analytics Platform 古き良き Data Warehouse から Data Lake への移行 injection には Kafka も利用 諸々気にせずに気軽にデータをストレージに置きたい Alibaba の例 Spark Structured Streming の上に SQL-like なものを載せた？ ストリームの mini-batch 処理の合間で compaction を行う (つらそう) スライド中の「パイプラインの途中でモダンな技術に流し込めるかどうか？」という言葉が印象的だった モダンなものに移行するとき現実的に重要 Spark v3.0の紹介 前半 Introduction new features in Spark 3.0 from Kazuaki Ishizaki スピーカー: @kiszk さん\nSQL の性能に関わる7大機能の話 Query planの新しい表示方法 Join hintsの強化 Adaptive query execution Dynamic partitioning pruning nested column pruning \u0026amp; pushdown の強化 Aggregation のコード生成の改良 ScalaとJavaの新バージョンのサポート 実行計画が見やすくなるのは本当にうれしい 人がチューニングしていた部分が Adaptive Query Plan や Dynamic Partition Pruning で自動化されるのもうれしい 後半 Apache Spark 3.0新機能紹介 - 拡張機能やWebUI関連のアップデート（Spark Meetup Tokyo #3 Online） from NTT DATA Technology \u0026amp; Innovation スピーカー: @raspberry1123 さん\nAccelarator Aware Scheduling Project Hydrogen プラグイン機能 executor や driver の機能を user が拡張できる executor については Spark 2.4 からあったそうだが知らなかった… Structured Straming web UI ストリーム処理にはこういった chart が必要だと思う SparkにPRを投げてみた Sparkにプルリク投げてみた from Noritaka Sekiyama スピーカー: @moomindani さん\nスピーカーは AWS で Glue の開発をしている方 Glue は Spark に強く依存しているため、機能追加にモチベーションがあったとのこと 私も投げてみたい！ LT: td-spark internals: AirframeでSparkの機能を拡張するテクニック td-spark internals: Extending Spark with Airframe - Spark Meetup Tokyo #3 2020 from Taro L. Saito スピーカー: @taroleo さん\nAirframe とは Scala の application 開発用ツール群のようなもの？ Spark の機能を拡張 LT: Spark 3.1 Feature Expectation LT: Spark 3.1 Feature Expectation from Takeshi Yamamuro スピーカー: @maropu さん\n新機能 Support Filter Pushdown JSON JSON の読み取りが早くなる Better Handling for Node Shutdown node 離脱時にその node が保持する shuffle や cache の情報が失われていた 計画された node の離脱に対して shuffle や cache ブロックを別 node に委譲 まとめ どんどん便利になっていくなという印象。\n手でパフォーマンスチューニングする要素はどんどん減っていくのだろう。\nDelta Lake が盛り上がっているように感じた。\nデータレイクについて勉強しないと…\n","permalink":"https://soonraah.github.io/posts/spark-meetup-tokyo-3/","summary":"\u003cp\u003e2020-07-31 にオンライン開催された \u003ca href=\"https://spark-meetup-tokyo.connpass.com/event/181422/\"\u003eSpark Meetup Tokyo #3 Online\u003c/a\u003e に参加した。\u003cbr\u003e\n感想などメモしておく。\u003c/p\u003e\n\u003ch1 id=\"全体感\"\u003e全体感\u003c/h1\u003e\n\u003cp\u003eトピックとしては主に\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSpark 3.0\u003c/li\u003e\n\u003cli\u003eSpark + AI Summit 2020\u003c/li\u003e\n\u003cli\u003eSpark 周辺要素\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eといったところだろうか。\u003cbr\u003e\n最近のコミュニティの動向や関心を日本語で聞くことができてよかった。\u003c/p\u003e\n\u003cp\u003e運営 \u0026amp; スピーカーの皆様、ありがとうございます。\u003c/p\u003e\n\u003ch1 id=\"発表\"\u003e発表\u003c/h1\u003e\n\u003cp\u003e発表資料は公開されたら追加していく。\u003c/p\u003e\n\u003ch2 id=\"sparkai-summit-2020-イベント概要\"\u003eSPARK+AI Summit 2020 イベント概要\u003c/h2\u003e\n\u003cp\u003eスピーカー: \u003ca href=\"https://twitter.com/tokyodataguy\"\u003e@tokyodataguy\u003c/a\u003e さん\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSummit は金融業界の参加者が増えているらしい\u003c/li\u003e\n\u003cli\u003eSpark で最も使われている言語は Python とのことだったが、Databricks の notebook サービスの話だった\n\u003cul\u003e\n\u003cli\u003eプロダクションコードではまた違うのだろう\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSpark 3.0 の update をざっくりと\u003c/li\u003e\n\u003cli\u003eSpark 周辺要素の話をざっくりと\n\u003cul\u003e\n\u003cli\u003eKoalas\u003c/li\u003e\n\u003cli\u003eDelta Lake\u003c/li\u003e\n\u003cli\u003eRedash\u003c/li\u003e\n\u003cli\u003eMLflow\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"introducing-koalas-10\"\u003eIntroducing Koalas 1.0\u003c/h2\u003e\n\n\u003ciframe src=\"//www.slideshare.net/slideshow/embed_code/key/42rsX5ywWZbk5b\" width=\"510\" height=\"420\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen\u003e \u003c/iframe\u003e \u003cdiv style=\"margin-bottom:5px\"\u003e \u003cstrong\u003e \u003ca href=\"//www.slideshare.net/ueshin/introducing-koalas-10-and-11\" title=\"Introducing Koalas 1.0 (and 1.1)\" target=\"_blank\"\u003eIntroducing Koalas 1.0 (and 1.1)\u003c/a\u003e \u003c/strong\u003e from \u003cstrong\u003e\u003ca href=\"//www.slideshare.net/ueshin\" target=\"_blank\"\u003eTakuya UESHIN\u003c/a\u003e\u003c/strong\u003e \u003c/div\u003e\n\n\u003cp\u003eスピーカー: \u003ca href=\"https://twitter.com/ueshin\"\u003e@ueshin\u003c/a\u003e さん\u003c/p\u003e","title":"勉強会メモ: Spark Meetup Tokyo #3 Online"},{"content":"Spark バッチ処理の問題を調べていたら分離レベルという概念にたどりついた。\n分離レベルについて調べたので、Spark の問題の内容と絡めて記しておく。\n考えてみれば当たり前でたいした話ではない。\n分離レベルとは トランザクションの挙動についての暗黙の理解 アドホックな分析クエリやプロダクションコード中のクエリを書くとき、その単一のクエリのトランザクションにおいて「同時に実行されている別のクエリの commit 前の状態や commit 結果に影響され、このクエリの結果がおかしくなるかもしれない」ということは通常考えない。\nトランザクションはデータベースのある時点の状態に対して正しく処理される、というほぼ無意識の理解をおそらくほとんどの開発者が持っている。\n多くの場合この理解は間違っていない。\nそれはなぜかというと DB 等のデータ処理フレームワークがある強さの分離レベルを提供しているからである。\nいろいろな分離レベル ACID 特性のうちの1つ、分離性 (Isolation) の程度を表すのが分離レベル。\nトランザクション中に行われる操作の過程が他の操作から隠蔽されることを指し、日本語では分離性、独立性または隔離性ともいう。より形式的には、独立性とはトランザクション履歴が直列化されていることと言える。この性質と性能はトレードオフの関係にあるため、一般的にはこの性質の一部を緩和して実装される場合が多い。 \u0026ndash; Wikipedia ACID (コンピュータ科学)\n分離レベルには名前のついたものがいくつかあり、分離性の保証の強さが異なる。\n具体的にはトランザクションの並行性の問題への対応力が異なる。\n名著「データ指向アプリケーションデザイン」の第7章で分離レベルについて詳しく述べられているので、以下ではそちらからの引用。\n分離レベルを弱い順に並べる。\nread uncommitted\nこのレベルではダーティライトは生じませんが、ダーティリードは妨げられません。\nread committed\nデータベースからの読み取りを行った際に見えるデータは、コミットされたもののみであること（ダーティリードは生じない）。 データベースへの書き込みを行う場合、上書きするのはコミットされたデータのみであること（ダーティライトは生じない）。 snapshot isolation\nスナップショット分離の考え方は、それぞれのトランザクションがデータベースの一貫性のあるスナップショットから読み取りを行うというものです。すなわち、トランザクションが読み取るデータは、すべてそのトランザクションの開始時点のデータベースにコミット済みのものだけということです。\nserializability\nこの分離レベルはトランザクションが並行して実行されていても、最終的な答えはそれぞれが1つずつ順番に、並行ではなく実行された場合と同じになることを保証します。\n日本語で「分離レベル」を検索すると snapshot isolation の代わりに repeatable read が出てくる事が多い。\nしかし repeatable read の名前は実装によって意味が違っていたりして扱いが難しいらしい。\n分離レベルと race condition の関係 以下に各分離レベルとトランザクションの並行性の問題 (race condition) の関係を示す。\n各 race condition の説明については割愛するが、複数のトランザクションが並行して実行されることにより起こりうる期待されていない挙動だと思えばよい。\n○はその分離レベルにおいてその race condition が発生しないことを示す。\n△は条件によっては発生する。\ndirty read dirty write read skew (nonrepeatable read) lost update write skew phantom read read uncommitted ○ - - - - - read committed ○ ○ - - - - snapshot isolation ○ ○ ○ △ - △ serializability ○ ○ ○ ○ ○ ○ 下に行くほど強い分離レベルとなっている。\n分離レベルが強くなるほど race condition が発生しにくくなるが、一方で lock 等によりパフォーマンスが下がっていくというトレードオフがある。\n各種データベースの分離レベル ここでは MySQL と Hive においてどの分離レベルが提供されているかを見てみる。\nMySQL の場合 MySQL の分離レベルについては以下のドキュメントで述べられている。\n15.7.2.1 Transaction Isolation Levels MySQL (というか InnoDB?) では次の4つの分離レベルを設定することができる。\nREAD UNCOMMITTED READ COMMITTED REPEATABLE READ (default) SERIALIZABLE デフォルトの分離レベルは REPEATABLE READ だが、これは前述の snapshot isolation に相当するらしい。\n分離レベルは、例えば set transaction 構文により次のようにして指定できる。\nset transaction isolation level SERIALIZABLE; この場合は現在のセッション内で実行される次のトランザクションについて適用される。\nすべてのセッションやすべてのトランザクション等の指定もできる。\n詳しくは以下。\n13.3.7 SET TRANSACTION Statement Hive の場合 Hive についてはドキュメントに次のような記載がある。\nAt this time only snapshot level isolation is supported. When a given query starts it will be provided with a consistent snapshot of the data.\n\u0026ndash; Hive Transactions\nHive は snapshot isolation のみ提供しているとのこと。\nThe default DummyTxnManager emulates behavior of old Hive versions: has no transactions and uses hive.lock.manager property to create lock manager for tables, partitions and databases.\n\u0026ndash; Hive Transactions\nlock は小さくとも partition の単位になるのだろうか。\nであるとすると予想通りだが MySQL よりもいかつい挙動になっている。\nこのように多くの DB では snapshot isolation の分離レベルが基本となっている。\nSpark クエリの分離レベル では Spark のクエリはどうだろうか。\nここからようやく本題となる。\nread committed 相当 Spark において DataFrame を用いたデータ処理を記述するとき、それは1つの SQL クエリを書くのと近い感覚になる。\nそもそも DataFrame は SQL-like な使い心地を目的として作られた API だから当然だ。\nDataFrame で記述された処理は実行時に RDD として翻訳されるが、分離レベルを考えるにあたって RDD の特性がキーとなってくる。\nBy default, each transformed RDD may be recomputed each time you run an action on it.\n\u0026ndash; RDD Operations\nあまり良い説明を見つけられなかったが、1回の action を伴う処理においても同じ RDD が複数回参照されるとき、その RDD までの計算は通常やり直されることになる。\nしたがって例えば self join のようなことをするとき、同じデータソースを2回読みに行くということが起こってしまう。\nHDFS や S3 上のファイル、JDBC 経由の外部 DB など Spark は様々なデータソースを扱うことができるが、通常 Spark がその2回の読み込みに対して lock をかけたりすることはできない。\nつまり non-repeatable read や phantom read を防ぐことができない。\nread committed という弱い分離レベルに相当するということになってしまう。\n分離レベルという言葉はトランザクションという概念に対して使われるものであり、DataFrame のクエリをトランザクションと呼んでいいのかはわからない。\nなので分離レベルという言葉をここで使うのが適切でないかもしれないということは述べておく。\n検証 MySQL からデータを読み取り Spark で処理することを考える。\nまず local の MySQL で次のような table を用意する。\nmysql\u0026gt; describe employees; +---------------+---------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------------+---------+------+-----+---------+-------+ | id | int(11) | NO | PRI | NULL | | | salary | int(11) | YES | | NULL | | | department_id | int(11) | YES | | NULL | | +---------------+---------+------+-----+---------+-------+ 3 rows in set (0.03 sec) 部署 (department) ごとの給料 (salary) の平均から各従業員の給料がどれくらい離れているかの差分を見たいものとする。\nSpark のコードは次のようになる。\nSpark のバージョンはこれを書いている時点での最新 3.0.0 とした。\npackage com.example import org.apache.spark.sql.functions.avg import org.apache.spark.sql.SparkSession object IsolationLevelExperiment { def main(args: Array[String]): Unit = { // Prepare SparkSession val spark = SparkSession .builder() .appName(\u0026#34;Isolation Level Experiment\u0026#34;) .master(\u0026#34;local[*]\u0026#34;) .getOrCreate() import spark.implicits._ // Read from MySQL val dfEmployee = spark .read .format(\u0026#34;jdbc\u0026#34;) .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:mysql://localhost\u0026#34;) .option(\u0026#34;dbtable\u0026#34;, \u0026#34;db_name.employees\u0026#34;) .option(\u0026#34;user\u0026#34;, \u0026#34;user_name\u0026#34;) .option(\u0026#34;password\u0026#34;, \u0026#34;********\u0026#34;) .option(\u0026#34;driver\u0026#34;, \u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;) .load .cache // Get average salary val dfAvg = dfEmployee .groupBy($\u0026#34;department_id\u0026#34;) .agg(avg($\u0026#34;salary\u0026#34;).as(\u0026#34;avg_salary\u0026#34;)) // Calculate diff val dfResult = dfEmployee .as(\u0026#34;e\u0026#34;) .join( dfAvg.as(\u0026#34;a\u0026#34;), $\u0026#34;e.department_id\u0026#34; === $\u0026#34;a.department_id\u0026#34;, \u0026#34;left_outer\u0026#34; ) .select( $\u0026#34;e.id\u0026#34;, $\u0026#34;e.department_id\u0026#34;, ($\u0026#34;e.salary\u0026#34; - $\u0026#34;a.avg_salary\u0026#34;).as(\u0026#34;salary_diff\u0026#34;) ) // Output results dfResult.show spark.stop() } } このコードを実行する前に MySQL の general query log を ON にする。\nmysql\u0026gt; set global general_log = \u0026#39;ON\u0026#39;; Query OK, 0 rows affected (0.02 sec) mysql\u0026gt; show global variables like \u0026#39;general_log%\u0026#39;; +------------------+--------------------------------------+ | Variable_name | Value | +------------------+--------------------------------------+ | general_log | ON | | general_log_file | /usr/local/var/mysql/MacBook-Pro.log | +------------------+--------------------------------------+ 2 rows in set (0.01 sec) これによって MySQL に対して発行されたクエリがログとして記録されるようになる。\n直感的に snapshot isolation になっているのであれば MySQL に対する select 文は1回だけ発行されるはずである。\nしかし前述のとおり RDD や DataFrame の処理は途中の状態を通常保存せず、同じ RDD や DataFrame を参照していたとしても再計算される。\n上記コードの例だと dfEmployee が2回参照されている。\nコードを実行すると general query log には次のように、データ取得のための select 文が2つ記録されていた。\nそれぞれ join() の左右の table のデータソースを示している。\n8 Query SELECT `id`,`salary`,`department_id` FROM test_fout_dsp.employees 7 Query SELECT `salary`,`department_id` FROM test_fout_dsp.employees WHERE (`department_id` IS NOT NULL) 2つの select 文はそれぞれ別のクエリ、トランザクションとして発行されている。\nしたがって前者の select 文が実行された後、後者の select 文が実行される前に別のトランザクションにより employees が更新されたり挿入・削除されたりすると non-repeatable read や phantom read が発生してしまうのである。\n今回はデータソースへのアクセスを確認するためにデータソースとして MySQL を使ったが、同じことはファイルや他の DB など別のデータソースで起こりうる。\n回避策 プロダクト運用上、non-repeatable read や phantom read が発生しうる状況というのは多くの場合で厄介である。\n一時的なデータソースの状態に依存して問題が発生するため、バグの原因追求がとても困難だからだ。\n見た目上の分離レベルを強くし、これらを避けるには2つの方法が考えられる。\nimmutable なデータにのみアクセスする 単純な話で non-repeatable read や phantom read が発生するようなデータソースを参照しなければよい。\n例えばユーザの行動ログのような蓄積されていくデータが hour 単位で partitioning されているような場合、基本的に過去の partition に対しては変更や挿入・削除は行われない。\nこのような partition にアクセスする分には前述のような厄介な問題は起こらない。\ncache する データソースから読み取った結果の DataFrame に対して cache() または persist() をするとよい。\nSpark SQL can cache tables using an in-memory columnar format by calling spark.catalog.cacheTable(\u0026quot;tableName\u0026quot;) or dataFrame.cache().\n\u0026ndash; Caching Data In Memory\n前述のコードで dfEmployee に対して .cache() をした場合、MySQL へのデータ取得のための select 文の発行は1回になった。\n大きなデータソースを cache() するときだけメモリや HDD 容量に気をつけておきたい。\nまとめ DataFrame はいわゆる RDBMS を操作しているように見えてしまうところが難しいところだろうか。\n「データ指向アプリケーションデザイン」に至極真っ当なことが書いてあったので、その言葉を借りて締めておく。\n何も考えずにツールを信じて依存するのではなく、並行性の問題にはどういったものがあるのか、そしてそれらを回避するにはどうしたら良いのかを、しっかり理解しなければなりません。そうすれば、信頼性があり、正しく動作するアプリケーションを手の届くツールを使って構築できるようになります。\n\u0026ndash; Martin Kleppmann データ指向アプリケーションデザイン\n","permalink":"https://soonraah.github.io/posts/weak_isolation_level_of_dataframe/","summary":"\u003cp\u003eSpark バッチ処理の問題を調べていたら分離レベルという概念にたどりついた。\u003cbr\u003e\n分離レベルについて調べたので、Spark の問題の内容と絡めて記しておく。\u003cbr\u003e\n考えてみれば当たり前でたいした話ではない。\u003c/p\u003e\n\u003ch1 id=\"分離レベルとは\"\u003e分離レベルとは\u003c/h1\u003e\n\u003ch2 id=\"トランザクションの挙動についての暗黙の理解\"\u003eトランザクションの挙動についての暗黙の理解\u003c/h2\u003e\n\u003cp\u003eアドホックな分析クエリやプロダクションコード中のクエリを書くとき、その単一のクエリのトランザクションにおいて「同時に実行されている別のクエリの commit 前の状態や commit 結果に影響され、このクエリの結果がおかしくなるかもしれない」ということは通常考えない。\u003cbr\u003e\nトランザクションはデータベースのある時点の状態に対して正しく処理される、というほぼ無意識の理解をおそらくほとんどの開発者が持っている。\u003c/p\u003e\n\u003cp\u003e多くの場合この理解は間違っていない。\u003cbr\u003e\nそれはなぜかというと DB 等のデータ処理フレームワークがある強さの分離レベルを提供しているからである。\u003c/p\u003e\n\u003ch2 id=\"いろいろな分離レベル\"\u003eいろいろな分離レベル\u003c/h2\u003e\n\u003cp\u003eACID 特性のうちの1つ、分離性 (Isolation) の程度を表すのが分離レベル。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eトランザクション中に行われる操作の過程が他の操作から隠蔽されることを指し、日本語では分離性、独立性または隔離性ともいう。より形式的には、独立性とはトランザクション履歴が直列化されていることと言える。この性質と性能はトレードオフの関係にあるため、一般的にはこの性質の一部を緩和して実装される場合が多い。 \u003c!-- raw HTML omitted --\u003e\u003cbr\u003e\n\u0026ndash; \u003c!-- raw HTML omitted --\u003e\u003ca href=\"https://ja.wikipedia.org/wiki/ACID_%28%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF%E7%A7%91%E5%AD%A6%29\"\u003eWikipedia ACID (コンピュータ科学)\u003c/a\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e分離レベルには名前のついたものがいくつかあり、分離性の保証の強さが異なる。\u003cbr\u003e\n具体的にはトランザクションの並行性の問題への対応力が異なる。\u003cbr\u003e\n名著「\u003ca href=\"https://www.amazon.co.jp/gp/product/4873118700/ref=as_li_tl?ie=UTF8\u0026amp;camp=247\u0026amp;creative=1211\u0026amp;creativeASIN=4873118700\u0026amp;linkCode=as2\u0026amp;tag=froglog02-22\u0026amp;linkId=19bf71b38427997bd19423b1bb219d3f\"\u003eデータ指向アプリケーションデザイン\u003c/a\u003e」の第7章で分離レベルについて詳しく述べられているので、以下ではそちらからの引用。\u003c/p\u003e\n\n\u003ca target=\"_blank\"  href=\"https://www.amazon.co.jp/gp/product/4873118700/ref=as_li_tl?ie=UTF8\u0026camp=247\u0026creative=1211\u0026creativeASIN=4873118700\u0026linkCode=as2\u0026tag=froglog02-22\u0026linkId=4d28525a2e352660b372d4d1027b72fb\"\u003e\u003cimg border=\"0\" src=\"//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8\u0026MarketPlace=JP\u0026ASIN=4873118700\u0026ServiceVersion=20070822\u0026ID=AsinImage\u0026WS=1\u0026Format=_SL250_\u0026tag=froglog02-22\" \u003e\u003c/a\u003e\u003cimg src=\"//ir-jp.amazon-adsystem.com/e/ir?t=froglog02-22\u0026l=am2\u0026o=9\u0026a=4873118700\" width=\"1\" height=\"1\" border=\"0\" alt=\"\" style=\"border:none !important; margin:0px !important;\" /\u003e\n\u003cbr\u003e\n\n\u003cp\u003e分離レベルを弱い順に並べる。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eread uncommitted\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eこのレベルではダーティライトは生じませんが、ダーティリードは妨げられません。\u003c/p\u003e\u003c/blockquote\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eread committed\u003c/p\u003e\n\u003cblockquote\u003e\n\u003col\u003e\n\u003cli\u003eデータベースからの読み取りを行った際に見えるデータは、コミットされたもののみであること（ダーティリードは生じない）。\u003c/li\u003e\n\u003cli\u003eデータベースへの書き込みを行う場合、上書きするのはコミットされたデータのみであること（ダーティライトは生じない）。\u003c/li\u003e\n\u003c/ol\u003e\u003c/blockquote\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003esnapshot isolation\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eスナップショット分離の考え方は、それぞれのトランザクションがデータベースの一貫性のあるスナップショットから読み取りを行うというものです。すなわち、トランザクションが読み取るデータは、すべてそのトランザクションの開始時点のデータベースにコミット済みのものだけということです。\u003c/p\u003e\u003c/blockquote\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eserializability\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eこの分離レベルはトランザクションが並行して実行されていても、最終的な答えはそれぞれが1つずつ順番に、並行ではなく実行された場合と同じになることを保証します。\u003c/p\u003e\u003c/blockquote\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e日本語で「分離レベル」を検索すると snapshot isolation の代わりに repeatable read が出てくる事が多い。\u003cbr\u003e\nしかし repeatable read の名前は実装によって意味が違っていたりして扱いが難しいらしい。\u003c/p\u003e","title":"Spark DataFrame クエリの弱い分離レベル"},{"content":"東京の IT 企業で働く中年エンジニアが勉強したことや考えていることなどを書きます。\nこのブログは Hugo で作成しました。\nTheme には PaperMod を利用しています。\n","permalink":"https://soonraah.github.io/about/","summary":"\u003cp\u003e東京の IT 企業で働く中年エンジニアが勉強したことや考えていることなどを書きます。\u003c/p\u003e\n\u003cp\u003eこのブログは \u003ca href=\"https://gohugo.io/\"\u003eHugo\u003c/a\u003e で作成しました。\u003cbr\u003e\nTheme には \u003ca href=\"https://themes.gohugo.io/hugo-papermod/\"\u003ePaperMod\u003c/a\u003e を利用しています。\u003c/p\u003e","title":"About"},{"content":"はじめに Apache Spark 3.0.0 がリリースされました。\nSpark Release 3.0.0 release note を見て個人的に気になったところなど簡単に調べました。\n書いてみると Databricks の記事へのリンクばっかになってしまった…\n全体感 こちらの記事を読めば全体感は OK.\nIntroducing Apache Spark 3.0 公式の release note には\nPython is now the most widely used language on Spark.\nとあってそうなん？ってなったけど、こちらの記事だと\nPython is now the most widely used language on Spark and, consequently, was a key focus area of Spark 3.0 development. 68% of notebook commands on Databricks are in Python.\nと書いてありどうやら Databricks の notebook の話らしく、だったらまあそうかなという感じ。\nプロダクトコードへの実装というよりは、アドホック分析や検証用途の話なんでしょう。\n[Project Hydrogen] Accelerator-aware Scheduler SPARK-24615 Spark 上で deep learning できるようにすることを目指す Project Hydrogen、その3つの大きな目標のうちの一つ。\nBig Data LDN 2018: PROJECT HYDROGEN: UNIFYING AI WITH APACHE SPARK YARN や Kubernetes では GPU や FPGA を扱えるようになっているので Spark でも扱えるようにしたいというモチベーション。\nSpark のドキュメント によると\nFor example, the user wants to request 2 GPUs for each executor. The user can just specify spark.executor.resource.gpu.amount=2 and Spark will handle requesting yarn.io/gpu resource type from YARN.\nのようにして executor に GPU リソースを要求できるみたいです。\nAdaptive Query Execution SPARK-31412 平たく言うと実行時に得られる統計情報を使って plan を最適化すると、静的に生成された plan より効率化できるよねという話。\nspark.sql.adaptive.enabled=true にすることで有効になります。\n処理の途中で中間生成物が materialize されるタイミングで、その時点の統計情報を使って残りの処理を最適化する、というのを繰り返します。\nSpark 3.0.0 では以下3つの AQE が実装されました。\nCoalescing Post Shuffle Partitions Converting sort-merge join to broadcast join Optimizing Skew Join Spark 2 以前だとこのあたりは実行しつつチューニングするような運用になりがちでした。\n特に skew の解消は salt を追加したりなど面倒だったりします。\nこれらが自動で最適化されるというのは運用上うれしいところ。\n急なデータ傾向の変化に対しても自動で最適化して対応できるという面があります。\nAQE に関してもやはり Databricks の解説記事がわかりやすいです。\n図もいい感じ。\nAdaptive Query Execution: Speeding Up Spark SQL at Runtime Dynamic Partition Pruning SPARK-11150 こちらも AQE 同様にクエリのパフォーマンスを改善する目的で導入されたもの。\n改善幅は AQE より大きいようです。\nやはり実行時の情報を使って partition pruning を行い、不要な partition の参照を減らすという方法。\n主に star schema における join 時のように、静的には partition pruning が行えない場合を想定しています。\n比較的小さいことが多いと思われる dimension table 側を broadcast する broadcast join において、broadcast された情報を fact table の partition pruning に利用するというやり口。\nDynamic Partition Pruning in Apache Spark Structured Streaming UI SPARK-29543 \u0026ldquo;Structured Streaming\u0026rdquo; というタブが UI に追加された件。\nSpark のドキュメントに例があります。\nStructured Streaming Tab\nSpark 2.4 系では Structured Streaming を動かしていてもせいぜい job や stage が増えていくという味気ないものしか見えませんでした。\nSpark 3.0.0 で実際に動かしてみたけど欲しかったやつ！という感じ。\nストリーム処理では入力データ量の変化の可視化がマストだと思ってます。\nCatalog plugin API SPARK-31121 SPIP: Spark API for Table Metadata これまでは CTAS (Create Table As Select) の操作はあったが、外部のデータソースに対して DDL 的な操作をする API が足りていませんでした。\nCTAS も挙動に実装依存の曖昧さがありました。\nそこで create, alter, load, drop 等のテーブル操作をできるようにしたという話。\nドキュメントの DDL Statements のあたりを読め何ができるかわかります。\n以前のバージョンでも一部のデータソースについてはできた模様 (ex. Hive)。\n今の自分の業務では Spark から DDL を扱うようなことはしないのでそれほど恩恵は感じられません。\nnotebook からアドホックな Spark のバッチを動かすというような使い方をしていればうれしいかもしれません。\nAdd an API that allows a user to define and observe arbitrary metrics on batch and streaming queries SPARK-29345 クエリの途中の段階で何らかの metrics を仕込んでおいて callback 的にその metrics にアクセスできる仕組み。\nDataset#observe() の API ドキュメント を読むのが一番早いです。\nこの例ではストリーム処理を扱っているが、バッチ処理の例を自分で書いて試してみました。\n// Register listener spark .listenerManager .register(new QueryExecutionListener { override def onSuccess(funcName: String, qe: QueryExecution, durationNs: Long): Unit = { val num = qe.observedMetrics .get(\u0026#34;my_metrics\u0026#34;) .map(_.getAs[Long](\u0026#34;num\u0026#34;)) .getOrElse(-100.0) println(s\u0026#34;num of data: $num\u0026#34;) } override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {} }) // Make DataFrame val df = Seq .range(0, 1000) .map((_, Seq(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;)(Random.nextInt(3)), math.random())) .toDF(\u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;, \u0026#34;value\u0026#34;) // Observe and process val dfResult = df .observe(\u0026#34;my_metrics\u0026#34;, count($\u0026#34;*\u0026#34;).as(\u0026#34;num\u0026#34;)) .groupBy($\u0026#34;type\u0026#34;) .agg(avg($\u0026#34;value\u0026#34;).as(\u0026#34;avg_value\u0026#34;)) // Run dfResult.show これを動かしたときの出力は次のようになりました。\n+----+------------------+ |type| avg_value| +----+------------------+ | c|0.5129435063033314| | b|0.4693004460694317| | a|0.4912087482418599| +----+------------------+ num of data: 1000 observe() はその出力の DataFrame に対して schema やデータの中身を変更することはありません。\nmetrics を仕込むのみ。\nlogical plan を出力してみると observe() を入れることにより途中に CollectMetrics という plan が挿入されていました。\nソースを見ると accumulator を使っている模様。\nなので observe() の集計のみで一度 job が動くわけではなく、クエリが2回走るという感じではありません。\n全体の処理の中でひっそりと accumulator で脇で集計しておくといった趣でしょうか。\nこれは結構有用だと思います。\n例えば何らかの集計をやるにしてもその最初や途中で、例えば入力データは何件あったか？みたいなことをログに出しておきたいことがあります。\nというか accumulator で頑張ってそういうものを作ったことがある…\nこれがフレームワーク側でサポートされるのはうれしいです。\nまとめ 2つのダイナミックな最適化に期待大。\n気が向いたら追加でまた調べるかもしれません。\n","permalink":"https://soonraah.github.io/posts/study-spark-3-0-0/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003eApache Spark 3.0.0 がリリースされました。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://spark.apache.org/releases/spark-release-3-0-0.html\"\u003eSpark Release 3.0.0\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003erelease note を見て個人的に気になったところなど簡単に調べました。\u003cbr\u003e\n書いてみると Databricks の記事へのリンクばっかになってしまった…\u003c/p\u003e\n\u003ch2 id=\"全体感\"\u003e全体感\u003c/h2\u003e\n\u003cp\u003eこちらの記事を読めば全体感は OK.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://databricks.com/jp/blog/2020/06/18/introducing-apache-spark-3-0-now-available-in-databricks-runtime-7-0.html\"\u003eIntroducing Apache Spark 3.0\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e公式の release note には\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePython is now the most widely used language on Spark.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eとあってそうなん？ってなったけど、こちらの記事だと\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePython is now the most widely used language on Spark and, consequently, was a key focus area of Spark 3.0 development. 68% of notebook commands on Databricks are in Python.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eと書いてありどうやら Databricks の notebook の話らしく、だったらまあそうかなという感じ。\u003cbr\u003e\nプロダクトコードへの実装というよりは、アドホック分析や検証用途の話なんでしょう。\u003c/p\u003e","title":"Apache Spark 3.0.0 について調べた"}]