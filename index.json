[{"content":"このポストについて Data Contract CLI を触ってみたところ、面白かったのとこれからのデータパイプライン開発について思うところがあったので書いてみる。\nData Contract CLI とは？ datacontract/datacontract-cli\nData Contract CLI は data contracts を運用するためのオープンソースのコマンドラインツールである。\ndata contracts の概念については以前の記事で詳しく書いているのでそちらをご参考いただければと。\nただしこちらの記事は1年前のものであり、今回取り上げる Data Contract CLI の登場などを含めて現在では data contracts を取り巻く状況も変わっている可能性があることに注意。\nData Contract CLI は Python で開発されており、pip でインストールすることができる。\nこの記事を書いている時点では v0.10.3 が最新であり、この記事の内容はこのバージョンに基づいている。\nData Contract CLI で扱う data contracts は YAML で定義される前提となっており、その仕様は datacontract/datacontract-specification で決められている。\nこの data contracts に対して Data Contract CLI では次のようなことが行える。\nlint によるフォーマットチェック データソースに接続した上での schema やデータ品質のテスト data contracts の破壊的な変更の検出 JSON Schema や dbt など、他の形式からの／へのインポートとエクスポート 以下の図がイメージしやすい。\ndatacontract/datacontract-cli より\nData Contract CLI の開発者が何を考えているか、この図から推測できる部分があるので後ほど考察したい。\nData Contract CLI を使ってみる 前提 手元に個人開発用の BigQuery の table があったので、これについて data contract を用意して Data Contract CLI を使ってみることにした。\n個人の MoneyForward の情報を取り込んでいる table であり、収支詳細のカテゴリを扱う table である。\ndata contract (YAML ファイル) の作成は基本的には best practice に従う。\ndata contract の作成後は export や変更の検出などを試してみる。\n準備 今回は Poetry で Python 環境を用意した。\ndatacontract-cli および BigQuery を扱うのに必要となる google-cloud-bigquery-storage を install しておく。\n[tool.poetry.dependencies] python = \u0026#34;^3.10\u0026#34; datacontract-cli = \u0026#34;^0.10.3\u0026#34; google-cloud-bigquery-storage = \u0026#34;^2.24.0\u0026#34; バージョンを確認。\n$ datacontract --version 0.10.3 DDL からの import して data contract を作成 まず data contract を用意する必要がある。\ndata contract は YAML ファイルなので仕様を見ながら一から書くこともできるが、既存の table などがある場合は import を使うことができる。\nちなみに v0.10.3 の時点で対応している import のソースは sql, avro, glue の3つ。\nBigQuery では INFORMATION_SCHEMA.TABLES から既存 table の DDL を得ることができるため、それを利用して money_forward_main_group.sql を用意した。\nCREATE TABLE `\u0026lt;PROJECT_ID\u0026gt;.\u0026lt;DATASET_ID\u0026gt;.money_forward_main_group` ( mf_main_group_id INT64, name STRING, is_income BOOL ); この DDL ファイルを使って import サブコマンドにより data contract を作成する。\n$ datacontract import --format sql --source money_forward_main_group.sql \u0026gt; datacontract_v1.yml 作成された datacontract_v1.yml の中は次のようになっている。\ndataContractSpecification: 0.9.3 id: my-data-contract-id info: title: My Data Contract version: 0.0.1 models: money_forward_main_group`: type: table fields: mf_main_group_id: type: integer name: type: string is_income: type: boolean この時点で一度 lint サブコマンドでフォーマットチェックしておく。\ndatacontract lint datacontract_v1.yml WARNING:root:Data Contract YAML is invalid. Validation error: data.models must be named by propertyName definition ERROR:root:Run operation failed: [lint] Check that data contract YAML is valid - None - failed - data.models must be named by propertyName definition - datacontract ╭────────┬────────────────────────────────────────┬───────┬────────────────────────────────────────────────────╮ │ Result │ Check │ Field │ Details │ ├────────┼────────────────────────────────────────┼───────┼────────────────────────────────────────────────────┤ │ failed │ Check that data contract YAML is valid │ │ data.models must be named by propertyName │ │ │ │ │ definition │ ╰────────┴────────────────────────────────────────┴───────┴────────────────────────────────────────────────────╯ 🔴 data contract is invalid, found the following errors: 1) data.models must be named by propertyName definition エラーになってしまった。\n実はこの時点で datacontract_v1.yml にいくつか問題があり、修正することに。\n不要な \u0026ldquo;`\u0026rdquo; が含まれている servers ブロックがない (確かに DDL を与えるだけだと BigQuery なのか Snowflake なのかわからんな…) 次のように修正して datacontract_v2.yml を作った。\n@@ -3,8 +3,13 @@ info: title: My Data Contract version: 0.0.1 +servers: + dev: + type: bigquery + project: \u0026lt;PROJECT_ID\u0026gt; + dataset: \u0026lt;DATASET_ID\u0026gt; models: - money_forward_main_group`: + money_forward_main_group: type: table fields: mf_main_group_id: 再度 lint サブコマンドを実行すると成功した。\nただし description がない旨の warning が出ている。\ntable や column の description や制約事項を追加して datacontract_v3.yml とする。\n@@ -3,6 +3,9 @@ info: title: My Data Contract version: 0.0.1 + description: | + MoneyForward の収支における分類 (項目)。 + ダウンロードできる収支詳細に記載される情報を元に作成されている。 servers: dev: type: bigquery @@ -11,10 +14,18 @@ models: money_forward_main_group: type: table + description: MoneyForward の収支における大項目 fields: mf_main_group_id: + description: 大項目 ID type: integer + required: true + primary: true + unique: true name: + description: 大項目の項目名 type: string + required: true is_income: + description: 大項目が収入の場合は true, 支出の場合は false type: boolean lint サブコマンドが成功するようになった。\n$ datacontract lint datacontract_v3.yml ╭────────┬──────────────────────────────────────────┬───────┬─────────╮ │ Result │ Check │ Field │ Details │ ├────────┼──────────────────────────────────────────┼───────┼─────────┤ │ passed │ Data contract is syntactically valid │ │ │ │ passed │ Linter \u0026#39;Field pattern is correct regex\u0026#39; │ │ │ │ passed │ Linter \u0026#39;Objects have descriptions\u0026#39; │ │ │ │ passed │ Linter \u0026#39;Field references existing field\u0026#39; │ │ │ │ passed │ Linter \u0026#39;Example(s) match model\u0026#39; │ │ │ │ passed │ Linter \u0026#39;Fields use valid constraints\u0026#39; │ │ │ │ passed │ Linter \u0026#39;Quality check(s) use model\u0026#39; │ │ │ │ passed │ Linter \u0026#39;noticePeriod in ISO8601 format\u0026#39; │ │ │ ╰────────┴──────────────────────────────────────────┴───────┴─────────╯ 🟢 data contract is valid. Run 8 checks. Took 0.304237 seconds. 初回テスト 次に test サブコマンドでテストを実行する。\nちなみに test では BiqQuery にアクセスするため、環境変数 DATACONTRACT_BIGQUERY_ACCOUNT_INFO_JSON_PATH に認証情報の JSON ファイルを指定しておく必要がある。\n$ datacontract test datacontract_v3.yml Testing datacontract_v3.yml Column,Event,Details is_income,:icon-fail: Type Mismatch, Expected Type: boolean; Actual Type: BOOL Type Mismatch, Expected Type: boolean; Actual Type: BOOL ╭────────┬──────────────────────────────────────────────────────────────────┬──────────────────┬────────────────────────────────────────────────────╮ │ Result │ Check │ Field │ Details │ ├────────┼──────────────────────────────────────────────────────────────────┼──────────────────┼────────────────────────────────────────────────────┤ │ passed │ Check that field mf_main_group_id is present │ │ │ │ passed │ Check that field mf_main_group_id has type integer │ │ │ │ passed │ Check that field name is present │ │ │ │ passed │ Check that field name has type string │ │ │ │ passed │ Check that field is_income is present │ │ │ │ failed │ Check that field is_income has type boolean │ │ Type Mismatch, Expected Type: boolean; Actual │ │ │ │ │ Type: BOOL │ │ passed │ Check that required field mf_main_group_id has no null values │ mf_main_group_id │ │ │ passed │ Check that unique field mf_main_group_id has no duplicate values │ mf_main_group_id │ │ │ passed │ Check that required field name has no null values │ name │ │ ╰────────┴──────────────────────────────────────────────────────────────────┴──────────────────┴────────────────────────────────────────────────────╯ 🔴 data contract is invalid, found the following errors: 1) Type Mismatch, Expected Type: boolean; Actual Type: BOOL エラーになってしまった。\ncolumn is_income の型が boolean ではなく BOOL だと言われている。\n確かに BiqQuery としての型は BOOL であるが、一方で data contracts の仕様としては boolean しか指定できない。\nバグっぽいのでいったん is_income の型指定をはずしておく…\n出力された内容を見るとどういったことがチェックされているかがわかる。\nこの時点では column の存在および型がチェックされていて、さらに制約事項を記載した column についてはその制約を満たしているかがチェックされている。\nデータ例の追加 次に data contract に examples ブロックを追加する。\n名前のとおりでデータの例を記載することができる。\nまず data contract を宣言して開発することを考えると、例が示されているのはとても助かる。\n次のように修正して datacontract_v4.yml を作った。\n@@ -28,4 +28,14 @@ required: true is_income: description: 大項目が収入の場合は true, 支出の場合は false - type: boolean + # type mismatch になるバグ？があるため boolean 型のチェックは除外 + # type: boolean +examples: + - type: csv + description: money_forward_main_group のレコードの例。 + model: money_forward_main_group + data: | + mf_main_group_id,name,is_income + 1,\u0026#34;収入\u0026#34;,true + 2,\u0026#34;食費\u0026#34;,false + 3,\u0026#34;日用品\u0026#34;,false オプション --examples をつけて test サブコマンドを実行すると、examples ブロックに追加したデータに対してテストを行うことができる。\n$ datacontract test --examples datacontract_v4.yml Testing datacontract_v4.yml ╭────────┬──────────────────────────────────────────────────────────────────┬──────────────────┬─────────╮ │ Result │ Check │ Field │ Details │ ├────────┼──────────────────────────────────────────────────────────────────┼──────────────────┼─────────┤ │ passed │ Check that field mf_main_group_id is present │ │ │ │ passed │ Check that field name is present │ │ │ │ passed │ Check that field is_income is present │ │ │ │ passed │ Check that required field mf_main_group_id has no null values │ mf_main_group_id │ │ │ passed │ Check that unique field mf_main_group_id has no duplicate values │ mf_main_group_id │ │ │ passed │ Check that required field name has no null values │ name │ │ ╰────────┴──────────────────────────────────────────────────────────────────┴──────────────────┴─────────╯ 🟢 data contract is valid. Run 6 checks. Took 0.232013 seconds. データ品質チェックの追加 quality ブロックを追加してデータ品質チェックを行うことができる。\nv0.10.3 の時点では SodaCL, Monte Carlo, greate expectation の記法で品質チェックを書くことができる。\nここでは SodaCL の記法で table の行数が1件以上あることを確認する簡単なチェックを追加して datacontract_v5.yml とする。\n@@ -39,3 +39,8 @@ 1,\u0026#34;収入\u0026#34;,true 2,\u0026#34;食費\u0026#34;,false 3,\u0026#34;日用品\u0026#34;,false +quality: + type: SodaCL + specification: + checks for money_forward_main_group: + - row_count \u0026gt; 0 examples と BigQuery それぞれに対して test サブコマンドを実行。\n$ datacontract test --examples datacontract_v5.yml Testing datacontract_v5.yml ╭────────┬──────────────────────────────────────────────────────────────────┬──────────────────┬─────────╮ │ Result │ Check │ Field │ Details │ ├────────┼──────────────────────────────────────────────────────────────────┼──────────────────┼─────────┤ │ passed │ Check that field mf_main_group_id is present │ │ │ │ passed │ Check that field name is present │ │ │ │ passed │ Check that field is_income is present │ │ │ │ passed │ row_count \u0026gt; 0 │ │ │ │ passed │ Check that required field mf_main_group_id has no null values │ mf_main_group_id │ │ │ passed │ Check that unique field mf_main_group_id has no duplicate values │ mf_main_group_id │ │ │ passed │ Check that required field name has no null values │ name │ │ ╰────────┴──────────────────────────────────────────────────────────────────┴──────────────────┴─────────╯ 🟢 data contract is valid. Run 7 checks. Took 0.48302 seconds. $ datacontract test datacontract_v5.yml Testing datacontract_v5.yml ╭────────┬──────────────────────────────────────────────────────────────────┬──────────────────┬─────────╮ │ Result │ Check │ Field │ Details │ ├────────┼──────────────────────────────────────────────────────────────────┼──────────────────┼─────────┤ │ passed │ Check that field mf_main_group_id is present │ │ │ │ passed │ Check that field mf_main_group_id has type integer │ │ │ │ passed │ Check that field name is present │ │ │ │ passed │ Check that field name has type string │ │ │ │ passed │ Check that field is_income is present │ │ │ │ passed │ row_count \u0026gt; 0 │ │ │ │ passed │ Check that required field mf_main_group_id has no null values │ mf_main_group_id │ │ │ passed │ Check that unique field mf_main_group_id has no duplicate values │ mf_main_group_id │ │ │ passed │ Check that required field name has no null values │ name │ │ ╰────────┴──────────────────────────────────────────────────────────────────┴──────────────────┴─────────╯ 🟢 data contract is valid. Run 9 checks. Took 5.690055 seconds. 行数 (row_count) のチェックが増えていることが確認できる。\nここまでのところで data contract (YAML) はいったん完成とする。\nこの時点での全体像を記載しておく。\ndataContractSpecification: 0.9.3 id: my-data-contract-id info: title: My Data Contract version: 0.0.1 description: | MoneyForward の収支における分類 (項目)。 ダウンロードできる収支詳細に記載される情報を元に作成されている。 servers: dev: type: bigquery project: \u0026lt;PROJECT_ID\u0026gt; dataset: \u0026lt;DATASET_ID\u0026gt; models: money_forward_main_group: type: table description: MoneyForward の収支における大項目 fields: mf_main_group_id: description: 大項目 ID type: integer required: true primary: true unique: true name: description: 大項目の項目名 type: string required: true is_income: description: 大項目が収入の場合は true, 支出の場合は false # type mismatch になるバグ？があるため boolean 型のチェックは除外 # type: boolean examples: - type: csv description: money_forward_main_group のレコードの例。 model: money_forward_main_group data: | mf_main_group_id,name,is_income 1,\u0026#34;収入\u0026#34;,true 2,\u0026#34;食費\u0026#34;,false 3,\u0026#34;日用品\u0026#34;,false quality: type: SodaCL specification: checks for money_forward_main_group: - row_count \u0026gt; 0 ここで作った data contract はフルの記載ではない。\n例えば servicelevels ブロックを追加して SLA を記載することもできる。\ndata contract のすべての仕様を知りたい場合は仕様ドキュメントを参照のこと。\nexport を試す JSON Schema data contract ができたところで export を試していきたい。\nv0.10.3 では14種類のフォーマットに対しての export がサポートされており、ここではそのうちのいくつかを使ってみる。\nまずは JSON Schema で出力してみる。\n$ datacontract export --format jsonschema datacontract_v5.yml { \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;mf_main_group_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;unique\u0026#34;: true }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;is_income\u0026#34;: {} }, \u0026#34;required\u0026#34;: [ \u0026#34;mf_main_group_id\u0026#34;, \u0026#34;name\u0026#34; ] } JSON Schema の記法でモデルが表現されたものが標準出力された。\n同様に Avro, Protobuf などの形式でも export することができる。\nデータ基盤的な話をすると、取り込みの対象であるアプリケーション (data provider) から送られてくるログは JSON 形式になっていることがよくある。\ndata contract をアプリケーション側とデータ基盤側で合意しておけば、アプリケーション側でログ生成時に JSON Schema でログに validation をかけることができる。\nこれによりログの schema の意図しない変更を防ぐ運用が可能になるだろう。\ndbt dbt の形式で export することもできる。\ndbt の YAML ドキュメントを出力してみる。\n$ datacontract export --format dbt --server bigquery datacontract_v5.yml version: 2 models: - name: money_forward_main_group config: meta: data_contract: my-data-contract-id materialized: table contract: enforced: true description: MoneyForward の収支における大項目 columns: - data_type: NUMBER description: 大項目 ID constraints: - type: not_null - type: unique name: mf_main_group_id - data_type: STRING description: 大項目の項目名 constraints: - type: not_null name: name - description: 大項目が収入の場合は true, 支出の場合は false name: is_income data_type が微妙で BigQuery の型定義になっていない。\n--server bigquery を与えているので配慮してほしいところではある。\nsources 用のドキュメントの形式でも export できる。\n$ datacontract export --format dbt-sources --server bigquery datacontract_v5.yml version: 2 sources: - name: my-data-contract-id tables: - name: money_forward_main_group description: MoneyForward の収支における大項目 columns: - tests: - dbt_expectations.dbt_expectations.expect_column_values_to_be_of_type: column_type: NUMBER - not_null - unique description: 大項目 ID name: mf_main_group_id - tests: - dbt_expectations.dbt_expectations.expect_column_values_to_be_of_type: column_type: STRING - not_null description: 大項目の項目名 name: name - description: 大項目が収入の場合は true, 支出の場合は false name: is_income description: \u0026#39;MoneyForward の収支における分類 (項目)。 ダウンロードできる収支詳細に記載される情報を元に作成されている。 \u0026#39; staging 用の SQL の形式の export もある。\n$ datacontract export --format dbt-staging-sql --server bigquery datacontract_v5.yml select mf_main_group_id, name, is_income from {{ source(\u0026#39;my-data-contract-id\u0026#39;, \u0026#39;money_forward_main_group\u0026#39;) }} このようにいくつかの dbt の形式で出力できるが、微妙な部分があるのでこのまま data contract を SSoT として dbt のファイルを CI で自動生成するという運用は現時点では難しいだろう。\nまたリッチなテストなども表現できないため、dbt をがっつり使っているチームには物足りなさがあると思われる。\ndata contract をまず作り、それをもとに dbt モデルを構築していくときのベースを作る用途なら現時点でもありかもしれない。\nHTML HTML 形式で export することもできる。\nホスティングして data contract を web で見せられるようにすることもできるだろう。\n$ datacontract export --format html datacontract_v5.yml \u0026gt; money_forward_main_group.html HTML での export 例\n頑張ればこれで data catalog を作ることもできそう。\nまた今回の data contract には含まれていないが、前述の servicelevels ブロックがあれば web 上で SLA を示せたりできて良さそうだ。\n考察 Data Contract CLI で何ができるかがわかったところで最初の図を再掲。\ndatacontract/datacontract-cli より\nこの図から Data Contract CLI としては data contracts ファーストな運用を考えていることがわかる。\nまず data provider と data consumer の合意のもとに宣言的に data contracts を作成する。\nそれに基づき、それぞれ data provider と data consumer で使われるツール用に data contracts から必要なファイルを自動生成していく。\n例えば data provider である application 向けにはログの validation 用に JSON Schema のファイルを作成する。\ndata consumer である DWH 向けには dbt のファイルを用意する、など。\nまた、data contract に記載の情報からデータに対してテストを実行することができる。\n簡単な column の制約から、SodaCL などを使ったより高度な品質チェックまで。\nデータについてのドキュメントも data contract から生成される。\nこのようにまず data contract ありきで、モデルのメタ情報としてはそれを SSoT とし、data provider / consumer で必要なファイルを CI で自動生成するという未来が見えてくるような気がする。\nただしその未来を実現するための課題も現状では多い。\nData Contract CLI の相互運用性 (interoperatbility) が最も大きなボトルネックになるだろう。\n例えば想定する dbt ドキュメントが一発で作れるか、package を使ったリッチなテストが書けるか、etc.\nそもそも前述したようにバグっぽい挙動や微妙な挙動もあり、このツール自体がまだ発展途上という感がある。\nこれから普及していくかというのは今のところ何とも言えない。\nちなみに作者の1人は Data Mesh Manager というサービスの制作者であり、(ここまでのところでは触れていなかったが) Data Contract CLI にはこの Data Mesh Manager との連携機能が組み込まれている。\ndata contract ファーストの世界のビジョンはとても面白いと思ったので、いい方向でツールが発展していくとうれしい。\n","permalink":"https://soonraah.github.io/posts/data-contract-cli/","summary":"このポストについて Data Contract CLI を触ってみたところ、面白かったのとこれからのデータパイプライン開発について思うところがあったので書いてみる。\nData Contract CLI とは？ datacontract/datacontract-cli\nData Contract CLI は data contracts を運用するためのオープンソースのコマンドラインツールである。\ndata contracts の概念については以前の記事で詳しく書いているのでそちらをご参考いただければと。\nただしこちらの記事は1年前のものであり、今回取り上げる Data Contract CLI の登場などを含めて現在では data contracts を取り巻く状況も変わっている可能性があることに注意。\nData Contract CLI は Python で開発されており、pip でインストールすることができる。\nこの記事を書いている時点では v0.10.3 が最新であり、この記事の内容はこのバージョンに基づいている。\nData Contract CLI で扱う data contracts は YAML で定義される前提となっており、その仕様は datacontract/datacontract-specification で決められている。\nこの data contracts に対して Data Contract CLI では次のようなことが行える。\nlint によるフォーマットチェック データソースに接続した上での schema やデータ品質のテスト data contracts の破壊的な変更の検出 JSON Schema や dbt など、他の形式からの／へのインポートとエクスポート 以下の図がイメージしやすい。","title":"Data Contract CLI から考える Data Contracts ファーストのデータパイプラインの未来"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第8章「データ統合と相互運用性」について。\n業務で扱っているデータ基盤はデータ統合が不完全であるため、なんとかしたいと考えている。\n以降、特に注釈のない引用は DMBOK2 第8章からの引用とする。\nデータストレージと相互運用性とは データ統合と相互運用性 (DII: Data Integration and Interoperability) は次のように定義されている。\nアプリケーションや組織内および相互間におけるデータの移動と統合を管理する\nデータの移動を効率的に管理することがそのビジネス上の意義となる。\nほとんどの組織には数多くのデータストアがあり、組織内・組織間でデータを移動させることは IT 組織の重要な任務となっている。\n複雑さとそれに伴うコストを管理するために、全社的な視点からデータ統合を設計しなければならない。\nデータウェアハウスなどのデータハブによりアプリケーション間のインターフェースの数を削減することができる。\nDII のゴールは以下\n法令を遵守しながら、必要とするフォーマットと時間枠でデータを安全に提供する。 共有のモデルとインターフェースを開発することでソリューションを管理するコストと複雑さを軽減する。 重要なイベントを特定し、アラートとアクションを自動的に起動する。 ビジネスインテリジェンス、アナリティクス、マスターデータ管理、業務効率化の取り組みをサポートする。 概念・用語など 抽出、変換、取込 DII の中心にあるのが、抽出 (Extract)、変換 (Transform)、取込 (Load) のいわゆる ETL という基本プロセス。\n抽出 ソースから必要なデータを選択し、抽出する 抽出されたデータはディスク上やメモリ上にステージングされる 業務システムで実行される場合は、少ないリソースを利用するように設計する 変換 ソースデータを変換してターゲットデータストアの構造と互換性を持つようにする フォーマット変更、構造の変更、意味的変換、重複排除、並べ替えなどがある 取込 ターゲットシステムに物理的に格納されるか、提供される ELT ターゲットシステムにより多くの変化機能がある場合は、プロセスの順序を ELT にすることができる データレイクへの取込を行うビッグデータ環境では一般的 レイテンシ ソースシステムでデータが生成されてから、ターゲットシステムでそのデータが利用可能になるまでの時間差。\nアプローチによってレイテンシの高低が異なる。\nバッチ 利用者や自動的な要求に応えて、定期的にアプリケーションや組織間を一定量まとまって移動させる レイテンシは高いが大量データを処理するときのパフォーマンスがいい 低レイテンシを実現するためのマイクロバッチもある 変更データキャプチャ データの変更 (挿入・変更・削除) のデータセットを監視し、その差分をターゲットのシステムに渡す DBMS のアクティビティログをコピーし、処理する形で行われることもある 準リアルタイムとイベント駆動 設定された予定により1日を通して分割された少量のデータセットで処理されたり、データ更新などのイベントが発生したときに処理されたりする 一般的にエンタープライズ・サービス・バスを利用して実装される 非同期 データ提供側は受信側の更新確認を待たずに処理を続行する リアルタイム、同期 次のトランザクションを実行する前に、他のアプリケーションからの確認を受け取るまで実行プロセスが待機する 非同期と比べて状態管理の負担が少ないが、他のトランザクションをブロックしたり遅延させたりすることもある 低レイテンシまたはストリーミング イベントが発生したときにシステムからリアルタイムで連続して流れる リプリケーション 分析やクエリによるパフォーマンス低下を防ぐために、トランザクション処理環境にリプリケーション (複製) を使用することがある。\n多くの DBMS にはリプリケーションを作るためのユーティリティ機能がある。\nアーカイブ 利用頻度が低いデータは、よりコストがかからない代替データ構造やストレージに移行できる。\nアーカイブ技術が変わっても古いデータにアクセスできるようにしておくことが重要。\nエンタープライズ・メッセージフォーマット／カノニカルモデル 組織やグループでデータを共有するための標準化されたフォーマット、共通モデルがカノニカルモデル。\n組織内で合意するのは大仕事だが、データ相互運用性の複雑さとサポートコストが大幅に削減される。\nデータ連携モデル データ転送のためのシステム間を接続する方法。\nポイント・ツー・ポイント データを共有するシステム間で互いに直接データを渡す 多数のシステムが同じソースから同じデータを必要とする場合は効率が下がる ハブ＆スポーク 多くのアプリケーションが利用できる中央データハブへ共有データを集約 データウェアハウス、データマート、オペレーショナル・データストア、マスターデータ管理のハブなどがその例 ソースシステムへのリソースへの影響やインターフェース構築のコストを最小にできる パブリッシュ・サブスクライブ データを提供するシステムはデータサービスのカタログにリストされ、データを利用するシステムはそれらのサービスをサブスクライブする DII アーキテクチャの概念 アプリケーションカップリング 2つのシステムが関連し合っている結合度合い サービス、API、メッセージキューなどの技法を用いて疎結合にするのが望ましい オーケストレーションとプロセスコントロール オーケストレーションとは復数のプロセスがシステム内でどのように構成され、実行されるかを説明するために使われる プロセスコントロールはデータの発信、配信、取込が正確かつ完全であることを保証するために必要な要素 バッチジョブのログ アラート ジョブ依存チャート etc. エンタープライズアプリケーション統合 ソフトウェアモジュールは明確に定義されたインタフェース呼び出し (API) だけを介して連携するというモデル エンタープライズ・サービスバス システム間の仲介役として機能し、システム間でメッセージをやり取りするシステム 疎結合の例ではアプリケーション間のサービスとして機能する サービス指向アーキテクチャ (SOA) アプリケーション間で明確に定義されたサービス呼び出しを利用することでデータを提供したり、データを更新したりする SOA によりアプリケーションの独立性が担保され、システムの置き換えが容易になる データについてのサービスはサービスカタログ上で定義される 複合イベント処理 (CEP) 復数のソースから得られるデータを結合して重要なイベントを識別すること 振る舞いや行動を予測し、リアルタイム応答を自動的に起動できるようになる データフェデレーションと仮想化 データフェデレーションでは構造に関係なく、個々のデータストアの組み合わせにアクセスできる 仮想化では復数の異種データストアを単一のデータベースとしてアクセスできる Data as a Service SaaS (Software as a Service) と同様の概念 DaaS ではベンダーがライセンスを提供し、必要に応じてデータを提供する クラウドベースの統合 データ、プロセス、SOA、アプリケーション統合などのユースケースに対応する、クラウドサービスとして提供されるシステム統合の一形態 SaaS の登場により、クラウドベースの統合を経て、組織が持つデータセンターの外にあるデータを統合するための新しい種類の需要が生まれた データ交換標準 データエレメントの構造に関する正式なルール。\nISO により制定されている。\nシステム間で交換書式やデータレイアウトを合意すれば、起業のデータ相互運用性を大幅に簡素化し、運用コストを下げることができる。\n所感 概ね業務などを通じて知っている内容だった。\nデータ統合は難しい。\nApache Flink には CEP の機能があるけど、よく使われているのだろうか？\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-8/","summary":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第8章「データ統合と相互運用性」について。\n業務で扱っているデータ基盤はデータ統合が不完全であるため、なんとかしたいと考えている。\n以降、特に注釈のない引用は DMBOK2 第8章からの引用とする。\nデータストレージと相互運用性とは データ統合と相互運用性 (DII: Data Integration and Interoperability) は次のように定義されている。\nアプリケーションや組織内および相互間におけるデータの移動と統合を管理する\nデータの移動を効率的に管理することがそのビジネス上の意義となる。\nほとんどの組織には数多くのデータストアがあり、組織内・組織間でデータを移動させることは IT 組織の重要な任務となっている。\n複雑さとそれに伴うコストを管理するために、全社的な視点からデータ統合を設計しなければならない。\nデータウェアハウスなどのデータハブによりアプリケーション間のインターフェースの数を削減することができる。\nDII のゴールは以下\n法令を遵守しながら、必要とするフォーマットと時間枠でデータを安全に提供する。 共有のモデルとインターフェースを開発することでソリューションを管理するコストと複雑さを軽減する。 重要なイベントを特定し、アラートとアクションを自動的に起動する。 ビジネスインテリジェンス、アナリティクス、マスターデータ管理、業務効率化の取り組みをサポートする。 概念・用語など 抽出、変換、取込 DII の中心にあるのが、抽出 (Extract)、変換 (Transform)、取込 (Load) のいわゆる ETL という基本プロセス。\n抽出 ソースから必要なデータを選択し、抽出する 抽出されたデータはディスク上やメモリ上にステージングされる 業務システムで実行される場合は、少ないリソースを利用するように設計する 変換 ソースデータを変換してターゲットデータストアの構造と互換性を持つようにする フォーマット変更、構造の変更、意味的変換、重複排除、並べ替えなどがある 取込 ターゲットシステムに物理的に格納されるか、提供される ELT ターゲットシステムにより多くの変化機能がある場合は、プロセスの順序を ELT にすることができる データレイクへの取込を行うビッグデータ環境では一般的 レイテンシ ソースシステムでデータが生成されてから、ターゲットシステムでそのデータが利用可能になるまでの時間差。\nアプローチによってレイテンシの高低が異なる。\nバッチ 利用者や自動的な要求に応えて、定期的にアプリケーションや組織間を一定量まとまって移動させる レイテンシは高いが大量データを処理するときのパフォーマンスがいい 低レイテンシを実現するためのマイクロバッチもある 変更データキャプチャ データの変更 (挿入・変更・削除) のデータセットを監視し、その差分をターゲットのシステムに渡す DBMS のアクティビティログをコピーし、処理する形で行われることもある 準リアルタイムとイベント駆動 設定された予定により1日を通して分割された少量のデータセットで処理されたり、データ更新などのイベントが発生したときに処理されたりする 一般的にエンタープライズ・サービス・バスを利用して実装される 非同期 データ提供側は受信側の更新確認を待たずに処理を続行する リアルタイム、同期 次のトランザクションを実行する前に、他のアプリケーションからの確認を受け取るまで実行プロセスが待機する 非同期と比べて状態管理の負担が少ないが、他のトランザクションをブロックしたり遅延させたりすることもある 低レイテンシまたはストリーミング イベントが発生したときにシステムからリアルタイムで連続して流れる リプリケーション 分析やクエリによるパフォーマンス低下を防ぐために、トランザクション処理環境にリプリケーション (複製) を使用することがある。","title":"読書メモ: DMBOK2 第8章 データ統合と相互運用性"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第5章「データストレージとオペレーション」について。\n主にデータベースの運用に関する内容となっており、いわゆるデータベースエンジニアの人には当たり前の話？が書かれている。\n以降、特に注釈のない引用は DMBOK2 第6章からの引用とする。\nデータストレージとオペレーションとは 以下のように定義されている。\nデータの価値を最大化するために、永続化されるデータを設計し、実装し、サポートすること\n主にデータベース管理者 (DBA: Database Administrators) が行うことになる。\n次の2つのアクティビティが含まれる。\nデータベースサポート: データベース環境の初期実装からデータの取得、バックアップ、廃棄までのデータライフサイクル関連アクティビティ データベース技術サポート: 技術要件を決め、技術的なアーキテクチャを定義し、技術を実装・管理する 事業がデータに依存する企業においてはデータストレージとオペレーションのアクティビティは事業の継続性のために必要不可欠である。\nゴールは次のとおり\nデータライフサイクル全体にわたるデータの可用性を管理する データ資産の完全性を保証する データ処理の性能を管理する 概念・用語など データベースアーキテクチャの種類 集中型データベース: 単一システム内で使うデータを一箇所にまとている 分散型データベース: 多数のノードにデータが配置される 連邦型データベース: 自立した複数のデータベースシステムを単一の連邦型データベースに割り当てる 仮想化／クラウドプラットフォーム: クラウド上のデータベースを実装 データベース処理のタイプ ACID: トランザクションの信頼性のための制約 原子性 (Atomicity): 操作はすべて実行されるかまったく実行されないかのどちらか 一貫性 (Consistency): トランザクションはシステムが定義するすべてのルールを常に満たさなければならない 独立性 (Isolation): 各トランザクションは実行中の他のトランザクションの影響を受けない 永続性 (Durability): トランザクションは完了すると元に戻せない BASE: データの量と多様性を受けた、ACID とは異なる考え 基本的に利用可能 (Basically Available): ノードの障害発生時もあるレベル以上の可用性を保証する ソフトステート (Soft State): データは一定の変動状態にある 最終的な一貫性の確保 (Eventual Consistency): データは最終的にすべてのノードで一貫性を保つが、各トランザクションの一貫性が常に確保されているわけではない CAP: 分散システムでは以下のどれか2つしか満たせない 一貫性 (Consistency): システムは常に想定どおり動作できなければならない 可用性 (Availability): システムは要求時に利用可能でなければならに 分断耐性 (Partition Tolerance): システムは部分的な障害の発生時に運用を続行できなければならない データベース構成 上から順により制御された構造であり、かつ古くからあるものとなっている。\n階層型 tree schema データは親子関係が必要な木構造として整理されている リレーショナル型 集合理論や関係代数にもとづく schema on write 書き込み時に構造を知っておく必要がある 非リレーショナル型 データを単純な文字列や完全なファイルとして保存することができる schema on read 様々な方法で読み取ることができる 一般的なデータベースプロセス すべてのデータベースは何らかの形で以下のプロセスを実現する。\nアーカイブ 最大容量と増加の予測 変更データキャプチャ (CDC: Change Data Capture) 廃棄 レプリケーション 耐障害性と復旧 保持 シャーディング (区画化) アクティビティ データストレージとオペレーションでは主にデータベース技術の管理とデータベースの管理の2つのアクティビティがある。\nデータベース技術の管理 データベース技術の特徴に対する理解 技術選定において候補となるデータベース技術の特徴を理解しておく必要がある 1つのデータベースアーキテクチャや DBMS があらゆるニーズに対応しているわけではない データベース技術の評価 戦略的に DBMS ソフトウェアを選定することが重要 小規模なパイロットプロジェクトや PoC の実施を推奨 データベース技術の管理と監視 データベース技術の実装、サポート、利用に関わる人にトレーニングが必要 定期的なバックアップとリカバリテスト データベースの管理 要件の理解 容量見積などを含むストレージ要件 トランザクション型、大規模な書き込み \u0026amp; 検索型などの使用パターン データアクセスのための言語、手法などのアクセス要件 事業継続性の計画 災害や有害事象への備え バックアップの作成 データのリカバリ データベースインスタンスの実装 物理ストレージ環境の管理 データベース・アクセス統制管理 ストレージコンテナの作成 物理データモデルの実装 データの取り込み データのレプリケーション管理 データベース性能の管理 SLA の設定 データベースの可用性の管理 データベースの稼働管理 サービスレベルの維持 トランザクション性能とバッチ性能 性能の問題の修復 代替環境の維持 テスト用データセットの管理 効率的なテストのための高品質なデータ データ移行の管理 まとめ この章はいわゆるオペレーショナルな RDBMS について書かれているようにも見えるが、抽象度が高いので組織レベルのデータ基盤や DWH にも当てはまる。\nDMBOK2 の内容は抽象的に書かれていることも多く、それゆえ現場エンジニアにとってぱっと理解しにくいものとなっていることもある。\nACID は大事だと思うが、データレイクの運用であまり考慮されていないケースを見かけることがある。\nCAP 定理は書籍「データ指向アプリケーションデザイン」で疑問を投げかけられてたな。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-6/","summary":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第5章「データストレージとオペレーション」について。\n主にデータベースの運用に関する内容となっており、いわゆるデータベースエンジニアの人には当たり前の話？が書かれている。\n以降、特に注釈のない引用は DMBOK2 第6章からの引用とする。\nデータストレージとオペレーションとは 以下のように定義されている。\nデータの価値を最大化するために、永続化されるデータを設計し、実装し、サポートすること\n主にデータベース管理者 (DBA: Database Administrators) が行うことになる。\n次の2つのアクティビティが含まれる。\nデータベースサポート: データベース環境の初期実装からデータの取得、バックアップ、廃棄までのデータライフサイクル関連アクティビティ データベース技術サポート: 技術要件を決め、技術的なアーキテクチャを定義し、技術を実装・管理する 事業がデータに依存する企業においてはデータストレージとオペレーションのアクティビティは事業の継続性のために必要不可欠である。\nゴールは次のとおり\nデータライフサイクル全体にわたるデータの可用性を管理する データ資産の完全性を保証する データ処理の性能を管理する 概念・用語など データベースアーキテクチャの種類 集中型データベース: 単一システム内で使うデータを一箇所にまとている 分散型データベース: 多数のノードにデータが配置される 連邦型データベース: 自立した複数のデータベースシステムを単一の連邦型データベースに割り当てる 仮想化／クラウドプラットフォーム: クラウド上のデータベースを実装 データベース処理のタイプ ACID: トランザクションの信頼性のための制約 原子性 (Atomicity): 操作はすべて実行されるかまったく実行されないかのどちらか 一貫性 (Consistency): トランザクションはシステムが定義するすべてのルールを常に満たさなければならない 独立性 (Isolation): 各トランザクションは実行中の他のトランザクションの影響を受けない 永続性 (Durability): トランザクションは完了すると元に戻せない BASE: データの量と多様性を受けた、ACID とは異なる考え 基本的に利用可能 (Basically Available): ノードの障害発生時もあるレベル以上の可用性を保証する ソフトステート (Soft State): データは一定の変動状態にある 最終的な一貫性の確保 (Eventual Consistency): データは最終的にすべてのノードで一貫性を保つが、各トランザクションの一貫性が常に確保されているわけではない CAP: 分散システムでは以下のどれか2つしか満たせない 一貫性 (Consistency): システムは常に想定どおり動作できなければならない 可用性 (Availability): システムは要求時に利用可能でなければならに 分断耐性 (Partition Tolerance): システムは部分的な障害の発生時に運用を続行できなければならない データベース構成 上から順により制御された構造であり、かつ古くからあるものとなっている。","title":"読書メモ: DMBOK2 第6章 データストレージとオペレーション"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第5章「データモデリングとデザイン」について。\n第4章でデータモデリングについて言及されていたが、ここで詳しく見ていく。\n以降、特に注釈のない引用は DMBOK2 第5章からの引用とする。\nデータモデリングとは データモデリングとは、データ要件を洗い出し、分析し、取扱スコープを決めるプロセスであり、データ要件を記述し伝えるために、明確に定義されたデータモデルと呼ばれる様式が用いられる。このプロセスは反復的であり、概念、論理、物理モデルが含まれる。\n第4章 データアーキテクチャ ではエンタープライズ・データモデルという言葉が登場したが、この第5章ではそのデータモデルがより具体的に解説されている。\nデータモデルは効果的なデータマネジメントを行うにあたり必要なものとされている。\n次のような意義がある。\nデータに関する共通語彙を提供する 組織のデータや情報システムに関しての明示的な知識を捉え文書化する プロジェクトにおいて主なコミュニケーションツールとして使われる アプリケーションをカスタマイズ、統合、リプレースする際の出発点となる データモデルは組織が把握した現状、またはあるべき姿として組織のデータを記述する。\nカテゴリー情報、リソース情報、業務イベント情報、詳細取引情報がその対象となる。\nデータモデルの構成要素 ほとんどのデータモデルはエンティティ、リレーションシップ、属性、ドメインにより構成される。\nエンティティ エンティティとはある組織が情報を収集する対象のこと。\n組織が使う名詞。\n以下は Marmaid の Entity Relationship Diagrams でエンティティを表現した例。\n学校を例にしており、学生、コース、インストラクタがエンティティとして表されている。\n--- title: Entity --- erDiagram Student Course Instructor リレーションシップ リレーションシップはエンティティ間の関連性を表す。\n以下の例は学生はコースを \u0026ldquo;履修する\u0026rdquo;、インストラクタはコースを \u0026ldquo;教える\u0026rdquo; ことを表している。\n--- title: Relationship --- erDiagram Student }|--|{ Course : take Instructor }|--|{ Course : teach 線の両端の形はリレーションシップの cardinality (多重度) を表している。\nこの場合ではそれぞれカラスの足状になっているが、それぞれ「1またはそれ以上」という意味で、「学生は1つ以上のコースを履修する」「コースは1人以上の学生に履修される」ということを表している。\nリレーションシップの arity (項数) は一般的には二項型 (バイナリリレーションシップ) であることが多く、2つのエンティティ間の関係を表す。\n一方で線の両端が同じエンティティにつながっている単項型、3のエンティティが関連する三項型もある。\n以下は単項型の例で、前提となるコースの履修が必要であることを示す。\n--- title: Unary Relationship --- erDiagram Course }|--|| Course : \"prerequisite course is required\" 2つのエンティティ間でリレーションシップがある場合、外部キーが暗黙的に作成されることがある。\n属性 属性はエンティティを識別、記述、評価するプロパティ。\n属性はドメインを持つことができる。(後述)\n以下の例では学生のエンティティに属性を付与した。\n--- title: Attribute --- erDiagram Student { int student_id PK string first_name string last_name date birthday } ここで学生番号 student_id は単体で PK (primary key) に指定されており、インスタンスを一意に識別する1つの属性、つまりシンプルキーであると言える。\nドメイン ドメイン (定義域) は属性が取りうる値一式のこと。\n例えばコース開始日という属性があった場合は、現実に存在する日付として有効な値がドメインに含まれることになる。\nコース開始日は開校日以降とする、のように範囲を限定することもできる。\n余談だが…\nエンティティ、リレーションシップ、属性などのデータモデルの構成要素は概念データモデルのレベルで説明した方がよかったかもしれない。\nしかし Mermaid の ER 図はおそらく物理データモデル用のものとなっており、物理っぽい例になってしまった。\nデータモデリング・スキーム スキームとはデータの表現方法のようなもので、DMBOK2 では以下に示す6種類のスキームが紹介されている。\n各スキームそれぞれに表記法がいくつか存在する。\nスキームごとにどの (物理的な) データベースで使用できるか、またどの詳細レベルで使用できるかが決まっている。\n例えばリレーショナルスキームでは3つの詳細レベルすべてで RDBMS 状にモデルを構築できるが、他のタイプのデータベースでは概念モデル (CDM) と論理モデル (LDM) のみを構築できる。\nスキーム RDBMS MDBMS オブジェクト型 ドキュメント型 カラム型 グラフ型 キーバリュー型 リレーショナル CDM LDM PDM CDM LDM CDM LDM CDM LDM CDM LDM CDM LDM CDM LDM ディメンショナル CDM LDM PDM CDM LDM PDM オブジェクト指向 CDM LDM PDM CDM LDM PDM ファクトベース CDM LDM PDM CDM LDM CDM LDM CDM LDM CDM LDM CDM LDM CDM LDM タイムベース PDM NoSQL PDM PDM PDM PDM PDM DMBOK2 表10 スキームとデータベースの相互参照表\nこのポストではこのうち2つだけ触れておく。\nリレーショナルスキーム 1970年、Edward Codd により提唱された。\n業務データを正確に表現し、冗長性を除去することが設計目標である。\nIE 表記法というのがもっとも一般的とのことで、Mermaid の ER 図も IE 表記法に近い。\nerDiagram Student }|--|{ Course : take ディメンショナルスキーム ディメンショナルモデルでは大量データに対する問い合わせと分析を最適化しようとしている。\n特定の業務プロセスに的を絞った業務上の設問に対応する。\n以下の例は入学申請というプロセスを表したものであり、例えば学生が所属するゾーンごとに分析できる。\nゾーンから地域、国へと分析を拡大できる。\nDMBOK2 図40 ディメンショナルモデルの分析軸表記\nこの分析軸表記法は DMBOK2 では有用とされているがネットで検索してもほとんど出てこない…\nただしデータウェアハウスの設計でよく目にする Kimball のディメンショナルモデリングは正にこれなので重要である。\nファクトテーブルやディメンションテーブルという概念もここで出てくる。\nちなみにフラットで単一なディメンショナル構造を正規化し、階層構造やネットワーク構造のコンポーネントを作ることをスノーフレーク化というとのこと。\nあの Snowflake の製品名はここから来ているのだろうか？\n詳細レベル データモデリングには概念・論理・物理の3つの詳細レベルがあり、新しいアプリケーションを開発するときなどはこの順にモデリングを進めることになる。\n概念モデル 概念データモデルには、関連する概念の集合体としてデータ要件の概要が取り込まれる。ここには、特定の領域や業務機能に関する基本的で重要なビジネスエンティティのみが含まれ、各エンティティの説明とエンティティ間のリレーションシップが含まれる。\nここは実装によらずあくまで業務を説明するものであり、したがって table 名や column 名ではなく業務の言葉で表現されることになる。\n論理モデル 論理データモデルは、詳細なデータ要件が表現されたものであり、通常、アプリケーション要件のような特定の使用シナリオに適応する。論理データモデルも技術や具体的な実装上の制約から独立している。論理データモデルは概念データモデルの拡張として始まることが多い。\nリレーショナルスキームの場合、概念モデルに属性を追加して拡張し、正規化を適用する。\n物理モデル 物理データモデル (PDM) は詳細な技術的ソリューションを表す。このモデルは論理データモデルを出発点として作成されることが多く、そこからハードウェア、ソフトウェア、ネットワークツールを組み合わせた環境に適応するように設計される。\nこのモデルは実装方法に依存しており、具体的な table 名や column 名で記述される。\nパフォーマンスのために非正規化されることがある。\nビューにするかやパーティションなどについても考慮する。\nデータモデリングの導入・推進 ビジネスアナリスト、データモデラーによって以下のように進められる。\nデータモデリング計画 データモデルの構築 データモデルのレビュー データモデルの維持と更新管理 データモデルの構築において、新しいアプリケーションを開発する際に要件定義から始めるプロセスをフォワードエンジニアリングと呼ぶ。\nフォワードエンジニアリングでは概念 -\u0026gt; 論理 -\u0026gt; 物理の順にデータモデリングを行い、具体的にしていく。\nその逆順で行われるのがリバースエンジニアリングであり、既存のデータベースを文書化するプロセスとなる。\nデータモデルのレビューにおいては Data Model Scorecard (Hoberman, 2015) というものが使われる。\nまとめ 曖昧な理解だったデータモデリングについて把握できた。\n例えば dbt はデータモデリングツールだと言われることがあるが、あればリレーショナル物理モデリングの一部を行っている、という言い方が正しそうだ。\nモダンなデータ基盤では概念・論理モデルはどのやって構築するのがベストプラクティスなのだろうか？\nそれともあまり概念・論理のモデリングは行われていないのだろうか？\n詳細レベルの話は結構大事だと思っていて、3つのレベルを分けずにまとめてやることが多かったのであまり良くなかったと反省した。\nまとめてやると物理を殴れるエンジニアがやるしかないが、レベルを分ければ概念モデルなどはデータアナリスト、営業などが参加しやすくなる。\n今の仕事のデータ追加のフローを改善できそうな気がしてきた。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-5/","summary":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第5章「データモデリングとデザイン」について。\n第4章でデータモデリングについて言及されていたが、ここで詳しく見ていく。\n以降、特に注釈のない引用は DMBOK2 第5章からの引用とする。\nデータモデリングとは データモデリングとは、データ要件を洗い出し、分析し、取扱スコープを決めるプロセスであり、データ要件を記述し伝えるために、明確に定義されたデータモデルと呼ばれる様式が用いられる。このプロセスは反復的であり、概念、論理、物理モデルが含まれる。\n第4章 データアーキテクチャ ではエンタープライズ・データモデルという言葉が登場したが、この第5章ではそのデータモデルがより具体的に解説されている。\nデータモデルは効果的なデータマネジメントを行うにあたり必要なものとされている。\n次のような意義がある。\nデータに関する共通語彙を提供する 組織のデータや情報システムに関しての明示的な知識を捉え文書化する プロジェクトにおいて主なコミュニケーションツールとして使われる アプリケーションをカスタマイズ、統合、リプレースする際の出発点となる データモデルは組織が把握した現状、またはあるべき姿として組織のデータを記述する。\nカテゴリー情報、リソース情報、業務イベント情報、詳細取引情報がその対象となる。\nデータモデルの構成要素 ほとんどのデータモデルはエンティティ、リレーションシップ、属性、ドメインにより構成される。\nエンティティ エンティティとはある組織が情報を収集する対象のこと。\n組織が使う名詞。\n以下は Marmaid の Entity Relationship Diagrams でエンティティを表現した例。\n学校を例にしており、学生、コース、インストラクタがエンティティとして表されている。\n--- title: Entity --- erDiagram Student Course Instructor リレーションシップ リレーションシップはエンティティ間の関連性を表す。\n以下の例は学生はコースを \u0026ldquo;履修する\u0026rdquo;、インストラクタはコースを \u0026ldquo;教える\u0026rdquo; ことを表している。\n--- title: Relationship --- erDiagram Student }|--|{ Course : take Instructor }|--|{ Course : teach 線の両端の形はリレーションシップの cardinality (多重度) を表している。\nこの場合ではそれぞれカラスの足状になっているが、それぞれ「1またはそれ以上」という意味で、「学生は1つ以上のコースを履修する」「コースは1人以上の学生に履修される」ということを表している。\nリレーションシップの arity (項数) は一般的には二項型 (バイナリリレーションシップ) であることが多く、2つのエンティティ間の関係を表す。","title":"読書メモ: DMBOK2 第5章 データモデリングとデザイン"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第4章「データアーキテクチャ」について。\nやや抽象度が高い内容となっており、理解が難しいと感じた。\n以降、特に注釈のない引用は DMBOK2 第4章からの引用とする。\nデータアーキテクチャとは データアーキテクチャの定義は以下のとおり。\n企業の (組織構造に関係なく) データニーズを明確にし、ニーズに合うマスターとなる青写真を設計し、維持する。マスターとなる青写真を使ってデータ統合を手引し、データ資産をコントロールし、ビジネス戦略に合わせてデータへの投資を行う。\n業務戦略と技術実装の間を橋渡しすることがデータアーキテクチャの目的。\n以下が成果物、つまり前述の定義の「マスターとなる青写真」となる。\nデータの保存と処理の要件 企業の現在のデータ要件と長期のデータ要件を満たす構造や計画の立案 これらを記述するためにエンタープライズ・データモデル (EDM) とデータフロー設計が必要となる。(後述)\nエンタープライズアーキテクチャ データアーキテクチャとビジネス、アプリケーション、テクニカルなど他の領域のアーキテクチャとの違いは下表のようになる。\nそれぞれが影響を及ぼし合う関係となっており、データアーキテクチャはビジネスアーキテクチャによって決められる。\nドメイン エンタープライズ・ビジネスアーキテクチャ エンタープライズ・データアーキテクチャ エンタープライズ・アプリケーションアーキテクチャ エンタープライズ・テクニカルアーキテクチャ 目的 企業が顧客や他のステークホルダに どのように価値提供しているかを 明らかにする データがどのように整理・管理されるべきか記述する 企業内アプリケーションの 構造と機能を記述する システムを稼働して価値を 提供するために必要な 物理実装技術を記述する 要素 ビジネスモデル、業務プロセス、業務能力、サービス、イベント、戦略、語彙集 データモデル、データ定義、データマッピング仕様、データフロー、構造化データの API ビジネスシステム、ソフトウェアパッケージ、データベース テクニカルプラットフォーム、ネットワーク、セキュリティ、統合ツール 依存関係 他のドメインに対する要件を設定する ビジネスアーキテクチャによって作られ、要求されるデータを管理する ビジネス要件に基づいて特定されたデータに対応する アプリケーションアーキテクチャを提供し実行する 役割 ビジネスアーキテクトとアナリスト、ビジネス・データスチュワード データアーキテクトとモデラー、データスチュワード アプリケーションアーキテクト インフラストラクチャアーキテクト DMBOK2 表6 アーキテクチャ領域\nweb 系エンジニア的には「アーキテクチャ」という言葉だけ聞くとクラウドサービスのアイコンを使ってアーキテクチャ図をお絵描きするよくあるアレを想像しがちだが、データアーキテクチャはそれではないということ。\n(DMBOK2 の分類ではそれはテクニカルアーキテクチャの領域)\nエンタープライズ・データモデル (EDM) EDM とは\n全体的でエンタープライズレベルの実装に依存しない概念または論理モデルであり、企業全体にわたるデータに関して一貫した共通のビューを提供する。\n次の図は概念・論理モデルと物理的なデータモデルとの関係を示す。\n垂直方向には対象領域ごとに対応する異なるレベルのモデルが関係している。\n水平方向には同じレベルのモデルがあり、対象領域をまたいでエンティティが関係し合う。\nDMBOK2 図23 エンタープライズ・データモデル\nこれらはトップダウンまたはボトムアップのアプローチで構築される。\nレベル別に段階的・反復的に構築するのがセオリーとのこと。\nいきなりすべてを記述しようとするのは良くない。\nデータモデリングについて詳しくは第5章で見ていくことになる。\nデータフロー設計 データフロー設計はデータベース、アプリケーション、その他にまたがるデータの処理と格納の要件。\nデータリネージのドキュメントであり、データがビジネスプロセスやシステムをどのように移動するかを示したデータフロー図により記述される。\n以下はデータフロー図の例。\nDMBOK2 図26 データフロー図例\nデータと以下の関係をマッピングする。\nアプリケーション データベース ネットワークセグメント ビジネス上の役割 地域的祭が生じている拠点 データアーキテクチャの導入・推進 データアーキテクチャの導入には導入失敗のリスクアセスメント、および (他の多くの DMBOK 知識領域と同様) 組織と文化の変革が必要になる。\n次のようにしてデータアーキテクチャの慣行を確立する。\n既存のデータアーキテクチャ設計書の評価 ロードマップの開発 各プロジェクトにおける全社要件の管理 開発プロジェクトはデータアーキテクチャによって確立された業務要件と標準に基づいて、データを取得・格納・配布するためのソリューションを実装するもの データアーキテクチャは各プロジェクトのスコープ境界を決めるのに影響する。\nプロジェクトのデータ要件定義 プロジェクトのデータ設計レビュー データリネージの影響判定 データアプリケーションの制御 データアーキテクチャ標準の徹底 データテクノロジーと更新の意思決定ガイド プロジェクトは一般的にプロジェクト固有のアーキテクチャの優先事項を重視するが、全社的なデータアーキテクチャには積極的に取り組むべきとされる。\nまとめ データアーキテクチャの概念は抽象度が高く、データ基盤を単に IT の一貫として捉えている組織では理解されにくいと感じた。\n私自身、本章を読むまではデータ基盤まわりのシステムアーキテクチャのことだと思っていたが、異なるレベルの話だった。\n逆に言うとデータマネジメント系の勉強会で発表するようなデータマネジメントが推進されている会社ではデータアーキテクチャについての考えが明確になっているような気がしている。\n複数社で働いてきた自分の経験からだが、マネジメント層でも抽象度が高い仕事ができる人はそれほど多くない。\nそのへんのケイパビリティが組織のデータマネジメントの成否に強く影響してくるのだろう。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-4/","summary":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第4章「データアーキテクチャ」について。\nやや抽象度が高い内容となっており、理解が難しいと感じた。\n以降、特に注釈のない引用は DMBOK2 第4章からの引用とする。\nデータアーキテクチャとは データアーキテクチャの定義は以下のとおり。\n企業の (組織構造に関係なく) データニーズを明確にし、ニーズに合うマスターとなる青写真を設計し、維持する。マスターとなる青写真を使ってデータ統合を手引し、データ資産をコントロールし、ビジネス戦略に合わせてデータへの投資を行う。\n業務戦略と技術実装の間を橋渡しすることがデータアーキテクチャの目的。\n以下が成果物、つまり前述の定義の「マスターとなる青写真」となる。\nデータの保存と処理の要件 企業の現在のデータ要件と長期のデータ要件を満たす構造や計画の立案 これらを記述するためにエンタープライズ・データモデル (EDM) とデータフロー設計が必要となる。(後述)\nエンタープライズアーキテクチャ データアーキテクチャとビジネス、アプリケーション、テクニカルなど他の領域のアーキテクチャとの違いは下表のようになる。\nそれぞれが影響を及ぼし合う関係となっており、データアーキテクチャはビジネスアーキテクチャによって決められる。\nドメイン エンタープライズ・ビジネスアーキテクチャ エンタープライズ・データアーキテクチャ エンタープライズ・アプリケーションアーキテクチャ エンタープライズ・テクニカルアーキテクチャ 目的 企業が顧客や他のステークホルダに どのように価値提供しているかを 明らかにする データがどのように整理・管理されるべきか記述する 企業内アプリケーションの 構造と機能を記述する システムを稼働して価値を 提供するために必要な 物理実装技術を記述する 要素 ビジネスモデル、業務プロセス、業務能力、サービス、イベント、戦略、語彙集 データモデル、データ定義、データマッピング仕様、データフロー、構造化データの API ビジネスシステム、ソフトウェアパッケージ、データベース テクニカルプラットフォーム、ネットワーク、セキュリティ、統合ツール 依存関係 他のドメインに対する要件を設定する ビジネスアーキテクチャによって作られ、要求されるデータを管理する ビジネス要件に基づいて特定されたデータに対応する アプリケーションアーキテクチャを提供し実行する 役割 ビジネスアーキテクトとアナリスト、ビジネス・データスチュワード データアーキテクトとモデラー、データスチュワード アプリケーションアーキテクト インフラストラクチャアーキテクト DMBOK2 表6 アーキテクチャ領域\nweb 系エンジニア的には「アーキテクチャ」という言葉だけ聞くとクラウドサービスのアイコンを使ってアーキテクチャ図をお絵描きするよくあるアレを想像しがちだが、データアーキテクチャはそれではないということ。\n(DMBOK2 の分類ではそれはテクニカルアーキテクチャの領域)\nエンタープライズ・データモデル (EDM) EDM とは\n全体的でエンタープライズレベルの実装に依存しない概念または論理モデルであり、企業全体にわたるデータに関して一貫した共通のビューを提供する。\n次の図は概念・論理モデルと物理的なデータモデルとの関係を示す。\n垂直方向には対象領域ごとに対応する異なるレベルのモデルが関係している。\n水平方向には同じレベルのモデルがあり、対象領域をまたいでエンティティが関係し合う。\nDMBOK2 図23 エンタープライズ・データモデル","title":"読書メモ: DMBOK2 第4章 データアーキテクチャ"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第15章「データマネジメント成熟度アセスメント」について。\nデータマネジメントの導入において重要な役割を果たしそうなので早めに確認しておきたかった。\n以降、特に注釈のない引用は DMBOK2 第15章からの引用とする。\nデータマネジメント成熟度アセスメントとは データマネジメント成熟度アセスメント (DMMA: Data Management Maturity Assessment) はその名の通り、組織のデータマネジメントのレベルの評価に基づくプロセス改善の取り組みのこと。\n能力成熟度アセスメント (CMA: Capability Maturity Assessment) というものがあり、それのデータマネジメント版が DMMA。\nCMA では\n成熟度モデルは進化の観点から定義され、それにはプロセスの特性を表すレベルが使用される。組織がプロセスの特性を理解すると、組織は成熟度を測り、その能力を向上させるための計画を立てることができる。(中略) 新しいレベルに上がる度にプロセスの実行はより一貫性を増し、予測可能な状態となり、信頼性が高くなる。\nレベルは通常0~5の6段階で表される。\nDMMA は\n全体的なデータマネジメントを評価するために使用したり、単一の知識領域や、単一のプロセスに焦点を当てて使用したりできる。どのような点に焦点を当てたとしても、DMMA はデータマネジメント業務の健全性と有効性について、業務と IT の視点のギャップを埋めるために役立つ。\n組織が DMMA を実施する理由は、規制への対応、データガバナンス、プロセス改善、etc.\nDMMA の第一のゴールはデータマネジメント活動の現状を評価することであり、それにより改善計画を立てることができるようになる。\nアセスメントレベル 以下はアセスメントレベルの概要。\nデータマネジメントの各知識領域 (ex. データガバナンス、メタデータ管理、etc.) ごとにレベルが評価される。\nlevel 0: 能力が欠如した状態 データマネジメントの取り組みがない level 1: 初期／場当たり的な状態 限られたツールセットを用いた一般的なデータマネジメント ガバナンスは低レベル データ処理は一部の専門家に依存し、役割や責任は部門別に定義されている level 2: 反復可能な状態 組織は一元化された共通ツールを使い始める 役割は明確化されており、一部の専門家のみに依存しない level 3: 定義された状態 拡張可能なプロセスの導入と制度化 組織全体である程度統制されたデータの複製 データ品質全体の総体的な向上 組織的なポリシー定義と統制 level 4: 管理された状態 新しいプロジェクトやタスクから得られる結果が予測され、リスクの管理が始まる データマネジメントに成果に対する評価尺度が含まれる データマネジメント用の標準ツールが集中管理計画とガバナンス機能に組み合わされている level 5: 最適化された状態 活動の成果は十分予測可能に 組織は継続的な改善に重点を置く 十分理解された評価尺度を使ってデータ品質とプロセスが管理・測定される 次のように各知識領域ごとに可視化することができる。\n現状ランクと求められるランクの乖離が大きいところが組織にとってのリスクとなる。\nDMBOK2 図105 データマネジメント成熟度アセスメントを視覚化した例\n既存のフレームワーク DMMA のフレームワークとして5つのフレームワークが挙げられている。\n多くのベンダーが独自のモデルを開発しているため、ベンダーを選ぶ前、または独自のフレームワークを開発する前に複数のフレームワークを比較検討する必要がある。\nCMMI データマネジメント成熟度モデル (DMM) EDM 協議会 DCAM IBM データガバナンス評議会成熟度モデル スタンフォード・データガバナンス成熟度モデル ガートナーのエンタープライズ・インフォメーションマネジメント成熟度モデル ※DMBOK2 の出版が2018年で情報が古くなっているものもあり、リンク先が公式ではないものも含む。\nデータマネジメント成熟度アセスメントの実施 次のサイクルで実施する。\nアセスメントアクティビティの計画 経営幹部や業務部門が理解できるようアセスメントの目的を形式化する 組織全体をスコープとするのは難しいこともあるため、最初のアセスメントでは単一の業務領域などスコープをしぼる 情報収集のアプローチ (ワークショップ、インタビュー、アンケート、成果物レビューなど) を定義する 成熟度アセスメントを実行する 情報収集の実施 評価付を行い、合意された見解に達する 評価結果を吟味する 目標とする次の状態にいたるステップを明確化 アセスメント結果を報告 改善を達成するためのターゲットプログラムを作成する 特定のデータマネジメント機能を改善するためのアクションを特定 そのスケジュール、および実施された場合に期待される DMMA の改善を示す 成熟度の再評価 定期的に再評価を行うべき 定期的に改善が測定できると組織全体のコミットメントと熱意が維持される まとめ DMMA はデータマネジメントの導入時に政治的な意味でも重要になりそうだと思った。\n「データマネジメント？なにそれおいしいの？」というレベルの経営層に現実を見せて動機づけにできそう。\nまあデータマネジメントにちゃんと取り組むならどっちにしろ最初にやるべき。\n関係ないけどかなーり昔に働いていた職場で上司が「CMMI やることになった」とバタバタしていたのを思い出した。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-15/","summary":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第15章「データマネジメント成熟度アセスメント」について。\nデータマネジメントの導入において重要な役割を果たしそうなので早めに確認しておきたかった。\n以降、特に注釈のない引用は DMBOK2 第15章からの引用とする。\nデータマネジメント成熟度アセスメントとは データマネジメント成熟度アセスメント (DMMA: Data Management Maturity Assessment) はその名の通り、組織のデータマネジメントのレベルの評価に基づくプロセス改善の取り組みのこと。\n能力成熟度アセスメント (CMA: Capability Maturity Assessment) というものがあり、それのデータマネジメント版が DMMA。\nCMA では\n成熟度モデルは進化の観点から定義され、それにはプロセスの特性を表すレベルが使用される。組織がプロセスの特性を理解すると、組織は成熟度を測り、その能力を向上させるための計画を立てることができる。(中略) 新しいレベルに上がる度にプロセスの実行はより一貫性を増し、予測可能な状態となり、信頼性が高くなる。\nレベルは通常0~5の6段階で表される。\nDMMA は\n全体的なデータマネジメントを評価するために使用したり、単一の知識領域や、単一のプロセスに焦点を当てて使用したりできる。どのような点に焦点を当てたとしても、DMMA はデータマネジメント業務の健全性と有効性について、業務と IT の視点のギャップを埋めるために役立つ。\n組織が DMMA を実施する理由は、規制への対応、データガバナンス、プロセス改善、etc.\nDMMA の第一のゴールはデータマネジメント活動の現状を評価することであり、それにより改善計画を立てることができるようになる。\nアセスメントレベル 以下はアセスメントレベルの概要。\nデータマネジメントの各知識領域 (ex. データガバナンス、メタデータ管理、etc.) ごとにレベルが評価される。\nlevel 0: 能力が欠如した状態 データマネジメントの取り組みがない level 1: 初期／場当たり的な状態 限られたツールセットを用いた一般的なデータマネジメント ガバナンスは低レベル データ処理は一部の専門家に依存し、役割や責任は部門別に定義されている level 2: 反復可能な状態 組織は一元化された共通ツールを使い始める 役割は明確化されており、一部の専門家のみに依存しない level 3: 定義された状態 拡張可能なプロセスの導入と制度化 組織全体である程度統制されたデータの複製 データ品質全体の総体的な向上 組織的なポリシー定義と統制 level 4: 管理された状態 新しいプロジェクトやタスクから得られる結果が予測され、リスクの管理が始まる データマネジメントに成果に対する評価尺度が含まれる データマネジメント用の標準ツールが集中管理計画とガバナンス機能に組み合わされている level 5: 最適化された状態 活動の成果は十分予測可能に 組織は継続的な改善に重点を置く 十分理解された評価尺度を使ってデータ品質とプロセスが管理・測定される 次のように各知識領域ごとに可視化することができる。","title":"読書メモ: DMBOK2 第15章 データマネジメント成熟度アセスメント"},{"content":"このポストについて 前回の記事 dlt 入門 - ELT の Extract と Load を担う data load tool では dlt の概要を説明した。\nこの記事ではそれを踏まえ、dlt を使って CSV ファイルを BigQuery に load するという一連の開発作業をやってみる。\n現実の CSV はそのまま使えなかったりするので、BigQuery に入れるまでに泥臭い前処理のような作業があることが多い。\nそのへんもまとめて dlt でやってみるとこんな感じ、というのが示せるとよい。\nやりたいこと 個人で管理しているお金の情報を個人用の BigQuery に置きたい、という要件を想定。\nデータ概要 具体的には MoneyForward のデータを load していく。\n個人では API を利用できないので、web UI から export できる CSV のデータで収入・支出詳細と資産推移月次を対象とする。\nCSV の export 方法は以下を参照。\n入出金履歴はダウンロードできますか – マネーフォワード MEサポートサイト データの内容は次のようになっている。\n収入・支出詳細_2023-11-01_2023-11-30.csv \u0026#34;計算対象\u0026#34;,\u0026#34;日付\u0026#34;,\u0026#34;内容\u0026#34;,\u0026#34;金額（円）\u0026#34;,\u0026#34;保有金融機関\u0026#34;,\u0026#34;大項目\u0026#34;,\u0026#34;中項目\u0026#34;,\u0026#34;メモ\u0026#34;,\u0026#34;振替\u0026#34;,\u0026#34;ID\u0026#34; \u0026#34;1\u0026#34;,\u0026#34;2023/11/30\u0026#34;,\u0026#34;AMAZON.CO.JP\u0026#34;,\u0026#34;-2830\u0026#34;,\u0026#34;楽天カード\u0026#34;,\u0026#34;食費\u0026#34;,\u0026#34;食料品\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;EPv92ZjQcOxgWQx_cLbhD1\u0026#34; \u0026#34;1\u0026#34;,\u0026#34;2023/11/24\u0026#34;,\u0026#34;東京ガス\u0026#34;,\u0026#34;-4321\u0026#34;,\u0026#34;楽天カード\u0026#34;,\u0026#34;水道・光熱費\u0026#34;,\u0026#34;ガス・灯油代\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;r6wuQPfrIRS6aFpNYZE5Eh\u0026#34; \u0026#34;1\u0026#34;,\u0026#34;2023/11/24\u0026#34;,\u0026#34;給与 カ) フロッグログ\u0026#34;,\u0026#34;700000\u0026#34;,\u0026#34;みずほ銀行\u0026#34;,\u0026#34;収入\u0026#34;,\u0026#34;給与\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;doettKpYyNp0Tml9KQQXm1\u0026#34; ヘッダーがあり、各列に名前が付いている。\nencoding が CP932 であることに注意。\nID の列があるので、行の識別に使えそう。\n資産推移月次.csv \u0026#34;日付\u0026#34;,\u0026#34;合計（円）\u0026#34;,\u0026#34;預金・現金・暗号資産（円）\u0026#34;,\u0026#34;株式(現物)（円）\u0026#34;,\u0026#34;投資信託（円）\u0026#34;,\u0026#34;債券（円）\u0026#34;,\u0026#34;年金（円）\u0026#34;,\u0026#34;ポイント（円）\u0026#34; \u0026#34;2023/12/17\u0026#34;,\u0026#34;11505000\u0026#34;,\u0026#34;5000000\u0026#34;,\u0026#34;300000\u0026#34;,\u0026#34;5000000\u0026#34;,\u0026#34;200000\u0026#34;,\u0026#34;1000000\u0026#34;,\u0026#34;5000\u0026#34; \u0026#34;2023/12/16\u0026#34;,\u0026#34;11505000\u0026#34;,\u0026#34;5000000\u0026#34;,\u0026#34;300000\u0026#34;,\u0026#34;5000000\u0026#34;,\u0026#34;200000\u0026#34;,\u0026#34;1000000\u0026#34;,\u0026#34;5000\u0026#34; \u0026#34;2023/12/15\u0026#34;,\u0026#34;11505000\u0026#34;,\u0026#34;5000000\u0026#34;,\u0026#34;300000\u0026#34;,\u0026#34;5000000\u0026#34;,\u0026#34;200000\u0026#34;,\u0026#34;1000000\u0026#34;,\u0026#34;5000\u0026#34; encoding などについては同じ。\n当月についてはすべての 日付 がある一方、それ以前については 2023/11/30, 2023/10/31, \u0026hellip; のように月末だけが含まれている。\n日付 で行の識別ができる。\nこれらはまず最初に GCS bucket に手動で置くものとする。\n開発手順 CSV そのままだと schema まわりで問題が起こりそうなので、そのあたり try \u0026amp; error で解決したい。\nというのをいきなり BigQuery 上でやると手間もかかるし汚くなるので、最初は試験的にローカル環境の DuckDB に load するようにする。\nDuckDB でうまくいったら BigQuery へと移行する。\n全体の流れとしては次のようになる。\ndlt のインストール DuckDB 用の pipeline project を作成 pipeline の実装 configuration pipeline の実行 Streamlit app による結果の確認 schema の調整 不要レコードのフィルタリング incremental loading への対応 BigQuery への移行 BigQuery への load の確認 開発作業 1. dlt のインストール destination としては DuckDB, BigQuery を使うので、以下のようにして一緒にインストールする。\n(Poetry の例)\npoetry add \u0026#39;dlt[duckdb,bigquery]\u0026#39; インストールされたバージョンを確認。\n$ dlt --version dlt 0.3.25 また、gsfs, pandas, streamlit, google-cloud-bigquery-storage も必要になるのでインストールしておく。\n2. pipeline project を作成 次のコマンドで pipeline project を用意する。\n$ dlt init filesystem duckdb これは verified source として Filesystem、destination として DuckDB を指定して pipeline project を作るという意味。\nFilesystem はローカルのファイルシステムや S3, GCS のようなクラウドストレージからファイルを読むことが可能。\nこのコマンドが成功すると次のようなディレクトリ構造が作られる。\n. ├── .dlt │ ├── .sources │ ├── config.toml │ └── secrets.toml ├── .gitignore ├── filesystem │ ├── README.md │ ├── __init__.py │ ├── helpers.py │ ├── readers.py │ └── settings.py └── filesystem_pipeline.py filesystem/ 以下には Filesystem を使うための関数定義や README などが生成されている。\nfilesystem_pipeline.py には pipeline の実装例がある。\nこれを元に修正していってもいいが、今回は新しく money_forward_pipeline.py を用意して実装していく。\n3. pipeline の実装 money_forward_pipeline.py は次のように実装した。\nimport dlt from filesystem import filesystem, read_csv, readers class MFFileSpec: def __init__(self, file_name: str, table_name: str): self.file_name = file_name self.table_name = table_name def run_pipeline() -\u0026gt; None: pipeline = dlt.pipeline( pipeline_name=\u0026#34;money_forward\u0026#34;, destination=\u0026#34;duckdb\u0026#34;, dataset_name=\u0026#34;raw_money_forward\u0026#34;, full_refresh=True, ) load_info = pipeline.run(money_forward()) print(load_info) @dlt.source def money_forward() -\u0026gt; dlt.extract.source.DltSource: file_specs = [ MFFileSpec(\u0026#34;収入・支出詳細_*.csv\u0026#34;, \u0026#34;income_expence_details\u0026#34;), MFFileSpec(\u0026#34;資産推移月次.csv\u0026#34;, \u0026#34;monthly_assets\u0026#34;), ] for file_spec in file_specs: records = filesystem( file_glob=f\u0026#34;money_forward/{file_spec.file_name}\u0026#34;, ) | read_csv(encoding=\u0026#34;cp932\u0026#34;) yield records.with_name(file_spec.table_name) if __name__ == \u0026#34;__main__\u0026#34;: run_pipeline() pipeline を作り、run() するときに source として money_forward() を与えている。\n試験的に何度か table を作り直すので full_refresh=True を指定している。\nちなみに source は resource をグルーピングしたものであり、resource ごとに table が作られると思っていい。\nmoney_forward() の中では収入・支出詳細と資産推移月次のそれぞれに対して resource が作られる。\nfilesystem() でファイルをリストアップし、それを read_csv() に渡してデータを読んでいる。\nここで GCS の bucket や認証情報が指定されていないことに気づくかもしれない。\n関数 filesystem() の定義は次のようになっている。\n@dlt.resource( primary_key=\u0026#34;file_url\u0026#34;, spec=FilesystemConfigurationResource, standalone=True ) def filesystem( bucket_url: str = dlt.secrets.value, credentials: Union[FileSystemCredentials, AbstractFileSystem] = dlt.secrets.value, file_glob: Optional[str] = \u0026#34;*\u0026#34;, files_per_page: int = DEFAULT_CHUNK_SIZE, extract_content: bool = False, ) -\u0026gt; Iterator[List[FileItem]]: ... bucket_url, credentials のデフォルト値は dlt.sercrets.value になっている。\nこれは設定ファイルや環境変数などから取得された設定値を使うことを意味する。\n4. configuration 認証情報などの設定は dlt の設定ファイルや環境変数で与えることができる。\nここでは設定ファイルを使うことにする。\nbucket や認証情報を次のように記載した。\n.dlt/config.toml [sources.money_forward_pipeline] bucket_url = \u0026#34;gs://\u0026lt;BUCKET_NAME\u0026gt;\u0026#34; .dlt/secrets.toml [sources.credentials] client_email = \u0026#34;\u0026lt;CLIENT_EMAIL\u0026gt;\u0026#34; private_key = \u0026#34;\u0026lt;PRIVATE_KEY\u0026gt;\u0026#34; project_id = \u0026#34;\u0026lt;PROJECT_ID\u0026gt;\u0026#34; .dlt/secrets.toml の情報から GcpServiceAccountCredentials の object が自動で生成され、それが filesystem() に渡されるようになる。\n認証情報の各値を得るには GCP で service account を作り、key を用意してやる必要がある。\nservice account には必要な権限を付与しておく。\n当然だが .dlt/secrets.toml は GitHub 等に push してはいけない。\nちなみに設定周りの便利な機能として、足りない設定があったときに設定できる場所の一覧を教えてくれるというものがある。\n例えば bucket_url を書き忘れて pipeline を実行した場合、次のようなエラーメッセージが出力される。\nFollowing fields are missing: [\u0026#39;bucket_url\u0026#39;] in configuration with spec FilesystemConfigurationResource for field \u0026#34;bucket_url\u0026#34; config providers and keys were tried in following order: In Environment Variables key MONEY_FORWARD__SOURCES__MONEY_FORWARD_PIPELINE__FILESYSTEM__BUCKET_URL was not found. In Environment Variables key MONEY_FORWARD__SOURCES__MONEY_FORWARD_PIPELINE__BUCKET_URL was not found. In Environment Variables key MONEY_FORWARD__SOURCES__BUCKET_URL was not found. In Environment Variables key MONEY_FORWARD__BUCKET_URL was not found. In secrets.toml key money_forward.sources.money_forward_pipeline.filesystem.bucket_url was not found. In secrets.toml key money_forward.sources.money_forward_pipeline.bucket_url was not found. In secrets.toml key money_forward.sources.bucket_url was not found. In secrets.toml key money_forward.bucket_url was not found. In config.toml key money_forward.sources.money_forward_pipeline.filesystem.bucket_url was not found. In config.toml key money_forward.sources.money_forward_pipeline.bucket_url was not found. In config.toml key money_forward.sources.bucket_url was not found. In config.toml key money_forward.bucket_url was not found. In Environment Variables key SOURCES__MONEY_FORWARD_PIPELINE__FILESYSTEM__BUCKET_URL was not found. In Environment Variables key SOURCES__MONEY_FORWARD_PIPELINE__BUCKET_URL was not found. In Environment Variables key SOURCES__BUCKET_URL was not found. In Environment Variables key BUCKET_URL was not found. In secrets.toml key sources.money_forward_pipeline.filesystem.bucket_url was not found. In secrets.toml key sources.money_forward_pipeline.bucket_url was not found. In secrets.toml key sources.bucket_url was not found. In secrets.toml key bucket_url was not found. In config.toml key sources.money_forward_pipeline.filesystem.bucket_url was not found. In config.toml key sources.money_forward_pipeline.bucket_url was not found. In config.toml key sources.bucket_url was not found. In config.toml key bucket_url was not found. bucket_url が設定される可能性のある環境変数や設定ファイル中のパスが優先度順で一覧となって表示されている。\nこれにより何か設定が間違っているときにどこに書き足せばいいかがわかるようになっている。\nちなみにパスの違いはスコープの違いを表していて、\nIn secrets.toml key sources.money_forward_pipeline.filesystem.bucket_url was not found. In secrets.toml key sources.money_forward_pipeline.bucket_url was not found. の2つを比べると前者は pipeline money_forward_pipeline の filesystem という resource のための bucket_url だが、後者はそれより広く filesystem 以外でも参照できるものとなっている。\n設定まわりの挙動について、詳しくは Configuration | dlt Docs を参照のこと。\n5. pipeline の実行 ここまでで pipeline を実行する準備が整ったので実行してみる。\n$ python money_forward_pipeline.py 成功していれば次のようなログが出力される。\nPipeline money_forward completed in 2.96 seconds 1 load package(s) were loaded to destination duckdb and into dataset raw_money_forward_20231218112802 The duckdb destination used duckdb:////Users/sonoyou/Dev/python/dlt_sample/money_forward.duckdb location to store data Load package 1702898884.890725 is LOADED and contains no failed jobs 6. Streamlit app による結果の確認 意図したとおりにデータが load されたのか結果を確認する方法はいくつかあるが、ここでは Streamlit を使ってみる。\n$ dlt pipeline money_forward show このコマンドを実行すると web ブラウザが立ち上がり、Streamlit app が表示される。\n左のサイドバーで \u0026ldquo;Explore data\u0026rdquo; を選択すると DuckDB と接続し、pipeline により作られた table の schema やデータを見ることができる。\nさらにクエリを実行することも可能。便利。\nStreamlit app の画面: Exprole data\n\u0026ldquo;Load info\u0026rdquo; を選択すると load 時の統計情報などを見ることができる。\nStreamlit app の画面: Load info\nさて、前者の画面で table income_expence_detals (収入・支出詳細) の schema を見ると明らかにおかしいことがわかる。\nこちらを修正していく必要がある。\n日本語 column 名の修正 _dlt の prefix がついた column は dlt の管理用なのでいったん無視するとして、それ以外だと x, id のみしかない。\n元の CSV のヘッダーに記載されていた column 名は日本語表記になっており、1つだけ ID という英字表記の column があった。\nどうやら日本語表記の column 名に問題がありそうだ。\ncolumn 名を指定する方法はいくつかあるが、read_csv() の引数 names を使うことにした。\n関数 money_forward() を次のように修正した。\n@dlt.source def money_forward() -\u0026gt; dlt.extract.source.DltSource: file_specs = [ MFFileSpec( file_name=\u0026#34;収入・支出詳細_*.csv\u0026#34;, table_name=\u0026#34;income_expence_details\u0026#34;, columns=[\u0026#34;is_calc_target\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;details\u0026#34;, \u0026#34;amount_yen\u0026#34;, \u0026#34;financial_institution\u0026#34;, \u0026#34;main_group\u0026#34;, \u0026#34;mid_group\u0026#34;, \u0026#34;memo\u0026#34;, \u0026#34;transfer\u0026#34;, \u0026#34;id\u0026#34;], ), MFFileSpec( file_name=\u0026#34;資産推移月次.csv\u0026#34;, table_name=\u0026#34;monthly_assets\u0026#34;, columns=[\u0026#34;date\u0026#34;, \u0026#34;total_yen\u0026#34;, \u0026#34;deposit_cash_crypto_yen\u0026#34;,\u0026#34;stock_yen\u0026#34;,\u0026#34;investment_trust_yen\u0026#34;,\u0026#34;bond_yen\u0026#34;,\u0026#34;pension_yen\u0026#34;,\u0026#34;point_yen\u0026#34;], ), ] for file_spec in file_specs: records = filesystem( file_glob=f\u0026#34;money_forward/{file_spec.file_name}\u0026#34;, ) | read_csv(encoding=\u0026#34;cp932\u0026#34;, names=file_spec.columns, header=0) yield records.with_name(file_spec.table_name) class MFFileSpec に field columns を追加し、read_csv() に names として渡すようにした。\ncolumns には CSV のヘッダー情報を英訳したものをハードコードしている。\nこちらの read_csv() では内部的に pandas が使われており、pandas.read_csv() の引数を取ることができる。\nnames で明示的に column 名を指定した形になる。\n再度 pipeline を実行し、できあがった table income_expence_details の schema を確認してみる。\nname data_type nullable 0 is_calc_target bigint true 1 date text true 2 details text true 3 amount_yen bigint true 4 financial_institution text true 5 main_group text true 6 mid_group text true 7 transfer bigint true 8 id text true 9 _dlt_load_id text false 10 _dlt_id text false 11 memo text true (なぜか memo が末尾になっているのが気になるが)\n大丈夫そうに見えるけど、もう少し手直しする。\n7. schema の調整 上記の column date に注目したい。\nこれは日付を表す column だが、型が text になっている。\ndate 型として扱えるようにしたい。\nここでは add_map() により resource に対して変換をかませる対応を行った。\nmoney_forward_pipeline.py に次の変換用関数を追加する。\ndef convert_date(d: dict) -\u0026gt; dict: if \u0026#34;date\u0026#34; in d: d[\u0026#34;date\u0026#34;] = datetime.datetime.strptime(d[\u0026#34;date\u0026#34;], \u0026#34;%Y/%m/%d\u0026#34;).date() return d 元の値としては \u0026quot;2023/11/30\u0026quot; のような文字列が入っているため、これを Python の datetime.date に変換する。\n関数 money_forward() の中で resource を yield しているところに ad_map() でこれを適用。\nfor file_spec in file_specs: records = filesystem( file_glob=f\u0026#34;money_forward/{file_spec.file_name}\u0026#34;, ) | read_csv(encoding=\u0026#34;cp932\u0026#34;, names=file_spec.column_names, header=0) yield records.add_map(convert_date).with_name(file_spec.table_name) 再度 pipeline を実行すると column date の型が date になることが確認できた。\nただし、本来はこのような schema 調整は import_schema_path, export_schema_path による YAML の編集で対応する方が好ましい。\n今回はそれがうまくいかなかったので add_map() による変換を使うやり方にした。\n詳しくは Adjust a schema | dlt Docs を参照。\nちなみに同じ要領で add_map() により個人情報をマスキングしたりもできる。\n8. 不要レコードのフィルタリング ここで資産推移月次の CSV ファイルについて思い出してみる。\n当月についてはすべての 日付 がある一方、それ以前については 2023/11/30, 2023/10/31, \u0026hellip; のように月末だけが含まれている。\n月次である以上、月ごとのデータのみが含まれていることが好ましい。\nつまり当月の 2023/12/17, 2023/12/17 のような途中のレコードは不要であり、2023/11/30, 2023/10/31 のような月末のレコードのみを残したい。\nそれには add_filter() を使う。\n次のようなフィルタリング用の関数を用意する。\ndate の値の日付が月末だった場合のみ True を返す。\ndef is_last_day_of_month(d: dict) -\u0026gt; bool: if \u0026#34;date\u0026#34; in d: date = d[\u0026#34;date\u0026#34;] return date.day == calendar.monthrange(date.year, date.month)[1] return True やはりこれを add_filter() により resource に加えるわけだが、今回は table monthly_assets (資産推移月次) にのみ適用したい。\n次のようにした。\nfor file_spec in file_specs: records = filesystem( file_glob=f\u0026#34;money_forward/{file_spec.file_name}\u0026#34;, ) | read_csv(encoding=\u0026#34;cp932\u0026#34;, names=file_spec.column_names, header=0) records = records.add_map(convert_date) if file_spec.table_name == \u0026#34;monthly_assets\u0026#34;: records = records.add_filter(is_last_day_of_month) yield records.with_name(file_spec.table_name) これで pipeline を再実行したところ、table monthly_assets に月末以外のレコードが含まれないようになった。\n9. incremental loading への対応 DWH への load を運用するにあたり、load を incremental に行えるかについても考えないといけない。\n例えば 収入・支出詳細_2023-11-01_2023-11-30.csv にレコードの追加や変更があった場合、同じファイルを再度 load したい。\n資産推移月次.csv については export 時に毎回全期間書き出しなのでこちらも同じファイルを load することになる。\nこのとき、単に新しくレコードが追加されるだけだと、例えば同じ買い物が2回計上されてしまうなどの問題が生じる。\ndlt では incremental loading をサポートしており、primary_key (または merge_key) により load 後のレコードの重複を避けることができる。\n前述のとおり収入・支出詳細は ID、資産推移月次は 日付 でレコードを一意にできる。\n@dlt.source def money_forward() -\u0026gt; dlt.extract.source.DltSource: file_specs = [ MFFileSpec( file_name=\u0026#34;収入・支出詳細_*.csv\u0026#34;, table_name=\u0026#34;income_expence_details\u0026#34;, columns=[\u0026#34;is_calc_target\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;details\u0026#34;, \u0026#34;amount_yen\u0026#34;, \u0026#34;financial_institution\u0026#34;, \u0026#34;main_group\u0026#34;, \u0026#34;mid_group\u0026#34;, \u0026#34;memo\u0026#34;, \u0026#34;transfer\u0026#34;, \u0026#34;id\u0026#34;], primary_keys=[\u0026#34;id\u0026#34;], ), MFFileSpec( file_name=\u0026#34;資産推移月次.csv\u0026#34;, table_name=\u0026#34;monthly_assets\u0026#34;, columns=[\u0026#34;date\u0026#34;, \u0026#34;total_yen\u0026#34;, \u0026#34;deposit_cash_crypto_yen\u0026#34;,\u0026#34;stock_yen\u0026#34;,\u0026#34;investment_trust_yen\u0026#34;,\u0026#34;bond_yen\u0026#34;,\u0026#34;pension_yen\u0026#34;,\u0026#34;point_yen\u0026#34;], primary_keys=[\u0026#34;date\u0026#34;], ), ] for file_spec in file_specs: records = filesystem( file_glob=f\u0026#34;money_forward/{file_spec.file_name}\u0026#34;, ) | read_csv(encoding=\u0026#34;cp932\u0026#34;, names=file_spec.columns, header=0) records = records.add_map(convert_date) if file_spec.table_name == \u0026#34;monthly_assets\u0026#34;: records = records.add_filter(is_last_day_of_month) @dlt.transformer(name=file_spec.table_name, primary_key=file_spec.primary_keys, write_disposition=\u0026#34;merge\u0026#34;) def dummy(items): return items yield records | dummy class MFFileSpec に primary_keys を追加した。\n生成される resource に primary_keys を指定するために @dlt.transformer として関数 dummy() を定義した。\ndummy() は transform の処理自体は何もしないが、primary_key および write_disposition を指定している。\nwrite_disposition は incremental loading の挙動を決めるパラメータであり、\u0026quot;replace\u0026quot;, \u0026quot;append\u0026quot;, \u0026quot;merge\u0026quot; の3つが指定できる。\nここでは primary_keys を使って追加・変更されたレコードを一意にしたかったので \u0026quot;merge\u0026quot; を指定した。\n詳しくは Pipeline Tutorial | dlt Docs を参照。\nDuckDB, BigQuery ともにすべての write_disposition をサポートしているが、destination によってはサポートされないものもあるので注意。\n(dummy() なんか使わずにもっときれいにできる方法があるかもしれないが…)\n10. BigQuery への移行 ローカルの DuckDB でやりたいことができるようになってきたのでいよいよ BigQuery へと移行する。\n2点変更すればよい。\nまずは pipeline の destination 指定を \u0026quot;duckdb\u0026quot; から \u0026quot;bigquery\u0026quot; に変更する。\npipeline = dlt.pipeline( pipeline_name=\u0026#34;money_forward\u0026#34;, destination=\u0026#34;bigquery\u0026#34;, dataset_name=\u0026#34;raw_money_forward\u0026#34;, full_refresh=True, ) 加えて .dlt/secrets.toml に destination 用の設定を追加する。\n[destination.bigquery] location = \u0026#34;asia-northeast1\u0026#34; [destination.bigquery.credentials] client_email = \u0026#34;\u0026lt;CLIENT_EMAIL\u0026gt;\u0026#34; private_key = \u0026#34;\u0026lt;PRIVATE_KEY\u0026gt;\u0026#34; project_id = \u0026#34;\u0026lt;PROJECT_ID\u0026gt;\u0026#34; 以上で destination 変更が完了。\npipeline を再実行すると成功した。\n11. BigQuery への load の確認 BigQuery にデータが load されているかを確認する。\nもちろん前述の Streamlit app から見る方法でも確認できるが、Cloud Shell 上から bq コマンドで確認する。\nまずは dataset を確認。\n$ bq ls datasetId ------------------------------------------ raw_money_forward_20231219112438 raw_money_forward_20231219112438_staging dlt.pipeline() で指定した dataset 名に timestamp っぽい文字列がついた名前で dataset が作成されている。\nこの timestamp は邪魔だと思ったが、除外する方法が分からなかった。\nload ごとに dataset を分けるべきという思想なのだろうか…？\n[追記]\ndlt.pipeline() の引数で full_refresh=True になっていたのが dataset 名に timestamp がついてしまう原因だった。\nfull_refresh=True は名前のとおり完全に作り直す挙動になっており、try \u0026amp; error で何度も作り直すためにつけていた。\nTrue のときに dataset 名が一意になるよう、timestamp をつけるという挙動は理解できる。\nこれを False にすると timestamp のない raw_money_forward という名前で dataset が作成されることを確認した。\n末尾に _staging がついているものは incremental loading のための一時的なデータ置き場のようなもの。\ndadtaset raw_money_forward_20231219112438 の table 一覧を見てみる。\n$ bq query --nouse_legacy_sql \u0026#39;select table_name from raw_money_forward_20231219112438.INFORMATION_SCHEMA.TABLES\u0026#39; +------------------------+ | table_name | +------------------------+ | _dlt_loads | | _dlt_version | | monthly_assets | | _dlt_pipeline_state | | income_expence_details | +------------------------+ monthly_assets, income_expence_details の2つの table が作成されていることを確認した。\nデータもちゃんと入っている模様。\n_dlt の prefix を持つ table には dlt 関連の管理情報が含まれている。\n意図して見ることは少ないと思うが、何か問題が起こったときには参照することになるだろう。\nというわけで BiqQuery への load まで成功した。\nいろいろ編集したが、最終的な money_forwared_pipeline.py のコードを貼っておく。\nimport calendar import datetime import typing import dlt from filesystem import filesystem, read_csv, readers class MFFileSpec: def __init__(self, file_name: str, table_name: str, columns: typing.List[str], primary_keys: typing.List[str]): self.file_name = file_name self.table_name = table_name self.columns = columns self.primary_keys = primary_keys def run_pipeline() -\u0026gt; None: pipeline = dlt.pipeline( pipeline_name=\u0026#34;money_forward\u0026#34;, destination=\u0026#34;bigquery\u0026#34;, dataset_name=\u0026#34;raw_money_forward\u0026#34;, full_refresh=False, ) load_info = pipeline.run(money_forward()) print(load_info) @dlt.source def money_forward() -\u0026gt; dlt.extract.source.DltSource: file_specs = [ MFFileSpec( file_name=\u0026#34;収入・支出詳細_*.csv\u0026#34;, table_name=\u0026#34;income_expence_details\u0026#34;, columns=[\u0026#34;is_calc_target\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;details\u0026#34;, \u0026#34;amount_yen\u0026#34;, \u0026#34;financial_institution\u0026#34;, \u0026#34;main_group\u0026#34;, \u0026#34;mid_group\u0026#34;, \u0026#34;memo\u0026#34;, \u0026#34;transfer\u0026#34;, \u0026#34;id\u0026#34;], primary_keys=[\u0026#34;id\u0026#34;], ), MFFileSpec( file_name=\u0026#34;資産推移月次.csv\u0026#34;, table_name=\u0026#34;monthly_assets\u0026#34;, columns=[\u0026#34;date\u0026#34;, \u0026#34;total_yen\u0026#34;, \u0026#34;deposit_cash_crypto_yen\u0026#34;,\u0026#34;stock_yen\u0026#34;,\u0026#34;investment_trust_yen\u0026#34;,\u0026#34;bond_yen\u0026#34;,\u0026#34;pension_yen\u0026#34;,\u0026#34;point_yen\u0026#34;], primary_keys=[\u0026#34;date\u0026#34;], ), ] for file_spec in file_specs: records = filesystem( file_glob=f\u0026#34;money_forward/{file_spec.file_name}\u0026#34;, ) | read_csv(encoding=\u0026#34;cp932\u0026#34;, names=file_spec.columns, header=0) records = records.add_map(convert_date) if file_spec.table_name == \u0026#34;monthly_assets\u0026#34;: records = records.add_filter(is_last_day_of_month) @dlt.transformer(name=file_spec.table_name, primary_key=file_spec.primary_keys, write_disposition=\u0026#34;merge\u0026#34;) def dummy(items): return items yield records | dummy def convert_date(d: dict) -\u0026gt; dict: if \u0026#34;date\u0026#34; in d: d[\u0026#34;date\u0026#34;] = datetime.datetime.strptime(d[\u0026#34;date\u0026#34;], \u0026#34;%Y/%m/%d\u0026#34;).date() return d def is_last_day_of_month(d: dict) -\u0026gt; bool: if \u0026#34;date\u0026#34; in d: date = d[\u0026#34;date\u0026#34;] return date.day == calendar.monthrange(date.year, date.month)[1] return True if __name__ == \u0026#34;__main__\u0026#34;: run_pipeline() まとめ ドキュメントを見ながら一通りのことができるものを実装することができた。\nコードに関しては慣れればもう少しきれいに書くことができそうな気がする。\n最後に destination を DuckDB から BiqQuery に変更する作業はとても簡単で体験が良かった。\n現在業務で DWH の移行を考えているが、こういう機能があると移行がとても楽だし、DuckDB のようなローカルで気楽に検証できる環境に切り替えられるのもすごくいい。\nDDD みを感じる。\nもちろん GCS 上の CSV ファイルを BiqQuery に読み込む方法は公式で提供されているので、dlt は必須ではない。\nしかし上記のように容易に destination が変えられたり verified source が提供されていたりというところや、前処理のようなごちゃごちゃしたこと pipeline の定義と一緒に Python でどうとでも書けるところにメリットがある。\nまた今回は触れていないが schema evolution についての配慮もある。\n仕事で使ってみてもいいと思った。\nただし dlt が動くマシンにデータを載せることになるので、基本的にはあまり大きなデータの移動には向かない。\nライトなユースケースがマッチするだろう。\n","permalink":"https://soonraah.github.io/posts/load-csv-data-into-bq-by-dlt/","summary":"このポストについて 前回の記事 dlt 入門 - ELT の Extract と Load を担う data load tool では dlt の概要を説明した。\nこの記事ではそれを踏まえ、dlt を使って CSV ファイルを BigQuery に load するという一連の開発作業をやってみる。\n現実の CSV はそのまま使えなかったりするので、BigQuery に入れるまでに泥臭い前処理のような作業があることが多い。\nそのへんもまとめて dlt でやってみるとこんな感じ、というのが示せるとよい。\nやりたいこと 個人で管理しているお金の情報を個人用の BigQuery に置きたい、という要件を想定。\nデータ概要 具体的には MoneyForward のデータを load していく。\n個人では API を利用できないので、web UI から export できる CSV のデータで収入・支出詳細と資産推移月次を対象とする。\nCSV の export 方法は以下を参照。\n入出金履歴はダウンロードできますか – マネーフォワード MEサポートサイト データの内容は次のようになっている。\n収入・支出詳細_2023-11-01_2023-11-30.csv \u0026#34;計算対象\u0026#34;,\u0026#34;日付\u0026#34;,\u0026#34;内容\u0026#34;,\u0026#34;金額（円）\u0026#34;,\u0026#34;保有金融機関\u0026#34;,\u0026#34;大項目\u0026#34;,\u0026#34;中項目\u0026#34;,\u0026#34;メモ\u0026#34;,\u0026#34;振替\u0026#34;,\u0026#34;ID\u0026#34; \u0026#34;1\u0026#34;,\u0026#34;2023/11/30\u0026#34;,\u0026#34;AMAZON.CO.JP\u0026#34;,\u0026#34;-2830\u0026#34;,\u0026#34;楽天カード\u0026#34;,\u0026#34;食費\u0026#34;,\u0026#34;食料品\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;EPv92ZjQcOxgWQx_cLbhD1\u0026#34; \u0026#34;1\u0026#34;,\u0026#34;2023/11/24\u0026#34;,\u0026#34;東京ガス\u0026#34;,\u0026#34;-4321\u0026#34;,\u0026#34;楽天カード\u0026#34;,\u0026#34;水道・光熱費\u0026#34;,\u0026#34;ガス・灯油代\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;r6wuQPfrIRS6aFpNYZE5Eh\u0026#34; \u0026#34;1\u0026#34;,\u0026#34;2023/11/24\u0026#34;,\u0026#34;給与 カ) フロッグログ\u0026#34;,\u0026#34;700000\u0026#34;,\u0026#34;みずほ銀行\u0026#34;,\u0026#34;収入\u0026#34;,\u0026#34;給与\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;doettKpYyNp0Tml9KQQXm1\u0026#34; ヘッダーがあり、各列に名前が付いている。\nencoding が CP932 であることに注意。","title":"現実の CSV ファイルのデータを BigQuery に load する仕組みを作るという泥臭い作業を dlt でやってみる"},{"content":"このポストについて このポストは datatech-jp Advent Calendar 2023 の18日目の投稿です。\nweb の記事で見かけた dlt というツールが気になったので調べてみた。\ndlt の概要について書いていく。\nWhat is dlt? https://dlthub.com/\ndlt とは \u0026ldquo;data load tool\u0026rdquo; の略。\n雑に言うとデータパイプラインにおける ELT の Extract と Load を行う ものとなっている。\n主にベルリンとニューヨークに拠点を持つ dltHub 社によって開発されており、OSS の Python ライブラリとして提供されている。\n次のような特徴を持つ。\nプラットフォームではなくあくまでライブラリであることが強調されている つまり Airflow, GitHub Actions, Google Cloud Functions, ローカル環境などどこでも動かすことができる スケールアウト可能な分散処理ではない extract と load にまつわる反復的で平凡な作業をなくすことを目指している schema 推論や schema evolution をサポート 宣言的なコードでメンテナンスを楽にする incremental loading をサポート 豊富な source GA, Salesforce, Kinesis などいろいろ例が挙げられている 要は API からの取得も含めて JSON-like な形式で Python で読めるものなら何でも 豊富な destination BigQuery, Snowflake など主要なクラウド DWH DuckDB はローカルでの動作確認に便利 Airflow, dbt などとの連携がある CLI の提供もある その他、Glossary を見ておくとドキュメントが読みやすくなる。\ndlt の仕組み dlt は内部的には extract, normalize, load の3つのステージを実行する。\nextract API その他から得られた JSON データを parse する normalize 正規化エンジンにより JSON 入れ子構造をリレーショナルテーブルに展開 schema 推論なども含む load destination へと冪等で atomic なロードを行う 荒れがちな取り込みのコードをこの型にはめられるというのがいいところかなと思う。\narchitecutre diagram from How dlt works | dlt Docs\nコード例 以下は Getting Started | dlt Docs に記載のもっともシンプルなコード例。\nimport dlt data = [ {\u0026#39;id\u0026#39;: 1, \u0026#39;name\u0026#39;: \u0026#39;Alice\u0026#39;}, {\u0026#39;id\u0026#39;: 2, \u0026#39;name\u0026#39;: \u0026#39;Bob\u0026#39;} ] pipeline = dlt.pipeline( pipeline_name=\u0026#39;quick_start\u0026#39;, destination=\u0026#39;duckdb\u0026#39;, dataset_name=\u0026#39;mydata\u0026#39; ) load_info = pipeline.run(data, table_name=\u0026#34;users\u0026#34;) print(load_info) これを quick_start_pipeline.py に保存して、\npython quick_start_pipeline.py のように普通に Python スクリプトとして実行すると DuckDB の mydata.users にデータが load される。\nまとめ ざっとだが dlt がどういったものかを解説した。\nFivetran や Airbyte を用意するほどでもない、データ量もそれほど多くといったライトなケースに dlt の選択肢が出てくるのだろうか。\n次のポストでは実際に dlt でパイプラインを構築する開発手順を追ってみたい。\n最後に dlthub に記載の Suraj Rajan 氏 (Snowflake の偉い人) の言葉を引用しておく。\nDLT + DBT can be a killer combination for customers.\nよさそう。\ndlt という命名も dbt を意識してる感じがする。\n","permalink":"https://soonraah.github.io/posts/what-is-dlt/","summary":"このポストについて このポストは datatech-jp Advent Calendar 2023 の18日目の投稿です。\nweb の記事で見かけた dlt というツールが気になったので調べてみた。\ndlt の概要について書いていく。\nWhat is dlt? https://dlthub.com/\ndlt とは \u0026ldquo;data load tool\u0026rdquo; の略。\n雑に言うとデータパイプラインにおける ELT の Extract と Load を行う ものとなっている。\n主にベルリンとニューヨークに拠点を持つ dltHub 社によって開発されており、OSS の Python ライブラリとして提供されている。\n次のような特徴を持つ。\nプラットフォームではなくあくまでライブラリであることが強調されている つまり Airflow, GitHub Actions, Google Cloud Functions, ローカル環境などどこでも動かすことができる スケールアウト可能な分散処理ではない extract と load にまつわる反復的で平凡な作業をなくすことを目指している schema 推論や schema evolution をサポート 宣言的なコードでメンテナンスを楽にする incremental loading をサポート 豊富な source GA, Salesforce, Kinesis などいろいろ例が挙げられている 要は API からの取得も含めて JSON-like な形式で Python で読めるものなら何でも 豊富な destination BigQuery, Snowflake など主要なクラウド DWH DuckDB はローカルでの動作確認に便利 Airflow, dbt などとの連携がある CLI の提供もある その他、Glossary を見ておくとドキュメントが読みやすくなる。","title":"dlt 入門 - ELT の Extract と Load を担う data load tool"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第12章「メタデータ管理」について。\n仕事でメタデータを扱い始めたので読んでおきたかった。\n以降、特に注釈のない引用は DMBOK2 第12章からの引用とする。\nメタデータとは 一般的な説明としては「データに関するデータ」とよく言われている。\nデータに関するデータはすべてメタデータなので、メタデータはとても幅広い内容となっている。\nDMBOK2 ではメタデータの説明として図書館の例を挙げている。\nそこには数十万の書籍と雑誌があるのに、図書目録がない。図書目録がなければ、読者は特定の本や特定のトピックの検索を開始する方法さえ分からないかもしれない。図書目録は、必要な情報 (図書館が所有する本と資料、保管場所) を提供するだけでなく、利用者が様々な着眼点 (対象分野、著者、タイトル) から資料を見つけることを可能にする。 (中略) メタデータを持たない組織は、図書目録のない図書館のようなものである。\nデータという資産を管理するためにも、データを利用するためにも、リスクマネジメントのためにもメタデータは必要となる。\nメタデータの種類 メタデータはビジネス、テクニカル、オペレーショナルの3つに分類することができる。\nビジネスメタデータ 主にデータの内容と状態に重点を置く。\nIT からは独立している。\ndataset, table, column の定義と説明 業務ルール、変換ルール、計算方法、導出方法 データモデル etc. テクニカルメタデータ 技術的詳細やシステムに関する情報。\n主に IT に関連している。\n物理 database の table, column の名称 column のプロパティ アクセス権 etc. オペレーショナルメタデータ データの処理とアクセスの詳細を示す。\n運用で得られる情報とも言える。\nバッチプログラムのジョブ実行ログ データの抽出とその結果などの履歴 運用スケジュールの以上 etc. 以上、各種のメタデータで例に挙げたのはあくまで一部であり、現実にはもっと多くのメタデータが存在する。\nメタデータを管理する意義 図書館の例からもわかるとおり、メタデータなしではデータを管理することはできない。\n信頼性が高く管理されたメタデータにより、次のようなことができるようになる。\nデータのコンテキストを提供し、それによりデータ品質を測定可能にして信頼性を向上させる 業務効率の向上、および古いデータや誤ったデータの利用防止 データ利用者とエンジニアの間のコミュニケーションの改善 法令遵守の支援 etc. メタデータの管理が不十分だと次のようなことが起こる。\n一貫性のないデータ利用と誤った定義によるリスク メタデータは複製されて保管されることによる冗長性 利用者の信頼性低下 etc. メタデータアーキテクチャ メタデータの内容は幅広いがしたがってその取得元も幅広く、ビジネス用語集、BI ツール、モデリングツール、等々が挙げられる。\nこれらを何らかの方法で集約し、一箇所のメタデータポータルで閲覧できるようにする必要がある。\nつまり「ここに来ればデータについてのことがわかる」という入り口を設けることになる。\nそのためのアーキテクチャの構成が4つ挙げられている。\n集中型メタデータアーキテクチャ 様々なソースから得られたメタデータを保持する1つの中央のメタデータリポジトリを含む。\nDMBOK2 図85 集中型メタデータアーキテクチャ\n分散型メタデータアーキテクチャ 単一のメタデータリポジトリは持たず、メタデータポータルでの検索時に各種ソースのメタデータを直接参照する。\n集中型は可用性や検索性など、分散型は鮮度やメンテナンス性などに重きをおいているという違いがある。\nDMBOK2 図86 分散型メタデータアーキテクチャ\nハイブリッド型メタデータアーキテクチャ 集中型と分散型のハイブリッド。\n手動で作ったメタデータなどは中央のメタデータリポジトリに起き、それ以外は各種ソースを参照する。\nDMBOK2 図87 ハイブリッド型メタデータアーキテクチャ\n双方向メタデータアーキテクチャ メタデータリポジトリまたはソース側のどこでもメタデータの編集を行うことができ、双方向に反映される。\n図はなし。\n…なんだけど、どうも図のキャプションが各アーキテクチャの説明とずれている気がしてならない。\n図85 -\u0026gt; 双方向メタデータアーキテクチャ 図86 -\u0026gt; 分散型メタデータアーキテクチャ 図87 -\u0026gt; 集中型メタデータアーキテクチャ なのでは？\nメタデータ管理の導入 メタデータ管理の活動は次のような流れになる。\nメタデータ戦略の策定 メタデータ要件の把握 メタデータアーキテクチャの定義 メタデータの作成と維持 メタデータのクエリ、レポート、分析 また\n組織へのリスクを最小限に抑え容易に受け入れてもらえるように、管理されたメタデータ環境を段階的に導入することを薦める。\nとある。\nまずはメタデータリポジトリを実装し、その後必要に応じてインタフェースなどを追加していく。\n(とはいえ最近のメタデータツールならリポジトリと UI が一緒になっていたりする)\nメタデータを管理しないことによるリスクのリスクアセスメント、メタデータを維持管理していくための組織と文化の変革も必要。\n所感 データマネジメントに関心がない人にメタデータの意義を説明するのは難しいと感じる。\nこのへん上手く話せるようになりたい。\nそもそもメタデータって？というところからになるが、図書館の例はわかりやすいので使っていこうと思う。\n「信頼性」という言葉がたびたび出てくるがデータを提供するにあたって信頼性はとても重要で、データ資産を利用してもらえるか、つまりデータ資産の価値に大きく影響を与えるとの認識。\n信頼性が大事だということを理解してもらうのも意外と難しい。\n現在業務で OpenMetadata を使ったメタデータ管理の導入を進めているので、これについても気が向いたら何か書きたい。\n私はデータエンジニアなので技術スタックやツールから考えがちだが、実際はメタデータリポジトリに何を入れるかやそれをどうメンテするかが肝だということがわかった。\nメタデータの付与や更新は中央のデータチームだけではまかなえず、データプロバイダーである組織内の各チームの協力は必須。\n付与だけでなくメンテナンスも含めて組織として体制をつくる必要があり、ガバナンスの力が効いてくるのだろうなと。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-12/","summary":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第12章「メタデータ管理」について。\n仕事でメタデータを扱い始めたので読んでおきたかった。\n以降、特に注釈のない引用は DMBOK2 第12章からの引用とする。\nメタデータとは 一般的な説明としては「データに関するデータ」とよく言われている。\nデータに関するデータはすべてメタデータなので、メタデータはとても幅広い内容となっている。\nDMBOK2 ではメタデータの説明として図書館の例を挙げている。\nそこには数十万の書籍と雑誌があるのに、図書目録がない。図書目録がなければ、読者は特定の本や特定のトピックの検索を開始する方法さえ分からないかもしれない。図書目録は、必要な情報 (図書館が所有する本と資料、保管場所) を提供するだけでなく、利用者が様々な着眼点 (対象分野、著者、タイトル) から資料を見つけることを可能にする。 (中略) メタデータを持たない組織は、図書目録のない図書館のようなものである。\nデータという資産を管理するためにも、データを利用するためにも、リスクマネジメントのためにもメタデータは必要となる。\nメタデータの種類 メタデータはビジネス、テクニカル、オペレーショナルの3つに分類することができる。\nビジネスメタデータ 主にデータの内容と状態に重点を置く。\nIT からは独立している。\ndataset, table, column の定義と説明 業務ルール、変換ルール、計算方法、導出方法 データモデル etc. テクニカルメタデータ 技術的詳細やシステムに関する情報。\n主に IT に関連している。\n物理 database の table, column の名称 column のプロパティ アクセス権 etc. オペレーショナルメタデータ データの処理とアクセスの詳細を示す。\n運用で得られる情報とも言える。\nバッチプログラムのジョブ実行ログ データの抽出とその結果などの履歴 運用スケジュールの以上 etc. 以上、各種のメタデータで例に挙げたのはあくまで一部であり、現実にはもっと多くのメタデータが存在する。\nメタデータを管理する意義 図書館の例からもわかるとおり、メタデータなしではデータを管理することはできない。\n信頼性が高く管理されたメタデータにより、次のようなことができるようになる。\nデータのコンテキストを提供し、それによりデータ品質を測定可能にして信頼性を向上させる 業務効率の向上、および古いデータや誤ったデータの利用防止 データ利用者とエンジニアの間のコミュニケーションの改善 法令遵守の支援 etc. メタデータの管理が不十分だと次のようなことが起こる。\n一貫性のないデータ利用と誤った定義によるリスク メタデータは複製されて保管されることによる冗長性 利用者の信頼性低下 etc. メタデータアーキテクチャ メタデータの内容は幅広いがしたがってその取得元も幅広く、ビジネス用語集、BI ツール、モデリングツール、等々が挙げられる。","title":"読書メモ: DMBOK2 第12章 メタデータ管理"},{"content":"このポストについて このポストは Distributed computing Advent Calendar 2023 の3日目の記事になります。\n1日目、2日目に続いて Apache Iceberg について書きますが、このポストでは Iceberg の実用例を書きます。\nAWS DMS による CDC の結果を Apache Iceberg 形式にして Amazon Athena でクエリできるようにするという内容になります。\nやっていることとしては Perform upserts in a data lake using Amazon Athena and Apache Iceberg | AWS Big Data Blog で紹介されている内容と近いですが、実務としての背景や工夫したところなどを書いていきます。\n背景 私の所属する事業会社では日々プロダクトから様々なデータが発生しており、プロダクトの分析やレポーティング、ML など様々な用途で利用されている。\nそれを支える基盤としてデータ基盤が存在している。\nデータ基盤ではクエリエンジンとして Amazon Athena を使っている。\nストレージとしては S3 を使用しており、主に分析用として Parquet 形式でデータが置かれる。\nここに業務用の operational な database から日次でデータを取り込んでいる。\nデータソースは RDS (Aurora MySQL) であり、比較的大きなデータとなっている。\nこれまではこの RDS -\u0026gt; S3 のデータ取り込みには RDS の S3 snapshot export という機能を利用していた。\nこの機能では比較的簡単な設定により、バックアップ用のスナップショットの内容を S3 に export することができる。\nちなみに対象 database のスナップショットのサイズは数十 TB ある。\n課題 RDS の S3 snapshot export には次のような課題があった。\n料金が高い 💸 料金はデータ送信の量で決まるとのこと 具体的な額は伏せるが、思った以上にかかっていた export に時間がかかる 🕓 数時間程度 まれに通常より遅延することがあり、1日以上かかることもあった export の頻度を高くできない 🐌 コストも時間もかかるので1日より短くするのは無理 データソース側で更新されたレコードをデータ基盤上で分析に利用できるまで、最大で1日+数時間のラグがある もっと早く分析できるようにしたいという要望もあった 対象 database の特にデータ量の多い table についてレコードの更新タイミングを確認したところ、日によって波はあるが1日の中で更新されているレコードはおおよそ1~4割だった。\nスナップショットをまるまる export する場合は更新されていないデータも対象となるため、本来は不要であるはずのデータ移動が発生してしまっている。\nソリューション概要 スナップショット全体のコピーには無駄が多い。\nCDC (Change Data Capture) で差分のみ送るようにすればよりデータの移動が少なくなり、上記の問題が解決できると考えた。\nCDC とは RDB のデータを外部にレプリケーションする方法の一つで、レコードの追加・更新・削除などのイベントをログとして送信するというやり方。\nこのようなイベントログがあれば送信先にて table の状態を復元することができる。\nLog-Based Change Data Capture, Change Data Capture (CDC): What it is and How it Works - Striim\nCDC については以下も参照。\nLog-Based Change Data Capture in Change Data Capture (CDC): What it is and How it Works - Striim Track Data Changes - SQL Server | Microsoft Learn CDC を実現する方法はいくつかある。\nmodern data stack 的には Debezium が最も有名だろう。\n今回は元々クラウドサービスとして AWS を利用していたため、比較的導入が容易な AWS DMS (Database Migration Service) を使うことにした。\nsource として RDS、target として S3 を指定して DMS の replication task を動かすことになる。\nちなみに CDC を始める前に現在の table 全体をコピーする full load という処理を実行する必要があるが、このポストでは割愛。\nDMS の CDC で S3 へと送信されたイベントログはそのままでは分析に使えない。\n元の table と同じ形に組み上げる必要がある。\nここでは Apache Iceberg という table 形式を使ってそれを行うことにした。\nIceberg については当アドベントカレンダーの他のポストや本ブログの Iceberg についてのポストも参考にしてほしい。\nIceberg を採用した理由は以下のとおり。\nAthena でサポートされている table 形式である SQL の merge into 文 (後述) により比較的容易にイベントログを元の table の形に復元できる MOR (Merge on Read、後述) により、コストの低い table 更新を実現できる Athena および Glue (後述) で使いたいので Iceberg catalog としては Glue Data Catalog を使うことになる。\n他に Athena で使えて似たことができる table 形式としては Apache Hudi がある。\nしかし以前技術検証したときに Athena のエンジンが Hudi の MOR に対応していなかったため、採用しなかった。\n(今はできるかもしれない、未確認)\nアーキテクチャは以下のようになる。\nsolution architecture\nソリューション詳細 主に Iceberg まわりについて詳しく見ていく。\nmerge into 文による Iceberg table の更新 アーキテクチャ図の 2. にあたる処理。\nDMS の CDC では near real time で S3 にイベントログが出力される。\nただ、今回はデータ基盤については near real time の更新の要件はなかったため、1時間ごとの更新を試みることにした。\nソースである RDS 上の table が 2023-11-11 09:59:59.999999 の時点で次のような状態 (A) だったとする。\nid name phone_number 1 Alice 123-456 2 Bob 456-789 これが1時間後には次のようになっていたとする。(B)\nid name phone_number 1 Alice 123-000 3 Charlie 456-789 このとき、DMS による1時間分の CDC 結果の出力は例えば次のようになっている。(C)\nOp timestamp id name phone_number I 2023-11-11 10:05:00.000000 3 Charlie 333-333 U 2023-11-11 10:10:00.000000 1 Alice 123-000 D 2023-11-11 10:15:00.000000 2 Bob 456-789 column Op は DMS によって付与される column であり、イベントの種別を表す。\nI: 挿入 (Insert) U: 更新 (Update) D: 削除 (Delete) column timestamp も DMS によって付与された column であり、ソース側でその変更が commit された時刻を表す。 (column 名は設定で指定可能)\nソースの table が (A) の状態から (C) に記載されているような変更を経て (B) の状態になった、ということである。\n前回までの変更を Iceberg table が追従できているのであれば (A) の状態になっているはずであり、ここで (C) の差分を適用して (B) の状態にする必要がある。\nこれを自分で実装するとなると結構めんどうだが、幸いに SQL の merge into 文を使って Iceberg table を更新することができる。\nWrites - Apache Iceberg この例だと例えば次のような SQL で挿入・更新・削除を一度に適用できる。\nmerge into my_catalog.my_database.iceberg_table as iceberg using ( select * from cdc_table as cdc -- CDC 結果を1時間分だけ読み込んだ DataFrame ) on -- この column の一致で同一レコードとみなす (複数指定も可) iceberg.id = cdc.id -- 一致する id があり Op=D の場合は削除 when matched and cdc.Op = \u0026#39;D\u0026#39; then delete -- 一致する id があり Op=I or U の場合は更新 when matched and cdc.Op != \u0026#39;D\u0026#39; then update set id = cdc.id, name = cdc.name, phone_number = cdc.phone_number -- 一致する id がなく Op=I or U の場合は挿入 when not matched and cdc.Op != \u0026#39;D\u0026#39; then insert(id, name, phone_number) values(cdc.id, cdc.name, cdc.phone_number) 今回の実装では column 名取得の都合などから Glue Job の Spark SQL でこの SQL を実行するものとした。\nちなみに merge into 文自体は Athena でもサポートしているので、Athena でも実行することができる。\nIceberg table は MOR (Merge on Read) 形式にしており、table 更新時は列指向形式データをすべて書き換えるのではなく差分のみが追加されるようになっている。\nこのため大きな table であっても比較的低コストでの table 更新が可能となる。\n一方で読み取りは COW (Copy on Write) の場合と比べて遅くなるが、基本的には列指向なので分析クエリに対してはそこそこのパフォーマンスとなる。\nMOR, COW については以下を参考。\nRow-Level Changes on the Lakehouse: Copy-On-Write vs. Merge-On-Read in Apache Iceberg | Dremio (Dremio さんの Iceberg 解説記事にはいつもお世話になっています！)\ndaily snapshot の作成 アーキテクチャ図の 4. にあたる処理。\nIceberg table は hourly で更新する一方、Iceberg table から daily でスナップショットとして Parquet 形式に書き出すという処理も追加した。\nIceberg には time travel の機能があり、Iceberg table 自身の中でスナップショットを持つことも可能である。\nしかしわざわざ daily スナップショットを外に書き出しているのは、チームとして Iceberg table の運用経験がなく、長期的に安定運用できるかわからなかったため。\n例えばもし Iceberg table の metadata がぶっ壊れたら…などと考えると、Iceberg の外にデータを別で持っておきたい気持ちがわかっていただけるのではないだろうか。\n元々が daily の更新だったため、最低限 daily のスナップショットが残っていれば少なくとも現在の業務は継続することができる。\n分析者視点だと\n直近1週間 (後述) の鮮度の高いデータの分析 -\u0026gt; hourly 更新の table (Iceberg) 長期間におけるデータの分析 -\u0026gt; daily 更新の table (Parquet) と使い分けてもらうことになる。\n後々 Iceberg table が安定運用できることが分かれば Iceberg 1本にしてもいい。\nIceberg table のメンテナンス 前後してアーキテクチャ図の 3. にあたる処理。\nここでは Iceberg table の compaction と古いスナップショットの削除を行う。\nこれらも daily で実行するものとする。\nMOR の場合、更新が何回も行われ差分の世代が多くなってくるとそれらを merge して table としてのデータを読み取る速度が遅くなってくる。\nこれを解消するためには差分を統合して1つのファイルにまとめる必要があり、これを compaction と言う。\nこの compaction の処理を追加している。\nFine-Tuning Apache Iceberg Tables - Dremio Blog | Dremio Iceberg table は内部にスナップショットという概念があり、table の状態を複数バージョン持っている。\nこれにより time travel query が実行できたり、ACID transction を提供できたりと大きなメリットとなっている。\n前述のとおり daily でデータを Parquet として書き出しているので、Iceberg 内で過去の古いスナップショットを持っているとデータの重複になってしまう。\nよって定期的に Iceberg 内の古いスナップショットを削除する処理を追加している。\nここでは1週間分残すようにし、それより古いスナップショットは削除するようにした。\ncompaction、古い snashot の削除はともに Spark SQL から実行することができる。\nそれぞれ rewrite_data_files(), expire_snapshots() という procedure が対応している。\nMaintenance - Apache Iceberg 運用上の工夫 DMS mirgration task の監視 ソースである RDS 側で手動または自動でフェイルオーバーが実行されたり、エンジンバージョンアップが行われたりすると、CDC を継続的に行っている DMS の replication task が Failed で死んでしまうことがある。\nしたがって replication task の実行状況を監視して通知する仕組みが必須となる。\nreplication task が失敗した場合は速やかに再度実行することになる。\nこのあたりの運用はちょっと面倒なので、再実行の自動化を検討している。\n遅延チェック 上記とも関係しているが、何らかの理由で CDC の結果が S3 に届くのが遅れることも想定される。\n遅れているのに気づかずに「HH 時台の CDC イベントログは全部届いたよね！」とみなして Iceberg table に更新をかけると table が意図しない状態になってしまうかもしれない。\nこれを防ぐために各時間の merge into 文を実行する前に過去 N 時間分の CDC の出力をチェックして、イベントログが遅れて届いていることがないことを確認するようにしている。\n遅れて届いているイベントログがあった場合、procedure rollback_to_snapshot() などで table の状態を rollback することになる。\nちなみにこういった処理の依存関係はおなじみ Airflow (Amazon MWAA) で管理している。\nまた、合わせて DMS の CloudWatch Mterics の CDCLatencySource や CDCLatencyTarget を見ておくとよい。\nそれぞれソースと replication instance, replication instance とターゲット間の遅延を表している。\nschema evolution column 追加など、ソース側の table の schema が変更されることは当然起こりうる。\nRDS の table を管理しているチームには schema を変更するときは前もって教えてもらえる一応伝えてある。\n我々のチームでは通常 Athena というか Glue Data Catalog 上の table の定義は CDK で IaC として管理している。(dbt は未導入)\nただし Iceberg table の schema 変更は metadata として管理される。\n当然 CDK というかその中身の CloudFormation では Iceberg の metadata は扱えない。\nIceberg table の schema については IaC に乗せることはできず、DDL で管理することになっている。\nmerge into 文における単一レコードの複数回操作への配慮 前述の merge into 文のサンプル SQL を見て、例えば同じ id のレコードに対して1時間の中で\n複数回更新が行われた 挿入の後、削除が行われた など複数回の操作があった場合はどうなるんだ？と思った方はごもっとも。\n実際上記のサンプル SQL では複数回の操作を考慮できていない。\nなので cdc のレコードをどの順で読み取るかによる、未定義な挙動になると考えられる。\nではどうすればいいかというと、ある id に対して1時間の中で最後に行われた操作だけを見ればよい。\n最後の操作が削除であればそのレコードは最終的に存在しないし、最後の操作が挿入または更新であれば存在する。\n具体的には using (...) の中の select 文を id ごとに最後の操作だけ見るように書き換えればよい。\nここで「最後」を定義するために column timestamp が必要になってくる。\ntimestamp と window 関数を使ってごにょごにょすると…？\n適用の結果 (まだない) この記事において、適用の結果 AWS コストが◯◯%削減できました！と言えればよかったのだが、まだこの仕組みは運用を始めたばかりであり実績値は出せない。\nプロジェクト着手前の試算では結構がっつり減らせる想定ではあった。\n前述のとおり処理が多段的になり運用の手間も増えているが、ペイできる程度のコスト削減を見込んでいる。\n気が向いたら結果を追記するかもしれない。\nまとめ 実際に Apache Iceberg を使っているプロジェクトの実務としての背景や運用について書いてみた。\nそんなに新しい内容はないが、ノウハウ的なところはあまり見かけない気がするのでまあ許していただきたい。\n今回は Iceberg に注目したので DMS について詳しく書かなかったが、こっちはこっちでいろいろあったりする。\nDistributed computing Advent Calendar 2023、明日は Delta Lake についてのお話のようですね。\nこちらも興味深いです。\n弊チームではこういったお仕事や modern data stack の導入などを一緒にやってくれるデータエンジニアを募集しています。\nご興味のある方は X にて DM ください。\nカジュアル面談しましょう。\n","permalink":"https://soonraah.github.io/posts/ingest-data-into-athena-by-cdc-and-iceberg/","summary":"このポストについて このポストは Distributed computing Advent Calendar 2023 の3日目の記事になります。\n1日目、2日目に続いて Apache Iceberg について書きますが、このポストでは Iceberg の実用例を書きます。\nAWS DMS による CDC の結果を Apache Iceberg 形式にして Amazon Athena でクエリできるようにするという内容になります。\nやっていることとしては Perform upserts in a data lake using Amazon Athena and Apache Iceberg | AWS Big Data Blog で紹介されている内容と近いですが、実務としての背景や工夫したところなどを書いていきます。\n背景 私の所属する事業会社では日々プロダクトから様々なデータが発生しており、プロダクトの分析やレポーティング、ML など様々な用途で利用されている。\nそれを支える基盤としてデータ基盤が存在している。\nデータ基盤ではクエリエンジンとして Amazon Athena を使っている。\nストレージとしては S3 を使用しており、主に分析用として Parquet 形式でデータが置かれる。\nここに業務用の operational な database から日次でデータを取り込んでいる。\nデータソースは RDS (Aurora MySQL) であり、比較的大きなデータとなっている。\nこれまではこの RDS -\u0026gt; S3 のデータ取り込みには RDS の S3 snapshot export という機能を利用していた。","title":"CDC + Apache Iceberg で Amazon Athena にデータを取り込む"},{"content":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第3章「データガバナンス」について。\nDAMA ホイール図において中心に置かれているので先に読んでおこうと思った次第。\n以降、特に注釈のない引用は DMBOK2 第3章からの引用とする。\nデータガバナンスとは データガバナンス (DG) の定義は、データ資産の管理 (マネジメント) に対して職務権限を通し統制 (コントロール) することである。統制とは計画を立て、実行を監視し、徹底させることを指す。\nこの定義からデータガバナンスがなぜ DAMA ホイール図の中心に位置しているかがわかる。\nデータガバナンスにより周囲の各知識領域の計画や実施を統制するという建付けになる。\nデータガバナンス (DG) のゴールは組織がデータを資産として管理できるようにすることである。DG はデータを資産として管理するための原則、ポリシー、プロセス、フレームワーク、評価指標を提供し、組織の各階層レベルでデータマネジメントアクティビティを牽引する。\nこれを可能にするためにデータガバナンスは持続可能であり、業務プロセスに組み込まれており、測定可能になっている必要がある。\nデータガバナンス組織 次の組織構成が一般的なデータガバナンスモデルであるとのこと。\nDMBOK2 データガバナンス組織構成\n右側がデータマネジメントを実施するロールになっており、左側がデータガバナンスによりデータマネジメントさせるロールになっている。\nデータガバナンス運営委員会が組織のデータガバナンスの頂点となっており、最も権限が強い。\n各部門にはデータガバナンス評議会 (DGC) が置かれており、これらがポリシー・評価指標の開発などのデータガバナンスの取り組みや課題、報告を管理する。\nデータガバナンスオフィス (DGO) はデータスチュワード (後述) などで構成され、企業レベルのデータ定義とデータマネジメントにフォーカスする。\n大規模、かつデータマネジメントの意識が高い組織でないとこういった組織構成はなかなか作れないのではと思った。\nライト版の図も欲しいところ。\nDMBOK2 データ問題の報告経路\nポリシーやステークホルダー利害の不一致、権限、データ品質などなど、データに関する問題は上記のような報告経路をたどる。\nデータスチュワード制 データスチュワード制はデータとプロセスの実行責任と結果責任を表す最も一般的な呼び名であり、データ資産の効率的な統制と利用を確かなものとする。\nちょっとこの説明ではイメージしにくいかもしれない。\nデータガバナンスの文脈において、データスチュワードは現場を含む組織の各レベルでデータガバナンスを効かせるための活動を行う実務者だと理解した。\nデータスチュワードという職務名があってもいいが、そうでなくてもよいらしい。\n次のようなことをやる。\n核となるメタデータの作成と管理 ルールと標準の文書化 データ品質の問題管理 データガバナンス運営アクティビティの実施 データスチュワードについては以下も参考。\n参考: データスチュワードとは？役割やメリット、育成方法、選定基準について解説！ | trocco®(トロッコ) データポリシー データポリシーとは、データとインフォーメーションを生成し、取得し、健全性を保ち、セキュリティを守り、品質を維持し、利用することを統制する基本的なルールに、原則と管理糸を盛り込む指示である。\nポリシーはデータガバナンスの \u0026ldquo;What\u0026rdquo; を説明する。\n通常はデータマネジメント・プロフェッショナルか業務ポリシー担当者が起草し、最終的に DGC により承認される。\n組織に対して効果的に伝達、実施される必要があり、そのためには簡潔で直感的な表現であるべき。\n社内ポータルなどオンラインで閲覧できるようになっているのがよい。\nポリシー大事。\nデータガバナンスの導入 効果的で長期に渡るデータガバナンス・プログラムにはデータに関する組織的思考や行動に関する文化の変革に加え、データについて将来のあるべき行動を実現するための新しい考えて方、行動、ピリシー、プロセスをサポートする継続的なチェンジマネジメント・プログラムが必要である。\nデータガバナンスを効かせるということは組織の文化の変革をともなうため、組織内で変化や学習への抵抗が起こる。\nデータガバナンスがいくら正しくても組織特有の文化に配慮しないと組織に浸透させることはできない。\nこのためチェンジマネジメントが重要になってくる。\n恥ずかしながら個人的にはチェンジマネジメントという言葉を知らなかったが、コンサル用語でそういうのがあるらしい。\n参考: チェンジマネジメントとは何か？成功事例と共に徹底解説！ このチェンジマネジメントという言葉は第3章で何度も登場する。\nチェンジマネジメントではチェンジマネジメントの計画、トレーニング、システム開発への組み込み、ポリシー導入、コミュニケーションなどを行う。\nその他、データガバナンスの導入には調整とコミュニケーションが必要となる。\n業務用語集 意外かもしれないが第3章では業務用語集についても何度か触れられている。\n業務用語集の目的は共通理解によるコミュニケーションの推進、データ誤用リスクの低減、IT資産と業務組織との連携の改善など。\n業務用語集の作成・管理にはデータスチュワードが責任を負う。\n所感 自社の置かれている状況から遠いというのと自分がマネジメント側でないのとで、具体的にデータガバナンスをどう効かせるか正直なところイメージしにくいと感じる部分が多かった。\nガバナンスには権威・権限が必要である一方、あまり大きくない会社だとトップ層がプロダクトマネジメントに関心がないことも多く、その辺どうやって巻き込んでいくかが難しいところだと思う。\nどうでもいいけど「データガバナンス評議会」っていう響きかっこよくないですか？\nメガネを光らせながら「データガバナンス評議会の者ですが」って言ってみたい。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-3/","summary":"このポストについて DMBOK2 を読み進めていくシリーズ。\n今回は第3章「データガバナンス」について。\nDAMA ホイール図において中心に置かれているので先に読んでおこうと思った次第。\n以降、特に注釈のない引用は DMBOK2 第3章からの引用とする。\nデータガバナンスとは データガバナンス (DG) の定義は、データ資産の管理 (マネジメント) に対して職務権限を通し統制 (コントロール) することである。統制とは計画を立て、実行を監視し、徹底させることを指す。\nこの定義からデータガバナンスがなぜ DAMA ホイール図の中心に位置しているかがわかる。\nデータガバナンスにより周囲の各知識領域の計画や実施を統制するという建付けになる。\nデータガバナンス (DG) のゴールは組織がデータを資産として管理できるようにすることである。DG はデータを資産として管理するための原則、ポリシー、プロセス、フレームワーク、評価指標を提供し、組織の各階層レベルでデータマネジメントアクティビティを牽引する。\nこれを可能にするためにデータガバナンスは持続可能であり、業務プロセスに組み込まれており、測定可能になっている必要がある。\nデータガバナンス組織 次の組織構成が一般的なデータガバナンスモデルであるとのこと。\nDMBOK2 データガバナンス組織構成\n右側がデータマネジメントを実施するロールになっており、左側がデータガバナンスによりデータマネジメントさせるロールになっている。\nデータガバナンス運営委員会が組織のデータガバナンスの頂点となっており、最も権限が強い。\n各部門にはデータガバナンス評議会 (DGC) が置かれており、これらがポリシー・評価指標の開発などのデータガバナンスの取り組みや課題、報告を管理する。\nデータガバナンスオフィス (DGO) はデータスチュワード (後述) などで構成され、企業レベルのデータ定義とデータマネジメントにフォーカスする。\n大規模、かつデータマネジメントの意識が高い組織でないとこういった組織構成はなかなか作れないのではと思った。\nライト版の図も欲しいところ。\nDMBOK2 データ問題の報告経路\nポリシーやステークホルダー利害の不一致、権限、データ品質などなど、データに関する問題は上記のような報告経路をたどる。\nデータスチュワード制 データスチュワード制はデータとプロセスの実行責任と結果責任を表す最も一般的な呼び名であり、データ資産の効率的な統制と利用を確かなものとする。\nちょっとこの説明ではイメージしにくいかもしれない。\nデータガバナンスの文脈において、データスチュワードは現場を含む組織の各レベルでデータガバナンスを効かせるための活動を行う実務者だと理解した。\nデータスチュワードという職務名があってもいいが、そうでなくてもよいらしい。\n次のようなことをやる。\n核となるメタデータの作成と管理 ルールと標準の文書化 データ品質の問題管理 データガバナンス運営アクティビティの実施 データスチュワードについては以下も参考。\n参考: データスチュワードとは？役割やメリット、育成方法、選定基準について解説！ | trocco®(トロッコ) データポリシー データポリシーとは、データとインフォーメーションを生成し、取得し、健全性を保ち、セキュリティを守り、品質を維持し、利用することを統制する基本的なルールに、原則と管理糸を盛り込む指示である。\nポリシーはデータガバナンスの \u0026ldquo;What\u0026rdquo; を説明する。\n通常はデータマネジメント・プロフェッショナルか業務ポリシー担当者が起草し、最終的に DGC により承認される。\n組織に対して効果的に伝達、実施される必要があり、そのためには簡潔で直感的な表現であるべき。\n社内ポータルなどオンラインで閲覧できるようになっているのがよい。","title":"読書メモ: DMBOK2 第3章 データガバナンス"},{"content":"このポストについて なんか個人的にデータマネジメントの機運が高まってきたので、ずっと積ん読していた DAMA-DMBOK を読んでいこうかなと。\nで、せっかくなのでデータ関係の皆さんがよくやっているように自分としても読書メモをまとめてみようと思った。\n内容を網羅するのではなく、現場のデータエンジニアとして活動した経験を踏まえて自分なりの観点でまとめてみたい。\n今回は第1章「データマネジメント」で、それ以降は関心がある章をつまみ食い的に読んでいく。\nDAMA-DMBOK とは DAMA とは DAta Management Association の略であり、\n世界各地に80の支部を持ち、8,000名を越える会員を擁する全世界のデータ専門家のための国際的な非営利団体です。 特定のベンダーや技術、手法に依存しないことを前提として、データや情報、知識をエンタープライズの重要な資産として管理する必要性の理解を促し、この分野の成長を推進しております。\n\u0026ndash; 一般社団法人 データマネジメント協会 日本支部(DAMA Japan)\nとのこと。\nこの DAMA が刊行しているのがデータマネジメント知識体系ガイド (The DAMA Guide to Data Management Body of Knowledge) であり、その略称が DMBOK である。\nＤＡＭＡ ＤＭＢＯＫは、データマネジメントプロフェッショナルにとって有益な資料かつ指針となることを目指し、データ管理のもっとも信頼できる入門書となるよう編集されています\n\u0026ndash; 一般社団法人 データマネジメント協会 日本支部(DAMA Japan)\nIT 業界歴の長い人なら PMBOK というプロジェクトマネジメントについて書かれた本をご存知かもしれないが、あれのデータマネジメント版だと思っていい。\n私としてはデータマネジメントの教科書的なものだと考えている。\n2023年現在における最新版は2018年の第2版となっている。\n以降では DMBOK2 とする。\n5年前なのでやや古いと思うかもしれないが、内容的には特定技術について書かれているわけではなく、ある程度抽象度が高い話になっているので陳腐化はしにくい。\nデータマネジメントとは 以降、特に注釈のない引用は DMBOK2 第1章からの引用とする。\nデータマネジメントとは、データとインフォメーションという資産の価値を提供し、管理し、守り、高めるために、それらのライフサイクルを通して計画、方針、スケジュール、手順などを開発、実施、監督することである。\nこの一文にいろいろと集約されているので見ていこう。\nデータとインフォーメーション ここでいう「データ」と「インフォメーション」は DIKW モデルにもとづく解釈でよい。\nデータはインフォメーションの原材料であり、インフォメーションは意味と目的、つまりコンテキストを付与されたデータと言える。\nex. 前四半期の売上レポートは「インフォメーション」、それの元になっている DWH のデータが「データ」 データマネジメントにおいては特にこのデータとインフォーメションを扱う。\n資産 資産とは経済的資源であり、所有可能、管理可能で、それ自体に価値があるか、価値を生み出すことができるものである。\nこの定義からデータは資産であるとみなされ、したがって資産として組織で管理される必要がある。\n金融資産などと同じ面もあるが異なっている面も多くあり、例えば使用しても失われない。\nこのような特性が管理方法に影響する。\n資産としてのデータは本来はデータにかかるコスト、データから得る利益を測定して経済的観点から評価されるべき。\nしかしまだそのための基準はないとのこと。\nなので実質的に経済的観点からの評価を実行するのはとてもむずかしいと思われる。\nデータ基盤がコストセンターだと認識されているような組織だとこのあたり必要になってくるのかもしれない。\n提供し データライフサイクル (後述) においてはデータの生成と利用が最も重要である。\nデータは利用されて初めて価値を生む。\nデータにはそれぞれ生産者と利用者がいる。\n社内の一つの部門内でデータ生産・利用されることもあるが、一方で生産と利用が別部門であることもある。\nよってデータライフサイクルを考えるためには組織横断の全社的な視点が必要になってくる。\n管理し データを資産として管理するためには質の高いメタデータが必要となる。\nメタデータとはデータについてのデータという意味。\nデータを記述するものがなければ意味のある管理はできないということ。\nメタデータもデータの一種であるためデータとして管理される必要がある。\n多くの場合、メタデータ管理がデータマネジメント全体を改善する出発点となる。\n守り データは資産であると同時にリスクでもある。\nなくなったり、盗まれたり、誤用されたりもするため、そうならないようにリスクを管理する。\n不正確・不完全・期限切れなど低品質のデータからは正しくないインフォーメーションが得られるリスクや誤用されるリスクがある。\nリスクを避けるためにデータを用いた意思決定に必要なデータ品質が必要となる。\n当然セキュリティの話もある。\n昨今は個人情報への関心も高まっており、それらは保護されなければならない。\n高める データ品質を高める、または高品質であることを保証しなければならない。\nDMBOK2 ではデータ品質に重きを置いており、\nこれがデータマネジメントの根幹である。\nとまで言い切っている。\n利用者からすると品質の問題が明らかになるまではデータの信頼性は高いものと想定されるが、一度信頼が失われると取り戻すのは簡単ではない。\nデータ基盤の同じデータに対して、5分前にクエリした結果と今クエリした結果が違っていると「これ大丈夫かな」ってなりますよね？\n(何らかの原因によるデータ生成処理の再実行など、運用上でよく起こりうる話)\nデータの品質問題にたいしょするために収益の10〜30%を費やしていると専門家は考えている。IBM は米国において低品質のデータのために費やしたコストは2016年で3.1兆ドルであったと推定している。\nとあるとおり、低品質のデータにはコストとリスクがかかる。\n一方で高品質のデータからはより高い生産性や競合に対する優位性などさまざまな恩恵が得られる。\nライフサイクル データにはその発生から破棄に至るまでのライフサイクルがある。\nDMBOK2 データライフサイクルの主要アクティビティ\n上記はその概念的な図だが、実務におけるデータライフサイクルを詳細に記述するのは困難を伴う。\n生産者から利用者までの経路、すなわちデータリネージを表現する必要がある。\nデータ品質、メタデータ、セキュリティなどはライフサイクル全体を通して管理していかなければなない。\n計画、方針、スケジュール データから偶然に価値が生まれることはなく、様々な側面から計画が必要である。\nより高品質なデータを目指す計画においては、アーキテクチャ、モデリング、その他設計機能に対して、戦略的なアプローチが必要である。業務と IT のリーダーが戦略的に連携することも欠かせない。\n組織横断であるため、(理想的には) CDO がリーダーシップを発揮、ビジョンや目的を示し、自らそれにコミットメントしないと効果的なデータマネジメントにならない。\nCDO がいる組織はまだ多くないため、その状況でこの役割を誰が担うかというのがデータマネジメントの成果に大きな影響を与えるはず。\nデータマネジメント・フレームワーク データマネジメントには様々な側面がある。\nデータマネジメントを総合的に理解し、その構成要素間の関係を理解するためにいくつかのフレームワークが提案されている。\nここでは DMBOK2 で紹介されたフレームワークのうちのいくつかを紹介する。\nDAMA-DMBOK フレームワーク データマネジメント全体を構成する知識領域についてまとめたもの。\nDAMA ホイール図, DAMA のご紹介 - DAMA Japan\nこの DAMA ホイール図はおそらく一番有名なもので、見たことある人も多いのでは。\nデータマネジメントにどういった知識領域があるのかが一覧できる。\nただデータガバナンスが中心にあるというだけで、それ以外の各領域の関係性はわかりにくいかもしれない。\n知識領域コンテキスト図, DMBOKv2 Image Downloads - DAMA International\nこちらの知識領域コンテキスト図も DAMA-DMBOK フレームワークの一部である。\nDMBOK1 のときに最も人気のあった図とのこと。\n最初ぱっとこれを見たときはピンとこなかったが、ある事業ドメインにおけるデータの流れやコンテキストを表しているというのに気づきとても有用だと思った。\nこの内容を各事業ドメインで整理すると、組織のデータについての理解が大きく捗りそう。(めっちゃたいへんだけど)\n上記の解釈は誤り。\nこの図は事業ドメインごとではなく、知識領域、つまり DAMA ホイール図に記載の「データガバナンス」「アーキテクチャ」などの各要素ごとに作られる。\nこの図により各知識領域を簡潔に定義する。\nDMBOK2 では各章でそれぞれの知識領域について書かれており、章の冒頭にこの知識領域コンテキスト図が記載されている。\nある知識領域の概略をつかむにはその図を見ればよい。\nDMBOK ピラミッド ほとんどの組織はデータマネジメントの戦略を決めてからデータ管理を始めるということができず、走りながらデータマネジメントの改善に取り組むことになる。\nその中でどういったステップをたどるかを示してくれるのが Peter Aiken\u0026rsquo;s Framework である。\nPeter Aiken\u0026rsquo;s Framework, Why HR Data Management Strategy is important in your HR Transformation | SAP Blogs (元ネタは DMBOK2)\nPhase 1 (青): データの保存、統合 Phase 2 (黃): データ品質、アーキテクチャ、メタデータ Phase 3 (緑): ガバナンスと利用推進 Phase 4 (赤): 分析などの高度な利用 組織のデータマネジメントは Phase 1 -\u0026gt; 4 の順で進めていく。\nPhase の順序は上下関係にはなっておらず、上下関係は知識領域間の依存関係を示している。\nこの図は実務でデータマネジメントを進めるにあたり、良い指針となる。\n所感 第1章はタイトルのとおりデータマネジメントについて俯瞰するのに良かった。\n品質、ライフサイクル、リスク管理、メタデータなどの各要素がそれぞれに影響しあっていることが理解できた。\nこれからその各要素を詳しく見ていくことになる。\n組織横断的なデータの扱いを始めてからそんなに経っていない組織だと、CDO はおろかデータマネジメントに関心があるリーダー層がいないというケースが多いのではないだろうか。\nCDO が無理なら CTO あたりに関心を持ってコミットしてもらいたい。\nそれもできない場合は現場のデータエンジニアやアナリストなどが進めていくしかないが、それは厳しい状況でのデータマネジメントになりそう。\nエライ人を巻き込む政治力のようなものが問われる。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-1/","summary":"このポストについて なんか個人的にデータマネジメントの機運が高まってきたので、ずっと積ん読していた DAMA-DMBOK を読んでいこうかなと。\nで、せっかくなのでデータ関係の皆さんがよくやっているように自分としても読書メモをまとめてみようと思った。\n内容を網羅するのではなく、現場のデータエンジニアとして活動した経験を踏まえて自分なりの観点でまとめてみたい。\n今回は第1章「データマネジメント」で、それ以降は関心がある章をつまみ食い的に読んでいく。\nDAMA-DMBOK とは DAMA とは DAta Management Association の略であり、\n世界各地に80の支部を持ち、8,000名を越える会員を擁する全世界のデータ専門家のための国際的な非営利団体です。 特定のベンダーや技術、手法に依存しないことを前提として、データや情報、知識をエンタープライズの重要な資産として管理する必要性の理解を促し、この分野の成長を推進しております。\n\u0026ndash; 一般社団法人 データマネジメント協会 日本支部(DAMA Japan)\nとのこと。\nこの DAMA が刊行しているのがデータマネジメント知識体系ガイド (The DAMA Guide to Data Management Body of Knowledge) であり、その略称が DMBOK である。\nＤＡＭＡ ＤＭＢＯＫは、データマネジメントプロフェッショナルにとって有益な資料かつ指針となることを目指し、データ管理のもっとも信頼できる入門書となるよう編集されています\n\u0026ndash; 一般社団法人 データマネジメント協会 日本支部(DAMA Japan)\nIT 業界歴の長い人なら PMBOK というプロジェクトマネジメントについて書かれた本をご存知かもしれないが、あれのデータマネジメント版だと思っていい。\n私としてはデータマネジメントの教科書的なものだと考えている。\n2023年現在における最新版は2018年の第2版となっている。\n以降では DMBOK2 とする。\n5年前なのでやや古いと思うかもしれないが、内容的には特定技術について書かれているわけではなく、ある程度抽象度が高い話になっているので陳腐化はしにくい。\nデータマネジメントとは 以降、特に注釈のない引用は DMBOK2 第1章からの引用とする。\nデータマネジメントとは、データとインフォメーションという資産の価値を提供し、管理し、守り、高めるために、それらのライフサイクルを通して計画、方針、スケジュール、手順などを開発、実施、監督することである。\nこの一文にいろいろと集約されているので見ていこう。\nデータとインフォーメーション ここでいう「データ」と「インフォメーション」は DIKW モデルにもとづく解釈でよい。\nデータはインフォメーションの原材料であり、インフォメーションは意味と目的、つまりコンテキストを付与されたデータと言える。\nex. 前四半期の売上レポートは「インフォメーション」、それの元になっている DWH のデータが「データ」 データマネジメントにおいては特にこのデータとインフォーメションを扱う。","title":"読書メモ: DMBOK2 第1章 データマネジメント"},{"content":"前回のポストでは merge on read で Apache Iceberg の table を near real time で更新するということを行った。\nこのポストではそのメンテナンスについて触れて、かつそれを実行してみる。\nmerge on read の課題 merge on read で table を更新する場合、copy on write の場合と違い table 全体を洗い替えする必要はなく差分のみを追記することになる。\nしたがって更新にかかる時間は copy on write よりも短くなる。\n一方で merge on read の名のとおり読み出し時に積み重なった差分とベースを merge して最新の snapshot とするため、読み出しの速度は copy on write より遅くなる。\n長時間更新され差分がたくさん存在しているとなおさら遅い。\nなので\n更新頻度が低く、参照頻度が高いユースケース -\u0026gt; copy on write 更新頻度が高く、参照頻度が低いユースケース -\u0026gt; merge on write という使い分けがよいとされている。\n前回ポストの例では一晩更新を続けた後の merge on read の table に対して簡単な select 文を実行したところ、6分程度かかってしまった。\nレコード数はたかだか128件程度であることを考えるとかなり遅いと言える。\nこのままでは使い物にならない。\nしかし更新頻度が高く、参照もよく行われる場合はどうすればいいか？\nというところで compaction が必要になってくる。\nCompaction compaction は追加された差分ファイルをベースファイルと merge して新たなベースファイルを作るという処理である。\ncompaction 後の select クエリは compaction 以前の差分ファイルを読む必要がなくなるため、読み込みが速くなる。\nしたがって長期的に運用される merge on read の table では定期的に compaction が行われることが望ましい。\nIceberg の公式ドキュメントに compaction の記載があるが、Dremio の記事の方が図もあってわかりやすい。\nCompact data files | Apache Iceberg Compaction in Apache Iceberg: Fine-Tuning Your Iceberg Table’s Data Files | Dremio Iceberg で Spark から compaction を行う方法は2つある。\n1つは SparkActions.rewriteDataFiles() を使う方法、もう1つは SparkSQL 内で procedure rewrite_data_files を呼び出す方法だ。\n今回は主に SparkSQL ベースで実装しているということもあり後者にした。\nちなみに compaction は Iceberg 固有の機能ではなく、Hudi や Delta Lake などでも存在している。\nmerge on read をサポートする table format においては一般的なトピックだと思われる。\nその他のメンテナンス compaction 以外でも以下2点も対応する。\n古い snapshot の削除 長期間 table の更新を続けると snapshot が蓄積していく。\nデータが大きくなり続けるため、定期的に snapshot を削除していくことが推奨されている。\nprocedure expire_snapshots により指定の時刻より古い snapshot を削除することができる。\n古い metadata file の削除 データ参照の入り口である metadata file も同様に増え続ける。\nこれも定期的に削除するが、頻繁に更新が行われる table ではこれも削除した方がよい。\nmetadata file の削除は create table 時に table property として write.metadata.delete-after-commit.enabled および write.metadata.previous-versions-max を指定することで自動で行われる。\nこのように Iceberg の table 構造は物理的・論理的に多層的になっているので色々なレベルで配慮が必要という印象。\nメンテナンスの実装 以上を踏まえて、前回ポスト時の実装に対して変更を追加した。\nまず前回作った更新用の app 中の create table を変更。\ntblproperties として古い metadata file を削除するための設定を追加した。(末尾の2つ)\n// Create table spark .sql( \u0026#34;\u0026#34;\u0026#34;create table | my_catalog.my_db.device_temperature |( | device_id int, | operation string, | temperature double, | ts timestamp |) |using iceberg |tblproperties ( | \u0026#39;format-version\u0026#39; = \u0026#39;2\u0026#39;, | \u0026#39;write.delete.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.update.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.merge.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.metadata.delete-after-commit.enabled\u0026#39; = \u0026#39;true\u0026#39;, | \u0026#39;write.metadata.previous-versions-max\u0026#39; = \u0026#39;100\u0026#39; |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) 次に更新処理とは別プロセスとしてメンテナンスを実施する app を追加した。\n通常の更新処理の mini batch の中にメンテナンスを組み込んでもよかったのだが、そうすると mini batch が遅れる可能性がある。\n運用を考えてもメンテナンス用の処理は別で実行できるようになっていた方がいいだろう。\nIceberg は後述の1点を気をつければ並列書き込みが可能であるため、別プロセスで実施する方針とした。\nwhile (true) { println(\u0026#34;Execute compaction\u0026#34;) retry(3) { () =\u0026gt; spark .sql( \u0026#34;\u0026#34;\u0026#34;call my_catalog.system.rewrite_data_files( | table =\u0026gt; \u0026#39;my_db.device_temperature\u0026#39;, | strategy =\u0026gt; \u0026#39;binpack\u0026#39; |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) .show(truncate = false) } val ts = Timestamp.from(Instant.now().minusSeconds(20 * 60)) println(s\u0026#34;Expire snapshots older than ${ts.toString}\u0026#34;) retry(3) { () =\u0026gt; spark .sql( s\u0026#34;\u0026#34;\u0026#34;call my_catalog.system.expire_snapshots( | table =\u0026gt; \u0026#39;my_db.device_temperature\u0026#39;, | older_than =\u0026gt; timestamp \u0026#39;${ts.toString}\u0026#39;, | retain_last =\u0026gt; 20 |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) .show(truncate = false) } Thread.sleep(10 * 60 * 1000) } 無限ループによりおよそ10分に1回、それぞれ procedure 呼び出しにより compaction と snapshot 削除が実行されるようになっている。\nここで retry() は自前の実装だが、名前のとおり失敗しても指定回数まで retry するというものになっている。\nなぜ retry が必要かというと、Iceberg は並列した書き込みができるが lock などは取らず、楽観的な実行となっている。(optimistic concurrency)\n一連の書き込み処理の準備が終わって最後に commit するときに、その table が他のプロセスにより更新されたことがわかると失敗となる。\n実際に retry なしの場合は次のようなエラーが出ることがあった。\n[error] java.lang.RuntimeException: Cannot commit rewrite because of a ValidationException or CommitFailedException. This usually means that this rewrite has conflicted with another concurrent Iceberg operation. To reduce the likelihood of conflicts, set partial-progress.enabled which will break up the rewrite into multiple smaller commits controlled by partial-progress.max-commits. Separate smaller rewrite commits can succeed independently while any commits that conflict with another Iceberg operation will be ignored. This mode will create additional snapshots in the table history, one for each commit. このメッセージでは partial-progress.enabled の設定が推奨されているが、今回の問題設定はそもそも細かい更新だったのでちょっと違うかなというところで設定していない。\nこれを設定したところで retry 的な配慮は結局必要になるというのもある。\nメンテナンスの実行結果 以上のように実装した2つの処理を並列実行した。\n30秒に1回、table を更新 約10分に1回、table をメンテナンス 2日以上これをまわしっぱなしにした。\nCompaction の効果 select 文を実行したところ、数秒で完了した。\ncompaction を実施する前は一晩更新を続けた後の select で6分かかっていたので、かなり速くなったと言える。\ncompaction により差分の merge のコストが小さくなったからだ。\nOLAP ならこれぐらいで十分だろう。\nSnapshot の削除 metadata file は JSON 形式になっており、次のような形で利用可能な snapshot の情報が記載されている。\n{ ..., \u0026#34;snapshots\u0026#34;: [ { \u0026#34;sequence-number\u0026#34;: 25, \u0026#34;snapshot-id\u0026#34;: 8639168923204820422, \u0026#34;parent-snapshot-id\u0026#34;: 5556833698284258695, \u0026#34;timestamp-ms\u0026#34;: 1684722810993, \u0026#34;summary\u0026#34;: { \u0026#34;operation\u0026#34;: \u0026#34;overwrite\u0026#34;, \u0026#34;spark.app.id\u0026#34;: \u0026#34;local-1684722133108\u0026#34;, \u0026#34;added-data-files\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;added-position-delete-files\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;added-delete-files\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;added-records\u0026#34;: \u0026#34;26\u0026#34;, \u0026#34;added-files-size\u0026#34;: \u0026#34;13020\u0026#34;, \u0026#34;added-position-deletes\u0026#34;: \u0026#34;27\u0026#34;, \u0026#34;changed-partition-count\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;total-records\u0026#34;: \u0026#34;141\u0026#34;, \u0026#34;total-files-size\u0026#34;: \u0026#34;52397\u0026#34;, \u0026#34;total-data-files\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;total-delete-files\u0026#34;: \u0026#34;25\u0026#34;, \u0026#34;total-position-deletes\u0026#34;: \u0026#34;51\u0026#34;, \u0026#34;total-equality-deletes\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;manifest-list\u0026#34;: \u0026#34;data/warehouse/my_db/device_temperature/metadata/snap-8639168923204820422-1-7ea46920-6ac3-444d-aed1-8dbc8d0c24fd.avro\u0026#34;, \u0026#34;schema-id\u0026#34;: 0 }, ... ], ... } 最新の metadata file に含まれる snapshot の数をカウントしてみる。\n$ cat data/warehouse/my_db/device_temperature/metadata/v6133.metadata.json | jq \u0026#39;.snapshots | length\u0026#39; 59 table は30秒に1回更新されて新しい snapshot が作られている。\nそれと並行して10分に1回、その時点より20分以上前の snapshot を削除しているため、snapshot の数はおおよそ40〜60ぐらいになる。(実際は compaction で作られた snapshot も入ってくるためこれより少し多くなる)\nsnapshot の削除が効いていることがわかった。\n他の metadata file の shapshot 数も確認したが、だいたい上記の範囲におさまっていた。\nmetadata file の削除 metadata file の削除が効いているかも確認。\n$ ls -l data/warehouse/my_db/device_temperature/metadata/v*.metadata.json | wc -l 101 create table では 'write.metadata.previous-versions-max' = '100' を指定していた。\n現行 version 1 件 + 過去の version 100 件ということで期待どおりのファイル数にコントロールされていることがわかった。\n雑感 compaction などのメンテナンスが期待どおりに実行されていることが確認できた。\nmerge on read の table を長期的に運用する場合、こういったメンテナンスの処理や設定を導入することは必須となるだろう。\nIceberg で merge on read の table を高頻度で更新するにあたり、課題だなと思ったことを3点挙げておく。\nファイル形式 Apache Hudi や Delta Lake で merge on read の table を更新する場合、ベースファイルは Parquet 等の列指向、差分ファイルは Avro などの行指向がデフォルトになっている。\n直感的に少量の差分は行指向になっているのが効率がいいように思うが、一方で Iceberg でこういった形での更新ができるのかがわからなかった。\n複数のファイル形式がまざっていても manifest で吸収できるので読み込みはできるはず、しかし Spark からこういった書き込みができるという記述をドキュメントで見つけることができなかった。\n更新にかかる時間 以下は merge on read で table 更新を行う Spark app において、Spark web UI 上で表示される各 mini batch の実行時間を表すグラフである。\n約10分周期で mini batch の実行時間が増加していき、ストンと下がるということが確認される。\nこの下がっている部分は compaction のタイミングであり、逆に言うと compaction しないと書き込みの時間が増大するということになる。\n\u0026ldquo;merge on read\u0026rdquo; の名のとおり、本来は読み込み時に差分を merge するのであって書き込み時はそうならないはず。\nしかし差分が増えると書き込み時間が増えていくというのはこの仕様に合わない。\n(Iceberg のソースコードを読めばわかりそうだが、今回はそこまでやってない)\n求められる技術力 Iceberg の table は ACID transaction, time travel, schema/partition evolution など運用上便利な機能がサポートされている。\n一方でこれを運用していくには table format についての知見や Spark などの分散処理の知見を持つデータエンジニアのリソースが必要となってくる。\nBigQuery や Snowflake などの DWH を中心としたアーキテクチャと比べて人材をそろえるハードルが高いと考えられる。\nこれは Iceberg というよりは Lakehouse Architecture と DWH の差だろう。\n…などと課題を挙げてはみたが、総じて Iceberg はいいものだと感じた。\nうまく抽象化されたレイヤーを挟むことでデータレイクのいろいろな課題を解決しており、かつ動作も比較的わかりやすい。\nエンジニアリングが強い組織だと Iceberg を使って Lakehouse Architecture のも悪くなさそう。\n","permalink":"https://soonraah.github.io/posts/maintain-iceberg-table-updated-in-near-real-time/","summary":"前回のポストでは merge on read で Apache Iceberg の table を near real time で更新するということを行った。\nこのポストではそのメンテナンスについて触れて、かつそれを実行してみる。\nmerge on read の課題 merge on read で table を更新する場合、copy on write の場合と違い table 全体を洗い替えする必要はなく差分のみを追記することになる。\nしたがって更新にかかる時間は copy on write よりも短くなる。\n一方で merge on read の名のとおり読み出し時に積み重なった差分とベースを merge して最新の snapshot とするため、読み出しの速度は copy on write より遅くなる。\n長時間更新され差分がたくさん存在しているとなおさら遅い。\nなので\n更新頻度が低く、参照頻度が高いユースケース -\u0026gt; copy on write 更新頻度が高く、参照頻度が低いユースケース -\u0026gt; merge on write という使い分けがよいとされている。\n前回ポストの例では一晩更新を続けた後の merge on read の table に対して簡単な select 文を実行したところ、6分程度かかってしまった。","title":"near real time で更新される Apache Iceberg の table のメンテナンス"},{"content":"Apache Iceberg の table を near real time に、つまり高頻度で更新するということをやってみた。\nApache Iceberg とは Apache Iceberg (以下 Iceberg) は分散ファイルシステムやクラウドストレージ上の table format であり、Apache Hudi や Delta Lake と並んで data lake や lakehouse architecture で用いられる。\n特徴的なのは table とデータ実体 (Parquet, Avro など) の間に metadata file, manifest list, manifest file の抽象的なレイヤーがあり、ファイル単位で table の状態を track できること。\nこれにより強い isolation level、パフォーマンス、schema evolution など様々な機能・性能を実現できるようになっている。\nApache Iceberg Iceberg Table Spec\n詳しくは公式ドキュメントを参照のこと。\n最近では SmartNews 社が Iceberg で data lake を構築したことを記事にしていた。\nFlink-based Iceberg Real-Time Data Lake in SmartNews (Part I) | by SmartNews | SmartNews, Inc | Apr, 2023 | Medium ベンダー提供の DWH 中心ではなく Lakehouse Architecture を目指すのであれば最有力の table format の1つだと言えそう。\nこのポストでやりたいこと このポストでは Iceberg の table を near real time 更新することを想定して実験を行う。\n多数の IoT デバイスで測定された温度情報が逐次的に送られていることを想定し、定期的にそれをデバイス ID ごとの最新の温度情報を持つ table へと書き出すものとする。\n次のような要件を仮定する。\n低レイテンシが要求されており、near real time での table 更新を行う 次のような schema のレコードが IoT デバイスから送信され得られる状態になっているものとする device_id: デバイスに対して一意に振られている ID operation: upsert or delete (後述) temperature: 測定された温度の値 ts: 測定時の timestamp IoT デバイスは登録削除されることがあり、その場合は table 上の当該デバイス ID のレコードを削除する 削除の場合は operation = 'delete' となっている。それ以外は 'upsert' 送られてくるレコードは timestamp (ts) の順になっているとは限らない (out of order) リアルタイム処理のフレームワークとしては Spark Structured Streaming を使用するものとする。\ntable 更新の実装 上記実験のための実装を行った。\n各言語・フレームワーク等は以下のバージョンを使っている。\nScala 2.13.10 Java 11 Spark 3.3.2 Iceberg 1.2.1 実行可能な sbt project の全体を GitHub repository soonraah/streaming_iceberg に置いているのでご参考まで。\n実装の主要な部分を次に示す。\n// Create SparkSession instance val spark = SparkSession .builder() .appName(\u0026#34;StreamingIcebergExperiment\u0026#34;) .master(\u0026#34;local[2]\u0026#34;) .config(\u0026#34;spark.sql.extensions\u0026#34;, \u0026#34;org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog\u0026#34;, \u0026#34;org.apache.iceberg.spark.SparkCatalog\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog.type\u0026#34;, \u0026#34;hadoop\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog.warehouse\u0026#34;, \u0026#34;data/warehouse\u0026#34;) .getOrCreate() spark.sparkContext.setLogLevel(\u0026#34;WARN\u0026#34;) import spark.implicits._ // Drop table spark .sql( \u0026#34;\u0026#34;\u0026#34;drop table if exists | my_catalog.my_db.device_temperature |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) // Create table spark .sql( \u0026#34;\u0026#34;\u0026#34;create table | my_catalog.my_db.device_temperature |( | device_id int, | operation string, | temperature double, | ts timestamp |) |using iceberg |tblproperties ( | \u0026#39;format-version\u0026#39; = \u0026#39;2\u0026#39;, | \u0026#39;write.delete.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.update.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.merge.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39; |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) val random = new Random() val addOutOfOrderness = udf { (timestamp: Timestamp) =\u0026gt; // add time randomly in [-5, +5) sec val ms = timestamp.getTime + random.nextInt(10000) - 5000 new Timestamp(ms) } // Prepare input data val dfInput = spark .readStream .format(\u0026#34;rate\u0026#34;) .option(\u0026#34;rowsPerSecond\u0026#34;, 1) .load() .select( // Generate device_id in 0~127 randomly (rand() * 128).cast(IntegerType).as(\u0026#34;device_id\u0026#34;), // 90% of operation is upsert and 10% is delete when(rand() \u0026gt; 0.1, \u0026#34;upsert\u0026#34;).otherwise(\u0026#34;delete\u0026#34;).as(\u0026#34;operation\u0026#34;), // Generate temperature randomly based on standard normal distribution (randn() * 5.0 + 20.0).as(\u0026#34;temperature\u0026#34;), // timestamp is out of order addOutOfOrderness($\u0026#34;timestamp\u0026#34;).as(\u0026#34;ts\u0026#34;) ) // Update table for each mini batch dfInput .writeStream .trigger(Trigger.ProcessingTime(30.seconds)) .foreachBatch { (dfBatch: DataFrame, batchId: Long) =\u0026gt; println(s\u0026#34;Processing batchId=$batchId\u0026#34;) // Eliminate duplicated device_id val dfDedup = dfBatch .withColumn( \u0026#34;row_num\u0026#34;, row_number() .over(Window.partitionBy($\u0026#34;device_id\u0026#34;).orderBy($\u0026#34;ts\u0026#34;.desc)) ) .where($\u0026#34;row_num\u0026#34; === 1) .drop($\u0026#34;row_num\u0026#34;) // createOrReplaceTempView() doesn\u0026#39;t work dfDedup.createOrReplaceGlobalTempView(\u0026#34;input\u0026#34;) // Insert, update and delete records by \u0026#39;merge into\u0026#39; spark .sql( \u0026#34;\u0026#34;\u0026#34;merge into | my_catalog.my_db.device_temperature |using( | select | * | from | global_temp.input |) as input |on | device_temperature.device_id = input.device_id | and device_temperature.ts \u0026lt; input.ts |when matched and input.operation = \u0026#39;upsert\u0026#39; then update set * |when matched and input.operation = \u0026#39;delete\u0026#39; then delete |when not matched then insert * |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) () } .start .awaitTermination() spark.stop() この中の各処理を以下で説明していく。\n// Create SparkSession instance val spark = SparkSession .builder() .appName(\u0026#34;StreamingIcebergExperiment\u0026#34;) .master(\u0026#34;local[2]\u0026#34;) .config(\u0026#34;spark.sql.extensions\u0026#34;, \u0026#34;org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog\u0026#34;, \u0026#34;org.apache.iceberg.spark.SparkCatalog\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog.type\u0026#34;, \u0026#34;hadoop\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog.warehouse\u0026#34;, \u0026#34;data/warehouse\u0026#34;) .getOrCreate() Spark から Iceberg を使うためには extensions の設定、および catalog の設定が必要となる。\nこれらは設定ファイルやコマンドラインからも指定できるが、今回は実験のためソースコード上で簡易に設定した。\n// Create table spark .sql( \u0026#34;\u0026#34;\u0026#34;create table | my_catalog.my_db.device_temperature |( | device_id int, | operation string, | temperature double, | ts timestamp |) |using iceberg |tblproperties ( | \u0026#39;format-version\u0026#39; = \u0026#39;2\u0026#39;, | \u0026#39;write.delete.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.update.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.merge.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39; |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) table 作成のクエリを実行している。\n頻度の高い更新・削除を想定しているため、ここでは各操作に対して copy-on-write ではなく merge-on-read を指定している。\nこれにより書き込み時にデータファイル全体をコピーするのではなく、差分のみ追加するという形となり書き込みのコストが小さくなる。\nmerge-on-read は format version 2 でしか指定できないため、その設定もしている。\n// Prepare input data val dfInput = spark .readStream .format(\u0026#34;rate\u0026#34;) .option(\u0026#34;rowsPerSecond\u0026#34;, 1) .load() .select( // Generate device_id in 0~127 randomly (rand() * 128).cast(IntegerType).as(\u0026#34;device_id\u0026#34;), // 90% of operation is upsert and 10% is delete when(rand() \u0026gt; 0.1, \u0026#34;upsert\u0026#34;).otherwise(\u0026#34;delete\u0026#34;).as(\u0026#34;operation\u0026#34;), // Generate temperature randomly based on standard normal distribution (randn() * 5.0 + 20.0).as(\u0026#34;temperature\u0026#34;), // timestamp is out of order addOutOfOrderness($\u0026#34;timestamp\u0026#34;).as(\u0026#34;ts\u0026#34;) ) ここでは IoT デバイスから送信されたとするストリームデータを作成している。\nまずは .readStream.foramt(\u0026quot;rate\u0026quot;) によりストリームを発生させ、select() の部分で想定する schema にして値を入れている。\n詳細はコメントを参照。\n1点だけ補足すると、addOutOfOrderness() の部分では timestamp に対してわざと±5秒以内のランダムなゆらぎを与えおり、レコードが timestamp どおりに流れてこない状況を作っている。\ndfInput .writeStream .trigger(Trigger.ProcessingTime(30.seconds)) .foreachBatch { ... } .start .awaitTermination() ストリームデータは foreachBatch により、30秒ごとの mini batch の単位で処理される。\nこの mini batch の中で table 更新を行う。\n// Eliminate duplicated device_id val dfDedup = dfBatch .withColumn( \u0026#34;row_num\u0026#34;, row_number() .over(Window.partitionBy($\u0026#34;device_id\u0026#34;).orderBy($\u0026#34;ts\u0026#34;.desc)) ) .where($\u0026#34;row_num\u0026#34; === 1) .drop($\u0026#34;row_num\u0026#34;) Iceberg の merge into (後述) では更新をかける側の table において結合のキーとなる column の重複は許されず、重複があった場合は次のようなエラーになってしまう。\norg.apache.spark.SparkException: The ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table. This could result in the target row being operated on more than once with an update or delete operation and is not allowed. これを避けるため結合のキーとなる device_id ごとに一意となるよう、最新の行だけ残して重複を排除している。\n// Insert, update and delete records by \u0026#39;merge into\u0026#39; spark .sql( \u0026#34;\u0026#34;\u0026#34;merge into | my_catalog.my_db.device_temperature |using( | select | * | from | global_temp.input |) as input |on | device_temperature.device_id = input.device_id | and device_temperature.ts \u0026lt; input.ts |when matched and input.operation = \u0026#39;upsert\u0026#39; then update set * |when matched and input.operation = \u0026#39;delete\u0026#39; then delete |when not matched then insert * |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) はい、ここが update, insert, delete の最もキモとなるところ。\n個人的には merge into という SQL 構文は知らなかったのだが、どうやら Iceberg 特有のものでもないらしい。\ntable device_temperature に対して update, delete, insert を行う SQL となっている。\nこのクエリが実行されると Iceberg の table が更新され、snapshot が1つ前に進むことになる。\nDDL のところで merge-on-read を指定したため、差分のみのファイルが作成される。\ntable 更新の確認 上記のコードを実行すると、table の更新処理が始まる。\nしばらく流した後、SparkSQL で\nspark .sql( \u0026#34;\u0026#34;\u0026#34;select | * |from | my_catalog.my_db.device_temperature |order by | device_id |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) .show(128, truncate = false) のようなクエリを実行すると、\n+---------+---------+------------------+-----------------------+ |device_id|operation|temperature |ts | +---------+---------+------------------+-----------------------+ |0 |upsert |15.879340359832794|2023-05-08 07:39:56.637| |1 |upsert |20.303530210621492|2023-05-08 07:40:26.157| |2 |upsert |21.07822922592327 |2023-05-08 07:41:10.937| |3 |upsert |20.81228214216027 |2023-05-08 07:37:09.756| |6 |upsert |20.571664434275807|2023-05-08 07:38:43.124| |7 |upsert |19.44731196983606 |2023-05-08 07:39:56.1 | |8 |upsert |27.964942467735458|2023-05-08 07:39:39.623| |9 |upsert |23.319385015293673|2023-05-08 07:40:59.377| |10 |upsert |22.392313247902365|2023-05-08 07:40:40.946| ... のような結果が得られる。\n少し間をおいて実行すると中身が変わっており、更新されていることがわかる。\nまた、ある程度時間を経た後においても欠番があり (ex. 上記の device_id = 4, 5)、削除も行われていることが確認できる。\n雑感 Apache Hudi に比べて動作が素直でわかりやすいように感じた。\n一方で Hudi で言うところの PRECOMBINE_FIELD のようなものがなく、自分で重複排除する必要がありちょっと面倒。\n必然的にタイムトラベルできる粒度は mini batch の粒度となる。\nmerge-on-read の table では差分ファイルが大きくなるにつれ、読み取りのコストも大きくなっていく。\n上記の更新コードを一晩実行した後だと、単純な select に6分程度かかってしまった。\n最新 snapshot としてはレコード数はたかだか128程度なのでこれはかなり遅い。\nというところで compaction などの table のメンテナンス操作が必要となってくる。\nこれについては次のポストで扱いたい。\n","permalink":"https://soonraah.github.io/posts/update-iceberg-table-in-near-real-time/","summary":"Apache Iceberg の table を near real time に、つまり高頻度で更新するということをやってみた。\nApache Iceberg とは Apache Iceberg (以下 Iceberg) は分散ファイルシステムやクラウドストレージ上の table format であり、Apache Hudi や Delta Lake と並んで data lake や lakehouse architecture で用いられる。\n特徴的なのは table とデータ実体 (Parquet, Avro など) の間に metadata file, manifest list, manifest file の抽象的なレイヤーがあり、ファイル単位で table の状態を track できること。\nこれにより強い isolation level、パフォーマンス、schema evolution など様々な機能・性能を実現できるようになっている。\nApache Iceberg Iceberg Table Spec\n詳しくは公式ドキュメントを参照のこと。\n最近では SmartNews 社が Iceberg で data lake を構築したことを記事にしていた。\nFlink-based Iceberg Real-Time Data Lake in SmartNews (Part I) | by SmartNews | SmartNews, Inc | Apr, 2023 | Medium ベンダー提供の DWH 中心ではなく Lakehouse Architecture を目指すのであれば最有力の table format の1つだと言えそう。","title":"Apache Iceberg の table を near real time で更新する"},{"content":"データエンジニアリングの領域で少し前から目にするようになった \u0026ldquo;data contract\u0026rdquo; という言葉。\nなんとなく今の業務で困っている課題の解決になりそうな気がしつつもよくわかっていなかったので調べてみた。\ndata contract について語られているいくつかのブログ記事などを参考にしている。\nData Contract とは データの schema というのはナマモノで、いろいろな理由で変更されることがある。\nschema を変更する場合、その schema のデータ (table や log) が所属する単一のビジネス機能や application のドメインで行われることになる。\nそのドメインの閉じた世界で考える分にはこれで問題ないのだが、DWH や data lake など組織レベルのデータ基盤でデータを流通していた場合はその先のことも考えないといけなくなる。\nこのようにチームを超える影響というのは、ビジネス機能に責任を持っているチームからは見えにくくなっていることが多い。\n上流の application 側で schema を変更したら下流のデータ基盤の ETL 処理がぶっ壊れてしまった、というのはデータ基盤運用あるあるではないだろうか。\nというところを解決して平和に過ごせるようにすることが data contract の主なモチベーションだと思われる。\n\u0026ldquo;contract\u0026rdquo; は日本語で言うところの「契約」。\n組織におけるデータ流通において、データの送り手である producer 側と受け手である consumer 側との間で合意した契約を遵守することにより、前述のような問題を避けることができるというのが data contract である。\n組織内のデータの見通しがよくなったり、パイプラインを宣言的に開発することができるようになるというメリットもある。\nエンジニアにとっては Datafold のブログ記事の例を読むとイメージしやすいかもしれない。\nTo provide another analogy, data contracts are what API is for the web services. Say we want to get data from Twitter. One way is to scrape it by downloading and parsing the HTML of Twitter’s webpage. This may work, but our scraper will likely break occasionally, if Twitter, for instance, changes a name of a CSS class or HTML structure. There is no contract between Twitter’s web page and our scraper. However, if we access the same data via Twitter’s API, we know exactly the structure of the response we’re going to get. An API has required inputs, predictable outputs, error codes, SLAs (service level agreements – e.g. uptime), and terms of use, and other important properties. Importantly, API is also versioned which helps ensure that changes to the API won’t break end user’s applications, and to take advantage of those changes users would graciously migrate to the new version.\nThe Best Data Contract is the Pull Request - datafold.com\nTwitter から情報を取ることを考えたとき、scraping する場合は UI の変更などで処理が壊れてしまうがちゃんと定義された API を使う場合は処理が壊れない。\nこの違いは約束事があるかどうかであり、data contract がない場合は約束事がない scraping のようなものだよね、という話。\n最初の例ではデータ基盤に流れてくるデータの schema が変わることを示したが consumer は中央集権的なデータ基盤である必要はなく、どちらかというと data mesh のような非中央集権的なコンテキストで話題に上がる方が多い。\nまた契約の対象となるのは schema だけではなく、次のような様々な情報を含んでよい。\nschema 所有者 利用者 セマンティクス 更新頻度 etc. 扱っている情報としては data catalog に近かったりもするが、data catalog はどうなっているかを示しているのに対して data contract はどうなっているべきかを示すという違いがある。\nData Contract の概況 data contract についてはいろいろな人が語っているが、なんとなく現時点でコミュニティにおいてある程度合意がありそうなことをいくつか挙げる。\n議論はまだまだこれからという感じなので、これらは時間が経つと変わっていく可能性が高いことに注意。\nスタンダードになるようなサービスや製品はまだない まるっと data contract をやってくれるようなサービスや製品はまだないので、data contract をやりたければある程度自社で作り込んだりルール整備したりする必要がある。\nこのようなサービスや製品がまだない理由については The Case for Data Contracts - uncomfortablyidiosyncratic.substack.com で議論があったのでご興味のある方はご確認ください。\nSchemata という OSS の schema modeling のフレームワークがあるがそんなに広まっていない？\nschema metadata についてはわかるけど schema score がどう使われるのかがイメージつかない。\nschema は IDL で表現して data contract に含める data contract の対象の中ではデータの schema は重要視されている。\nschema は IDL (Interface Description Language) で記述される。\nIDL として代表的なものとして Protocol Buffer と Apache Avro が挙げられる。\nschema を記述するための規定の方法が open source で提供されており、これを使ってレコード単位のチェックを行うことができる。\n他にも JSON Schema もあるが、DB の世界との間で型の扱いに差があるので個人的にはおすすめできない。\n契約の形は様々 契約 (contract) の形は data contract を語る人によっていろいろあってこれが議論になっている模様。\nproducer 側が主体的な場合もあれば consumer 側が主体的な場合もある。\n比較的よく挙げられれているのが pull-request のレビューをもって契約の合意とするという方式。\ndata contract が規定の形式のドキュメントになっていれば、その追加や変更についての二者間の合意はエンジニアにとって慣れ親しんだ pull-request という形で行うことができる。\nThe Best Data Contract is the Pull Request - datafold.com の記事が正にこれについて書かれたものとなっている。\n後述の実装例でもいくつか紹介する。\nData Contract の実装例 そうは言うてもどないして data contract を実現するんや？ってなりますよね。\nというところでその例をいくつか以下に記載してみる。\n詳細はリンク先を参照。\n例 1 An Engineer\u0026rsquo;s Guide to Data Contracts - Pt. 1 - dataproducts.substac.com\nentity、つまり DB の table の変更を流通させる場合の例 Pt. 2 の記事では event log の流通について書かれている data contract は Protocol Buffers で記載 data contract は CI/CD でテストや互換性チェックが行われ、Schema Registry に登録される production 環境では table の変更が CDC で Kafka の topic に送られている Kafka に送られたレコードを KSQL や Flink のフレームワーク + Schema Registry でチェックして新たな topic に transactional outbox pattern についても配慮 例 2 Aurimas Griciūnas’ Post - LinkedIn\n別の例だが例 1 に近い やはり Kafka や Flink を利用 例 3 Implementing Data Contracts at GoCardless - Medium\nPub/Sub や BigQuery に入る前に data contract による validation が行われる データ所有者が Jsonnet で定義された data contract を Git で merge すると、必要なリソースやサービスが生成される 宣言的でかっこいい 契約の主体が producer 側、つまりデータ所有者に寄っている印象 所有権を意識させる 例 4 Fine, let\u0026rsquo;s talk about data contracts - benn.substack.com\ndbt を使った例 なのでバッチ処理的なチェック 契約の主体が consumer 側、つまりデータ基盤に寄っている印象 (data contract というよりもデータ品質テストに近い気が) 所感 私がお仕事で運用しているデータ基盤では、最初に挙げたあるあるのような上流のデータの問題で ETL が影響を受けるということが度々あり課題となっていた。\nなので data contract という概念には期待がある。\n前回の記事 では Glue Schema Registry の導入を諦めた旨を書いたが、data contract という文脈で再度 Glue Schema Registry の採用を検討してみてもいいかもしれない。\n一方、国内ではあまり data contract の事例を聞いたことがなく、この辺不思議に思っている。\n理由としては\nこのような課題が存在しない -\u0026gt; んなわけない 課題はあるが認識されていない -\u0026gt; あるかもしれない 認識されて data contract 的なことをしているが data contract という言葉が使われていない -\u0026gt; 某社の事例を噂で聞いたことがある などが考えられる。\n国内企業の事例も聞いてみたいところだし、自分のところで事例を出せるといいなとも思っている。\nあと CDC とかのリアルタイム処理はもう当たり前になってるんだな。\n参考 Data Contracts — From Zero To Hero The Best Data Contract is the Pull Request The Case for Data Contracts An Engineer\u0026rsquo;s Guide to Data Contracts - Pt. 1 Aurimas Griciūnas Post - LinkedIn Implementing Data Contracts at GoCardless Fine, let\u0026rsquo;s talk about data contracts ","permalink":"https://soonraah.github.io/posts/looked-into-data-contracts/","summary":"データエンジニアリングの領域で少し前から目にするようになった \u0026ldquo;data contract\u0026rdquo; という言葉。\nなんとなく今の業務で困っている課題の解決になりそうな気がしつつもよくわかっていなかったので調べてみた。\ndata contract について語られているいくつかのブログ記事などを参考にしている。\nData Contract とは データの schema というのはナマモノで、いろいろな理由で変更されることがある。\nschema を変更する場合、その schema のデータ (table や log) が所属する単一のビジネス機能や application のドメインで行われることになる。\nそのドメインの閉じた世界で考える分にはこれで問題ないのだが、DWH や data lake など組織レベルのデータ基盤でデータを流通していた場合はその先のことも考えないといけなくなる。\nこのようにチームを超える影響というのは、ビジネス機能に責任を持っているチームからは見えにくくなっていることが多い。\n上流の application 側で schema を変更したら下流のデータ基盤の ETL 処理がぶっ壊れてしまった、というのはデータ基盤運用あるあるではないだろうか。\nというところを解決して平和に過ごせるようにすることが data contract の主なモチベーションだと思われる。\n\u0026ldquo;contract\u0026rdquo; は日本語で言うところの「契約」。\n組織におけるデータ流通において、データの送り手である producer 側と受け手である consumer 側との間で合意した契約を遵守することにより、前述のような問題を避けることができるというのが data contract である。\n組織内のデータの見通しがよくなったり、パイプラインを宣言的に開発することができるようになるというメリットもある。\nエンジニアにとっては Datafold のブログ記事の例を読むとイメージしやすいかもしれない。\nTo provide another analogy, data contracts are what API is for the web services. Say we want to get data from Twitter.","title":"Data Contract について調べた"},{"content":"業務で AWS Glue Schema Registry を使おうとしたけど、やっぱりやめたというお話。\nGlue Schema Registry What\u0026rsquo;s Schema Registry? AWS Glue Schema Registry は2020年に発表された AWS の機能だ。\nControl the evolution of data streams using the AWS Glue Schema Registry 一方、私が最初に schema registry 的なものを見たのは Confluent の例。\nSchema Registry の概要 - Confluent AWS の Glue Schema Registry はこれより後のリリースであり、同等のものの AWS マネージド版といったところだろうか。\nschema registry で何ができるかは Confluent のリンク先の図がとてもわかりやすいので参考にしていただきたい。\nGlue Schema Registry もだいたい同じで、ストリーム処理のための機能である。\nGlue Schema Registry で解決したい課題とその機能 データ基盤上のストリーム処理における schema 管理はバッチ処理のそれとは異なる難しさがある。\nこれは schema evolution と呼ばれる問題で以前のポストでも述べている。\nバッチ処理おじさんがストリーム処理のシステムを開発するにあたって調べたこと 難しい点として以下のようなことが挙げられる。\n処理は常に行われており、producer の application 等をしばらく停止させない限りはオンラインで schema 変更をするしかない ただしデータ基盤起因で producer を止めることはなるべくしたくない オンラインの場合、out of orderness に配慮する必要がある deploy 順序、ネットワーク遅延など様々な要因から、新しい schema のレコードが古い schema のレコードより後に届くとはかぎらない consumer 側で新旧 schema に対する互換性 (compatibility) に配慮する必要がある Glue Schema Registry はこの課題に対応する。\nざっと説明すると次のようにして利用される。\nbroker (Apache Kafka, Amazon Kinesis Data Stremas, etc.) を挟んで producer と consumer の両方から参照できる場所にバージョン管理された schema 定義を配置する (これが Glue Schema Registry) producer からレコードを serialize して送信。レコードには schema のバージョン情報を含む consumer はレコードに含まれるバージョン情報と schema registry を参照してレコードの deserialize・解釈を行う これを実現するために Glue Schema Registry では次のような機能が提供されている。\nschema の登録と互換性ルールにもとづく schema のバージョン管理 producer, consumer で利用される SerDe ライブラリの提供 producer 側でのレコード検証、serialize、配信 consumer 側での deserialize これによりストリーム処理における producer と consumer の間で schema 変更を運用することが可能になる。\nやりたかったこと …という Glue Schema Registry が自分の業務でも使えるのではないか、そんなふうに考えていた時期が私にもありました。\n現在私は B to B to C のビジネスをやっている企業でデータ基盤の開発・運用を行うチームに所属している。\nその中で、ある application から発生する JSON 形式のログを near real time でデータ基盤に取り込み、分析したいという案件があった。\n以下のようなアーキテクチャを考えた。\napplication 側から SerDe ライブラリを使い、Kinesis Data Streams (KDS) に逐次的にレコードを配信 Spark Structured Streaming で Glue streaming ETL の job を実装し、KDS を購読してSerDe ライブラリ経由でデータ取得 Spark Structured Streaming から Apache Hudi の形式 (Merge on Read) で S3 へと保存 schema の変更がある場合は Glue Data Catalog 上の Hudi table の schema を変更 Athena でクエリ (ちなみに Glue Schema Registry のアイコンは AWS では提供されていない)\napplication はビジネスロジックに近いシステムを担当しているチームの管理であり、それ以降がデータ基盤チームの管理となっている。\nnear real time 以外でもデータソース側とデータ基盤側の間での schema 管理は以前から課題があり、この Glue Schema Registry を使えばうまくやる前例を作れるのではという期待があった。\n4 の schema 変更は自動で行われ、したがって application ログの schema と Hudi table の schema を別で管理・メンテする必要がない。\napplication 側とデータ基盤側の中間で schema を定義・管理できるのが素晴らしい、そんなふうに考えていた時期が私にもありました。\n断念した理由 が、結局は Glue Schema Registry の採用は断念するという結論になった。\n次のような理由にもとづく。\nSerDe 導入の壁 JSON Schema の折り合い 互換性の運用の難しさ 以下、それぞれについて述べる。\nSerDe 導入の壁 前述のようなアーキテクチャを実現するには、application 側のチームに SerDe ライブラリを導入してもらう必要がある。\nSerDe は Java のライブラリとして提供されているため、application 側が Java 系の言語で開発されている必要がある。\nここに大きなハードルがあると感じた。\nちなみに KDS まわりであれば SDK や KPL 経由で使うことも可能である。\n今回は application は Scala だったので、頼み込めば今回の案件については導入してもらうこともできたかもしれない。\nただ、うまくいけば今回の案件に限らず横展開したいと考えていたので、それを考えるとかなりハードルが高いという判断となった。\nJSON Schema の折り合い application とデータ基盤の間の schema 管理については data contract という考え方が出てきている。\nそれについて書かれたブログ記事 7 Lessons From GoCardless’ Implementation of Data Contracts で\nFor example, most teams didn’t want to use AVRO so we decided to use JSON as the interchange format for the contracts because it’s extensible.\nとあるように、Avro 形式を各 application チームに使ってもらうというのはまあまあハードルが高い。\nGlue Schema Registry ではレコードの形式として Avro, Protobuf, JSON をサポートしている。\n本来はできれば Avro や Protobuf のような、型がしっかりしている形式を使いたいところだが、このようなハードルがあり我々も JSON の仕様を前提に考えていた。\nJSON の場合は JSON Schema の仕様に従うことになる。\nこの JSON Schema、当たり前だが JSON の世界を表現するためのものとなっている。\n何が言いたいかというと JSON の世界の型の表現と Hudi table を含めた DB 的な世界の型の表現とが一致しないのだ。\n例えば数値表現を見てみると、JSON Schema においては integer, number, Multiples, Range の4つの型が定義されている。\n一般的な整数型 short, int, long 等の違いを表現することができない。\nGlue Schema Registry に登録された JSON Schema により Hudi の table の schema を自動更新したかったので、これは困ったぞ、となってしまった。\n互換性の運用の難しさ これはちょっと言葉にしにくいのだが、前方／後方互換性まわりの仕様が直感的に理解しにくかった。\nどういったときにフィールド追加が許容されるのか、互換性の起点となる checkpoint はどういったときに更新されるのか、etc.\nこのあたりいろいろ調査すれば理解できると思うのだが、ここまでの課題もあり気持ちが折れてしまった。\nまた、チームとしてこれを踏まえた schema 更新を運用するのはハードルが高いと感じた。\n結局どうしたか Glue Schema Registry は使わず、では結局どうしたかというと application 側とデータ基盤側の間でシステム的に schema を共有することなく連携する形にした。\nシステム的に共有しないので、人が共有してやるしかない。\nschema 変更する場合は次のようにチーム間で協調して進める。\nチーム間で認識合わせ データ基盤チームによる Glue Data Catalog の schema の変更作業 consumer に前方互換性があるという前提 application チームによる application ログの schema の変更作業 このあたりもっとうまくできる方法があるのではと考えている。\nとりあえず data contract の発展に期待しておく。\nまとめ ストリーム処理における Glue Schema Registry の思想、考え方は正しいものであると感じる。\nただし、扱いが難しい面があり、今回のユースケースやそれを踏まえた横展開には合わないという判断となった。\nチームやシステムをまたいだ schema 管理は簡単ではない。\n","permalink":"https://soonraah.github.io/posts/give-up-on-schema-registry/","summary":"業務で AWS Glue Schema Registry を使おうとしたけど、やっぱりやめたというお話。\nGlue Schema Registry What\u0026rsquo;s Schema Registry? AWS Glue Schema Registry は2020年に発表された AWS の機能だ。\nControl the evolution of data streams using the AWS Glue Schema Registry 一方、私が最初に schema registry 的なものを見たのは Confluent の例。\nSchema Registry の概要 - Confluent AWS の Glue Schema Registry はこれより後のリリースであり、同等のものの AWS マネージド版といったところだろうか。\nschema registry で何ができるかは Confluent のリンク先の図がとてもわかりやすいので参考にしていただきたい。\nGlue Schema Registry もだいたい同じで、ストリーム処理のための機能である。\nGlue Schema Registry で解決したい課題とその機能 データ基盤上のストリーム処理における schema 管理はバッチ処理のそれとは異なる難しさがある。\nこれは schema evolution と呼ばれる問題で以前のポストでも述べている。","title":"Glue Schema Registry の導入を断念した話"},{"content":"「データレイク」という言葉は使う人によって異なった意味があるように感じており、気になっていた。\nこのポストではアーキテクチャ目線でのデータレイクと内容物目線でのデータレイクの違いについて書いてみる。\n便宜上前者を「データレイク」、後者を「データレイク層」と呼ぶことにする。\nアーキテクチャ目線の「データレイク」 「データレイク」については以前こちらのポストで書いたのでここでは詳しく触れない。\n詳細はリンク先を見ていただきたい。\nここでキーとなるのが、\n加工前データや非構造化データを含むあらゆるデータを保存 一元的なデータ管理 という部分だ。\nあらゆるデータを一元的に管理するという思想であり、これができるアーキテクチャがデータレイクということだ。\n例えば AWS や Azure のドキュメントを見るとデータレイクの中が zone に分けられており、生データを保持する raw zone や加工されたデータを置いておく curated zone などがある。\n(zone の命名にもいくつかの流派があるようだ…)\nReference architecture - Data Analytics Lens Data lake zones and containers - Cloud Adoption Framework | Microsoft Docs 次の Robinhood 社の例でもデータレイク中に生データとその派生データが存在している。\nFresher Data Lake on AWS S3 | by Balaji Varadarajan | Robinhood 内容物目線の「データレイク層」 一方でデータレイクには生データのみを置くべき、という考えもある。\n本書におけるデータレイク（DataLake）層とは、元のデータをコピーして、1つのシステムに集約したものを指します。 データソース（＝水源）から流れてきたデータを蓄える場所なのでレイク（湖）と呼びます。\nECサイトの注文履歴データを、分析用DBにコピーしている場合、それがデータレイクと言えます。データレイクのデータは、データソースと一対一の関係にあります。何も加工していない、ただのコピーだからです。\n何も加工していない、ただのコピーであることが重要です。仮にデータの中身に誤りがあったとしても、修正や加工をせず、そのまま集約しましょう。\n\u0026ndash; ゆずたそ,渡部 徹太郎,伊藤 徹郎. 実践的データ基盤への処方箋〜 ビジネス価値創出のためのデータ・システム・ヒトのノウハウ (Japanese Edition) (pp.57-58). Kindle 版.\nこちらの書籍にならってこのポストではこれを「データレイク層」と呼ぶことにする。\nこの考え方では生データの「データレイク層」の他に加工されたデータを置くための「DWH 層」「データマート層」がある。\n本書においてはアーキテクチャというよりは中に何を入れるかで「データレイク層」が規定される。\nこの「データレイク層」の考えは日本企業でよく見かける気がしている。(が、私が知らないだけで海外事例もあるかもしれない)\n以下はその例。\n分析者から見た使いにくいデータ基盤の話 | リクルート　メンバーズブログ データの民主化を目指して ~ データ基盤ができるまで ~ - LIFULL Creators Blog 2024-02-04 追記 海外でも「データレイク層」の意味で data lake という言葉が使われているケースがある模様。\nSnowflake data zones : r/snowflake 「データレイク」と「データレイク層」の比較 「データレイク」も「データレイク層」もやっていることは同じで、ただ何を「データレイク (層)」と呼んでいるかが違っている。\nどちらも生データ、加工データ等を zone や層として分けて管理する。\n以下のようなニュアンスの違いがあると認識している。\nデータレイク データレイク層 内容物 生データ、加工データ 生データのみ アーキテクチャ オブジェクトストレージ等で一元的 データレイク層と DWH 層は別でもよい 使われている場所 海外 国内？ 個人的には「データレイク層」は「raw data 層」というような命名の方が混乱を避けつつ実を表しておりいいのではという感じがする。\n(つまり raw zone ですね…)\n一方で「データレイク層」と呼びたい気持ちもわかる。\nまとめ 誰かが「データレイク」についてしゃべっているときはどちらの「データレイク」のことを言っているのか、気をつけた方がよい。\nもっと言うと世の中にはまた別の定義もあるかもしれない。\n私がこれまで一緒に仕事をしたエンジニアの中でも優秀な人たちは言葉の定義に敏感な人が多かった。\nこういったところ気をつけていきたい。\n","permalink":"https://soonraah.github.io/posts/data-lake-and-data-lake-layer/","summary":"「データレイク」という言葉は使う人によって異なった意味があるように感じており、気になっていた。\nこのポストではアーキテクチャ目線でのデータレイクと内容物目線でのデータレイクの違いについて書いてみる。\n便宜上前者を「データレイク」、後者を「データレイク層」と呼ぶことにする。\nアーキテクチャ目線の「データレイク」 「データレイク」については以前こちらのポストで書いたのでここでは詳しく触れない。\n詳細はリンク先を見ていただきたい。\nここでキーとなるのが、\n加工前データや非構造化データを含むあらゆるデータを保存 一元的なデータ管理 という部分だ。\nあらゆるデータを一元的に管理するという思想であり、これができるアーキテクチャがデータレイクということだ。\n例えば AWS や Azure のドキュメントを見るとデータレイクの中が zone に分けられており、生データを保持する raw zone や加工されたデータを置いておく curated zone などがある。\n(zone の命名にもいくつかの流派があるようだ…)\nReference architecture - Data Analytics Lens Data lake zones and containers - Cloud Adoption Framework | Microsoft Docs 次の Robinhood 社の例でもデータレイク中に生データとその派生データが存在している。\nFresher Data Lake on AWS S3 | by Balaji Varadarajan | Robinhood 内容物目線の「データレイク層」 一方でデータレイクには生データのみを置くべき、という考えもある。\n本書におけるデータレイク（DataLake）層とは、元のデータをコピーして、1つのシステムに集約したものを指します。 データソース（＝水源）から流れてきたデータを蓄える場所なのでレイク（湖）と呼びます。\nECサイトの注文履歴データを、分析用DBにコピーしている場合、それがデータレイクと言えます。データレイクのデータは、データソースと一対一の関係にあります。何も加工していない、ただのコピーだからです。\n何も加工していない、ただのコピーであることが重要です。仮にデータの中身に誤りがあったとしても、修正や加工をせず、そのまま集約しましょう。\n\u0026ndash; ゆずたそ,渡部 徹太郎,伊藤 徹郎. 実践的データ基盤への処方箋〜 ビジネス価値創出のためのデータ・システム・ヒトのノウハウ (Japanese Edition) (pp.","title":"「データレイク」と「データレイク層」"},{"content":"ポエムです。\n事業フェーズごとのデータサイエンティストの役割 まずはこちらの発表。\n事業立ち上げにデータサイエンティストは必要なのか？ | CA BASE NEXT とても納得できる内容だった。\n一部抜き出して要約すると\n事業の立ち上げフェーズ データがまだなかったり、整備されていない状態 データサイエンスによる改善がしにくい 事業のグロースフェーズ 大規模なデータが使える状態 データサイエンスによる改善がやりやすい とのこと。異論はない。\nでは事業が立ち上がり、グロースが落ち着いたその後の成熟フェーズではどうなのだろうかという話。\n成熟フェーズにおける改善の難しさ 端的に言うと成熟フェーズでは ML によるさらなる改善は困難になってくると思う。\nここで言う成熟フェーズにおいてはプロダクトの進化とともに機械学習もそれなりに適用されてきたものとする。\n成熟フェーズということで既存の ML モデル、特にビジネスインパクトが大きい箇所はこれまでいろいろな改善が重ねられてきている。\nそのモデルの精度をさらに上げるとなると、より高度なアルゴリズム、より複雑なデータ等を扱う必要がある。\nしかし技術的によっぽど大きなブレークスルーがない限りは精度の改善幅はグロースフェーズよりもかなり小さいものとなるだろう。\n精度が上がれば上がるほど、次の1%を上げるためのコストは大きくなっていく。\n改善が進むほどに次の改善業務は困難になっていく。\n(蛇足だがある程度大きな組織でなければ高度で state-of-the-art な ML アルゴリズムは運用しない方がいいと考えている)\nでは既存ではない新しい適用箇所に ML を使えばいいのではとなるかもしれない。\nしかしやはりそれも難しい。\nビジネスインパクトが大きく、かつわかりやすい適用箇所にはおそらくすでに ML が適用されているからだ。\nその状態から更によい適用箇所を見つけるには深いドメイン知識が必要になったりする。\nという感じでいわゆるキラキラした「ML でビジネスをドライブ！」みたいなことは成熟フェーズでは難しいことが多いのではないか。\nしかしデータサイエンティストにやることがないわけではない。\n成熟フェーズで何ができるか ぱっと思いつくのは次のような仕事。\nデータドリブンな施策の立案・評価 これは事業フェーズ問わずあるべき ドメイン知識が必要 ML エンジニアリング パイプラインの改善や属人性をなくすお仕事 ML モデルの受動的なメンテナンス 精度が変化したときの調査 内部的・外部的要因によるデータの変化への対応 やっぱり ML モデルの精度改善 成熟フェーズということでビジネスもスケールしていれば 0.1% の精度改善でも売上的なインパクトは大きいかもしれない いわゆる狭義のデータサイエンスではなく、ドメイン知識であったりアナリストやエンジニア的な視点が絡んだ仕事が増えてくる。\nよくある「ML だけじゃなく◯◯もできると強いよね」みたいな話になってしまった。\nおわりに …という話が少し前に Twitter で知人との話題に上がった。\n若者が歴史的にいろんな人が改善に取り組んできた ML モデルの改善にアサインされている、というのが近いところで観測されたのでたいへんそうだなあと思いつつこの件を思い出したので書いてみた。\nここまで書いて思ったけど、成熟フェーズでキラキラしたことがやりづらくなるのはデータサイエンティストだけじゃないよな。\nでも狭義のデータサイエンスのスキルは特に、事業の存続期間と比べて大きく貢献できる期間が短いのかもしれない、と個人的には考えている。\n成熟フェーズでは高度なスキル、高い賃金に見合ったプロダクトへの貢献が得にくくなっていくのではないだろうろうか。\nでも反論もいっぱいありそうな気はします。\n「ML でビジネスインパクトどっかんどっかんやで！！」みたいな仕事をしたい人はそれなりの頻度で事業部を移る or 転職するというのが賢い動きになるんでしょうかね。\n","permalink":"https://soonraah.github.io/posts/ds-in-maturation-phase/","summary":"ポエムです。\n事業フェーズごとのデータサイエンティストの役割 まずはこちらの発表。\n事業立ち上げにデータサイエンティストは必要なのか？ | CA BASE NEXT とても納得できる内容だった。\n一部抜き出して要約すると\n事業の立ち上げフェーズ データがまだなかったり、整備されていない状態 データサイエンスによる改善がしにくい 事業のグロースフェーズ 大規模なデータが使える状態 データサイエンスによる改善がやりやすい とのこと。異論はない。\nでは事業が立ち上がり、グロースが落ち着いたその後の成熟フェーズではどうなのだろうかという話。\n成熟フェーズにおける改善の難しさ 端的に言うと成熟フェーズでは ML によるさらなる改善は困難になってくると思う。\nここで言う成熟フェーズにおいてはプロダクトの進化とともに機械学習もそれなりに適用されてきたものとする。\n成熟フェーズということで既存の ML モデル、特にビジネスインパクトが大きい箇所はこれまでいろいろな改善が重ねられてきている。\nそのモデルの精度をさらに上げるとなると、より高度なアルゴリズム、より複雑なデータ等を扱う必要がある。\nしかし技術的によっぽど大きなブレークスルーがない限りは精度の改善幅はグロースフェーズよりもかなり小さいものとなるだろう。\n精度が上がれば上がるほど、次の1%を上げるためのコストは大きくなっていく。\n改善が進むほどに次の改善業務は困難になっていく。\n(蛇足だがある程度大きな組織でなければ高度で state-of-the-art な ML アルゴリズムは運用しない方がいいと考えている)\nでは既存ではない新しい適用箇所に ML を使えばいいのではとなるかもしれない。\nしかしやはりそれも難しい。\nビジネスインパクトが大きく、かつわかりやすい適用箇所にはおそらくすでに ML が適用されているからだ。\nその状態から更によい適用箇所を見つけるには深いドメイン知識が必要になったりする。\nという感じでいわゆるキラキラした「ML でビジネスをドライブ！」みたいなことは成熟フェーズでは難しいことが多いのではないか。\nしかしデータサイエンティストにやることがないわけではない。\n成熟フェーズで何ができるか ぱっと思いつくのは次のような仕事。\nデータドリブンな施策の立案・評価 これは事業フェーズ問わずあるべき ドメイン知識が必要 ML エンジニアリング パイプラインの改善や属人性をなくすお仕事 ML モデルの受動的なメンテナンス 精度が変化したときの調査 内部的・外部的要因によるデータの変化への対応 やっぱり ML モデルの精度改善 成熟フェーズということでビジネスもスケールしていれば 0.1% の精度改善でも売上的なインパクトは大きいかもしれない いわゆる狭義のデータサイエンスではなく、ドメイン知識であったりアナリストやエンジニア的な視点が絡んだ仕事が増えてくる。\nよくある「ML だけじゃなく◯◯もできると強いよね」みたいな話になってしまった。\nおわりに …という話が少し前に Twitter で知人との話題に上がった。","title":"成熟フェーズの事業におけるデータサイエンティスト"},{"content":"ストリーム処理のフレームワークが備える backpressure という機能がある。\nこのポストでは Apache Flink の backpressure について調べたことを記載する。\nBackpressure の目的 backpressure はストリーム処理システムにおける負荷管理の仕組みの一つ。\n一時的な入力データ量の増大に対応する。\nインターネットユーザの行動履歴やセンサーデータなどは常に一定量のデータが流れているわけではなく、単位時間あたりのデータ量は常に変動している。\n一時的にスパイクしてデータ量が増大するようなことも起こりうる。\n複数の operator からなる dataflow graph により構成されるストリーム処理システムにおいては、処理スピードのボトルネックとなる operator が存在する。\n一時的に入力データ量が増えてボトルネックの operator の処理速度を上回ってしまった場合に、データの取りこぼしが発生するのを防ぐのが backpressure の目的となる。\nBackpressure の仕組み Buffer-based ここでは以前のブログでも紹介した、ストリーム処理で必要とされる機能について書かれた Fragkoulis et al. 1 を引用して一般論としての backpressure について述べたい。\n上流／下流の operator をそれぞれ producer, consumer とする。\nproducer, consumer (それらの subtask と言ってもいいかも) がそれぞれ異なる物理マシンに deploy されているケースが Figure 12b となる。\n各 subtask は input と output の buffer を持っており、\nproducer は処理結果を output buffer に書き出す TCP 等の物理的な接続でデータを送信 consumer 側の output buffer にデータを格納 consumer がそれを読み込んで処理する というような流れになる。\nbuffer はマシンごとの buffer pool で管理されており、input/output で buffer が必要となった場合はこの buffer pool に buffer が要求される。\nここで赤い丸で示されている subtask の処理速度が入力データの速度よりも遅かったとする。\nconsumer 側の input buffer の待機列が長くなり、さらにこの状況が続くとやがて buffer pool の buffer を使い果たす。\nすると producer 側から新しいデータを送信することができなくなり、producer 側の output buffer を使い始める。\n同様に producer 側でも output buffer を追加できなくなると producer は処理を待たざるを得なくなる。\nこのようにボトルネックとなる operator から上流に向かって buffer が埋まっていくことになる。\nこれが backpressure だ。\ndataflow graph を構成する operator 全体、物理的にはそれらのマシンのメモリにより一時的なデータ量の増加を buffer するという形になる。\nFigure 2a は producer と consumer が同じマシンにある場合の例であり、この場合はネットワークを介さずに buffer 上でやりとりができる。\nボトルネックがあれば同様に buffer を使い切り、それが上流に向かって伝播していくことになる。\nCredit-based 上記のような buffer-based な流量制御の場合、複数の channel が同じ下流のマシンにデータを送信する場合、同じ TCP socket を使うことになる。\n下流のある一部の channel が遅延して backpressure が働くと (data skew) 上流のすべての channel が影響を受けるという問題がある。\nこれを解決するのが credit-based な流量制御であり、Figure 13 はそれを示したものである。\nデータ送信を試みる前に credit という形で consumer 側から producer 側に buffer 状況を送信する。\nある channel で consumer の buffer がなくなると credit=0 となり、producer 側でその channel に送信できなるなり backpressure が発生する。\n一方、並列する他の channel には backpressure はかからず、TCP socket は利用可能となっている。\nFlink の Backpressure 残念ながら Flink 公式のドキュメントには backpressure についてあまり詳しく説明されていない。\nモニタリングについて書かれているのみである。\nMonitoring Back Pressure backpressure が起こっているかどうかは web UI 上から確認できるとのことだ。\n一方で Flink のブログや Alibaba のブログ等では内部的な挙動が詳しく書かれている。\nAnalysis of Network Flow Control and Back Pressure: Flink Advanced Tutorials A Deep-Dive into Flink\u0026rsquo;s Network Stack 前述のように buffer-based な仕組みの上に credit-based な挙動が採用されていることがわかる。\nここで Flink の設定の中で backpressure に影響がありそうなものを見ておく。\ntaskmanager.network.memory.buffers-per-channel こちらは channel 単位の排他的な buffer の数を指定する。\nskew 発生時において、この値が大きすぎると遅延している channel 以外で buffer が遊ぶことになり、逆に小さすぎると遅延していない channel でも処理が滞りやすくなると考えられる。\ntaskmanager.network.memory.floating-buffers-per-gate すべての input channel で共有される floating buffer の数。\nこの部分で skew をある程度吸収しようとするのだろう。\ntaskmanager.network.memory.max-buffers-per-channel channel ごとの最大 buffer 数。\n最大 buffer 数を制限することにより skew 時に backpressure が起こりやすくなり、結果として checkpoint のアラインメントを速くする効果があるとのこと。\n最大 buffer 数の制限がゆるいとボトルネックの operator で長い待ち行列を待つ必要があり、checkpoint barrier が移動するのに時間がかかってしまうということだろうか。\nweb.backpressure.cleanup-interval web.backpressure.delay-between-samples web.backpressure.num-samples web.backpressure.refresh-interval 上の4つは web UI によるモニタリング関連の設定値であり、backpressure 関連の挙動に直接影響を与えるものではない。\n…と書いてみたものの通常はこれらの値をチューニングすることはあまりないのではという印象。\nまとめ Flink の backpressure がどのように働くかがだいたい概観できた。\nそもそもなぜ backpressure を調べたかというと今開発している Flink アプリケーションで checkpoint size が増大し続ける問題があって backpressure の影響を疑っていた。\n結局 backpressure は関係なさそうだな…\nFragkoulis, M., Carbone, P., Kalavri, V., \u0026amp; Katsifodimos, A. (2020). A Survey on the Evolution of Stream Processing Systems.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://soonraah.github.io/posts/backpressure-for-flink/","summary":"ストリーム処理のフレームワークが備える backpressure という機能がある。\nこのポストでは Apache Flink の backpressure について調べたことを記載する。\nBackpressure の目的 backpressure はストリーム処理システムにおける負荷管理の仕組みの一つ。\n一時的な入力データ量の増大に対応する。\nインターネットユーザの行動履歴やセンサーデータなどは常に一定量のデータが流れているわけではなく、単位時間あたりのデータ量は常に変動している。\n一時的にスパイクしてデータ量が増大するようなことも起こりうる。\n複数の operator からなる dataflow graph により構成されるストリーム処理システムにおいては、処理スピードのボトルネックとなる operator が存在する。\n一時的に入力データ量が増えてボトルネックの operator の処理速度を上回ってしまった場合に、データの取りこぼしが発生するのを防ぐのが backpressure の目的となる。\nBackpressure の仕組み Buffer-based ここでは以前のブログでも紹介した、ストリーム処理で必要とされる機能について書かれた Fragkoulis et al. 1 を引用して一般論としての backpressure について述べたい。\n上流／下流の operator をそれぞれ producer, consumer とする。\nproducer, consumer (それらの subtask と言ってもいいかも) がそれぞれ異なる物理マシンに deploy されているケースが Figure 12b となる。\n各 subtask は input と output の buffer を持っており、\nproducer は処理結果を output buffer に書き出す TCP 等の物理的な接続でデータを送信 consumer 側の output buffer にデータを格納 consumer がそれを読み込んで処理する というような流れになる。","title":"Apache Flink の Backpressure の仕組みについて調べた"},{"content":"はじめに 前回のポストではデータレイクとはどういうものかというのを調べた。\n今回はデータレイクの文脈でどのような OSS が注目されているのかを見ていきたい。\n以下は NTT データさんによる講演資料であり、その中で「近年登場してきた、リアルタイム分析に利用可能なOSSストレージレイヤソフト」というのが3つ挙げられている。\n大規模データ活用向けストレージレイヤソフトのこれまでとこれから（NTTデータ テクノロジーカンファレンス 2019 講演資料、2019/09/05） from NTT DATA Technology \u0026amp; Innovation Delta Lake Apache Hudi Apache Kudu これらはすべて論理的なストレージレイヤーを担う。\nこちらの講演資料に付け足すようなこともないかもしれないが、このポストではデータレイクという文脈から自分で調べて理解した内容をまとめるということを目的にする。\n当然 Hadoop, Hive, Spark 等もデータレイクの文脈において超重要だが、「データレイク」という言葉がよく聞かれるようになる前から普及していたのでこのポストでは触れないことにする。\nDelta Lake https://delta.io/\nDelta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.\nDelta Lake\nDelta Lake は Apache Spark の読み書きに ACID な transaction を提供するストレージレイヤーの OSS である。\nDatabricks が作り、2019年4月に v0.1.0 がリリースされたのが最初だ。\n使い方はめちゃ簡単で、dependency を設定した上で Spark で\ndataframe .write .format(\u0026#34;delta\u0026#34;) .save(\u0026#34;/data\u0026#34;) のように読み書きすればよい。\nデータレイク化が進むと多種多様なデータが一元的に管理され、それらデータに対して横断的なクエリを実行できるようになる。\n各データの更新タイミングも様々であり、そのような状況では ACID 特性の中でも特に Isolation (独立性) が問題となってくる。\nSpark を処理エンジンとして使う場合、データソース・読み方によっては isolation level が弱くなってしまうことがあるというのは過去のポストでも述べた。\nおそらくこのことが Delta Lake の開発の強い動機となっているのではないだろうか。\nDelta Lake は最も強い isolation level である \u0026ldquo;serializability\u0026rdquo; を提供する。\nACID transaction の他には schema に合わないデータを弾いたり過去のデータのスナップショットにアクセスしたりなどの機能がある。\nどれもデータレイクの治安を守る方向であり、データスワンプ化に抵抗したいようだ。\nこれらを実現しているのが transaction log という仕組みとのこと。\nDelta Lake は table に対する変更の transaction を atomic な commit という単位に分け、commit ごとに操作を JSON file に書き出していく。\nJSON file には 000000.json, 000001.json のように連番が振られており、2つの application が同時に table を更新するような場合は各 application が次の番号の JSON file を作れるかどうかで衝突を制御している。\nJSON file はずっと蓄積していくため再計算のコストが大きくなっていくが、その時点での table の完全な状態を parquet にした checkpoint file というものを時折出力するという、compaction のようなことも行われる。\n(Delta Lake は parquet 形式のデータ保存を前提としている)\n詳しくは Databricks の blog を参照。\nDiving Into Delta Lake: Unpacking The Transaction Log Apache Hudi https://hudi.apache.org/\nHudi brings stream processing to big data, providing fresh data while being an order of magnitude efficient over traditional batch processing.\nApache Hudi\nApache Hudi で何ができるかを一言で説明するのは難しい。\n簡単にまとめると HDFS や S3 等にある table にリアルタイムに近いデータ取り込みと、処理速度とデータの新鮮さのトレードオフに配慮した読み込みを提供する。\n2016年から Uber が開発をしており、2020年に Apache Software Fundation の top-level project となった。\nデータレイクではあらゆる種類のデータを一元管理するが、その中には当然リアルタイム性の高いデータも含むことになる。\nデータレイク中のリアルタイム性の高いデータについては次のような要求が出てくる。\ntable へのデータの取り込みはリアルタイムに近いスピードで細かくやりたい 分析クエリ等の table の読み込みにおけるクエリの速度は速くしたい (例えば列指向形式で保存されたデータの様に) 取り込まれたデータをすぐに読めるようにしたい Hudi はこれに応えるものとなっている。\nこれらを実現するのが Merge On Read のデータ構造だ。\nHudi の table には Copy On Write と Merge On Read の2種類がある。\nここでは Hudi の肝である後者について触れておきたい。\nApache Hudi Merge On Read TablePermalink\ntable に加えられた変更についての情報は timeline に追加される。\n時系列になった変更についてのメタデータのようなものだろうか。\nこの timeline によって snapshot isolation が保証される。\n最初にデータが追加されたときは parquet 等の列指向のフォーマットで保存される。\nその一方でその後のデータの追加・更新については Avro 等の行指向のフォーマットの delta log に記載される。\nここがミソであり、列指向のフォーマットでは1レコードずつなどの細かい追加・更新が高コストになるのでその部分を行指向の delta log にまかせている。\n追加・更新が増えてくると delta log がどんどん長くなってしまうので、あるタイミングで compaction を行って直近までの delta log の変更内容を反映した列指向ファイルを作成する。\nこれを読む方法は2通りある。\nSnapshot Queries では読み取り時に列指向ファイルと行指向の delta log をどちらも読んで merge して最新の結果を返す。\n(これが \u0026ldquo;Merge On Read\u0026rdquo; ということ)\n一方で Read Optimized Queries では直近で compaction が行われた時点での列指向ファイルだけを読み、その時点の結果を返す。\nつまりデータの新鮮さが重要な場合は Snapshot Queries, データの新鮮さよりもクエリの速度を優先したい場合は Read Optimized Queries が有利ということだ。\nこれらはトレードオフとなるので状況に応じて使いわけることになる。\n2通りと述べたが実際はもう1つ、 Incremental Queries というものもある。\nこれはある時点からの差分のみを読み出して処理するというものとなっている。\nevent time と processing time の差があるものを DFS 上に書き出すのに適している。\nちなみに Merge On Read ではないもう1つの table type である Copy On Write は、Merge On Read の構造から delta log を消したものとなっている。\nすなわち書き込み時に常に列指向ファイルの更新がおこり、新しく作り直される。(という意味で \u0026ldquo;Copy On Write\u0026rdquo;)\n書き込みのたびに常に compaction が発生していると言ってもよい。\n更新頻度が低いデータならこちらの table type を使うのが適しているだろう。\nHudi の table は Spark, Hive, Presto 等からクエリすることができる。\n公式ドキュメント的には Spark 推しの感がある。\nApache Kudu https://kudu.apache.org/\nApache Kudu is an open source distributed data storage engine that makes fast analytics on fast and changing data easy.\nApache Kudu はストリーム処理などの追加・更新の速いデータをすぐに分析できるようにすることを目的としている。\nCloudera 社の内部プロジェクトとして始まり、2016年から Apache Software Foundation の top-level project となった。\nKudu の目指すところは Hudi とよく似ているが、次の点で異なっている。\nKudu は OLTP つまり小さなデータアクセスを大量にさばくのにも向いている Kudu は Hudi の incremental queries のようなことはできない Kudu は HDFS や S3 のような cloud storage 上にデータを持つのでははく、Raft の合意で制御された独自のサーバー群を要する Kudu では各 table のデータが tablet という単位により構成される。\ntablet はいわゆる partition によく似た概念となっており、key の範囲による分割、ハッシュ値による分割、またはその組み合わせにより分割される。\n1つの tablet は複数の tablet server に replication されており、そのうちの1つが leader として振る舞い書き込みを受け付ける。\nleader と follower の関係は Raft 合意アルゴリズム により管理される。\n一方で master server では tablet のメタデータ等が管理されており、client はまず master と通信することになる。\nApache Kudu Architectural Overview\n読み書きに関する内部的な振る舞いについては Cloudera 社のブログ記事 (日本語訳) が参考になる。\nCLOUDERA Blog Apache Kudu Read \u0026amp; Write Paths\nclient 側からはおそらく見えないが、内部的には\nメモリ上の MemRowSet, DeltaMemStore 列指向の base data file 差分を表す delta file (UNDO/REDO records) の3段の構成になっている。 (それと WAL も)\ndelta file を使うのは Hudi 等と同じだが一度メモリ上で変更を受けるという段があるのが特徴的だ。\n挿入はある tablet のメモリ上の MemRowSet にまず追加される。\nまた任意の timestamp の snapshot を得るために、MemRowSet 上のデータへの更新・削除はの差分は REDO records へと保存される。\nMemRowSet がいっぱいになると最新の状態が列指向の base data file へ書き出され、更新・削除前の状態は UNDO records へと書き出される。\n読み取りのときは MemRowSet とディスク上の base data file + delta file をスキャンすることになる。\nしたがって delta file の数やサイズが大きくなると遅くなる。\nやはりここでも compaction が必要となってくる。\nこのように memory を使うため server が必要であり、データレイクでよく言われるコンピューティングとストレージの分離が完全にはできない。\nCloudera 的と言えるかもしれない。\n読み取りには2つのモードがある。\nデフォルトは READ_LATEST であり、名前のとおり snapshot をとってすぐにデータを読むとのこと。\nREAD_LATEST は比較的弱い read committed の isolation level を示す。\nこれはおそらく Raft や WAL を経て変更が可視になるまでに時間を要するためだ。\nread committed は実用では問題が起こることもある。(ex. Spark クエリの分離レベル)\nもう一つは READ_AT_SNAPSHOT であり、明示的 (推奨) または暗黙的に読み取る対象の timestamp を指定する。\n書き込みの operation が完了し、その timestamp までの変更が安全に読めるようになるまで待って結果を返すことになる。\nisolation level はおそらく最も強い serializable となっている。\nしたがって2つのモードはデータの新鮮さと isolation level (consistency も？) のトレードオフとなっている。\nまとめ Delta Lake, Apache Hudi, Apache Kudu の3つを見比べて見てとても面白いのは、課題感は少しずつ違っているのにどれも列指向ファイル + 差分ファイル (delta file) というアーキテクチャを中心に置いているということだ。\nDelta Lake では transaction を重視している一方で Hudi ではリアルタイムデータをすぐに分析することを目指し、かつ Kudu ではさらに OLTP もサポートする。\nおそらく導入は Delta Lake が最も簡単であり、Kudu に至っては server を用意する必要があるのでハードルが1段高い。\n同じアーキテクチャということもあり、例えば time travel の機能などは共通して提供されている。\nバランス的には Apache Hudi がよさそうだが、どれを使うべきかは work load 次第だろう。\nhourly のデータ更新に慣れすぎていて fast data - fast analysis の需要に気づけないといこともよくありそう。\n","permalink":"https://soonraah.github.io/posts/oss-for-data-lake/","summary":"はじめに 前回のポストではデータレイクとはどういうものかというのを調べた。\n今回はデータレイクの文脈でどのような OSS が注目されているのかを見ていきたい。\n以下は NTT データさんによる講演資料であり、その中で「近年登場してきた、リアルタイム分析に利用可能なOSSストレージレイヤソフト」というのが3つ挙げられている。\n大規模データ活用向けストレージレイヤソフトのこれまでとこれから（NTTデータ テクノロジーカンファレンス 2019 講演資料、2019/09/05） from NTT DATA Technology \u0026amp; Innovation Delta Lake Apache Hudi Apache Kudu これらはすべて論理的なストレージレイヤーを担う。\nこちらの講演資料に付け足すようなこともないかもしれないが、このポストではデータレイクという文脈から自分で調べて理解した内容をまとめるということを目的にする。\n当然 Hadoop, Hive, Spark 等もデータレイクの文脈において超重要だが、「データレイク」という言葉がよく聞かれるようになる前から普及していたのでこのポストでは触れないことにする。\nDelta Lake https://delta.io/\nDelta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.\nDelta Lake\nDelta Lake は Apache Spark の読み書きに ACID な transaction を提供するストレージレイヤーの OSS である。\nDatabricks が作り、2019年4月に v0.1.0 がリリースされたのが最初だ。","title":"データレイク関連の OSS - Delta Lake, Apache Hudi, Apache Kudu"},{"content":"最近よく聞かれるようになった「データレイク」という概念にあまりついていけていなかったため、いまさらながらざっと調べてみた。\nデータレイクとは Wikipedia によると最初にこの言葉を使ったのは Pentaho 社の CTO である James Dixon らしい。\nその時の彼のブログ (10年前…) を読むと、既にあったデータマートに対して\nOnly a subset of the attributes are examined, so only pre-determined questions can be answered. The data is aggregated so visibility into the lowest levels is lost\n\u0026ndash;Pentaho, Hadoop, and Data Lakes - James Dixon’s Blog というような問題意識からデータレイクというコンセプトを提案したようだ。\n最近？のデータレイクについてはベンダー等の記事が参考になる。\nデータレイクとは - AWS データレイクとは？ - talend データレイクとは？データレイクの落とし穴と効果 - Informatica 書籍だと『AWSではじめるデータレイク: クラウドによる統合型データリポジトリ構築入門』がいいだろうか。\nデータレイクの概要と AWS が考えている構築・運用がざっとわかる。\nAmazon で検索した限りだと現時点でタイトルに「データレイク」を含む和書はこれのみだった。\n上に挙げたような文献ではデータレイクはデータウェアハウスとの対比という形でよく語られている。\n共通している内容は概ね以下のとおり。\n加工前データや非構造化データを含むあらゆるデータを保存 データウェアハウスでは加工され構造化されたデータのみを含む データレイクでは加工前の半構造化、非構造化データも含む ex. ログ、画像、音声 Scheme on Read 書き込み時にデータの構造を決める (Scheme on Write) のではなく、使用時に決める なので詳細なスキーマ設計なしに様々なデータを置いていくことができる 一元的なデータ管理 データがサイロ化しないよう、組織全体のデータを一元的に管理 なのでデータへのアクセス権の管理が重要になる 多様な分析用途に対応 データウェアハウスはビジネスアナリストが決まったレポートを出すために使われる データレイクでは機械学習など更に高度な分析をデータサイエンティストが行う 機械学習の普及がデータレイクを強く後押ししているように思う。\n機械学習をやっていれば様々な特徴量を扱いたいというのは自然な欲求であり、データレイクはそれを実現する。\nまた、クラウドベンダーは以下のような点も強調している。\n従量課金のクラウドストレージによるメリット 運用開始前の時点でどんな生データがどれだけ来るか、見積もるのはとても難しい 従量課金のクラウドストレージ (Amazon S3, Google Cloud Storage, etc.) なら必要なときに必要なサイズだけ追加できる 安価なクラウドストレージの普及がデータレイクを後押し ストレージとコンピューティングの分離 処理側のリソースを処理内容に応じて確保できる 処理側のバージョンアップや変更が容易 (オンプレ Hadoop クラスタのバージョンアップの辛さを思い出してください) なるほどです。\n沼にはまらないために データレイクとの対比でデータスワンプ (沼) という言葉がある。\n以下の説明がわかりやすいだろうか。\nデータスワンプ（Data Swamp）とは、データの沼地（Swamp）という意味です。これの対比語としてデータレイク（Data Lake：データの湖）があります。沼には、いろんな魚が住んでいるかもしれませんが、水が濁っているため、どこにどんな魚がいるか全く見えません。また、全く見えないため「魚が住んでいないんじゃないか」とも思い、魚を捕るのも諦めてしまいがちです。その一方で、湖は、水が澄んでいるため、魚を見ることができ「おっ！魚がいるな。何とか捕まえてみよう」と思えます。\nこの沼と湖にいる魚を、データの例えとして使っているのが、データスワンプと、データレイクという言葉です。\n\u0026ndash; データスワンプ - Realize\nデータスワンプにならないようにということについては次の資料が参考になる。\nThe difference between a data swamp and a data lake? 5 signs Metadata Separates Data Lakes From Data Swamps From Data-Swamp to Data-Lake on AWS (Part 1) すごくざっくりまとめるとデータレイクの構築・運用に当たって次の点に配慮するとよい。\nメタデータ、カタログの整備 どんなに優れたデータレイクを構築したとしても、利用者がどこに何のデータがあるかわからないと意味がない 多種のデータ資産を一元的に管理するデータレイクにおいて目的のデータを見つけるために必要な機能 データガバナンス 誰がどのデータに対して何ができるのか 統制をきつくしすぎると自由度が減るため、これらのバランスを考慮しないといけない その他、常識的な運用 クラウドベンダーが推進するデータレイク クラウドベンダーがどのようなデータレイクを考えているかというのも見ておく。\nGCP Google Cloud データレイクとしての Cloud Storage\nデータレイクとしての Cloud Storage というドキュメントに GCP が考えるデータレイクの構成が記載されている。\n構成としては以下のとおり。\n保存: Cloud Storage 変換: Cloud Dataproc, Cloud Dataprep, Cloud Dataflow 分析: BigQuery, Cloud ML Engine, Cloud Datalab, etc. データ保存のストレージとして、みんな大好きな BigQuery ではなく Cloud Storage を想定しているのは非構造化データを扱うためだろう。\nいわゆるリレーショナル的なものより Cloud Storage や S3 のようなオブジェクトストレージの方がこの要件に適している。\nもちろん Cloud Storage に保存された生データを処理してから BigQuery のデータウェアハウスに再びつっこむのも悪くない。\n変換や分析には様々なサービスが使えるし、今後も幅は広がっていくと考えられる。\nクラウドにおけるストレージとコンピューティングの分離の恩恵と言えるだろう。\n現時点ではこのドキュメントにはカタログ化についての記述はないが、GCP には Data Catalog というサービスがあり Cloud Storage のメタデータを扱えるようだ。\nAWS AWS Sample AWS data lake platform\n前述の書籍『AWSではじめるデータレイク: クラウドによる統合型データリポジトリ構築入門』に加え、Building Big Data Storage Solutions (Data Lakes) for Maximum Flexibility も読んでおくとよい。\n保存: Amazon S3 (, AWS Glue Data Catalog) 変換: AWS Glue ETL, Amazon EMR, AWS Lambda, etc. 分析: Amazon Athena, Amazon Redshift Spectrum, Amazon ML, etc. やはり中心となるのは S3 だ。\nGCP も AWS もストレージのコストメリットを推している。\nGlue Data Catalog がメタデータを管理する。\nGCP 同様、変換・分析には様々なオプションがある。\nBuilding Big Data Storage Solutions (Data Lakes) for Maximum Flexibility には記載がないのだが、AWS Lake Formation というサービスもある。\n上に挙げたようなサービスの上に一枚被せて、データレイクとして運用しやすくするといったようなものらしい。\n特に権限管理的な意味合いが強いように思う。\n組織のデータを一元的に管理するデータレイクには様々な部署・役割の人からのアクセスがある。\nどのデータを誰が扱えるのか、複数のサービスを横断して IAM で権限を管理のはかなりきつそうな印象でそういうのを楽にしてくれるのだろう。\nまとめ データレイクの概観やベンダーが考えていることがわかった。\n普段業務で使っているということもあり、全体的に AWS 寄りになってしまったように思うのでその点をご留意していただきたい。\nこの後はデータレイク関連の OSS について調べておきたいところ。\n","permalink":"https://soonraah.github.io/posts/what-is-a-data-lake/","summary":"最近よく聞かれるようになった「データレイク」という概念にあまりついていけていなかったため、いまさらながらざっと調べてみた。\nデータレイクとは Wikipedia によると最初にこの言葉を使ったのは Pentaho 社の CTO である James Dixon らしい。\nその時の彼のブログ (10年前…) を読むと、既にあったデータマートに対して\nOnly a subset of the attributes are examined, so only pre-determined questions can be answered. The data is aggregated so visibility into the lowest levels is lost\n\u0026ndash;Pentaho, Hadoop, and Data Lakes - James Dixon’s Blog というような問題意識からデータレイクというコンセプトを提案したようだ。\n最近？のデータレイクについてはベンダー等の記事が参考になる。\nデータレイクとは - AWS データレイクとは？ - talend データレイクとは？データレイクの落とし穴と効果 - Informatica 書籍だと『AWSではじめるデータレイク: クラウドによる統合型データリポジトリ構築入門』がいいだろうか。\nデータレイクの概要と AWS が考えている構築・運用がざっとわかる。\nAmazon で検索した限りだと現時点でタイトルに「データレイク」を含む和書はこれのみだった。\n上に挙げたような文献ではデータレイクはデータウェアハウスとの対比という形でよく語られている。\n共通している内容は概ね以下のとおり。","title":"いまさらながらのデータレイク"},{"content":"ストリーム処理における CSV ファイルの読み込み Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。\nしかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。\nstar schema における dimension table 的な情報をストリームに結合したい場合 等が考えられる。\nこのポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。\nFlink は現時点の stable である v1.11 を想定。\nCSV ファイルを読む方法 DataStream API ベースの実装で CSV ファイルを読むには StreamExecutionEnvironment のメソッドである readFile() を使う。\noverload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。\nまず1つめは FileInputFormat\u0026lt;OUT\u0026gt; inputFormat であり、こちらは data stream の生成に用いる入力フォーマットを指定する。\nおそらく最も一般的なのが TextInputFormat だと思われる。\nもちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。\nPojoCsvInputFormat RowCsvInputFormat TupleCsvInputFormat なんとなく名前でわかると思うが、それぞれ readFile() の結果として返される DataStreamSource が内包する型が異なる。\nこれについては後述の実験にて確認する。\n次に FileProcessingMode watchType も見ておきたい。\nこの引数ではデータソースの監視についてのモードを指定する。\nモードは2つある。\nFileProcessingMode.PROCESS_CONTINUOUSLY 対象のファイルが更新され、その更新に追随する必要がある場合に利用 指定のインターバルでファイルの更新をチェック 更新があった場合はファイル全体を読む FileProcessingMode.PROCESS_ONCE 対象のファイルの更新がない、更新について考えない場合に利用 最初に一度だけファイルを読む おそらく多くの場合は前者が必要になるのではないだろうか。\n利用にあたっては更新があった場合にファイル全体が読まれるということに注意が必要だ。\n例えばファイル末尾にレコードを1件追加するような更新であったとしても、全レコードが再度ストリームに流されるということである。\n詳しくは ドキュメント を参照。\nこれはファイル全体で1つの atomic な単位だとみなされているものと思われる。\nレコード単位で処理していくストリーム処理にファイルというバルクな単位のデータを流そうとしているのでこうなってしまう。\nそう考えるとやはり static なファイルのデータは dimension table として情報を付加するような、ストリームの本川に合流する支川のような使い方が想定されているのだろう。\nちなみに CsvReader というものもあるが、こちらは DataSet API、つまりバッチ処理向けのようなので今回は扱わない。\n実験 実際にコードを書いて readFile() で CSV を読んでみる。\nここでは PojoCsvInputFormat と TupleCsvInputFormat を切り替えられるようにした。\nコード package com.example.entry import org.apache.flink.api.common.typeinfo.BasicTypeInfo import org.apache.flink.api.java.io.{PojoCsvInputFormat, TupleCsvInputFormat} import org.apache.flink.api.java.tuple.Tuple3 import org.apache.flink.api.java.typeutils.{PojoField, PojoTypeInfo, TupleTypeInfo} import org.apache.flink.core.fs.Path import org.apache.flink.streaming.api.functions.source.FileProcessingMode import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, createTypeInformation} import scala.collection.JavaConverters._ import scala.concurrent.duration._ /** * The experiment to read CSV file by Flink. * It reads CSV file as POJOs or tuples and just prints on console. */ object ReadCsvFileExperimentRunner { /** POJO */ case class Company(name: String, ticker: String, numEmployees: Int) /** Tuple */ type CompanyTuple = Tuple3[String, String, Int] def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val companiesFilePath = \u0026#34;data/companies.csv\u0026#34; val interval = 10.seconds args.headOption match { case None | Some(\u0026#34;pojo\u0026#34;) =\u0026gt; val inputFormat = createPojoCsvInputFormat(companiesFilePath) env .readFile(inputFormat, companiesFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, interval.toMillis) .map(_.toString) .print() case Some(\u0026#34;tuple\u0026#34;) =\u0026gt; val inputFormat = createTupleCsvInputFormat(companiesFilePath) env .readFile(inputFormat, companiesFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, interval.toMillis) .map(_.toString) .print() case _ =\u0026gt; throw new RuntimeException(s\u0026#34;Unsupported input format: ${args(0)}\u0026#34;) } env.execute() } private def createPojoCsvInputFormat(csvFilePath: String): PojoCsvInputFormat[Company] = { val clazz = classOf[Company] val pojoFields = Seq( new PojoField(clazz.getDeclaredField(\u0026#34;name\u0026#34;), BasicTypeInfo.STRING_TYPE_INFO), new PojoField(clazz.getDeclaredField(\u0026#34;ticker\u0026#34;), BasicTypeInfo.STRING_TYPE_INFO), new PojoField(clazz.getDeclaredField(\u0026#34;numEmployees\u0026#34;), BasicTypeInfo.INT_TYPE_INFO) ).asJava val pojoTypeInfo = new PojoTypeInfo[Company](clazz, pojoFields) val fieldNames = Array(\u0026#34;name\u0026#34;, \u0026#34;ticker\u0026#34;, \u0026#34;numEmployees\u0026#34;) val inputFormat = new PojoCsvInputFormat[Company](new Path(csvFilePath), pojoTypeInfo, fieldNames) inputFormat.setSkipFirstLineAsHeader(true) inputFormat } private def createTupleCsvInputFormat(csvFilePath: String): TupleCsvInputFormat[CompanyTuple] = { val types = Seq( BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.INT_TYPE_INFO ) val tupleTypeInfo = new TupleTypeInfo[CompanyTuple](classOf[CompanyTuple], types: _*) val inputFormat = new TupleCsvInputFormat[CompanyTuple](new Path(csvFilePath), tupleTypeInfo) inputFormat.setSkipFirstLineAsHeader(true) inputFormat } } ポイントは POJO 版も tuple 版も型情報を作ってやる必要があるということだ。\nそれぞれ PojoTypeInfo, TupleTypeInfo を用意してやる必要があり、これがやや癖があって面倒。\nあるフィールドを数値として読むことは可能だが、日付の parse のようなことはできないようである。\nというのを考えると TextInputFormat で読んで自分で parse するのと比べてあまりうれしくないような…\nデータ 実験用のデータとして会社情報を示す簡単な CSV ファイルを適当に作って data/companies.csv に配置。\nname,ticker,num_employees Alphabet Inc,GOOG,98771 Apple Inc,AAPL,147000 Facebook Inc,FB,49942 Amazon.com Inc,AMZN,798000 実行 まずは POJO 版を実行してみた。\nプログラムが起動するとすぐに以下が出力された。\n[info] running com.example.entry.ReadCsvFileExperimentRunner pojo 2\u0026gt; Company(Alphabet Inc,GOOG,98771) 1\u0026gt; Company(Facebook Inc,FB,49942) 2\u0026gt; Company(Apple Inc,AAPL,147000) 1\u0026gt; Company(Amazon.com Inc,AMZN,798000) Company インスタンスとして CSV ファイルの内容を取得できている。\nプログラムは止まっていないが CSV ファイルの内容を一通り吐き出したところで出力は止まった。\nここで CSV ファイルに次の1行を追加してみる。\nMicrosoft Corporation,MSFT,163000 すると出力は\n[info] running com.example.entry.ReadCsvFileExperimentRunner pojo 2\u0026gt; Company(Alphabet Inc,GOOG,98771) 1\u0026gt; Company(Facebook Inc,FB,49942) 2\u0026gt; Company(Apple Inc,AAPL,147000) 1\u0026gt; Company(Amazon.com Inc,AMZN,798000) 2\u0026gt; Company(Alphabet Inc,GOOG,98771) 1\u0026gt; Company(Amazon.com Inc,AMZN,798000) 2\u0026gt; Company(Apple Inc,AAPL,147000) 1\u0026gt; Company(Microsoft Corporation,MSFT,163000) 2\u0026gt; Company(Facebook Inc,FB,49942) となり、最初に出力された4行に加えて新たに5行追加された。\nCSV ファイルには1行追加しただけだが、既存の行も含む CSV ファイル全体が再度出力された。\nドキュメントに記載されているとおりの仕様となっている。\ntuple 版で実行すると出力は次のようになった。\n[info] running com.example.entry.ReadCsvFileExperimentRunner tuple 1\u0026gt; (Facebook Inc,FB,49942) 1\u0026gt; (Amazon.com Inc,AMZN,798000) 2\u0026gt; (Alphabet Inc,GOOG,98771) 2\u0026gt; (Apple Inc,AAPL,147000) tuple として読めているようだ。\nwatchType が同じなので CSV ファイルの更新についての挙動は同様だった。\nちなみに実行可能なプロジェクトは GitHub に置いている。\nsbt 'runMain com.example.entry.ReadCsvFileExperimentRunner pojo' または sbt 'runMain com.example.entry.ReadCsvFileExperimentRunner tuple' で実行できる。\n(Ctrl + C で終了)\nまとめ TextInputFormat で読んで自分で parse するのと比べ、*CsvInputFormat を使う方法はコーディングとしてはあまりメリットが感じられなかった。\nまた、ストリーム処理においてやはりファイルというデータソースは傍流なんだなという感じ。\nちなみに Table API で CSV を読むこともおそらく可能。\n気が向いたら書く。\n","permalink":"https://soonraah.github.io/posts/read-csv-by-flink-datastream-api/","summary":"ストリーム処理における CSV ファイルの読み込み Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。\nしかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。\nstar schema における dimension table 的な情報をストリームに結合したい場合 等が考えられる。\nこのポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。\nFlink は現時点の stable である v1.11 を想定。\nCSV ファイルを読む方法 DataStream API ベースの実装で CSV ファイルを読むには StreamExecutionEnvironment のメソッドである readFile() を使う。\noverload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。\nまず1つめは FileInputFormat\u0026lt;OUT\u0026gt; inputFormat であり、こちらは data stream の生成に用いる入力フォーマットを指定する。\nおそらく最も一般的なのが TextInputFormat だと思われる。\nもちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。\nPojoCsvInputFormat RowCsvInputFormat TupleCsvInputFormat なんとなく名前でわかると思うが、それぞれ readFile() の結果として返される DataStreamSource が内包する型が異なる。","title":"Apache Flink の DataStream API 利用時の CSV ファイル読み込み"},{"content":"ちょっと昔話 かつて参画したプロジェクトの話。\nそのプロジェクトでは他社から受注した受託開発として機械学習系のシステムを開発していた。\n当時としては新しいフレームワークを使い、かなり頑張ってなんとか納期内で完成させた。\nその中の1つの機能として A/B テストができるようにしていた。\nパラメータチューニングによりパフォーマンスを改善することを想定していた。\nしかし結局その機能は使われることがなかった。\nなぜか。\nA/B テストを実施するためのクライアントの追加の予算がつかなかったためである。\n受託なのでなおさらなのだが、売上にならなければ工数をかけるこはできない。\n工数を使ってパフォーマンス改善することはできなかった。\n手はあるのに。\n機械学習の精度は必ずしも利益に結びつかない この昔話で何が言いたいかというと、機械学習の精度改善は必ずしも利益に結びつかないということである。\nそのことを示しているとても素晴らしい資料がこちら。\n機械学習の精度と売上の関係 from Tokoroten Nakayama 前述の昔話の例はこの資料で言うところの③ロジスティック型 (=外注) となる。\nいったん売上が立った後、追加予算がつかなかったので精度改善では売上は増えなかったのだ。\n倫理感による精度改善 受託開発を主としている組織であれば工数にはシビアなので、売上の立たない工数をかけることはあまりないだろう。\n(よっぽどの炎上鎮火とかでなければ)\nしかし自社で製品やサービスを作って提供しているような組織の場合、利益にならない精度改善をしているのを時折見かける。\nなぜそのようなことが起こるかと言うと多くの場合はデータサイエンティスト／機械学習エンジニアとしての倫理感からなのではないだろうか。\n「◯◯予測という機能なのでできるだけ良い予測精度を示すべきだ」\n「ユーザには気づかれない部分だが精度が悪いので改善したい」\n倫理感や興味が先行してしまっているのだ。\nしかしその精度を上げた先に利益があるとは限らない。\n機械学習で職を得ている人間は自分の仕事を機械学習の精度を上げるゲームだとみなす傾向があるように思う。\n例えばインターネット広告の CTR 予測。\nこれは予測精度が高いほど利益は改善するし、広告主に価値も提供できる。\n精度改善に倫理と利益が伴っている、とても機械学習がハマる例だと思う。\n本来はこれらを兼ね備えているのが良い適用先であるはずだ。\nイシューは行き渡っているのか 利益に結びつかない、または間接的にしか結びつかないような精度改善をやることが許されるというのは組織に余裕があるということで悪いことではないのかもしれない。\nしかし単によいイシューの設定ができてないだけという可能性もある。\n自社で製品やサービスを作って提供しているような組織において、単純なロジスティック回帰でコアなところのビジネスを大きく加速させることができた時期を過ぎると機械学習で解くのに適したよい問題を恒常的に見つけ出すのは実は難しいのではないだろうかと最近考えるようになった。\nビジネスの領域拡大よりも既存領域への機械学習の適用の方が速いということは十分ありうる。\nもちろんチームの規模にもよる。\n機械学習チームの人的リソースの規模に対して機械学習で解くべきよいイシューを見つけ出せているのか、ということだ。\n少し前にちょっと話題になったこちらの件もイシューが大事だと言っている。\n全ての機械学習の論文は新しいアルゴリズムを提案しているのですか？ - Quora キャリアの行く末 事業会社においてビジネスの領域拡大よりも既存領域への機械学習の適用の方が速く、よいイシューを提供しにくいということがよく起こるのであれば、機械学習チームのリソースは余剰気味になりやすいということになる。\nこれが続くと今後機械学習しかやらない人材の市場価値は下がっていくのかもしれない。\nもしくは自社で製品やサービスを持っている組織ではなく、受託開発やコンサルが主戦場になっていくのかもしれない。\n何にせよ特定のプロダクトに commit したいのであれば機械学習エンジニアは機械学習以外のスキルも磨いていく必要があるように思う。\nおわりに 見える範囲にいる人が利益にならない精度改善をしているのを横目で見てこのようなことを考えていた。\n難しいけどできるだけ金を生んでいきたい。\n","permalink":"https://soonraah.github.io/posts/ml-accuracy-profit-ethic-issue/","summary":"ちょっと昔話 かつて参画したプロジェクトの話。\nそのプロジェクトでは他社から受注した受託開発として機械学習系のシステムを開発していた。\n当時としては新しいフレームワークを使い、かなり頑張ってなんとか納期内で完成させた。\nその中の1つの機能として A/B テストができるようにしていた。\nパラメータチューニングによりパフォーマンスを改善することを想定していた。\nしかし結局その機能は使われることがなかった。\nなぜか。\nA/B テストを実施するためのクライアントの追加の予算がつかなかったためである。\n受託なのでなおさらなのだが、売上にならなければ工数をかけるこはできない。\n工数を使ってパフォーマンス改善することはできなかった。\n手はあるのに。\n機械学習の精度は必ずしも利益に結びつかない この昔話で何が言いたいかというと、機械学習の精度改善は必ずしも利益に結びつかないということである。\nそのことを示しているとても素晴らしい資料がこちら。\n機械学習の精度と売上の関係 from Tokoroten Nakayama 前述の昔話の例はこの資料で言うところの③ロジスティック型 (=外注) となる。\nいったん売上が立った後、追加予算がつかなかったので精度改善では売上は増えなかったのだ。\n倫理感による精度改善 受託開発を主としている組織であれば工数にはシビアなので、売上の立たない工数をかけることはあまりないだろう。\n(よっぽどの炎上鎮火とかでなければ)\nしかし自社で製品やサービスを作って提供しているような組織の場合、利益にならない精度改善をしているのを時折見かける。\nなぜそのようなことが起こるかと言うと多くの場合はデータサイエンティスト／機械学習エンジニアとしての倫理感からなのではないだろうか。\n「◯◯予測という機能なのでできるだけ良い予測精度を示すべきだ」\n「ユーザには気づかれない部分だが精度が悪いので改善したい」\n倫理感や興味が先行してしまっているのだ。\nしかしその精度を上げた先に利益があるとは限らない。\n機械学習で職を得ている人間は自分の仕事を機械学習の精度を上げるゲームだとみなす傾向があるように思う。\n例えばインターネット広告の CTR 予測。\nこれは予測精度が高いほど利益は改善するし、広告主に価値も提供できる。\n精度改善に倫理と利益が伴っている、とても機械学習がハマる例だと思う。\n本来はこれらを兼ね備えているのが良い適用先であるはずだ。\nイシューは行き渡っているのか 利益に結びつかない、または間接的にしか結びつかないような精度改善をやることが許されるというのは組織に余裕があるということで悪いことではないのかもしれない。\nしかし単によいイシューの設定ができてないだけという可能性もある。\n自社で製品やサービスを作って提供しているような組織において、単純なロジスティック回帰でコアなところのビジネスを大きく加速させることができた時期を過ぎると機械学習で解くのに適したよい問題を恒常的に見つけ出すのは実は難しいのではないだろうかと最近考えるようになった。\nビジネスの領域拡大よりも既存領域への機械学習の適用の方が速いということは十分ありうる。\nもちろんチームの規模にもよる。\n機械学習チームの人的リソースの規模に対して機械学習で解くべきよいイシューを見つけ出せているのか、ということだ。\n少し前にちょっと話題になったこちらの件もイシューが大事だと言っている。\n全ての機械学習の論文は新しいアルゴリズムを提案しているのですか？ - Quora キャリアの行く末 事業会社においてビジネスの領域拡大よりも既存領域への機械学習の適用の方が速く、よいイシューを提供しにくいということがよく起こるのであれば、機械学習チームのリソースは余剰気味になりやすいということになる。\nこれが続くと今後機械学習しかやらない人材の市場価値は下がっていくのかもしれない。\nもしくは自社で製品やサービスを持っている組織ではなく、受託開発やコンサルが主戦場になっていくのかもしれない。\n何にせよ特定のプロダクトに commit したいのであれば機械学習エンジニアは機械学習以外のスキルも磨いていく必要があるように思う。\nおわりに 見える範囲にいる人が利益にならない精度改善をしているのを横目で見てこのようなことを考えていた。\n難しいけどできるだけ金を生んでいきたい。","title":"機械学習の精度と利益と倫理とイシューと"},{"content":"はじめに このポストではストリーム処理の survay 論文の話題に対して Apache Flink における例を挙げて紹介する。\n論文概要 Fragkoulis, M., Carbone, P., Kalavri, V., \u0026amp; Katsifodimos, A. (2020). A Survey on the Evolution of Stream Processing Systems.\n2020年の論文。\n過去30年ぐらいのストリーム処理のフレームワークを調査し、その発展を論じている。\nストリーム処理に特徴的に求められるいくつかの機能性 (functionality) についてその実現方法をいくつか挙げ、比較的古いフレームワークと最近のフレームワークでの対比を行っている。\nこのポストのスコープ このポストでは前述のストリーム処理システムに求められる機能性とそれがなぜ必要となるかについて簡単にまとめる。\n論文ではそこからさらにその実現方法がいくつか挙げられるが、ここでは個人的に興味がある Apache Flink ではどのように対処しているかを見ていく。\nちなみに論文中では Apache Flink はモダンなフレームワークの1つとしてちょいちょい引き合いに出されている。\nここでは Flink v1.11 をターゲットとする。\n以下では論文で挙げられている機能性に沿って記載していく。\nOut-of-order Data Management Out-of-order ストリーム処理システムにやってくるデータの順序は外的・内的要因により期待される順序になっていないことがある。\n外的要因としてよくあるのはネットワークの問題。\nデータソース (producer) からストリーム処理システムに届くまでのルーティング、負荷など諸々の条件により各レコードごとに転送時間は一定にはならない。\n各 operator の処理などストリーム処理システムの内的な要因で順序が乱されることもある。\nout-of-order は処理の遅延や正しくない結果の原因となることがある。\nout-of-order を管理するためにストリーム処理システムは処理の進捗を検出する必要がある。\n\u0026ldquo;進捗\u0026rdquo; とはある時間経過でレコードの処理がどれだけ進んだかというもので、レコードの順序を表す属性 A (ex. event time) により定量化される。\nある期間で処理された最古の A を進捗の尺度とみなすことができる。\nApache Flink の場合 Flink ではこの進捗を測るのに watermark という概念が使われている。\nEvent Time and Watermarks Apache Flink Event Time and Watermarks\n(図が見にくい場合はページ上部の太陽みたいなマークをクリックして light mode にしてください)\nこちらの図でストリーム中の破線で描かれているのが watermark であり、W(11) の wartermark は「timestamp が11以下の event はこの後もう来ないものとみなす」ということを下流の operator に伝えるものである。\nwatermark は metadata 的なものだが、通常の event と同じようにストリーム中を流れている (これを panctuation という)。\n下流の operator が window 処理をしていた場合、W(11) が届いた時点で timestamp が11までのところの window 処理を完結してさらに下流に output することができる。\nwatermark がいつ・どのような値で発生するかについては Flink application の開発者の実装次第ということになる。\nしかし現実的には Writing a Periodic WatermarkGenerator の例にある BoundedOutOfOrdernessGenerator のように、 WatermarkGenerator にやってきた event の event time を元に決めることが多いと思われる。\nState Management ストリーム処理における状態 \u0026ldquo;状態\u0026rdquo; とは継続的なストリーム処理の中で内部的な副作用をとらえたもの。\nアクティブな window、レコードのかたまり、aggregation の進捗など。\nユーザ定義のものも含まれる。\n状態については以下のようなトピックがある。\nProgrammability プログラミングモデルにおいて状態がどのように定義・管理されるか 定義と管理についてそれぞれシステムとユーザの場合がある Scalability and Persistency 最近のストリーム処理は scalable の時流を汲んでおり、scale out するときに状態をどのように扱うか 内外の記憶装置に状態を永続化するという方法がよく取られる Consistency transaction level の保証について Apache Flink の場合 ドキュメントの TOP ページ における Flink を表す一文\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams.\nにおいても \u0026ldquo;stateful\u0026rdquo; という言葉が使われているとおり、状態の扱いは Flink の設計思想の中でもかなり重要な部分となっている。\nFlink における状態の扱いについてはこちらを参照。\nStateful Stream Processing Programmability Flink では application 開発者が任意の状態を定義することができる。\nWorking with State 一方で状態の管理はフレームワーク側でやってくれるので、開発者は checkpoint や restore 等のことは特に配慮する必要はない。\n論文中ではこれを \u0026ldquo;User-Declared System-Managed State\u0026rdquo; と呼んでおり、最近のストリーム処理システムの傾向となっている。\nScalability Flink では keyBy() により key-level の状態を持つことができる。\nkey ごとに並列 task 内での partitioning し、分散することが可能ということである。\nPersistency 論文では永続化については scalability と絡めて述べられていたが、Flink のドキュメントでは fault tolerance の文脈で永続化について書かれている。\nFlink の fault tolerance の肝は stream replay と checkpointing である。\ncheckpointing とはストリームと operator の状態の一貫性のあるスナップショットをとることである。\nApache Flink Snapshotting Operator State\nこの checkpoint を作成する過程で各 operator の状態が state backend へと永続化される。\nstate backend では RocksDB の key/value store に各 checkpoint, 各 operator の状態が保存される。\n(RocksDB 以外にもメモリやファイルシステムなどもある)\nConsistency Flink では checkpoint のインターバルの期間の単位 (epoch という) で一貫性のある状態を永続化する。\n上図の barriers がその単位を決めている。\nChandy Lamport algorithm という分散スナップショットの手法がインスパイアされており、unaligned/aligned で各 operator の状態のスナップショットを取るようになっている。\nFault Tolerance \u0026amp; High Availability Fault Tolerance ストリーム処理システムにとって fault tolerance は2つの理由から重要である。\nストリーム処理システムは stateful な計算を終わりのないデータに対して行っている fault tolerance がなければ、障害があったときに最初から状態を計算しなおさなければならない 一方で、多くの場合過去に処理されたデータは既に失われている 最近のストリーム処理システムは分散アーキテクチャを採用している 物理マシンの数だけ問題が起こりやすくなる output commit problem についても考慮する必要がある。\nこれは出力が公開された位置から状態を復元できることが確かな場合のみ、システムは外界に出力を公開するというもの。\n言い換えると、障害からの復旧時などに同じ出力を2回してしまわない、出力を exactly-once にできるかというものである。\nHigh Availability 過去の研究においてストリーム処理システムの可用性は recovery time, performance overhead (throughput \u0026amp; latency), resource utilization により定量化されてきた。\nこの論文では\nA streaming system is available when it can provide output based on the processing of its current input.\nを可用性の定義として提案する。\n時間ごとの processing time と event time の差により定量化される。\nApache Flink の場合 Fault Tolerance 論文中では Flink は output commit problem については Kafka などの出力先の外部システムの責任とするスタンスだとしている。\nKafka には idempotent producer という機能があり、たぶんこれのことを言っている。\nまた一方で TwoPhaseCommitSinkFunction の2相コミットによって exectly-once semantics を提供するという方法も示されている。\nAn Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!) checkpointing における JobManager を2相コミットの coordinator とみなし、checkpoint barrier が最後の operator に到達するまでをコミット要求相、その後の JobManager からの checkpointing 完了通知をコミット相としている。\nコミット相において外部システムへの書き出しの transaction が完了する形となる。\nfault tolerance については State Management の項も参照。\nHigh Availability Flink のプロセスには JobManager と TaskManager があり、前者は cluster に1つだけ動く。\nしたがって JobManager が SPOF になり、可用性に影響しうる。\nhigh availability (高可用性) を実現するためには JobManager が SPOF となることを避けることができる。\nstandalone または YARN の cluster として deploy した場合は JobManager が SPOF となることを避けることができる。\n以下は standalone の例。\n1つの JobManager が leader として動いているが、それが crash すると standby のインスタンスが leader を引き継ぐ。\n(論文中では passive replication として紹介)\nApache Flink Standalone Cluster High Availability\nLoad Management, Elasticity \u0026amp; Reconfiguration Load Management ストリーム処理システムは、外部のデータソースがデータを送る流速を制御することができない。\n入力データの流速がシステムのキャパより大きいことによるパフォーマンス劣化を防ぐための対応が必要となる。\n次のような手法がある。\nload shedding 多すぎる入力データを落とす方法 back-pressure 入力データを落とせないときに buffering と組み合わせて使う dataflow graph 上に速度制限が波及していく elasticity 分散アーキテクチャと cloud にもとづく方法 いわゆる scale out Apache Flink の場合 Flink では back pressure および elasticity の組み合わせとなっている。\nback pressure は一時的な入力データの増加に対応する。\n各 operator (subtask?) は入出力の buffer を持っており、これにより operator 間の処理速度の違いをある程度吸収できる。\nしかし入力データが著しく多くなると\nボトルネックとなる operator の処理が滞る その operator の入力 buffer がいっぱいになる (ボトルネックではない) 上流の operator の出力 buffer がいっぱいになる 上流の operator の処理が滞る (以降繰り返し) のように、dataflow graph の上流へ上流へと遅延が波及する。\nelasticity の面では、JobManager や TaskManager の追加や削除ができるようになっている。\nAdding JobManager/TaskManager Instances to a Cluster TaskManager の追加や削除においては状態の再配分が行われる。\n再配分される状態は key group という単位で partitioning されており、consistent hash 的な方法で各 TaskManager 配下の operator へと配分される。\nちなみに AWS が提供する Flink の managed service である Amazon Kinesis Data Analytics for Apache Flink では CPU 使用率をモニタリングして自動的に scale out が行われるようになっている。\nApplication Scaling in Kinesis Data Analytics for Apache Flink まとめ バッチ処理ではあまりクリティカルにならないような問題でもストリーム処理では重大な影響を及ぼすことがある。\nストリーム処理に求められる機能性を実現するに当たり、Apache Flink では checkpoint の仕組みが中心的な役割を果たしているということが理解できた。\n","permalink":"https://soonraah.github.io/posts/functionality-of-streaming-system/","summary":"はじめに このポストではストリーム処理の survay 論文の話題に対して Apache Flink における例を挙げて紹介する。\n論文概要 Fragkoulis, M., Carbone, P., Kalavri, V., \u0026amp; Katsifodimos, A. (2020). A Survey on the Evolution of Stream Processing Systems.\n2020年の論文。\n過去30年ぐらいのストリーム処理のフレームワークを調査し、その発展を論じている。\nストリーム処理に特徴的に求められるいくつかの機能性 (functionality) についてその実現方法をいくつか挙げ、比較的古いフレームワークと最近のフレームワークでの対比を行っている。\nこのポストのスコープ このポストでは前述のストリーム処理システムに求められる機能性とそれがなぜ必要となるかについて簡単にまとめる。\n論文ではそこからさらにその実現方法がいくつか挙げられるが、ここでは個人的に興味がある Apache Flink ではどのように対処しているかを見ていく。\nちなみに論文中では Apache Flink はモダンなフレームワークの1つとしてちょいちょい引き合いに出されている。\nここでは Flink v1.11 をターゲットとする。\n以下では論文で挙げられている機能性に沿って記載していく。\nOut-of-order Data Management Out-of-order ストリーム処理システムにやってくるデータの順序は外的・内的要因により期待される順序になっていないことがある。\n外的要因としてよくあるのはネットワークの問題。\nデータソース (producer) からストリーム処理システムに届くまでのルーティング、負荷など諸々の条件により各レコードごとに転送時間は一定にはならない。\n各 operator の処理などストリーム処理システムの内的な要因で順序が乱されることもある。\nout-of-order は処理の遅延や正しくない結果の原因となることがある。\nout-of-order を管理するためにストリーム処理システムは処理の進捗を検出する必要がある。\n\u0026ldquo;進捗\u0026rdquo; とはある時間経過でレコードの処理がどれだけ進んだかというもので、レコードの順序を表す属性 A (ex. event time) により定量化される。","title":"ストリーム処理システムに求められる機能性、および Apache Flink におけるその対応"},{"content":"ほとんどバッチ処理しか書いたことのない者だがストリーム処理のシステムを開発することになった。\nそれにあたって独学で調べたことなどまとめておく。\nストリーム処理とは そもそも \u0026ldquo;ストリーム処理\u0026rdquo; とは何を指しているのか。\n以下の引用が簡潔に示している。\na type of data processing engine that is designed with infinite data sets in mind. Nothing more.\n\u0026ndash; Streaming 101: The world beyond batch\nこちらは \u0026ldquo;streaming system\u0026rdquo; について述べたものだが、つまり終わりのないデータを扱うのがストリーム処理ということである。\n例えば web サービスから生まれ続けるユーザ行動ログを逐次的に処理するというのがストリーム処理。\nweb サービスが終了しないかぎりはユーザ行動ログの生成には終わりがない。\nこれに対して \u0026ldquo;1日分のユーザ行動ログ\u0026rdquo; 等のように有限の量のデータを切り出して処理する場合、これはバッチ処理となる。\nストリーム処理とバッチ処理の違いは扱うデータが無限なのか有限なのかということだ。\nこの後触れていくが、この終わりのないデータを継続的に処理し続けるというところにバッチ処理にはない難しさがある。\nなぜストリーム処理なのか なぜストリーム処理なのか。\nひとえに逐次的な入力データに対する迅速なフィードバックが求められているからと言えるだろう。\n迅速なフィードバックがビジネス上のメリットとなることは自明だ。\nSNS の配信 カーシェアリングにおける配車や料金設定 クレジットカードや広告クリックなどの不正検知 もしこれらの application が例えば hourly のバッチ処理で実装されていたらどうだろうか。\nまあ待っていられない。\n一般的なストリーム処理の構成 モダンな…と言っていいのかわからないが、ストリーム処理を行うための一般的なシステムは次の3つの要素で構成される。\nproducer broker consumer producer は最初にレコードを生成する、ストリームデータの発生源となるものである。\n例えばログを生成する web application であったり、何らかのセンサーを持つ IoT 機器であったりがこれに該当する。\nproducer は絶え間なくログを生成し、それを broker へと送る。\nbroker は producer から送られたログを格納し、任意のタイミングで取り出せるようにするものである。\n誤解を恐れずに言うとメッセージキューに近いイメージだ。\nApache Kafka クラスタや Amazon Kinesis Data Streams 等がこれに該当する。\nconsumer は broker からログを取り出し、それに対し何かしらの処理を行うものだ。\ntime window 集計であったりログからの異常検知であったり、処理した結果として何かビジネス上意味があるものを得るのである。\nこれを行うフレームワークとしては Spark Streaming や Apache Flink 等がメジャーなのだろうか。\nproducer と consumer の間に broker を挟むメリットとしては次のようなことが挙げられる。\nproducer が M 個、consumer が N 個の場合に M * N の関係になるところを broker を挟めば M + N にできる producer, consumer に多数のシステムがあったとしても各自は broker との接続だけを考えればよい 任意のタイミングでデータを読み出せる producer または consumer に問題が発生してもデータロスが起こりにくくできる その分 broker には高い可用性が求められる Kafka はクラスタで冗長構成 Kinesis Data Streams は複数 AZ でレプリケーション 時間の概念 ストリーム処理では時間の概念がいくつかあり、集計などの処理をどの時間をベースにして実行するのか、意識する必要がある。\nevent time producer 側でログイベントが発生した時間 ingestion time broker にそのログイベントのレコードが挿入された時間 processing time consumer 側でレコードを処理した時間 processing time を使うのが一番簡単なのだが、おそらく分析系の処理であれば window 集計等では event time を使うことが多いのではないだろうか。\ningestion time はおそらく実際のプロダクトではあまり使われないのではと思われる。\n(ネットワークのパフォーマンスを見るぐらい？)\nWindowing ストリーム処理の中で sum, count などを伴う集計処理を行う場合、通常は時間方向の window で切って処理するということになるのではないだろうか。\nwindow で切らずに完全なデータセットがそろうまで待つことはできないし、データが来るたびに逐次的に全体の結果を更新するしていくというのも割に合わない。\nwindow の切り方もいくつかある。\ntumbling window 固定長でオーバーラップしない sliding window 固定長でオーバーラップを含む session window いわゆる web の session のように、ある種のイベントがある期間発生しないことにより window が区切られる これらについては Flink のドキュメントが図もあってわかりやすい。\n個人的な感想だが、この time window の集計がない単なる map 的なストリーム処理であれば traditional なアーキテクチャでも難しくはない。\nしかし time window 集計が必要となった場合は Spark Streaming 等のモダンなフレームワークが威力を発揮してくる。\nWatermark 時間で window を切るときは、前述のどの時間の定義を用いるかを考えなければいけない。\nprocessing time を用いる場合は簡単だが event time はやや難しい。\nconsumer 側では event のレコードがどれくらい遅れてやってくるかわからないためだ。\nネットワークその他の影響により、event のレコードが producer -\u0026gt; broker -\u0026gt; consumer という経路で consumer に届くまでの時間というのは一定にはならない。\nまた、古い event が新しい event より後に届くというように順番が前後することも起こりうる。\nここで \u0026ldquo;watermark\u0026rdquo; という考え方が必要になってくる。\nA watermark with a value of time X makes the statement: \u0026quot;all input data with event times less than X have been observed.\u0026quot;\n\u0026ndash; Streaming 102: The world beyond batch\nある processing time において「event time X より前のレコードはすべて到着したよ」というのが watermark である。\n別の言い方をすると watermark により event のレコードがどの程度遅延してもよいかが定義される。\nevent time X より前のレコードが真の意味ですべて到着した、というのは難しい。\n実際には heuristic にどの程度遅れていいかを決め、それより遅れた場合はある event time 期間における window 処理には含めないということになる。\nwatermark の決め方はフレームワーク次第だろうか。\n例えば Spark Structured Streaming の例だと図もあって比較的わかりやすい。\nSchema Evolution 何らかの業務システムや web システム等をある程度運用したことがある人ならわかると思うが、データの schema というのはナマモノだ。\n一度決めたら終わりというわけではなくプロダクトやビジネスの変化に応じて変化していく。\nカラムが増えたり、削除されたり、名前や型が変わったり…\nこのようにデータの構造が変化していくこと、またはそれを扱うことを \u0026ldquo;schema evolution\u0026rdquo; という。\nバッチ処理において schema の変更に追従することを考えるのはそれほど難しくない。\nhourly のバッチ処理であったとしても、バッチ処理とバッチ処理の間の時間で application を更新すればいいだけだ。\n(が、実際に行うのは困難が伴うことも多い)\nではストリーム処理ではどうだろうか。\nいわゆるストリーム処理においては処理と処理の間というものがなく、application がずっと稼働しっぱなしということになる。\nバッチ処理のような更新はできない。\nもっと言うと producer で生まれた新しい schema のレコードがいつ届くかもわからない。\nおそらくこの問題には2つの対応方法がある。\n1つめは consumer 側のシステムで前方互換性を保つという方法である。\nこの場合、新しいフィールドは必ず末尾に追加される等、producer 側での schema 更新についてある程度のルールが必要となるだろう。\nproducer 側で生成されるレコードの schema の変更が必ず事前にわかるというのであれば後方互換性でもいいが、多くの場合は難しい。\nところで前方互換と後方互換、どっちがどっちなのか覚えられません。\n2つめの方法として schema 情報をレコード自体に入れ込んでしまうという方法もある。\nApach Avro のような serialization の方法を取っているとレコード自体に schema の情報を付与することができる。\nおそらく最もエレガントにこれをやるのが Confluent の Schema Registry という機能だ。\nproducer から送出されるレコードには schema ID を付与する。\nschema の実体は Schema Registry という broker とは別の場所で管理されており、consumer 側では受け取ったレコードに付与されている schema ID と Schema Registry に登録されている shcema の実体を参照してレコードを deserialize することができる。\nDeploy ストリーム処理を行うシステムは終わりのないデータを処理するためのものであり、ずっと動き続けることが期待されている。\nしかし通常システムは一度立ち上げれば終わりということではなく、運用されている中で更新していく必要がある。\nずっと動かしながらどのように deploy, release するのか。\nこの問題は主に consumer 側のシステムで配慮が必要になると思われる。\n正直これについてはちゃんと調べられていないが、2点ほど述べておきたい。\nまず1点目、application を中断・更新・再開するのにどの程度の時間がかかるのかを知っておく必要があるということ。\nアーキテクチャやフレームワーク、処理の内容や checkpoint (後述) を使うか等によりこの時間は変わってくる。\n一例だが、AWS 環境において\nAWS Glue + Spark Structured Streaming Amazon Kinesis Data Analytics + Flink の比較をしたことがある。\n前者は再開に数分かかったのに対し、後者は1分未満で再開できた。\n再開までの時間が十分に短いと判断できるのであればそのまま deploy, release してしまっていいだろう。\n一方そうでない場合はどうすべきかという話が2点目。\n再開までの時間が長く、システム要件的に許容できないというのであれば、release 時は二重で動かすというような措置が必要かもしれない。\nおそらく Blue-Green Deployment のようなことを考えることになるだろう。\nCheckpoint 前述のとおり、ストリーム処理を行うシステムはずっと動き続けることが期待されている。\nしかし予定された application の更新や不測のエラー等、何らかの理由で一時的に中断されるということが実際の運用中には起こる。\n中断されたとき速やかに復帰する仕組みとして \u0026ldquo;checkpoint\u0026rdquo; というものがいくつかの consumer 側のフレームワークで提供されている。\n雑に説明すると、処理のある時点における進捗や内部状態などをディスク等に永続化し、そこから処理を再開できるようにするものである。\nRecovering from Failures with Checkpointing - Apache Spark Checkpointing - Apache Flink 上記は Spark Structured Streaming と Flink の例だ。\ncheckpoint には次のようなメリットがあり、運用上有用だと言える。\n内部の状態を保持しているため、速やかに復帰できる 中断した位置から再開できるので出力に穴が開かない 一方で落とし穴もある。\ncheckpoint では内部の状態が永続化されるわけだが、内部の状態というのは当然 application の実装が決めているものである。\napplication のコードを変更したとき、変更の内容によっては永続化された checkpoint と application が合わなくなることがあるのだ。\n未定義の挙動となることもあるので、checkpoint の運用には十分に配慮する必要がある。\nどのような変更なら checkpoint が安全に利用できるのかはフレームワークのドキュメントに記載があるので確認しておきたい。\nRDB の世界との折り合い みんな大好きな RDB の世界では table を操作してデータの処理を行う。\n基本的には table というものはある時点における完全なデータセットを表すものである。 (ex. isolation)\n他方、ストリーム処理はやってきたデータを逐次的に処理するものである (mini-batch の場合もあるが)。\n直感的にこの2つは相性が悪そうに見える。\nしかし Spark や Flink では table ベースの操作でストリーム処理を行うための API が提供されている。\nおそらく\nストリーム処理の周辺のデータソースとして RDB が存在する RDB 的な table 操作があまりにも浸透している というところが API が必要である理由なのだろう。\nストリームデータを table 的に扱うというのが、やや直感的な理解をしにくいものとなっている。\nフレームワークのドキュメントを確認しておきたい。\n例えば Spark Structured Streaming であれば処理の出力のための3つの output mode が示されている。\nAppend mode: 追加された行だけ出力 Complete mode: table 全体を出力 Update mode: 更新された行だけ出力 どれを選ぶかにより必要とする内部メモリの大きさも影響される。\nまとめ 思ったより長文になってしまった。\n結局ストリーム処理の難しさは以下の2点に尽きるだろう。\n複数の時間の概念 常時稼働のシステム 独学なので抜け漏れがあったり、話が新しくなかったりすることもあると思われる。\n参考 Streaming 101: The world beyond batch Apache Beam PMC によるストリーム処理の解説ポスト。必読 Streaming 102: The world beyond batch 上の続きであり watermark について触れている Analytics Lens - AWS Well-Architected Framework AWS の資料。ストリーム処理のシステムの全体感がつかめる Structured Streaming Programming Guide - Apache Spark Spark Structured Streaming のドキュメント。consumer の気持ちがわかる Flink DataStream API Programming Guide - Apache Flink Flink のドキュメントの方がより詳しい。DataStream API の解説を中心に読むとよい ","permalink":"https://soonraah.github.io/posts/study-streaming-system/","summary":"ほとんどバッチ処理しか書いたことのない者だがストリーム処理のシステムを開発することになった。\nそれにあたって独学で調べたことなどまとめておく。\nストリーム処理とは そもそも \u0026ldquo;ストリーム処理\u0026rdquo; とは何を指しているのか。\n以下の引用が簡潔に示している。\na type of data processing engine that is designed with infinite data sets in mind. Nothing more.\n\u0026ndash; Streaming 101: The world beyond batch\nこちらは \u0026ldquo;streaming system\u0026rdquo; について述べたものだが、つまり終わりのないデータを扱うのがストリーム処理ということである。\n例えば web サービスから生まれ続けるユーザ行動ログを逐次的に処理するというのがストリーム処理。\nweb サービスが終了しないかぎりはユーザ行動ログの生成には終わりがない。\nこれに対して \u0026ldquo;1日分のユーザ行動ログ\u0026rdquo; 等のように有限の量のデータを切り出して処理する場合、これはバッチ処理となる。\nストリーム処理とバッチ処理の違いは扱うデータが無限なのか有限なのかということだ。\nこの後触れていくが、この終わりのないデータを継続的に処理し続けるというところにバッチ処理にはない難しさがある。\nなぜストリーム処理なのか なぜストリーム処理なのか。\nひとえに逐次的な入力データに対する迅速なフィードバックが求められているからと言えるだろう。\n迅速なフィードバックがビジネス上のメリットとなることは自明だ。\nSNS の配信 カーシェアリングにおける配車や料金設定 クレジットカードや広告クリックなどの不正検知 もしこれらの application が例えば hourly のバッチ処理で実装されていたらどうだろうか。\nまあ待っていられない。\n一般的なストリーム処理の構成 モダンな…と言っていいのかわからないが、ストリーム処理を行うための一般的なシステムは次の3つの要素で構成される。\nproducer broker consumer producer は最初にレコードを生成する、ストリームデータの発生源となるものである。\n例えばログを生成する web application であったり、何らかのセンサーを持つ IoT 機器であったりがこれに該当する。","title":"バッチ処理おじさんがストリーム処理のシステムを開発するにあたって調べたこと"},{"content":"前提 ここでは web システムで使われている機械学習のモデルやアルゴリズムを改善するための online の A/B テストを考える。\n具体的に述べると web 広告における CTR 予測や EC サイトのレコメンデーション等が対象である。\nよくあるやつ。\nweb システムにおいて online の A/B テストは KPI 改善の根幹でありとても重要だ。\nそれが重くなるとつらい、という話。\nここで「重い」と言っているのは計算資源のことではなく、A/B テストを実施する担当者の運用コストについて。\nA/B テストの運用が重い場合のデメリット デメリット 1. KPI 改善が遅くなる デメリットと言えばこれが一番大きい。\n単純に A/B テストを1回まわすのに時間がかかってしまうし、それがゆえに online の A/B テストに入るまでの offline のテストが厚くなりここでも時間がかかってしまう。\nKPI 改善に時間がかかるというのはつまり売上や利益を大きくするのに時間がかかってしまうということである。\nデメリット 2. KPI 改善における offline テストの比重が大きくなる 前述のとおりだが online の A/B テストが重いとそこで失敗できなくなり、結果としてその前段の offline のテストを厚くするということになる。\noffline のテストが厚いことの何が問題だろうか。\nここで前提としている CTR 予測やレコメンデーションのようなタスクの場合、offline のデータは既存のモデルやアルゴリズムの影響を受けることになる。\n例えばレコメンデーションの場合を考えると、新しいモデルを offline で評価するための実験データの正例 (コンテンツの閲覧等) は既存モデルによって生み出される。\n既存モデルが「このコンテンツがいいよ」といってユーザに出したリスト、その中からコンテンツの閲覧が行われ正例となるからだ。\nこのような状況下での offline テストにおいては既存モデルと近い好みを持ったモデルのスコアが高くなる傾向がある。\nもしかしたら既存モデルの影響を受けないようなデータをあえて用意しているような場合もあるかもしれないが、レアケースだろう。\n既存モデルの影響を受けないということは恩恵を受けないということなのでトレードオフでもある。\nデメリット 3. 新モデル／アルゴリズムを却下しにくくなる A/B テストの運用が重いと何度も繰り返すことができない。\nなので一発の A/B テストで既存モデル／アルゴリズムを置き換える成果を出さなければならないという圧が強くかかってしまう。\n既存モデル／アルゴリズムが勝って新しいものが負けてしまったときの埋没費用が大きいからだ。\nそうなると次のような事が起こる。\n「新モデルをチューニングしてみよう！」 数ヶ月前のデータでチューニングされていない既存モデル vs. 直近のデータでチューニングされた新モデル 「新モデルが KPI で勝っている間に意思決定しよう！」 勝ったり負けたりする中で… つまりフェアなテストではなくなってしまう。\nなぜ A/B テスト運用が重くなるのか 理由 1. リリース作業のコスト A/B テストの運用が重くなる理由の1として、A/B テストを始めるときのオペレーション、つまり新しいモデル／アルゴリズムをリリースする際の作業コストが重いことが挙げられる。\nこれにはモデル／アルゴリズムが web のシステムに対してどのような形でデプロイされているかが影響する。\nmercari が次のような資料を公開しているので参考にしたい。\n機械学習システムの設計パターンを公開します。 例えば Synchronous pattern のように web サーバと推論をする場所が分離している場合に比べて Web single pattern のように一体化している場合は新しいモデル／アルゴリズムのリリースに繊細にならざるを得ないのではないだろうか。\nコードベースもそうだし、マシンリソースの管理も後者の方が難しい。\n特にリリース担当者がデータサイエンティスト的な人だった場合、リリースの心理的障壁が上がる。\n理由 2. はっきりしない指標 A/B テストするときはテストの指標として何らかの KPI を置く。\nこの KPI が複数ある場合がある。\n例えば web 広告における CTR 予測モデルであれば CTR と impression 等。\n現実のビジネスは複雑なので複数の KPI があるのは珍しくないと思われる。\n一方で KPI が複数になると評価が難しくなる。\n「KPI A は5上がったけど B は3下がった、この場合はどうなの？」ということになり \u0026ldquo;マネージャの肌感\u0026rdquo; みたいなものが必要になってしまう。\n当然機械的に判定することによる自動化なども行えない。\nA/B テストの経過における現状の良し悪しの判断にコストがかかってしまうことになる。\nまた判断基準が複雑なことによりおとり効果のようなことも発生しうるだろう。\nこれについての1つの解として OEC: Overall Evaluation Criterion という考え方がある。\nWhat does \u0026ldquo;Overall Evaluation Criterion\u0026rdquo; mean? 複数の KPI を重みをつけて組み合わせてただ1つの値として評価できるようにする、というものらしい。\n逆に考えると OEC を定義せずに複数の KPI で A/B テストをするということは KPI 間のバランスの合意なしで走ってしまっているということになる。\n理由 3. 煩雑な流量オペレーション これは実際に見たことだが、A/B テストの運用に際して組織の一部の人間しか理解していない煩雑なオペレーションを伴うことがある。\n最初は control group を 0.1% だけ流して、1日経って問題がなければ 1% に上げて、その次は5%で…などのような操作を求められるのである。\n担当者にすればまあまあのストレスだし、時間がかかってしまう。\nまとめ ソフトウェア開発の分野でも大きな変更をえいやーでリリースするのはキツイということで git-flow であったり GitHub Flow であったりが導入されてきたはず。\nA/B テストが重いということはそれの逆をいっている状態となる。つらい。\n","permalink":"https://soonraah.github.io/posts/heavy-ab-testing-operation/","summary":"前提 ここでは web システムで使われている機械学習のモデルやアルゴリズムを改善するための online の A/B テストを考える。\n具体的に述べると web 広告における CTR 予測や EC サイトのレコメンデーション等が対象である。\nよくあるやつ。\nweb システムにおいて online の A/B テストは KPI 改善の根幹でありとても重要だ。\nそれが重くなるとつらい、という話。\nここで「重い」と言っているのは計算資源のことではなく、A/B テストを実施する担当者の運用コストについて。\nA/B テストの運用が重い場合のデメリット デメリット 1. KPI 改善が遅くなる デメリットと言えばこれが一番大きい。\n単純に A/B テストを1回まわすのに時間がかかってしまうし、それがゆえに online の A/B テストに入るまでの offline のテストが厚くなりここでも時間がかかってしまう。\nKPI 改善に時間がかかるというのはつまり売上や利益を大きくするのに時間がかかってしまうということである。\nデメリット 2. KPI 改善における offline テストの比重が大きくなる 前述のとおりだが online の A/B テストが重いとそこで失敗できなくなり、結果としてその前段の offline のテストを厚くするということになる。\noffline のテストが厚いことの何が問題だろうか。\nここで前提としている CTR 予測やレコメンデーションのようなタスクの場合、offline のデータは既存のモデルやアルゴリズムの影響を受けることになる。\n例えばレコメンデーションの場合を考えると、新しいモデルを offline で評価するための実験データの正例 (コンテンツの閲覧等) は既存モデルによって生み出される。\n既存モデルが「このコンテンツがいいよ」といってユーザに出したリスト、その中からコンテンツの閲覧が行われ正例となるからだ。","title":"A/B テストの運用が重くてつらいという話"},{"content":"前回の記事 では Apache Flink における stream data と static data の join において、DataStream API における broadcast state pattern を使う方法を示した。\n今回の記事では Table API の temporal table function を用いた実験を行う。\nTable API Table API は名前のとおりで class Table を中心として SQL-like な DSL により処理を記述するという、DataStream API より high-level な API となっている。\nこれらの関係は Apaceh Spark の RDD と DataFrame (DataSet) の関係に似ている。\nSQL-like な API で記述された処理が実行時に最適化されて low-level API の処理に翻訳されるところも同じだ。\nRDB の table の概念を元にしているものと考えられるが、本質的に table の概念とストリーム処理はあまりマッチしないと思う。\ntable はある時点のデータセット全体を表すのに対し、ストリーム処理ではやってくるレコードを逐次的に処理したい。\nここを合わせているため、ストリーム処理における Table API による処理の挙動の理解には注意が必要だ。\nStreaming Concepts 以下のドキュメントを確認しておきたい。\nTemporal Table Function star schema における、変更されうる dimension table を stream data と結合する方法として、temporal table という仕組みが提供されている。\nドキュメントでは為替レートの例が示されている。\nstream でやってくる fact table 的なレコードに対して、為替のように時々刻々と変化する dimension table をそのレコードの時刻における snap shot としてぶつけるような形となる。\nレコードの時刻としては processing time または event time を扱うことができる。\nevent time の場合であっても watermark で遅延の許容を定義できるため dimension table のすべての履歴を状態として保持する必要はなく、processing time または event time の watermark に応じて過去の履歴は捨てることが可能となっている。\nTable API において temporal table を使うには temporal table function という形を取ることになる。\n実験 実験概要 やることは 前回の記事 とまったく同じで乱数で作った株価のデータを扱う。\n前回と違うのは DataStream API ではなく Table API で処理を記述したところである。\nコード 上記を実行するためのコードを以下に示す。\n実行可能なプロジェクトは GitHub に置いておいた。\nEntry Point toTable() により入力データの DataStream を Table に変換した後、処理を記述した。\nfunc が temporal table function に当たる。\n今回は processing time を基準として join しているが、実際のシステムでは event time を基準としたいことが多いのではないだろうか。\npackage example import org.apache.flink.api.java.io.TextInputFormat import org.apache.flink.api.scala._ import org.apache.flink.core.fs.Path import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration import org.apache.flink.streaming.api.TimeCharacteristic import org.apache.flink.streaming.api.functions.source.FileProcessingMode import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment} import org.apache.flink.table.api.bridge.scala.{StreamTableEnvironment, _} import org.apache.flink.table.api.{AnyWithOperations, EnvironmentSettings, FieldExpression, call} import org.apache.flink.test.util.MiniClusterWithClientResource object FlinkTableJoinTest { // See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html#testing-flink-jobs private val FlinkCluster = new MiniClusterWithClientResource( new MiniClusterResourceConfiguration .Builder() .setNumberSlotsPerTaskManager(2) .setNumberTaskManagers(1) .build ) def main(args: Array[String]): Unit = { FlinkCluster.before() // for batch programs use ExecutionEnvironment instead of StreamExecutionEnvironment val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime) env.setParallelism(2) // create settings val setting = EnvironmentSettings .newInstance() .useBlinkPlanner() .inStreamingMode() .build() // create a TableEnvironment val tableEnv = StreamTableEnvironment.create(env, setting) // create a Table instance for Company val companiesMasterFilePath = \u0026#34;data/companies.csv\u0026#34; val companies = readCompaniesMaster(companiesMasterFilePath, env) .toTable(tableEnv, $\u0026#34;ticker\u0026#34;.as(\u0026#34;c_ticker\u0026#34;), $\u0026#34;name\u0026#34;, $\u0026#34;c_proc_time\u0026#34;.proctime) // temporal table function val func = companies.createTemporalTableFunction($\u0026#34;c_proc_time\u0026#34;, $\u0026#34;c_ticker\u0026#34;) // create a Table instance for Stock val stocks = env .fromCollection(new UnboundedStocks) .toTable(tableEnv, $\u0026#34;ticker\u0026#34;.as(\u0026#34;s_ticker\u0026#34;), $\u0026#34;price\u0026#34;, $\u0026#34;s_proc_time\u0026#34;.proctime) // join with a temporal table function val results = stocks .joinLateral(call(func, $\u0026#34;s_proc_time\u0026#34;), $\u0026#34;s_ticker\u0026#34; === $\u0026#34;c_ticker\u0026#34;) .select($\u0026#34;s_ticker\u0026#34;, $\u0026#34;name\u0026#34;, $\u0026#34;price\u0026#34;) .toAppendStream[(String, String, Double)] .print env.execute() FlinkCluster.after() } private def readCompaniesMaster(companiesMasterFilePath: String, env: StreamExecutionEnvironment): DataStream[Company] = { env .readFile( new TextInputFormat(new Path(companiesMasterFilePath)), companiesMasterFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 10 * 1000 ) .map { line =\u0026gt; val items = line.split(\u0026#34;,\u0026#34;) Company(items(0), items(1)) } } } 実行結果 上記を実行すると以下のような stream と static が結合された結果レコードが流れ続ける。\n2\u0026gt; (AMZN,Amazon,110.05826176785374) 2\u0026gt; (AMZN,Amazon,237.82717323588966) 1\u0026gt; (FB,Facebook,147.96046700184428) 1\u0026gt; (GOOGL,Google,393.58555322242086) 2\u0026gt; (AMZN,Amazon,104.18843434881401) 前回と同様に data/companies.csv の中身を更新するとその結果が反映される。\n削除が反映されないのも同じだった。\nおそらく physical な処理としてはほぼ同じようになっていると思われる。\nまとめ 前回と同様の stream data と static data の join を、Table API + temporal table function で行えることを確認した。\ntemporal table function の概念さえ把握できれば Straem API のときに比べて簡潔に処理を記述できた。\n","permalink":"https://soonraah.github.io/posts/flink-join-by-temporal-table-function/","summary":"前回の記事 では Apache Flink における stream data と static data の join において、DataStream API における broadcast state pattern を使う方法を示した。\n今回の記事では Table API の temporal table function を用いた実験を行う。\nTable API Table API は名前のとおりで class Table を中心として SQL-like な DSL により処理を記述するという、DataStream API より high-level な API となっている。\nこれらの関係は Apaceh Spark の RDD と DataFrame (DataSet) の関係に似ている。\nSQL-like な API で記述された処理が実行時に最適化されて low-level API の処理に翻訳されるところも同じだ。\nRDB の table の概念を元にしているものと考えられるが、本質的に table の概念とストリーム処理はあまりマッチしないと思う。\ntable はある時点のデータセット全体を表すのに対し、ストリーム処理ではやってくるレコードを逐次的に処理したい。\nここを合わせているため、ストリーム処理における Table API による処理の挙動の理解には注意が必要だ。","title":"Apache Flink の Temporary Table Function を用いた stream data と static data の join"},{"content":"star schema における fact table と dimension table の join のようなことを Apache Flink におけるストリーム処理で行いたい。\nstream data と static data の join ということになる。\nただし dimension table 側も更新されるため、完全な static というわけではない。\nこのポストでは Flink v1.11 を前提とした。\njoin の方法 今回は DataStream API でこれを実現することを考える。\nFlink のドキュメントを読むと broadcast state pattern でできそうだ。\nThe Broadcast State Pattern やり方としては次のようになる。\nstatic data のファイルを FileProcessingMode.PROCESS_CONTINUOUSLY で読み込み DataStream 化 1 を broadcast() stream data の DataStream と 2 を connect() static data を PROCESS_CONTINUOUSLY で読むのは変更を得るため。\nPROCESS_ONCE で読んでしまうとストリーム処理の開始時に1回読むだけになり、dimension table の変更を得られない。\nこのあたりの仕様については Data Sources を参照。\nその後は broadcast state pattern にそのまま従う。\nしたがって BroadcastProcessFunction or KeyedBroadcastProcessFunction を実装する必要がある。\nその中で static data を取り込んで state として持ち、stream data 側で参照すればよい。\n2つのデータの各 term に対する関係性を以下に示す。\nin star schema stream or static broadcast or not dimension table static data broadcasted fact table stream data non-broadcasted 実験 実験概要 stream data として株価のデータを考える。\n適当に乱数で作った株価が \u0026ldquo;GOOGL\u0026rdquo; 等の ticker とともに流れてくる。\n一方、会社情報が記載された dimension table 的なファイルも用意する。\n流れ続ける株価データに対して ticker を key にして会社情報を紐付ける、ということを行う。\nコード 上記を実行するためのコードを以下に示す。\n実行可能なプロジェクトを GitHub に置いたので興味があればどうぞ。\nEntry Point main() の実装。\nMiniClusterWithClientResource は本来は単体テスト用だが、簡単に local で cluster を動かすためにここで使用している。\npackage example import org.apache.flink.api.common.state.MapStateDescriptor import org.apache.flink.api.java.io.TextInputFormat import org.apache.flink.core.fs.Path import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration import org.apache.flink.streaming.api.functions.source.FileProcessingMode import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment, createTypeInformation} import org.apache.flink.test.util.MiniClusterWithClientResource object FlinkJoinTest { // See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html#testing-flink-jobs private val FlinkCluster = new MiniClusterWithClientResource( new MiniClusterResourceConfiguration .Builder() .setNumberSlotsPerTaskManager(2) .setNumberTaskManagers(1) .build ) def main(args: Array[String]): Unit = { FlinkCluster.before() val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val companiesMasterFilePath = \u0026#34;data/companies.csv\u0026#34; val companies = readCompaniesMaster(companiesMasterFilePath, env) .broadcast(new MapStateDescriptor( \u0026#34;CompanyState\u0026#34;, classOf[String], // ticker classOf[String] // name )) // the broadcast state pattern // See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html env .fromCollection(new UnboundedStocks) .connect(companies) .process(new StockBroadcastProcessFunction) .print() env.execute(\u0026#34;flink join test\u0026#34;) FlinkCluster.after() } private def readCompaniesMaster(companiesMasterFilePath: String, env: StreamExecutionEnvironment): DataStream[Company] = { env .readFile( new TextInputFormat(new Path(companiesMasterFilePath)), companiesMasterFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 10 * 1000 ) .map { line =\u0026gt; val items = line.split(\u0026#34;,\u0026#34;) Company(items(0), items(1)) } } } Records 各種レコードを表す case class。\nUnboundedStocks は一定のインターバルで Stock を無限に返す iterator であり、stream data 生成に利用する。\ncase class Company(ticker: String, name: String) case class Stock(ticker: String, price: Double) /** * Iterator to generate unbounded stock data */ class UnboundedStocks extends Iterator[Stock] with Serializable { override def hasNext: Boolean = true // unbounded override def next(): Stock = { Thread.sleep(1000) val tickers = Seq(\u0026#34;GOOGL\u0026#34;, \u0026#34;AAPL\u0026#34;, \u0026#34;FB\u0026#34;, \u0026#34;AMZN\u0026#34;) val ticker = tickers(Random.nextInt(tickers.size)) // one of GAFA val price = 100 + Random.nextDouble() * 300 // random price Stock(ticker, price) } } BroadcastProcessFunction 肝である BroadcastProcessFunction の実装。\npackage example import org.apache.flink.api.common.state.MapStateDescriptor import org.apache.flink.streaming.api.functions.co.BroadcastProcessFunction import org.apache.flink.util.Collector class StockBroadcastProcessFunction extends BroadcastProcessFunction[Stock, Company, (String, String, Double)] { private val StateDescriptor = new MapStateDescriptor( \u0026#34;CompanyState\u0026#34;, classOf[String], // ticker classOf[String] // name ) override def processElement(value: Stock, ctx: BroadcastProcessFunction[Stock, Company, (String, String, Double)]#ReadOnlyContext, out: Collector[(String, String, Double)]): Unit = { val companyName = ctx.getBroadcastState(StateDescriptor).get(value.ticker) out.collect((value.ticker, Option(companyName).getOrElse(\u0026#34;-\u0026#34;), value.price)) } override def processBroadcastElement(value: Company, ctx: BroadcastProcessFunction[Stock, Company, (String, String, Double)]#Context, out: Collector[(String, String, Double)]): Unit = { ctx.getBroadcastState(StateDescriptor).put(value.ticker, value.name) } } 実行結果 上記を実行すると以下のような stream と static が結合された結果レコードが流れ続ける。\n2\u0026gt; (FB,Facebook,158.76057838239333) 1\u0026gt; (GOOGL,Google,288.4271251901199) 2\u0026gt; (AAPL,Apple,191.00515338617706) 1\u0026gt; (FB,Facebook,121.98205452369652) 2\u0026gt; (FB,Facebook,140.05023554456997) この状態で会社情報が記載されている data/companies.csv を更新することを考える。\n例えば \u0026ldquo;GOOGL\u0026rdquo; の社名を \u0026ldquo;Google\u0026rdquo; から \u0026ldquo;Alphabet\u0026rdquo; に変更して保存してみた。\nするとしばらくしてその修正が反映された結果が流れてくるようになる。\n1\u0026gt; (GOOGL,Alphabet,288.1008081843843) 2\u0026gt; (AMZN,Amazon,137.11135563851838) 1\u0026gt; (GOOGL,Alphabet,121.78368168964735) 2\u0026gt; (FB,Facebook,236.53483047124948) 1\u0026gt; (FB,Facebook,220.44300865769645) static data の更新が反映されることが確認できた。\n今回は10秒に1回のインターバルで元ファイルを確認するようにファイルを読んでいるため、変更してからそれが反映されるまで最大10秒程度かかる。\n懸念点 join はできたが次のような懸念点がある。\nレコードの削除に対応していない state を上書きしているだけなので companies.csv から削除されたレコードは感知できない ファイルの更新時に処理が重くなる可能性がある companies.csv の更新タイミングでその中の全レコードを処理してしまう checkpoint が大きくなる state が broadcast されているため、task ごとに重複した state が保存されてしまう See Important Considerations まとめ このように broadcast state pattern によって stream data と static data との join 処理を行うことができた。\nただし、まだちゃんと調べていないが DataStream API ではなく Table API を使えばもう少しカジュアルな感じで近いことができるかもしれない。\n気が向いたらそちらも試してみる。\n追記 Table API を使った場合についての記事を追加しました。\nApache Flink の Temporary Table Function を用いた stream data と static data の join ","permalink":"https://soonraah.github.io/posts/flink-join-by-broadcast-state-pattern/","summary":"star schema における fact table と dimension table の join のようなことを Apache Flink におけるストリーム処理で行いたい。\nstream data と static data の join ということになる。\nただし dimension table 側も更新されるため、完全な static というわけではない。\nこのポストでは Flink v1.11 を前提とした。\njoin の方法 今回は DataStream API でこれを実現することを考える。\nFlink のドキュメントを読むと broadcast state pattern でできそうだ。\nThe Broadcast State Pattern やり方としては次のようになる。\nstatic data のファイルを FileProcessingMode.PROCESS_CONTINUOUSLY で読み込み DataStream 化 1 を broadcast() stream data の DataStream と 2 を connect() static data を PROCESS_CONTINUOUSLY で読むのは変更を得るため。","title":"Apache Flink の Broadcast State Pattern を用いた stream data と static data の join"},{"content":"GitHub Flow ベースの開発においてはあまり大きな pull request を作ってほしくないという話。\nなんというか今更わざわざ言わなくてもいいんだけど…\n仕事で何度か大きな pull request が投げられているのを見てしまったので、それはあまりよくないよというのを自分でも指摘しやすくするためにまとめておく。\nReference 最初に参考資料を挙げておく。\n100 Duck-Sized Pull Requests 「巨大プルリク1件vs細かいプルリク100件」問題を考える（翻訳） Optimal pull request size これらを読めば特に私から言うこともないのだが…\nこれらに書いてあるとおりだが補足しておく。\nPull Request を小分けにしたときのメリット module を適切に切り出すモチベーションが得られる 次のような話がある。\n共通化という考え方はアンチパターンを生み出すだけ説 これを理由に module を分けるべきところが分けられず、いろんなことができてしまうベタッと大きな module が生まれるのを見た。\nもちろんよく考えられていない共通化は駄目だが、上記ポストでは一方で\nただし共通化という名の下におこなわれるのは「同じロジックを持つコードをまとめる」行為であって、抽象化のようにそのコード単位の意味を捉える作業はその範疇にない。抽象化というのはロジックを意味単位ごとにひとくくりにしていく行為で、これがどういうことなのかは次以降で述べていく。\n\u0026ndash; 共通化という考え方はアンチパターンを生み出すだけ説 - タオルケット体操\nと述べており抽象化、つまりコードの意味を考慮した上で適切な単位でまとめておくことは否定していない。\npull request を小さく分けるという行為のためには module や機能を適度なまとまりで切り分けること、つまり抽象化を考えていくことが必要となる。\nしたがって何でもできてしまう大きな module が生まれるのを防ぐ方向に働く。\n(もちろんここで駄目な共通化がなされてしまうこともあるだろう)\nリリースまでの期間が短く済む 前述の参考資料においても pull request が大きいと\nDevelopers would put off reviews until they had time/energy and the development process would come to a halt.\n\u0026ndash; 100 Duck-Sized Pull Requests\ndevelopment continues on the master branch, it often results in merge conflicts, rebases, and other fun.\n\u0026ndash; 100 Duck-Sized Pull Requests\nとなると述べられている。\n仮にすぐにレビューされ、かつ conflict なども発生しなかっとしても大きな1つの pull request をレビューする場合とそれを3つに分けた場合の開発プロセスの進行を比較すると\n大きな1つの pull request 全体の開発 全体のレビュー 3つに分割された pull request part-1 の開発 part-1 のレビュー \u0026amp; part-2 の開発 part-2 のレビュー \u0026amp; part-3 の開発 part-3 のレビュー のようになり、開発とレビューを並列で進められる部分があるので全体としての開発期間を短くすることができる。\nもちろんこれはうまく噛み合ったときの例だが。\n早めにフィードバックが得られるのも大きい。\nPull Request を小分けにしたときのデメリット 小分けにすることでレビュワーが全体感をつかみにくくなるというのはあるかもしれない。\npart-1 と part-2 で同じレビュワーになるとも限らない。\nこれに対しては開発に着手する前にまず全体のざっくりとした設計についてレビューを受けることで対応できる。\nこのステップを入れることで「とりあえず手を動かそう」となりにくくなる。\n手を動かすと仕事してる感が出るのでとりあえず手を動かしたくなりがちだが、ちょっと待てよと。\nまず何を作るか、全体の工程を考えましょうと。\nWorking under these constraints causes developers to break problems down into incremental deliverables. It helps avoid the temptation of jumping into development without a clear plan.\n\u0026ndash; 100 Duck-Sized Pull Requests\nまとめ pull request を分けてくれ、頼む。\n","permalink":"https://soonraah.github.io/posts/no-more-huge-pull-request/","summary":"GitHub Flow ベースの開発においてはあまり大きな pull request を作ってほしくないという話。\nなんというか今更わざわざ言わなくてもいいんだけど…\n仕事で何度か大きな pull request が投げられているのを見てしまったので、それはあまりよくないよというのを自分でも指摘しやすくするためにまとめておく。\nReference 最初に参考資料を挙げておく。\n100 Duck-Sized Pull Requests 「巨大プルリク1件vs細かいプルリク100件」問題を考える（翻訳） Optimal pull request size これらを読めば特に私から言うこともないのだが…\nこれらに書いてあるとおりだが補足しておく。\nPull Request を小分けにしたときのメリット module を適切に切り出すモチベーションが得られる 次のような話がある。\n共通化という考え方はアンチパターンを生み出すだけ説 これを理由に module を分けるべきところが分けられず、いろんなことができてしまうベタッと大きな module が生まれるのを見た。\nもちろんよく考えられていない共通化は駄目だが、上記ポストでは一方で\nただし共通化という名の下におこなわれるのは「同じロジックを持つコードをまとめる」行為であって、抽象化のようにそのコード単位の意味を捉える作業はその範疇にない。抽象化というのはロジックを意味単位ごとにひとくくりにしていく行為で、これがどういうことなのかは次以降で述べていく。\n\u0026ndash; 共通化という考え方はアンチパターンを生み出すだけ説 - タオルケット体操\nと述べており抽象化、つまりコードの意味を考慮した上で適切な単位でまとめておくことは否定していない。\npull request を小さく分けるという行為のためには module や機能を適度なまとまりで切り分けること、つまり抽象化を考えていくことが必要となる。\nしたがって何でもできてしまう大きな module が生まれるのを防ぐ方向に働く。\n(もちろんここで駄目な共通化がなされてしまうこともあるだろう)\nリリースまでの期間が短く済む 前述の参考資料においても pull request が大きいと\nDevelopers would put off reviews until they had time/energy and the development process would come to a halt.","title":"あまり大きな Pull Request を作ってほしくない"},{"content":"2020-07-31 にオンライン開催された Spark Meetup Tokyo #3 Online に参加した。\n感想などメモしておく。\n全体感 トピックとしては主に\nSpark 3.0 Spark + AI Summit 2020 Spark 周辺要素 といったところだろうか。\n最近のコミュニティの動向や関心を日本語で聞くことができてよかった。\n運営 \u0026amp; スピーカーの皆様、ありがとうございます。\n発表 発表資料は公開されたら追加していく。\nSPARK+AI Summit 2020 イベント概要 スピーカー: @tokyodataguy さん\nSummit は金融業界の参加者が増えているらしい Spark で最も使われている言語は Python とのことだったが、Databricks の notebook サービスの話だった プロダクションコードではまた違うのだろう Spark 3.0 の update をざっくりと Spark 周辺要素の話をざっくりと Koalas Delta Lake Redash MLflow Introducing Koalas 1.0 Introducing Koalas 1.0 (and 1.1) from Takuya UESHIN スピーカー: @ueshin さん\nWhat\u0026rsquo;s Koalas open source の pure Python library pandas の API で Spark を動かせるようにする 小さいデータも大きなデータも同じように扱えるように 最近 v1.1 をリリース (個人的には pandas の API があまり好きではなく…) SPARK+AI Summit 2020 のセッションハイライト Spark + AI Summit 2020セッションのハイライト（Spark Meetup Tokyo #3 Online発表資料） from NTT DATA Technology \u0026amp; Innovation スピーカー: @masaru_dobashi さん\nSummit のセッションから case study 的なセッションを2つピックアップ USCIS の例 Lessons Learned from Modernizing USCIS Data Analytics Platform 古き良き Data Warehouse から Data Lake への移行 injection には Kafka も利用 諸々気にせずに気軽にデータをストレージに置きたい Alibaba の例 Spark Structured Streming の上に SQL-like なものを載せた？ ストリームの mini-batch 処理の合間で compaction を行う (つらそう) スライド中の「パイプラインの途中でモダンな技術に流し込めるかどうか？」という言葉が印象的だった モダンなものに移行するとき現実的に重要 Spark v3.0の紹介 前半 Introduction new features in Spark 3.0 from Kazuaki Ishizaki スピーカー: @kiszk さん\nSQL の性能に関わる7大機能の話 Query planの新しい表示方法 Join hintsの強化 Adaptive query execution Dynamic partitioning pruning nested column pruning \u0026amp; pushdown の強化 Aggregation のコード生成の改良 ScalaとJavaの新バージョンのサポート 実行計画が見やすくなるのは本当にうれしい 人がチューニングしていた部分が Adaptive Query Plan や Dynamic Partition Pruning で自動化されるのもうれしい 後半 Apache Spark 3.0新機能紹介 - 拡張機能やWebUI関連のアップデート（Spark Meetup Tokyo #3 Online） from NTT DATA Technology \u0026amp; Innovation スピーカー: @raspberry1123 さん\nAccelarator Aware Scheduling Project Hydrogen プラグイン機能 executor や driver の機能を user が拡張できる executor については Spark 2.4 からあったそうだが知らなかった… Structured Straming web UI ストリーム処理にはこういった chart が必要だと思う SparkにPRを投げてみた Sparkにプルリク投げてみた from Noritaka Sekiyama スピーカー: @moomindani さん\nスピーカーは AWS で Glue の開発をしている方 Glue は Spark に強く依存しているため、機能追加にモチベーションがあったとのこと 私も投げてみたい！ LT: td-spark internals: AirframeでSparkの機能を拡張するテクニック td-spark internals: Extending Spark with Airframe - Spark Meetup Tokyo #3 2020 from Taro L. Saito スピーカー: @taroleo さん\nAirframe とは Scala の application 開発用ツール群のようなもの？ Spark の機能を拡張 LT: Spark 3.1 Feature Expectation LT: Spark 3.1 Feature Expectation from Takeshi Yamamuro スピーカー: @maropu さん\n新機能 Support Filter Pushdown JSON JSON の読み取りが早くなる Better Handling for Node Shutdown node 離脱時にその node が保持する shuffle や cache の情報が失われていた 計画された node の離脱に対して shuffle や cache ブロックを別 node に委譲 まとめ どんどん便利になっていくなという印象。\n手でパフォーマンスチューニングする要素はどんどん減っていくのだろう。\nDelta Lake が盛り上がっているように感じた。\nデータレイクについて勉強しないと…\n","permalink":"https://soonraah.github.io/posts/spark-meetup-tokyo-3/","summary":"2020-07-31 にオンライン開催された Spark Meetup Tokyo #3 Online に参加した。\n感想などメモしておく。\n全体感 トピックとしては主に\nSpark 3.0 Spark + AI Summit 2020 Spark 周辺要素 といったところだろうか。\n最近のコミュニティの動向や関心を日本語で聞くことができてよかった。\n運営 \u0026amp; スピーカーの皆様、ありがとうございます。\n発表 発表資料は公開されたら追加していく。\nSPARK+AI Summit 2020 イベント概要 スピーカー: @tokyodataguy さん\nSummit は金融業界の参加者が増えているらしい Spark で最も使われている言語は Python とのことだったが、Databricks の notebook サービスの話だった プロダクションコードではまた違うのだろう Spark 3.0 の update をざっくりと Spark 周辺要素の話をざっくりと Koalas Delta Lake Redash MLflow Introducing Koalas 1.0 Introducing Koalas 1.0 (and 1.1) from Takuya UESHIN スピーカー: @ueshin さん","title":"勉強会メモ: Spark Meetup Tokyo #3 Online"},{"content":"Spark バッチ処理の問題を調べていたら分離レベルという概念にたどりついた。\n分離レベルについて調べたので、Spark の問題の内容と絡めて記しておく。\n考えてみれば当たり前でたいした話ではない。\n分離レベルとは トランザクションの挙動についての暗黙の理解 アドホックな分析クエリやプロダクションコード中のクエリを書くとき、その単一のクエリのトランザクションにおいて「同時に実行されている別のクエリの commit 前の状態や commit 結果に影響され、このクエリの結果がおかしくなるかもしれない」ということは通常考えない。\nトランザクションはデータベースのある時点の状態に対して正しく処理される、というほぼ無意識の理解をおそらくほとんどの開発者が持っている。\n多くの場合この理解は間違っていない。\nそれはなぜかというと DB 等のデータ処理フレームワークがある強さの分離レベルを提供しているからである。\nいろいろな分離レベル ACID 特性のうちの1つ、分離性 (Isolation) の程度を表すのが分離レベル。\nトランザクション中に行われる操作の過程が他の操作から隠蔽されることを指し、日本語では分離性、独立性または隔離性ともいう。より形式的には、独立性とはトランザクション履歴が直列化されていることと言える。この性質と性能はトレードオフの関係にあるため、一般的にはこの性質の一部を緩和して実装される場合が多い。 \u0026ndash; Wikipedia ACID (コンピュータ科学)\n分離レベルには名前のついたものがいくつかあり、分離性の保証の強さが異なる。\n具体的にはトランザクションの並行性の問題への対応力が異なる。\n名著「データ指向アプリケーションデザイン」の第7章で分離レベルについて詳しく述べられているので、以下ではそちらからの引用。\n分離レベルを弱い順に並べる。\nread uncommitted\nこのレベルではダーティライトは生じませんが、ダーティリードは妨げられません。\nread committed\nデータベースからの読み取りを行った際に見えるデータは、コミットされたもののみであること（ダーティリードは生じない）。 データベースへの書き込みを行う場合、上書きするのはコミットされたデータのみであること（ダーティライトは生じない）。 snapshot isolation\nスナップショット分離の考え方は、それぞれのトランザクションがデータベースの一貫性のあるスナップショットから読み取りを行うというものです。すなわち、トランザクションが読み取るデータは、すべてそのトランザクションの開始時点のデータベースにコミット済みのものだけということです。\nserializability\nこの分離レベルはトランザクションが並行して実行されていても、最終的な答えはそれぞれが1つずつ順番に、並行ではなく実行された場合と同じになることを保証します。\n日本語で「分離レベル」を検索すると snapshot isolation の代わりに repeatable read が出てくる事が多い。\nしかし repeatable read の名前は実装によって意味が違っていたりして扱いが難しいらしい。\n分離レベルと race condition の関係 以下に各分離レベルとトランザクションの並行性の問題 (race condition) の関係を示す。\n各 race condition の説明については割愛するが、複数のトランザクションが並行して実行されることにより起こりうる期待されていない挙動だと思えばよい。\n○はその分離レベルにおいてその race condition が発生しないことを示す。\n△は条件によっては発生する。\ndirty read dirty write read skew (nonrepeatable read) lost update write skew phantom read read uncommitted ○ - - - - - read committed ○ ○ - - - - snapshot isolation ○ ○ ○ △ - △ serializability ○ ○ ○ ○ ○ ○ 下に行くほど強い分離レベルとなっている。\n分離レベルが強くなるほど race condition が発生しにくくなるが、一方で lock 等によりパフォーマンスが下がっていくというトレードオフがある。\n各種データベースの分離レベル ここでは MySQL と Hive においてどの分離レベルが提供されているかを見てみる。\nMySQL の場合 MySQL の分離レベルについては以下のドキュメントで述べられている。\n15.7.2.1 Transaction Isolation Levels MySQL (というか InnoDB?) では次の4つの分離レベルを設定することができる。\nREAD UNCOMMITTED READ COMMITTED REPEATABLE READ (default) SERIALIZABLE デフォルトの分離レベルは REPEATABLE READ だが、これは前述の snapshot isolation に相当するらしい。\n分離レベルは、例えば set transaction 構文により次のようにして指定できる。\nset transaction isolation level SERIALIZABLE; この場合は現在のセッション内で実行される次のトランザクションについて適用される。\nすべてのセッションやすべてのトランザクション等の指定もできる。\n詳しくは以下。\n13.3.7 SET TRANSACTION Statement Hive の場合 Hive についてはドキュメントに次のような記載がある。\nAt this time only snapshot level isolation is supported. When a given query starts it will be provided with a consistent snapshot of the data.\n\u0026ndash; Hive Transactions\nHive は snapshot isolation のみ提供しているとのこと。\nThe default DummyTxnManager emulates behavior of old Hive versions: has no transactions and uses hive.lock.manager property to create lock manager for tables, partitions and databases.\n\u0026ndash; Hive Transactions\nlock は小さくとも partition の単位になるのだろうか。\nであるとすると予想通りだが MySQL よりもいかつい挙動になっている。\nこのように多くの DB では snapshot isolation の分離レベルが基本となっている。\nSpark クエリの分離レベル では Spark のクエリはどうだろうか。\nここからようやく本題となる。\nread committed 相当 Spark において DataFrame を用いたデータ処理を記述するとき、それは1つの SQL クエリを書くのと近い感覚になる。\nそもそも DataFrame は SQL-like な使い心地を目的として作られた API だから当然だ。\nDataFrame で記述された処理は実行時に RDD として翻訳されるが、分離レベルを考えるにあたって RDD の特性がキーとなってくる。\nBy default, each transformed RDD may be recomputed each time you run an action on it.\n\u0026ndash; RDD Operations\nあまり良い説明を見つけられなかったが、1回の action を伴う処理においても同じ RDD が複数回参照されるとき、その RDD までの計算は通常やり直されることになる。\nしたがって例えば self join のようなことをするとき、同じデータソースを2回読みに行くということが起こってしまう。\nHDFS や S3 上のファイル、JDBC 経由の外部 DB など Spark は様々なデータソースを扱うことができるが、通常 Spark がその2回の読み込みに対して lock をかけたりすることはできない。\nつまり non-repeatable read や phantom read を防ぐことができない。\nread committed という弱い分離レベルに相当するということになってしまう。\n分離レベルという言葉はトランザクションという概念に対して使われるものであり、DataFrame のクエリをトランザクションと呼んでいいのかはわからない。\nなので分離レベルという言葉をここで使うのが適切でないかもしれないということは述べておく。\n検証 MySQL からデータを読み取り Spark で処理することを考える。\nまず local の MySQL で次のような table を用意する。\nmysql\u0026gt; describe employees; +---------------+---------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------------+---------+------+-----+---------+-------+ | id | int(11) | NO | PRI | NULL | | | salary | int(11) | YES | | NULL | | | department_id | int(11) | YES | | NULL | | +---------------+---------+------+-----+---------+-------+ 3 rows in set (0.03 sec) 部署 (department) ごとの給料 (salary) の平均から各従業員の給料がどれくらい離れているかの差分を見たいものとする。\nSpark のコードは次のようになる。\nSpark のバージョンはこれを書いている時点での最新 3.0.0 とした。\npackage com.example import org.apache.spark.sql.functions.avg import org.apache.spark.sql.SparkSession object IsolationLevelExperiment { def main(args: Array[String]): Unit = { // Prepare SparkSession val spark = SparkSession .builder() .appName(\u0026#34;Isolation Level Experiment\u0026#34;) .master(\u0026#34;local[*]\u0026#34;) .getOrCreate() import spark.implicits._ // Read from MySQL val dfEmployee = spark .read .format(\u0026#34;jdbc\u0026#34;) .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:mysql://localhost\u0026#34;) .option(\u0026#34;dbtable\u0026#34;, \u0026#34;db_name.employees\u0026#34;) .option(\u0026#34;user\u0026#34;, \u0026#34;user_name\u0026#34;) .option(\u0026#34;password\u0026#34;, \u0026#34;********\u0026#34;) .option(\u0026#34;driver\u0026#34;, \u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;) .load .cache // Get average salary val dfAvg = dfEmployee .groupBy($\u0026#34;department_id\u0026#34;) .agg(avg($\u0026#34;salary\u0026#34;).as(\u0026#34;avg_salary\u0026#34;)) // Calculate diff val dfResult = dfEmployee .as(\u0026#34;e\u0026#34;) .join( dfAvg.as(\u0026#34;a\u0026#34;), $\u0026#34;e.department_id\u0026#34; === $\u0026#34;a.department_id\u0026#34;, \u0026#34;left_outer\u0026#34; ) .select( $\u0026#34;e.id\u0026#34;, $\u0026#34;e.department_id\u0026#34;, ($\u0026#34;e.salary\u0026#34; - $\u0026#34;a.avg_salary\u0026#34;).as(\u0026#34;salary_diff\u0026#34;) ) // Output results dfResult.show spark.stop() } } このコードを実行する前に MySQL の general query log を ON にする。\nmysql\u0026gt; set global general_log = \u0026#39;ON\u0026#39;; Query OK, 0 rows affected (0.02 sec) mysql\u0026gt; show global variables like \u0026#39;general_log%\u0026#39;; +------------------+--------------------------------------+ | Variable_name | Value | +------------------+--------------------------------------+ | general_log | ON | | general_log_file | /usr/local/var/mysql/MacBook-Pro.log | +------------------+--------------------------------------+ 2 rows in set (0.01 sec) これによって MySQL に対して発行されたクエリがログとして記録されるようになる。\n直感的に snapshot isolation になっているのであれば MySQL に対する select 文は1回だけ発行されるはずである。\nしかし前述のとおり RDD や DataFrame の処理は途中の状態を通常保存せず、同じ RDD や DataFrame を参照していたとしても再計算される。\n上記コードの例だと dfEmployee が2回参照されている。\nコードを実行すると general query log には次のように、データ取得のための select 文が2つ記録されていた。\nそれぞれ join() の左右の table のデータソースを示している。\n8 Query SELECT `id`,`salary`,`department_id` FROM test_fout_dsp.employees 7 Query SELECT `salary`,`department_id` FROM test_fout_dsp.employees WHERE (`department_id` IS NOT NULL) 2つの select 文はそれぞれ別のクエリ、トランザクションとして発行されている。\nしたがって前者の select 文が実行された後、後者の select 文が実行される前に別のトランザクションにより employees が更新されたり挿入・削除されたりすると non-repeatable read や phantom read が発生してしまうのである。\n今回はデータソースへのアクセスを確認するためにデータソースとして MySQL を使ったが、同じことはファイルや他の DB など別のデータソースで起こりうる。\n回避策 プロダクト運用上、non-repeatable read や phantom read が発生しうる状況というのは多くの場合で厄介である。\n一時的なデータソースの状態に依存して問題が発生するため、バグの原因追求がとても困難だからだ。\n見た目上の分離レベルを強くし、これらを避けるには2つの方法が考えられる。\nimmutable なデータにのみアクセスする 単純な話で non-repeatable read や phantom read が発生するようなデータソースを参照しなければよい。\n例えばユーザの行動ログのような蓄積されていくデータが hour 単位で partitioning されているような場合、基本的に過去の partition に対しては変更や挿入・削除は行われない。\nこのような partition にアクセスする分には前述のような厄介な問題は起こらない。\ncache する データソースから読み取った結果の DataFrame に対して cache() または persist() をするとよい。\nSpark SQL can cache tables using an in-memory columnar format by calling spark.catalog.cacheTable(\u0026quot;tableName\u0026quot;) or dataFrame.cache().\n\u0026ndash; Caching Data In Memory\n前述のコードで dfEmployee に対して .cache() をした場合、MySQL へのデータ取得のための select 文の発行は1回になった。\n大きなデータソースを cache() するときだけメモリや HDD 容量に気をつけておきたい。\nまとめ DataFrame はいわゆる RDBMS を操作しているように見えてしまうところが難しいところだろうか。\n「データ指向アプリケーションデザイン」に至極真っ当なことが書いてあったので、その言葉を借りて締めておく。\n何も考えずにツールを信じて依存するのではなく、並行性の問題にはどういったものがあるのか、そしてそれらを回避するにはどうしたら良いのかを、しっかり理解しなければなりません。そうすれば、信頼性があり、正しく動作するアプリケーションを手の届くツールを使って構築できるようになります。\n\u0026ndash; Martin Kleppmann データ指向アプリケーションデザイン\n","permalink":"https://soonraah.github.io/posts/weak_isolation_level_of_dataframe/","summary":"Spark バッチ処理の問題を調べていたら分離レベルという概念にたどりついた。\n分離レベルについて調べたので、Spark の問題の内容と絡めて記しておく。\n考えてみれば当たり前でたいした話ではない。\n分離レベルとは トランザクションの挙動についての暗黙の理解 アドホックな分析クエリやプロダクションコード中のクエリを書くとき、その単一のクエリのトランザクションにおいて「同時に実行されている別のクエリの commit 前の状態や commit 結果に影響され、このクエリの結果がおかしくなるかもしれない」ということは通常考えない。\nトランザクションはデータベースのある時点の状態に対して正しく処理される、というほぼ無意識の理解をおそらくほとんどの開発者が持っている。\n多くの場合この理解は間違っていない。\nそれはなぜかというと DB 等のデータ処理フレームワークがある強さの分離レベルを提供しているからである。\nいろいろな分離レベル ACID 特性のうちの1つ、分離性 (Isolation) の程度を表すのが分離レベル。\nトランザクション中に行われる操作の過程が他の操作から隠蔽されることを指し、日本語では分離性、独立性または隔離性ともいう。より形式的には、独立性とはトランザクション履歴が直列化されていることと言える。この性質と性能はトレードオフの関係にあるため、一般的にはこの性質の一部を緩和して実装される場合が多い。 \u0026ndash; Wikipedia ACID (コンピュータ科学)\n分離レベルには名前のついたものがいくつかあり、分離性の保証の強さが異なる。\n具体的にはトランザクションの並行性の問題への対応力が異なる。\n名著「データ指向アプリケーションデザイン」の第7章で分離レベルについて詳しく述べられているので、以下ではそちらからの引用。\n分離レベルを弱い順に並べる。\nread uncommitted\nこのレベルではダーティライトは生じませんが、ダーティリードは妨げられません。\nread committed\nデータベースからの読み取りを行った際に見えるデータは、コミットされたもののみであること（ダーティリードは生じない）。 データベースへの書き込みを行う場合、上書きするのはコミットされたデータのみであること（ダーティライトは生じない）。 snapshot isolation\nスナップショット分離の考え方は、それぞれのトランザクションがデータベースの一貫性のあるスナップショットから読み取りを行うというものです。すなわち、トランザクションが読み取るデータは、すべてそのトランザクションの開始時点のデータベースにコミット済みのものだけということです。\nserializability\nこの分離レベルはトランザクションが並行して実行されていても、最終的な答えはそれぞれが1つずつ順番に、並行ではなく実行された場合と同じになることを保証します。\n日本語で「分離レベル」を検索すると snapshot isolation の代わりに repeatable read が出てくる事が多い。\nしかし repeatable read の名前は実装によって意味が違っていたりして扱いが難しいらしい。\n分離レベルと race condition の関係 以下に各分離レベルとトランザクションの並行性の問題 (race condition) の関係を示す。\n各 race condition の説明については割愛するが、複数のトランザクションが並行して実行されることにより起こりうる期待されていない挙動だと思えばよい。\n○はその分離レベルにおいてその race condition が発生しないことを示す。","title":"Spark DataFrame クエリの弱い分離レベル"},{"content":"東京の IT 企業で働く中年エンジニアが勉強したことや考えていることなどを書きます。\nこのブログは Hugo で作成しました。\nTheme には PaperMod を利用しています。\n","permalink":"https://soonraah.github.io/about/","summary":"東京の IT 企業で働く中年エンジニアが勉強したことや考えていることなどを書きます。\nこのブログは Hugo で作成しました。\nTheme には PaperMod を利用しています。","title":"About"},{"content":"はじめに Apache Spark 3.0.0 がリリースされました。\nSpark Release 3.0.0 release note を見て個人的に気になったところなど簡単に調べました。\n書いてみると Databricks の記事へのリンクばっかになってしまった…\n全体感 こちらの記事を読めば全体感は OK.\nIntroducing Apache Spark 3.0 公式の release note には\nPython is now the most widely used language on Spark.\nとあってそうなん？ってなったけど、こちらの記事だと\nPython is now the most widely used language on Spark and, consequently, was a key focus area of Spark 3.0 development. 68% of notebook commands on Databricks are in Python.\nと書いてありどうやら Databricks の notebook の話らしく、だったらまあそうかなという感じ。\nプロダクトコードへの実装というよりは、アドホック分析や検証用途の話なんでしょう。\n[Project Hydrogen] Accelerator-aware Scheduler SPARK-24615 Spark 上で deep learning できるようにすることを目指す Project Hydrogen、その3つの大きな目標のうちの一つ。\nBig Data LDN 2018: PROJECT HYDROGEN: UNIFYING AI WITH APACHE SPARK YARN や Kubernetes では GPU や FPGA を扱えるようになっているので Spark でも扱えるようにしたいというモチベーション。\nSpark のドキュメント によると\nFor example, the user wants to request 2 GPUs for each executor. The user can just specify spark.executor.resource.gpu.amount=2 and Spark will handle requesting yarn.io/gpu resource type from YARN.\nのようにして executor に GPU リソースを要求できるみたいです。\nAdaptive Query Execution SPARK-31412 平たく言うと実行時に得られる統計情報を使って plan を最適化すると、静的に生成された plan より効率化できるよねという話。\nspark.sql.adaptive.enabled=true にすることで有効になります。\n処理の途中で中間生成物が materialize されるタイミングで、その時点の統計情報を使って残りの処理を最適化する、というのを繰り返します。\nSpark 3.0.0 では以下3つの AQE が実装されました。\nCoalescing Post Shuffle Partitions Converting sort-merge join to broadcast join Optimizing Skew Join Spark 2 以前だとこのあたりは実行しつつチューニングするような運用になりがちでした。\n特に skew の解消は salt を追加したりなど面倒だったりします。\nこれらが自動で最適化されるというのは運用上うれしいところ。\n急なデータ傾向の変化に対しても自動で最適化して対応できるという面があります。\nAQE に関してもやはり Databricks の解説記事がわかりやすいです。\n図もいい感じ。\nAdaptive Query Execution: Speeding Up Spark SQL at Runtime Dynamic Partition Pruning SPARK-11150 こちらも AQE 同様にクエリのパフォーマンスを改善する目的で導入されたもの。\n改善幅は AQE より大きいようです。\nやはり実行時の情報を使って partition pruning を行い、不要な partition の参照を減らすという方法。\n主に star schema における join 時のように、静的には partition pruning が行えない場合を想定しています。\n比較的小さいことが多いと思われる dimension table 側を broadcast する broadcast join において、broadcast された情報を fact table の partition pruning に利用するというやり口。\nDynamic Partition Pruning in Apache Spark Structured Streaming UI SPARK-29543 \u0026ldquo;Structured Streaming\u0026rdquo; というタブが UI に追加された件。\nSpark のドキュメントに例があります。\nStructured Streaming Tab\nSpark 2.4 系では Structured Streaming を動かしていてもせいぜい job や stage が増えていくという味気ないものしか見えませんでした。\nSpark 3.0.0 で実際に動かしてみたけど欲しかったやつ！という感じ。\nストリーム処理では入力データ量の変化の可視化がマストだと思ってます。\nCatalog plugin API SPARK-31121 SPIP: Spark API for Table Metadata これまでは CTAS (Create Table As Select) の操作はあったが、外部のデータソースに対して DDL 的な操作をする API が足りていませんでした。\nCTAS も挙動に実装依存の曖昧さがありました。\nそこで create, alter, load, drop 等のテーブル操作をできるようにしたという話。\nドキュメントの DDL Statements のあたりを読め何ができるかわかります。\n以前のバージョンでも一部のデータソースについてはできた模様 (ex. Hive)。\n今の自分の業務では Spark から DDL を扱うようなことはしないのでそれほど恩恵は感じられません。\nnotebook からアドホックな Spark のバッチを動かすというような使い方をしていればうれしいかもしれません。\nAdd an API that allows a user to define and observe arbitrary metrics on batch and streaming queries SPARK-29345 クエリの途中の段階で何らかの metrics を仕込んでおいて callback 的にその metrics にアクセスできる仕組み。\nDataset#observe() の API ドキュメント を読むのが一番早いです。\nこの例ではストリーム処理を扱っているが、バッチ処理の例を自分で書いて試してみました。\n// Register listener spark .listenerManager .register(new QueryExecutionListener { override def onSuccess(funcName: String, qe: QueryExecution, durationNs: Long): Unit = { val num = qe.observedMetrics .get(\u0026#34;my_metrics\u0026#34;) .map(_.getAs[Long](\u0026#34;num\u0026#34;)) .getOrElse(-100.0) println(s\u0026#34;num of data: $num\u0026#34;) } override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {} }) // Make DataFrame val df = Seq .range(0, 1000) .map((_, Seq(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;)(Random.nextInt(3)), math.random())) .toDF(\u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;, \u0026#34;value\u0026#34;) // Observe and process val dfResult = df .observe(\u0026#34;my_metrics\u0026#34;, count($\u0026#34;*\u0026#34;).as(\u0026#34;num\u0026#34;)) .groupBy($\u0026#34;type\u0026#34;) .agg(avg($\u0026#34;value\u0026#34;).as(\u0026#34;avg_value\u0026#34;)) // Run dfResult.show これを動かしたときの出力は次のようになりました。\n+----+------------------+ |type| avg_value| +----+------------------+ | c|0.5129435063033314| | b|0.4693004460694317| | a|0.4912087482418599| +----+------------------+ num of data: 1000 observe() はその出力の DataFrame に対して schema やデータの中身を変更することはありません。\nmetrics を仕込むのみ。\nlogical plan を出力してみると observe() を入れることにより途中に CollectMetrics という plan が挿入されていました。\nソースを見ると accumulator を使っている模様。\nなので observe() の集計のみで一度 job が動くわけではなく、クエリが2回走るという感じではありません。\n全体の処理の中でひっそりと accumulator で脇で集計しておくといった趣でしょうか。\nこれは結構有用だと思います。\n例えば何らかの集計をやるにしてもその最初や途中で、例えば入力データは何件あったか？みたいなことをログに出しておきたいことがあります。\nというか accumulator で頑張ってそういうものを作ったことがある…\nこれがフレームワーク側でサポートされるのはうれしいです。\nまとめ 2つのダイナミックな最適化に期待大。\n気が向いたら追加でまた調べるかもしれません。\n","permalink":"https://soonraah.github.io/posts/study-spark-3-0-0/","summary":"はじめに Apache Spark 3.0.0 がリリースされました。\nSpark Release 3.0.0 release note を見て個人的に気になったところなど簡単に調べました。\n書いてみると Databricks の記事へのリンクばっかになってしまった…\n全体感 こちらの記事を読めば全体感は OK.\nIntroducing Apache Spark 3.0 公式の release note には\nPython is now the most widely used language on Spark.\nとあってそうなん？ってなったけど、こちらの記事だと\nPython is now the most widely used language on Spark and, consequently, was a key focus area of Spark 3.0 development. 68% of notebook commands on Databricks are in Python.\nと書いてありどうやら Databricks の notebook の話らしく、だったらまあそうかなという感じ。","title":"Apache Spark 3.0.0 について調べた"}]