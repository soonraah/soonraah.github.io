[{"content":"このポストについて なんか個人的にデータマネジメントの機運が高まってきたので、ずっと積ん読していた DAMA-DMBOK を読んでいこうかなと。\nで、せっかくなのでデータ関係の皆さんがよくやっているように自分としても読書メモをまとめてみようと思った。\n内容を網羅するのではなく、現場のデータエンジニアとして活動した経験を踏まえて自分なりの観点でまとめてみたい。\n今回は第1章「データマネジメント」で、それ以降は関心がある章をつまみ食い的に読んでいく。\nDAMA-DMBOK とは DAMA とは DAta Management Association の略であり、\n世界各地に80の支部を持ち、8,000名を越える会員を擁する全世界のデータ専門家のための国際的な非営利団体です。 特定のベンダーや技術、手法に依存しないことを前提として、データや情報、知識をエンタープライズの重要な資産として管理する必要性の理解を促し、この分野の成長を推進しております。\n\u0026ndash; 一般社団法人 データマネジメント協会 日本支部(DAMA Japan)\nとのこと。\nこの DAMA が刊行しているのがデータマネジメント知識体系ガイド (The DAMA Guide to Data Management Body of Knowledge) であり、その略称が DMBOK である。\nＤＡＭＡ ＤＭＢＯＫは、データマネジメントプロフェッショナルにとって有益な資料かつ指針となることを目指し、データ管理のもっとも信頼できる入門書となるよう編集されています\n\u0026ndash; 一般社団法人 データマネジメント協会 日本支部(DAMA Japan)\nIT 業界歴の長い人なら PMBOK というプロジェクトマネジメントについて書かれた本をご存知かもしれないが、あれのデータマネジメント版だと思っていい。\n私としてはデータマネジメントの教科書的なものだと考えている。\n2023年現在における最新版は2018年の第2版となっている。\n以降では DMBOK2 とする。\n5年前なのでやや古いと思うかもしれないが、内容的には特定技術について書かれているわけではなく、ある程度抽象度が高い話になっているので陳腐化はしにくい。\nデータマネジメントとは 以降、特に注釈のない引用は DMBOK2 第1章からの引用とする。\nデータマネジメントとは、データとインフォメーションという資産の価値を提供し、管理し、守り、高めるために、それらのライフサイクルを通して計画、方針、スケジュール、手順などを開発、実施、監督することである。\nこの一文にいろいろと集約されているので見ていこう。\nデータとインフォーメーション ここでいう「データ」と「インフォメーション」は DIKW モデルにもとづく解釈でよい。\nデータはインフォメーションの原材料であり、インフォメーションは意味と目的、つまりコンテキストを付与されたデータと言える。\nex. 前四半期の売上レポートは「インフォメーション」、それの元になっている DWH のデータが「データ」 データマネジメントにおいては特にこのデータとインフォーメションを扱う。\n資産 資産とは経済的資源であり、所有可能、管理可能で、それ自体に価値があるか、価値を生み出すことができるものである。\nこの定義からデータは資産であるとみなされ、したがって資産として組織で管理される必要がある。\n金融資産などと同じ面もあるが異なっている面も多くあり、例えば使用しても失われない。\nこのような特性が管理方法に影響する。\n資産としてのデータは本来はデータにかかるコスト、データから得る利益を測定して経済的観点から評価されるべき。\nしかしまだそのための基準はないとのこと。\nなので実質的に経済的観点からの評価を実行するのはとてもむずかしいと思われる。\nデータ基盤がコストセンターだと認識されているような組織だとこのあたり必要になってくるのかもしれない。\n提供し データライフサイクル (後述) においてはデータの生成と利用が最も重要である。\nデータは利用されて初めて価値を生む。\nデータにはそれぞれ生産者と利用者がいる。\n社内の一つの部門内でデータ生産・利用されることもあるが、一方で生産と利用が別部門であることもある。\nよってデータライフサイクルを考えるためには組織横断の全社的な視点が必要になってくる。\n管理し データを資産として管理するためには質の高いメタデータが必要となる。\nメタデータとはデータについてのデータという意味。\nデータを記述するものがなければ意味のある管理はできないということ。\nメタデータもデータの一種であるためデータとして管理される必要がある。\n多くの場合、メタデータ管理がデータマネジメント全体を改善する出発点となる。\n守り データは資産であると同時にリスクでもある。\nなくなったり、盗まれたり、誤用されたりもするため、そうならないようにリスクを管理する。\n不正確・不完全・期限切れなど低品質のデータからは正しくないインフォーメーションが得られるリスクや誤用されるリスクがある。\nリスクを避けるためにデータを用いた意思決定に必要なデータ品質が必要となる。\n当然セキュリティの話もある。\n昨今は個人情報への関心も高まっており、それらは保護されなければならない。\n高める データ品質を高める、または高品質であることを保証しなければならない。\nDMBOK2 ではデータ品質に重きを置いており、\nこれがデータマネジメントの根幹である。\nとまで言い切っている。\n利用者からすると品質の問題が明らかになるまではデータの信頼性は高いものと想定されるが、一度信頼が失われると取り戻すのは簡単ではない。\nデータ基盤の同じデータに対して、5分前にクエリした結果と今クエリした結果が違っていると「これ大丈夫かな」ってなりますよね？\n(何らかの原因によるデータ生成処理の再実行など、運用上でよく起こりうる話)\nデータの品質問題にたいしょするために収益の10〜30%を費やしていると専門家は考えている。IBM は米国において低品質のデータのために費やしたコストは2016年で3.1兆ドルであったと推定している。\nとあるとおり、低品質のデータにはコストとリスクがかかる。\n一方で高品質のデータからはより高い生産性や競合に対する優位性などさまざまな恩恵が得られる。\nライフサイクル データにはその発生から破棄に至るまでのライフサイクルがある。\nDMBOK2 データライフサイクルの主要アクティビティ\n上記はその概念的な図だが、実務におけるデータライフサイクルを詳細に記述するのは困難を伴う。\n生産者から利用者までの経路、すなわちデータリネージを表現する必要がある。\nデータ品質、メタデータ、セキュリティなどはライフサイクル全体を通して管理していかなければなない。\n計画、方針、スケジュール データから偶然に価値が生まれることはなく、様々な側面から計画が必要である。\nより高品質なデータを目指す計画においては、アーキテクチャ、モデリング、その他設計機能に対して、戦略的なアプローチが必要である。業務と IT のリーダーが戦略的に連携することも欠かせない。\n組織横断であるため、(理想的には) CDO がリーダーシップを発揮、ビジョンや目的を示し、自らそれにコミットメントしないと効果的なデータマネジメントにならない。\nCDO がいる組織はまだ多くないため、その状況でこの役割を誰が担うかというのがデータマネジメントの成果に大きな影響を与えるはず。\nデータマネジメント・フレームワーク データマネジメントには様々な側面がある。\nデータマネジメントを総合的に理解し、その構成要素間の関係を理解するためにいくつかのフレームワークが提案されている。\nここでは DMBOK2 で紹介されたフレームワークのうちのいくつかを紹介する。\nDAMA-DMBOK フレームワーク データマネジメント全体を構成する知識領域についてまとめたもの。\nDAMA ホイール図, DAMA のご紹介 - DAMA Japan\nこの DAMA ホイール図はおそらく一番有名なもので、見たことある人も多いのでは。\nデータマネジメントにどういった知識領域があるのかが一覧できる。\nただデータガバナンスが中心にあるというだけで、それ以外の各領域の関係性はわかりにくいかもしれない。\n知識領域コンテキスト図, DMBOKv2 Image Downloads - DAMA International\nこちらの知識領域コンテキスト図も DAMA-DMBOK フレームワークの一部である。\nDMBOK1 のときに最も人気のあった図とのこと。\n最初ぱっとこれを見たときはピンとこなかったが、ある事業ドメインにおけるデータの流れやコンテキストを表しているというのに気づきとても有用だと思った。\nこの内容を各事業ドメインで整理すると、組織のデータについての理解が大きく捗りそう。(めっちゃたいへんだけど)\nDMBOK ピラミッド ほとんどの組織はデータマネジメントの戦略を決めてからデータ管理を始めるということができず、走りながらデータマネジメントの改善に取り組むことになる。\nその中でどういったステップをたどるかを示してくれるのが Peter Aiken\u0026rsquo;s Framework である。\nPeter Aiken\u0026rsquo;s Framework, Why HR Data Management Strategy is important in your HR Transformation | SAP Blogs (元ネタは DMBOK2)\nPhase 1 (青): データの保存、統合 Phase 2 (黃): データ品質、アーキテクチャ、メタデータ Phase 3 (緑): ガバナンスと利用推進 Phase 4 (赤): 分析などの高度な利用 組織のデータマネジメントは Phase 1 -\u0026gt; 4 の順で進めていく。\nPhase の順序は上下関係にはなっておらず、上下関係は知識領域間の依存関係を示している。\nこの図は実務でデータマネジメントを進めるにあたり、良い指針となる。\n所感 第1章はタイトルのとおりデータマネジメントについて俯瞰するのに良かった。\n品質、ライフサイクル、リスク管理、メタデータなどの各要素がそれぞれに影響しあっていることが理解できた。\nこれからその各要素を詳しく見ていくことになる。\n組織横断的なデータの扱いを始めてからそんなに経っていない組織だと、CDO はおろかデータマネジメントに関心があるリーダー層がいないというケースが多いのではないだろうか。\nCDO が無理なら CTO あたりに関心を持ってコミットしてもらいたい。\nそれもできない場合は現場のデータエンジニアやアナリストなどが進めていくしかないが、それは厳しい状況でのデータマネジメントになりそう。\nエライ人を巻き込む政治力のようなものが問われる。\n","permalink":"https://soonraah.github.io/posts/dmbok-chapter-1/","summary":"このポストについて なんか個人的にデータマネジメントの機運が高まってきたので、ずっと積ん読していた DAMA-DMBOK を読んでいこうかなと。\nで、せっかくなのでデータ関係の皆さんがよくやっているように自分としても読書メモをまとめてみようと思った。\n内容を網羅するのではなく、現場のデータエンジニアとして活動した経験を踏まえて自分なりの観点でまとめてみたい。\n今回は第1章「データマネジメント」で、それ以降は関心がある章をつまみ食い的に読んでいく。\nDAMA-DMBOK とは DAMA とは DAta Management Association の略であり、\n世界各地に80の支部を持ち、8,000名を越える会員を擁する全世界のデータ専門家のための国際的な非営利団体です。 特定のベンダーや技術、手法に依存しないことを前提として、データや情報、知識をエンタープライズの重要な資産として管理する必要性の理解を促し、この分野の成長を推進しております。\n\u0026ndash; 一般社団法人 データマネジメント協会 日本支部(DAMA Japan)\nとのこと。\nこの DAMA が刊行しているのがデータマネジメント知識体系ガイド (The DAMA Guide to Data Management Body of Knowledge) であり、その略称が DMBOK である。\nＤＡＭＡ ＤＭＢＯＫは、データマネジメントプロフェッショナルにとって有益な資料かつ指針となることを目指し、データ管理のもっとも信頼できる入門書となるよう編集されています\n\u0026ndash; 一般社団法人 データマネジメント協会 日本支部(DAMA Japan)\nIT 業界歴の長い人なら PMBOK というプロジェクトマネジメントについて書かれた本をご存知かもしれないが、あれのデータマネジメント版だと思っていい。\n私としてはデータマネジメントの教科書的なものだと考えている。\n2023年現在における最新版は2018年の第2版となっている。\n以降では DMBOK2 とする。\n5年前なのでやや古いと思うかもしれないが、内容的には特定技術について書かれているわけではなく、ある程度抽象度が高い話になっているので陳腐化はしにくい。\nデータマネジメントとは 以降、特に注釈のない引用は DMBOK2 第1章からの引用とする。\nデータマネジメントとは、データとインフォメーションという資産の価値を提供し、管理し、守り、高めるために、それらのライフサイクルを通して計画、方針、スケジュール、手順などを開発、実施、監督することである。\nこの一文にいろいろと集約されているので見ていこう。\nデータとインフォーメーション ここでいう「データ」と「インフォメーション」は DIKW モデルにもとづく解釈でよい。\nデータはインフォメーションの原材料であり、インフォメーションは意味と目的、つまりコンテキストを付与されたデータと言える。\nex. 前四半期の売上レポートは「インフォメーション」、それの元になっている DWH のデータが「データ」 データマネジメントにおいては特にこのデータとインフォーメションを扱う。","title":"読書メモ: DMBOK2 第1章 データマネジメント"},{"content":"Apache Iceberg の table を near real time に、つまり高頻度で更新するということをやってみた。\nApache Iceberg とは Apache Iceberg (以下 Iceberg) は分散ファイルシステムやクラウドストレージ上の table format であり、Apache Hudi や Delta Lake と並んで data lake や lakehouse architecture で用いられる。\n特徴的なのは table とデータ実体 (Parquet, Avro など) の間に metadata file, manifest list, manifest file の抽象的なレイヤーがあり、ファイル単位で table の状態を track できること。\nこれにより強い isolation level、パフォーマンス、schema evolution など様々な機能・性能を実現できるようになっている。\nApache Iceberg Iceberg Table Spec\n詳しくは公式ドキュメントを参照のこと。\n最近では SmartNews 社が Iceberg で data lake を構築したことを記事にしていた。\nFlink-based Iceberg Real-Time Data Lake in SmartNews (Part I) | by SmartNews | SmartNews, Inc | Apr, 2023 | Medium ベンダー提供の DWH 中心ではなく Lakehouse Architecture を目指すのであれば最有力の table format の1つだと言えそう。\nこのポストでやりたいこと このポストでは Iceberg の table を near real time 更新することを想定して実験を行う。\n多数の IoT デバイスで測定された温度情報が逐次的に送られていることを想定し、定期的にそれをデバイス ID ごとの最新の温度情報を持つ table へと書き出すものとする。\n次のような要件を仮定する。\n低レイテンシが要求されており、near real time での table 更新を行う 次のような schema のレコードが IoT デバイスから送信され得られる状態になっているものとする device_id: デバイスに対して一意に振られている ID operation: upsert or delete (後述) temperature: 測定された温度の値 ts: 測定時の timestamp IoT デバイスは登録削除されることがあり、その場合は table 上の当該デバイス ID のレコードを削除する 削除の場合は operation = 'delete' となっている。それ以外は 'upsert' 送られてくるレコードは timestamp (ts) の順になっているとは限らない (out of order) リアルタイム処理のフレームワークとしては Spark Structured Streaming を使用するものとする。\ntable 更新の実装 上記実験のための実装を行った。\n各言語・フレームワーク等は以下のバージョンを使っている。\nScala 2.13.10 Java 11 Spark 3.3.2 Iceberg 1.2.1 実行可能な sbt project の全体を GitHub repository soonraah/streaming_iceberg に置いているのでご参考まで。\n実装の主要な部分を次に示す。\n// Create SparkSession instance val spark = SparkSession .builder() .appName(\u0026#34;StreamingIcebergExperiment\u0026#34;) .master(\u0026#34;local[2]\u0026#34;) .config(\u0026#34;spark.sql.extensions\u0026#34;, \u0026#34;org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog\u0026#34;, \u0026#34;org.apache.iceberg.spark.SparkCatalog\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog.type\u0026#34;, \u0026#34;hadoop\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog.warehouse\u0026#34;, \u0026#34;data/warehouse\u0026#34;) .getOrCreate() spark.sparkContext.setLogLevel(\u0026#34;WARN\u0026#34;) import spark.implicits._ // Drop table spark .sql( \u0026#34;\u0026#34;\u0026#34;drop table if exists | my_catalog.my_db.device_temperature |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) // Create table spark .sql( \u0026#34;\u0026#34;\u0026#34;create table | my_catalog.my_db.device_temperature |( | device_id int, | operation string, | temperature double, | ts timestamp |) |using iceberg |tblproperties ( | \u0026#39;format-version\u0026#39; = \u0026#39;2\u0026#39;, | \u0026#39;write.delete.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.update.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.merge.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39; |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) val random = new Random() val addOutOfOrderness = udf { (timestamp: Timestamp) =\u0026gt; // add time randomly in [-5, +5) sec val ms = timestamp.getTime + random.nextInt(10000) - 5000 new Timestamp(ms) } // Prepare input data val dfInput = spark .readStream .format(\u0026#34;rate\u0026#34;) .option(\u0026#34;rowsPerSecond\u0026#34;, 1) .load() .select( // Generate device_id in 0~127 randomly (rand() * 128).cast(IntegerType).as(\u0026#34;device_id\u0026#34;), // 90% of operation is upsert and 10% is delete when(rand() \u0026gt; 0.1, \u0026#34;upsert\u0026#34;).otherwise(\u0026#34;delete\u0026#34;).as(\u0026#34;operation\u0026#34;), // Generate temperature randomly based on standard normal distribution (randn() * 5.0 + 20.0).as(\u0026#34;temperature\u0026#34;), // timestamp is out of order addOutOfOrderness($\u0026#34;timestamp\u0026#34;).as(\u0026#34;ts\u0026#34;) ) // Update table for each mini batch dfInput .writeStream .trigger(Trigger.ProcessingTime(30.seconds)) .foreachBatch { (dfBatch: DataFrame, batchId: Long) =\u0026gt; println(s\u0026#34;Processing batchId=$batchId\u0026#34;) // Eliminate duplicated device_id val dfDedup = dfBatch .withColumn( \u0026#34;row_num\u0026#34;, row_number() .over(Window.partitionBy($\u0026#34;device_id\u0026#34;).orderBy($\u0026#34;ts\u0026#34;.desc)) ) .where($\u0026#34;row_num\u0026#34; === 1) .drop($\u0026#34;row_num\u0026#34;) // createOrReplaceTempView() doesn\u0026#39;t work dfDedup.createOrReplaceGlobalTempView(\u0026#34;input\u0026#34;) // Insert, update and delete records by \u0026#39;merge into\u0026#39; spark .sql( \u0026#34;\u0026#34;\u0026#34;merge into | my_catalog.my_db.device_temperature |using( | select | * | from | global_temp.input |) as input |on | device_temperature.device_id = input.device_id | and device_temperature.ts \u0026lt; input.ts |when matched and input.operation = \u0026#39;upsert\u0026#39; then update set * |when matched and input.operation = \u0026#39;delete\u0026#39; then delete |when not matched then insert * |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) () } .start .awaitTermination() spark.stop() この中の各処理を以下で説明していく。\n// Create SparkSession instance val spark = SparkSession .builder() .appName(\u0026#34;StreamingIcebergExperiment\u0026#34;) .master(\u0026#34;local[2]\u0026#34;) .config(\u0026#34;spark.sql.extensions\u0026#34;, \u0026#34;org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog\u0026#34;, \u0026#34;org.apache.iceberg.spark.SparkCatalog\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog.type\u0026#34;, \u0026#34;hadoop\u0026#34;) .config(\u0026#34;spark.sql.catalog.my_catalog.warehouse\u0026#34;, \u0026#34;data/warehouse\u0026#34;) .getOrCreate() Spark から Iceberg を使うためには extensions の設定、および catalog の設定が必要となる。\nこれらは設定ファイルやコマンドラインからも指定できるが、今回は実験のためソースコード上で簡易に設定した。\n// Create table spark .sql( \u0026#34;\u0026#34;\u0026#34;create table | my_catalog.my_db.device_temperature |( | device_id int, | operation string, | temperature double, | ts timestamp |) |using iceberg |tblproperties ( | \u0026#39;format-version\u0026#39; = \u0026#39;2\u0026#39;, | \u0026#39;write.delete.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.update.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39;, | \u0026#39;write.merge.mode\u0026#39; = \u0026#39;merge-on-read\u0026#39; |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) table 作成のクエリを実行している。\n頻度の高い更新・削除を想定しているため、ここでは各操作に対して copy-on-write ではなく merge-on-read を指定している。\nこれにより書き込み時にデータファイル全体をコピーするのではなく、差分のみ追加するという形となり書き込みのコストが小さくなる。\nmerge-on-read は format version 2 でしか指定できないため、その設定もしている。\n// Prepare input data val dfInput = spark .readStream .format(\u0026#34;rate\u0026#34;) .option(\u0026#34;rowsPerSecond\u0026#34;, 1) .load() .select( // Generate device_id in 0~127 randomly (rand() * 128).cast(IntegerType).as(\u0026#34;device_id\u0026#34;), // 90% of operation is upsert and 10% is delete when(rand() \u0026gt; 0.1, \u0026#34;upsert\u0026#34;).otherwise(\u0026#34;delete\u0026#34;).as(\u0026#34;operation\u0026#34;), // Generate temperature randomly based on standard normal distribution (randn() * 5.0 + 20.0).as(\u0026#34;temperature\u0026#34;), // timestamp is out of order addOutOfOrderness($\u0026#34;timestamp\u0026#34;).as(\u0026#34;ts\u0026#34;) ) ここでは IoT デバイスから送信されたとするストリームデータを作成している。\nまずは .readStream.foramt(\u0026quot;rate\u0026quot;) によりストリームを発生させ、select() の部分で想定する schema にして値を入れている。\n詳細はコメントを参照。\n1点だけ補足すると、addOutOfOrderness() の部分では timestamp に対してわざと±5秒以内のランダムなゆらぎを与えおり、レコードが timestamp どおりに流れてこない状況を作っている。\ndfInput .writeStream .trigger(Trigger.ProcessingTime(30.seconds)) .foreachBatch { ... } .start .awaitTermination() ストリームデータは foreachBatch により、30秒ごとの mini batch の単位で処理される。\nこの mini batch の中で table 更新を行う。\n// Eliminate duplicated device_id val dfDedup = dfBatch .withColumn( \u0026#34;row_num\u0026#34;, row_number() .over(Window.partitionBy($\u0026#34;device_id\u0026#34;).orderBy($\u0026#34;ts\u0026#34;.desc)) ) .where($\u0026#34;row_num\u0026#34; === 1) .drop($\u0026#34;row_num\u0026#34;) Iceberg の merge into (後述) では更新をかける側の table において結合のキーとなる column の重複は許されず、重複があった場合は次のようなエラーになってしまう。\norg.apache.spark.SparkException: The ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table. This could result in the target row being operated on more than once with an update or delete operation and is not allowed. これを避けるため結合のキーとなる device_id ごとに一意となるよう、最新の行だけ残して重複を排除している。\n// Insert, update and delete records by \u0026#39;merge into\u0026#39; spark .sql( \u0026#34;\u0026#34;\u0026#34;merge into | my_catalog.my_db.device_temperature |using( | select | * | from | global_temp.input |) as input |on | device_temperature.device_id = input.device_id | and device_temperature.ts \u0026lt; input.ts |when matched and input.operation = \u0026#39;upsert\u0026#39; then update set * |when matched and input.operation = \u0026#39;delete\u0026#39; then delete |when not matched then insert * |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) はい、ここが update, insert, delete の最もキモとなるところ。\n個人的には merge into という SQL 構文は知らなかったのだが、どうやら Iceberg 特有のものでもないらしい。\ntable device_temperature に対して update, delete, insert を行う SQL となっている。\nこのクエリが実行されると Iceberg の table が更新され、snapshot が1つ前に進むことになる。\nDDL のところで merge-on-read を指定したため、差分のみのファイルが作成される。\ntable 更新の確認 上記のコードを実行すると、table の更新処理が始まる。\nしばらく流した後、SparkSQL で\nspark .sql( \u0026#34;\u0026#34;\u0026#34;select | * |from | my_catalog.my_db.device_temperature |order by | device_id |\u0026#34;\u0026#34;\u0026#34;.stripMargin ) .show(128, truncate = false) のようなクエリを実行すると、\n+---------+---------+------------------+-----------------------+ |device_id|operation|temperature |ts | +---------+---------+------------------+-----------------------+ |0 |upsert |15.879340359832794|2023-05-08 07:39:56.637| |1 |upsert |20.303530210621492|2023-05-08 07:40:26.157| |2 |upsert |21.07822922592327 |2023-05-08 07:41:10.937| |3 |upsert |20.81228214216027 |2023-05-08 07:37:09.756| |6 |upsert |20.571664434275807|2023-05-08 07:38:43.124| |7 |upsert |19.44731196983606 |2023-05-08 07:39:56.1 | |8 |upsert |27.964942467735458|2023-05-08 07:39:39.623| |9 |upsert |23.319385015293673|2023-05-08 07:40:59.377| |10 |upsert |22.392313247902365|2023-05-08 07:40:40.946| ... のような結果が得られる。\n少し間をおいて実行すると中身が変わっており、更新されていることがわかる。\nまた、ある程度時間を経た後においても欠番があり (ex. 上記の device_id = 4, 5)、削除も行われていることが確認できる。\n雑感 Apache Hudi に比べて動作が素直でわかりやすいように感じた。\n一方で Hudi で言うところの PRECOMBINE_FIELD のようなものがなく、自分で重複排除する必要がありちょっと面倒。\n必然的にタイムトラベルできる粒度は mini batch の粒度となる。\nmerge-on-read の table では差分ファイルが大きくなるにつれ、読み取りにコストも大きくなっていく。\n上記の更新コードを一晩実行した後だと、単純な select に6分程度かかってしまった。\n最新 snapshot としてはレコード数はたかだか128程度なのでこれはかなり遅い。\nというところで compaction などの table のメンテナンス操作が必要となってくる。\nこれについては次のポストで扱いたい。\n","permalink":"https://soonraah.github.io/posts/update-iceberg-table-in-near-real-time/","summary":"Apache Iceberg の table を near real time に、つまり高頻度で更新するということをやってみた。\nApache Iceberg とは Apache Iceberg (以下 Iceberg) は分散ファイルシステムやクラウドストレージ上の table format であり、Apache Hudi や Delta Lake と並んで data lake や lakehouse architecture で用いられる。\n特徴的なのは table とデータ実体 (Parquet, Avro など) の間に metadata file, manifest list, manifest file の抽象的なレイヤーがあり、ファイル単位で table の状態を track できること。\nこれにより強い isolation level、パフォーマンス、schema evolution など様々な機能・性能を実現できるようになっている。\nApache Iceberg Iceberg Table Spec\n詳しくは公式ドキュメントを参照のこと。\n最近では SmartNews 社が Iceberg で data lake を構築したことを記事にしていた。\nFlink-based Iceberg Real-Time Data Lake in SmartNews (Part I) | by SmartNews | SmartNews, Inc | Apr, 2023 | Medium ベンダー提供の DWH 中心ではなく Lakehouse Architecture を目指すのであれば最有力の table format の1つだと言えそう。","title":"Apache Iceberg の table を near real time で更新する"},{"content":"データエンジニアリングの領域で少し前から目にするようになった \u0026ldquo;data contract\u0026rdquo; という言葉。\nなんとなく今の業務で困っている課題の解決になりそうな気がしつつもよくわかっていなかったので調べてみた。\ndata contract について語られているいくつかのブログ記事などを参考にしている。\nData Contract とは データの schema というのはナマモノで、いろいろな理由で変更されることがある。\nschema を変更する場合、その schema のデータ (table や log) が所属する単一のビジネス機能や application のドメインで行われることになる。\nそのドメインの閉じた世界で考える分にはこれで問題ないのだが、DWH や data lake など組織レベルのデータ基盤でデータを流通していた場合はその先のことも考えないといけなくなる。\nこのようにチームを超える影響というのは、ビジネス機能に責任を持っているチームからは見えにくくなっていることが多い。\n上流の application 側で schema を変更したら下流のデータ基盤の ETL 処理がぶっ壊れてしまった、というのはデータ基盤運用あるあるではないだろうか。\nというところを解決して平和に過ごせるようにすることが data contract の主なモチベーションだと思われる。\n\u0026ldquo;contract\u0026rdquo; は日本語で言うところの「契約」。\n組織におけるデータ流通において、データの送り手である producer 側と受け手である consumer 側との間で合意した契約を遵守することにより、前述のような問題を避けることができるというのが data contract である。\n組織内のデータの見通しがよくなったり、パイプラインを宣言的に開発することができるようになるというメリットもある。\nエンジニアにとっては Datafold のブログ記事の例を読むとイメージしやすいかもしれない。\nTo provide another analogy, data contracts are what API is for the web services. Say we want to get data from Twitter. One way is to scrape it by downloading and parsing the HTML of Twitter’s webpage. This may work, but our scraper will likely break occasionally, if Twitter, for instance, changes a name of a CSS class or HTML structure. There is no contract between Twitter’s web page and our scraper. However, if we access the same data via Twitter’s API, we know exactly the structure of the response we’re going to get. An API has required inputs, predictable outputs, error codes, SLAs (service level agreements – e.g. uptime), and terms of use, and other important properties. Importantly, API is also versioned which helps ensure that changes to the API won’t break end user’s applications, and to take advantage of those changes users would graciously migrate to the new version.\nThe Best Data Contract is the Pull Request - datafold.com\nTwitter から情報を取ることと考えたとき、scraping する場合は UI の変更などで処理が壊れてしまうがちゃんと定義された API を使う場合は処理が壊れない。\nこの違いは約束事があるかどうかであり、data contract がない場合は約束事がない scraping のようなものだよね、という話。\n最初の例ではデータ基盤に流れてくるデータの schema が変わることを示したが consumer は中央集権的なデータ基盤である必要はなく、どちらかというと data mesh のような非中央集権的なコンテキストで話題に上がる方が多い。\nまた契約の対象となるのは schema だけではなく、次のような様々な情報を含んでよい。\nschema 所有者 利用者 セマンティクス 更新頻度 etc. 扱っている情報としては data catalog に近かったりもするが、data catalog はどうなっているかを示しているのに対して data contract はどうなっているべきかを示すという違いがある。\nData Contract の概況 data contract についてはいろいろな人が語っているが、なんとなく現時点でコミュニティにおいてある程度合意がありそうなことをいくつか挙げる。\n議論はまだまだこれからという感じなので、これらは時間が経つと変わっていく可能性が高いことに注意。\nスタンダードになるようなサービスや製品はまだない まるっと data contract をやってくれるようなサービスや製品はまだないので、data contract をやりたければある程度自社で作り込んだりルール整備したりする必要がある。\nこのようなサービスや製品がまだない理由については The Case for Data Contracts - uncomfortablyidiosyncratic.substack.com で議論があったのでご興味のある方はご確認ください。\nSchemata という OSS の schema modeling のフレームワークがあるがそんなに広まっていない？\nschema metadata についてはわかるけど schema score がどう使われるのかがイメージつかない。\nschema は IDL で表現して data contract に含める data contract の対象の中ではデータの schema は重要視されている。\nschema は IDL (Interface Description Language) で記述される。\nIDL として代表的なものとして Protocol Buffer と Apache Avro が挙げられる。\nschema を記述するための規定の方法が open source で提供されており、これを使ってレコード単位のチェックを行うことができる。\n他にも JSON Schema もあるが、DB の世界との間で型の扱いに差があるので個人的にはおすすめできない。\n契約の形は様々 契約 (contract) の形は data contract を語る人によっていろいろあってこれが議論になっている模様。\nproducer 側が主体的な場合もあれば consumer 側が主体的な場合もある。\n比較的よく挙げられれているのが pull-request のレビューをもって契約の合意とするという方式。\ndata contract が規定の形式のドキュメントになっていれば、その追加や変更についての二者間の合意はエンジニアにとって慣れ親しんだ pull-request という形で行うことができる。\nThe Best Data Contract is the Pull Request - datafold.com の記事が正にこれについて書かれたものとなっている。\n後述の実装例でもいくつか紹介する。\nData Contract の実装例 そうは言うてもどないして data contract を実現するんや？ってなりますよね。\nというところでその例をいくつか以下に記載してみる。\n詳細はリンク先を参照。\n例 1 An Engineer\u0026rsquo;s Guide to Data Contracts - Pt. 1 - dataproducts.substac.com\nentity、つまり DB の table の変更を流通させる場合の例 Pt. 2 の記事では event log の流通について書かれている data contract は Protocol Buffers で記載 data contract は CI/CD でテストや互換性チェックが行われ、Schema Registry に登録される production 環境では table の変更が CDC で Kafka の topic に送られている Kafka に送られたレコードを KSQL や Flink のフレームワーク + Schema Registry でチェックして新たな topic に transactional outbox pattern についても配慮 例 2 Aurimas Griciūnas’ Post - LinkedIn\n別の例だが例 1 に近い やはり Kafka や Flink を利用 例 3 Implementing Data Contracts at GoCardless - Medium\nPub/Sub や BigQuery に入る前に data contract による validation が行われる データ所有者が Jsonnet で定義された data contract を Git で merge すると、必要なリソースやサービスが生成される 宣言的でかっこいい 契約の主体が producer 側、つまりデータ所有者に寄っている印象 所有権を意識させる 例 4 Fine, let\u0026rsquo;s talk about data contracts - benn.substack.com\ndbt を使った例 なのでバッチ処理的なチェック 契約の主体が consumer 側、つまりデータ基盤に寄っている印象 (data contract というよりもデータ品質テストに近い気が) 所感 私がお仕事で運用しているデータ基盤では、最初に挙げたあるあるのような上流のデータの問題で ETL が影響を受けるということが度々あり課題となっていた。\nなので data contract という概念には期待がある。\n前回の記事 では Glue Schema Registry の導入を諦めた旨を書いたが、data contract という文脈で再度 Glue Schema Registry の採用を検討してみてもいいかもしれない。\n一方、国内ではあまり data contract の事例を聞いたことがなく、この辺不思議に思っている。\n理由としては\nこのような課題が存在しない -\u0026gt; んなわけない 課題はあるが認識されていない -\u0026gt; あるかもしれない 認識されて data contract 的なことをしているが data contract という言葉が使われていない -\u0026gt; 某社の事例を噂で聞いたことがある などが考えられる。\n国内企業の事例も聞いてみたいところだし、自分のところで事例を出せるといいなとも思っている。\nあと CDC とかのリアルタイム処理はもう当たり前になってるんだな。\n参考 Data Contracts — From Zero To Hero The Best Data Contract is the Pull Request The Case for Data Contracts An Engineer\u0026rsquo;s Guide to Data Contracts - Pt. 1 Aurimas Griciūnas Post - LinkedIn Implementing Data Contracts at GoCardless Fine, let\u0026rsquo;s talk about data contracts ","permalink":"https://soonraah.github.io/posts/looked-into-data-contracts/","summary":"データエンジニアリングの領域で少し前から目にするようになった \u0026ldquo;data contract\u0026rdquo; という言葉。\nなんとなく今の業務で困っている課題の解決になりそうな気がしつつもよくわかっていなかったので調べてみた。\ndata contract について語られているいくつかのブログ記事などを参考にしている。\nData Contract とは データの schema というのはナマモノで、いろいろな理由で変更されることがある。\nschema を変更する場合、その schema のデータ (table や log) が所属する単一のビジネス機能や application のドメインで行われることになる。\nそのドメインの閉じた世界で考える分にはこれで問題ないのだが、DWH や data lake など組織レベルのデータ基盤でデータを流通していた場合はその先のことも考えないといけなくなる。\nこのようにチームを超える影響というのは、ビジネス機能に責任を持っているチームからは見えにくくなっていることが多い。\n上流の application 側で schema を変更したら下流のデータ基盤の ETL 処理がぶっ壊れてしまった、というのはデータ基盤運用あるあるではないだろうか。\nというところを解決して平和に過ごせるようにすることが data contract の主なモチベーションだと思われる。\n\u0026ldquo;contract\u0026rdquo; は日本語で言うところの「契約」。\n組織におけるデータ流通において、データの送り手である producer 側と受け手である consumer 側との間で合意した契約を遵守することにより、前述のような問題を避けることができるというのが data contract である。\n組織内のデータの見通しがよくなったり、パイプラインを宣言的に開発することができるようになるというメリットもある。\nエンジニアにとっては Datafold のブログ記事の例を読むとイメージしやすいかもしれない。\nTo provide another analogy, data contracts are what API is for the web services. Say we want to get data from Twitter.","title":"Data Contract について調べた"},{"content":"業務で AWS Glue Schema Registry を使おうとしたけど、やっぱりやめたというお話。\nGlue Schema Registry What\u0026rsquo;s Schema Registry? AWS Glue Schema Registry は2020年に発表された AWS の機能だ。\nControl the evolution of data streams using the AWS Glue Schema Registry 一方、私が最初に schema registry 的なものを見たのは Confluent の例。\nSchema Registry の概要 - Confluent AWS の Glue Schema Registry はこれより後のリリースであり、同等のものの AWS マネージド版といったところだろうか。\nschema registry で何ができるかは Confluent のリンク先の図がとてもわかりやすいので参考にしていただきたい。\nGlue Schema Registry もだいたい同じで、ストリーム処理のための機能である。\nGlue Schema Registry で解決したい課題とその機能 データ基盤上のストリーム処理における schema 管理はバッチ処理のそれとは異なる難しさがある。\nこれは schema evolution と呼ばれる問題で以前のポストでも述べている。\nバッチ処理おじさんがストリーム処理のシステムを開発するにあたって調べたこと 難しい点として以下のようなことが挙げられる。\n処理は常に行われており、producer の application 等をしばらく停止させない限りはオンラインで schema 変更をするしかない ただしデータ基盤起因で producer を止めることはなるべくしたくない オンラインの場合、out of orderness に配慮する必要がある deploy 順序、ネットワーク遅延など様々な要因から、新しい schema のレコードが古い schema のレコードより後に届くとはかぎらない consumer 側で新旧 schema に対する互換性 (compatibility) に配慮する必要がある Glue Schema Registry はこの課題に対応する。\nざっと説明すると次のようにして利用される。\nbroker (Apache Kafka, Amazon Kinesis Data Stremas, etc.) を挟んで producer と consumer の両方から参照できる場所にバージョン管理された schema 定義を配置する (これが Glue Schema Registry) producer からレコードを serialize して送信。レコードには schema のバージョン情報を含む consumer はレコードに含まれるバージョン情報と schema registry を参照してレコードの deserialize・解釈を行う これを実現するために Glue Schema Registry では次のような機能が提供されている。\nschema の登録と互換性ルールにもとづく schema のバージョン管理 producer, consumer で利用される SerDe ライブラリの提供 producer 側でのレコード検証、serialize、配信 consumer 側での deserialize これによりストリーム処理における producer と consumer の間で schema 変更を運用することが可能になる。\nやりたかったこと …という Glue Schema Registry が自分の業務でも使えるのではないか、そんなふうに考えていた時期が私にもありました。\n現在私は B to B to C のビジネスをやっている企業でデータ基盤の開発・運用を行うチームに所属している。\nその中で、ある application から発生する JSON 形式のログを near real time でデータ基盤に取り込み、分析したいという案件があった。\n以下のようなアーキテクチャを考えた。\napplication 側から SerDe ライブラリを使い、Kinesis Data Streams (KDS) に逐次的にレコードを配信 Spark Structured Streaming で Glue streaming ETL の job を実装し、KDS を購読してSerDe ライブラリ経由でデータ取得 Spark Structured Streaming から Apache Hudi の形式 (Merge on Read) で S3 へと保存 schema の変更がある場合は Glue Data Catalog 上の Hudi table の schema を変更 Athena でクエリ (ちなみに Glue Schema Registry のアイコンは AWS では提供されていない)\napplication はビジネスロジックに近いシステムを担当しているチームの管理であり、それ以降がデータ基盤チームの管理となっている。\nnear real time 以外でもデータソース側とデータ基盤側の間での schema 管理は以前から課題があり、この Glue Schema Registry を使えばうまくやる前例を作れるのではという期待があった。\n4 の schema 変更は自動で行われ、したがって application ログの schema と Hudi table の schema を別で管理・メンテする必要がない。\napplication 側とデータ基盤側の中間で schema を定義・管理できるのが素晴らしい、そんなふうに考えていた時期が私にもありました。\n断念した理由 が、結局は Glue Schema Registry の採用は断念するという結論になった。\n次のような理由にもとづく。\nSerDe 導入の壁 JSON Schema の折り合い 互換性の運用の難しさ 以下、それぞれについて述べる。\nSerDe 導入の壁 前述のようなアーキテクチャを実現するには、application 側のチームに SerDe ライブラリを導入してもらう必要がある。\nSerDe は Java のライブラリとして提供されているため、application 側が Java 系の言語で開発されている必要がある。\nここに大きなハードルがあると感じた。\nちなみに KDS まわりであれば SDK や KPL 経由で使うことも可能である。\n今回は application は Scala だったので、頼み込めば今回の案件については導入してもらうこともできたかもしれない。\nただ、うまくいけば今回の案件に限らず横展開したいと考えていたので、それを考えるとかなりハードルが高いという判断となった。\nJSON Schema の折り合い application とデータ基盤の間の schema 管理については data contract という考え方が出てきている。\nそれについて書かれたブログ記事 7 Lessons From GoCardless’ Implementation of Data Contracts で\nFor example, most teams didn’t want to use AVRO so we decided to use JSON as the interchange format for the contracts because it’s extensible.\nとあるように、Avro 形式を各 application チームに使ってもらうというのはまあまあハードルが高い。\nGlue Schema Registry ではレコードの形式として Avro, Protobuf, JSON をサポートしている。\n本来はできれば Avro や Protobuf のような、型がしっかりしている形式を使いたいところだが、このようなハードルがあり我々も JSON の仕様を前提に考えていた。\nJSON の場合は JSON Schema の仕様に従うことになる。\nこの JSON Schema、当たり前だが JSON の世界を表現するためのものとなっている。\n何が言いたいかというと JSON の世界の型の表現と Hudi table を含めた DB 的な世界の型の表現とが一致しないのだ。\n例えば数値表現を見てみると、JSON Schema においては integer, number, Multiples, Range の4つの型が定義されている。\n一般的な整数型 short, int, long 等の違いを表現することができない。\nGlue Schema Registry に登録された JSON Schema により Hudi の table の schema を自動更新したかったので、これは困ったぞ、となってしまった。\n互換性の運用の難しさ これはちょっと言葉にしにくいのだが、前方／後方互換性まわりの仕様が直感的に理解しにくかった。\nどういったときにフィールド追加が許容されるのか、互換性の起点となる checkpoint はどういったときに更新されるのか、etc.\nこのあたりいろいろ調査すれば理解できると思うのだが、ここまでの課題もあり気持ちが折れてしまった。\nまた、チームとしてこれを踏まえた schema 更新を運用するのはハードルが高いと感じた。\n結局どうしたか Glue Schema Registry は使わず、では結局どうしたかというと application 側とデータ基盤側の間でシステム的に schema を共有することなく連携する形にした。\nシステム的に共有しないので、人が共有してやるしかない。\nschema 変更する場合は次のようにチーム間で協調して進める。\nチーム間で認識合わせ データ基盤チームによる Glue Data Catalog の schema の変更作業 consumer に前方互換性があるという前提 application チームによる application ログの schema の変更作業 このあたりもっとうまくできる方法があるのではと考えている。\nとりあえず data contract の発展に期待しておく。\nまとめ ストリーム処理における Glue Schema Registry の思想、考え方は正しいものであると感じる。\nただし、扱いが難しい面があり、今回のユースケースやそれを踏まえた横展開には合わないという判断となった。\nチームやシステムをまたいだ schema 管理は簡単ではない。\n","permalink":"https://soonraah.github.io/posts/give-up-on-schema-registry/","summary":"業務で AWS Glue Schema Registry を使おうとしたけど、やっぱりやめたというお話。\nGlue Schema Registry What\u0026rsquo;s Schema Registry? AWS Glue Schema Registry は2020年に発表された AWS の機能だ。\nControl the evolution of data streams using the AWS Glue Schema Registry 一方、私が最初に schema registry 的なものを見たのは Confluent の例。\nSchema Registry の概要 - Confluent AWS の Glue Schema Registry はこれより後のリリースであり、同等のものの AWS マネージド版といったところだろうか。\nschema registry で何ができるかは Confluent のリンク先の図がとてもわかりやすいので参考にしていただきたい。\nGlue Schema Registry もだいたい同じで、ストリーム処理のための機能である。\nGlue Schema Registry で解決したい課題とその機能 データ基盤上のストリーム処理における schema 管理はバッチ処理のそれとは異なる難しさがある。\nこれは schema evolution と呼ばれる問題で以前のポストでも述べている。","title":"Glue Schema Registry の導入を断念した話"},{"content":"「データレイク」という言葉は使う人によって異なった意味があるように感じており、気になっていた。\nこのポストではアーキテクチャ目線でのデータレイクと内容物目線でのデータレイクの違いについて書いてみる。\n便宜上前者を「データレイク」、後者を「データレイク層」と呼ぶことにする。\nアーキテクチャ目線の「データレイク」 「データレイク」については以前こちらのポストで書いたのでここでは詳しく触れない。\n詳細はリンク先を見ていただきたい。\nここでキーとなるのが、\n加工前データや非構造化データを含むあらゆるデータを保存 一元的なデータ管理 という部分だ。\nあらゆるデータを一元的に管理するという思想であり、これができるアーキテクチャがデータレイクということだ。\n例えば AWS や Azure のドキュメントを見るとデータレイクの中が zone に分けられており、生データを保持する raw zone や加工されたデータを置いておく curated zone などがある。\n(zone の命名にもいくつかの流派があるようだ…)\nReference architecture - Data Analytics Lens Data lake zones and containers - Cloud Adoption Framework | Microsoft Docs 次の Robinhood 社の例でもデータレイク中に生データとその派生データが存在している。\nFresher Data Lake on AWS S3 | by Balaji Varadarajan | Robinhood 内容物目線の「データレイク層」 一方でデータレイクには生データのみを置くべき、という考えもある。\n本書におけるデータレイク（DataLake）層とは、元のデータをコピーして、1つのシステムに集約したものを指します。 データソース（＝水源）から流れてきたデータを蓄える場所なのでレイク（湖）と呼びます。\nECサイトの注文履歴データを、分析用DBにコピーしている場合、それがデータレイクと言えます。データレイクのデータは、データソースと一対一の関係にあります。何も加工していない、ただのコピーだからです。\n何も加工していない、ただのコピーであることが重要です。仮にデータの中身に誤りがあったとしても、修正や加工をせず、そのまま集約しましょう。\n\u0026ndash; ゆずたそ,渡部 徹太郎,伊藤 徹郎. 実践的データ基盤への処方箋〜 ビジネス価値創出のためのデータ・システム・ヒトのノウハウ (Japanese Edition) (pp.57-58). Kindle 版.\nこちらの書籍にならってこのポストではこれを「データレイク層」と呼ぶことにする。\nこの考え方では生データの「データレイク層」の他に加工されたデータを置くための「DWH 層」「データマート層」がある。\n本書においてはアーキテクチャというよりは中に何を入れるかで「データレイク層」が規定される。\nこの「データレイク層」の考えは日本企業でよく見かける気がしている。(が、私が知らないだけで海外事例もあるかもしれない)\n以下はその例。\n分析者から見た使いにくいデータ基盤の話 | リクルート　メンバーズブログ データの民主化を目指して ~ データ基盤ができるまで ~ - LIFULL Creators Blog 「データレイク」と「データレイク層」の比較 「データレイク」も「データレイク層」もやっていることは同じで、ただ何を「データレイク (層)」と呼んでいるかが違っている。\nどちらも生データ、加工データ等を zone や層として分けて管理する。\n以下のようなニュアンスの違いがあると認識している。\nデータレイク データレイク層 内容物 生データ、加工データ 生データのみ アーキテクチャ オブジェクトストレージ等で一元的 データレイク層と DWH 層は別でもよい 使われている場所 海外 国内？ 個人的には「データレイク層」は「raw data 層」というような命名の方が混乱を避けつつ実を表しておりいいのではという感じがする。\n(つまり raw zone ですね…)\n一方で「データレイク層」と呼びたい気持ちもわかる。\nまとめ 誰かが「データレイク」についてしゃべっているときはどちらの「データレイク」のことを言っているのか、気をつけた方がよい。\nもっと言うと世の中にはまた別の定義もあるかもしれない。\n私がこれまで一緒に仕事をしたエンジニアの中でも優秀な人たちは言葉の定義に敏感な人が多かった。\nこういったところ気をつけていきたい。\n","permalink":"https://soonraah.github.io/posts/data-lake-and-data-lake-layer/","summary":"「データレイク」という言葉は使う人によって異なった意味があるように感じており、気になっていた。\nこのポストではアーキテクチャ目線でのデータレイクと内容物目線でのデータレイクの違いについて書いてみる。\n便宜上前者を「データレイク」、後者を「データレイク層」と呼ぶことにする。\nアーキテクチャ目線の「データレイク」 「データレイク」については以前こちらのポストで書いたのでここでは詳しく触れない。\n詳細はリンク先を見ていただきたい。\nここでキーとなるのが、\n加工前データや非構造化データを含むあらゆるデータを保存 一元的なデータ管理 という部分だ。\nあらゆるデータを一元的に管理するという思想であり、これができるアーキテクチャがデータレイクということだ。\n例えば AWS や Azure のドキュメントを見るとデータレイクの中が zone に分けられており、生データを保持する raw zone や加工されたデータを置いておく curated zone などがある。\n(zone の命名にもいくつかの流派があるようだ…)\nReference architecture - Data Analytics Lens Data lake zones and containers - Cloud Adoption Framework | Microsoft Docs 次の Robinhood 社の例でもデータレイク中に生データとその派生データが存在している。\nFresher Data Lake on AWS S3 | by Balaji Varadarajan | Robinhood 内容物目線の「データレイク層」 一方でデータレイクには生データのみを置くべき、という考えもある。\n本書におけるデータレイク（DataLake）層とは、元のデータをコピーして、1つのシステムに集約したものを指します。 データソース（＝水源）から流れてきたデータを蓄える場所なのでレイク（湖）と呼びます。\nECサイトの注文履歴データを、分析用DBにコピーしている場合、それがデータレイクと言えます。データレイクのデータは、データソースと一対一の関係にあります。何も加工していない、ただのコピーだからです。\n何も加工していない、ただのコピーであることが重要です。仮にデータの中身に誤りがあったとしても、修正や加工をせず、そのまま集約しましょう。\n\u0026ndash; ゆずたそ,渡部 徹太郎,伊藤 徹郎. 実践的データ基盤への処方箋〜 ビジネス価値創出のためのデータ・システム・ヒトのノウハウ (Japanese Edition) (pp.","title":"「データレイク」と「データレイク層」"},{"content":"ポエムです。\n事業フェーズごとのデータサイエンティストの役割 まずはこちらの発表。\n事業立ち上げにデータサイエンティストは必要なのか？ | CA BASE NEXT とても納得できる内容だった。\n一部抜き出して要約すると\n事業の立ち上げフェーズ データがまだなかったり、整備されていない状態 データサイエンスによる改善がしにくい 事業のグロースフェーズ 大規模なデータが使える状態 データサイエンスによる改善がやりやすい とのこと。異論はない。\nでは事業が立ち上がり、グロースが落ち着いたその後の成熟フェーズではどうなのだろうかという話。\n成熟フェーズにおける改善の難しさ 端的に言うと成熟フェーズでは ML によるさらなる改善は困難になってくると思う。\nここで言う成熟フェーズにおいてはプロダクトの進化とともに機械学習もそれなりに適用されてきたものとする。\n成熟フェーズということで既存の ML モデル、特にビジネスインパクトが大きい箇所はこれまでいろいろな改善が重ねられてきている。\nそのモデルの精度をさらに上げるとなると、より高度なアルゴリズム、より複雑なデータ等を扱う必要がある。\nしかし技術的によっぽど大きなブレークスルーがない限りは精度の改善幅はグロースフェーズよりもかなり小さいものとなるだろう。\n精度が上がれば上がるほど、次の1%を上げるためのコストは大きくなっていく。\n改善が進むほどに次の改善業務は困難になっていく。\n(蛇足だがある程度大きな組織でなければ高度で state-of-the-art な ML アルゴリズムは運用しない方がいいと考えている)\nでは既存ではない新しい適用箇所に ML を使えばいいのではとなるかもしれない。\nしかしやはりそれも難しい。\nビジネスインパクトが大きく、かつわかりやすい適用箇所にはおそらくすでに ML が適用されているからだ。\nその状態から更によい適用箇所を見つけるには深いドメイン知識が必要になったりする。\nという感じでいわゆるキラキラした「ML でビジネスをドライブ！」みたいなことは成熟フェーズでは難しいことが多いのではないか。\nしかしデータサイエンティストにやることがないわけではない。\n成熟フェーズで何ができるか ぱっと思いつくのは次のような仕事。\nデータドリブンな施策の立案・評価 これは事業フェーズ問わずあるべき ドメイン知識が必要 ML エンジニアリング パイプラインの改善や属人性をなくすお仕事 ML モデルの受動的なメンテナンス 精度が変化したときの調査 内部的・外部的要因によるデータの変化への対応 やっぱり ML モデルの精度改善 成熟フェーズということでビジネスもスケールしていれば 0.1% の精度改善でも売上的なインパクトは大きいかもしれない いわゆる狭義のデータサイエンスではなく、ドメイン知識であったりアナリストやエンジニア的な視点が絡んだ仕事が増えてくる。\nよくある「ML だけじゃなく◯◯もできると強いよね」みたいな話になってしまった。\nおわりに …という話が少し前に Twitter で知人との話題に上がった。\n若者が歴史的にいろんな人が改善に取り組んできた ML モデルの改善にアサインされている、というのが近いところで観測されたのでたいへんそうだなあと思いつつこの件を思い出したので書いてみた。\nここまで書いて思ったけど、成熟フェーズでキラキラしたことがやりづらくなるのはデータサイエンティストだけじゃないよな。\nでも狭義のデータサイエンスのスキルは特に、事業の存続期間と比べて大きく貢献できる期間が短いのかもしれない、と個人的には考えている。\n成熟フェーズでは高度なスキル、高い賃金に見合ったプロダクトへの貢献が得にくくなっていくのではないだろうろうか。\nでも反論もいっぱいありそうな気はします。\n「ML でビジネスインパクトどっかんどっかんやで！！」みたいな仕事をしたい人はそれなりの頻度で事業部を移る or 転職するというのが賢い動きになるんでしょうかね。\n","permalink":"https://soonraah.github.io/posts/ds-in-maturation-phase/","summary":"ポエムです。\n事業フェーズごとのデータサイエンティストの役割 まずはこちらの発表。\n事業立ち上げにデータサイエンティストは必要なのか？ | CA BASE NEXT とても納得できる内容だった。\n一部抜き出して要約すると\n事業の立ち上げフェーズ データがまだなかったり、整備されていない状態 データサイエンスによる改善がしにくい 事業のグロースフェーズ 大規模なデータが使える状態 データサイエンスによる改善がやりやすい とのこと。異論はない。\nでは事業が立ち上がり、グロースが落ち着いたその後の成熟フェーズではどうなのだろうかという話。\n成熟フェーズにおける改善の難しさ 端的に言うと成熟フェーズでは ML によるさらなる改善は困難になってくると思う。\nここで言う成熟フェーズにおいてはプロダクトの進化とともに機械学習もそれなりに適用されてきたものとする。\n成熟フェーズということで既存の ML モデル、特にビジネスインパクトが大きい箇所はこれまでいろいろな改善が重ねられてきている。\nそのモデルの精度をさらに上げるとなると、より高度なアルゴリズム、より複雑なデータ等を扱う必要がある。\nしかし技術的によっぽど大きなブレークスルーがない限りは精度の改善幅はグロースフェーズよりもかなり小さいものとなるだろう。\n精度が上がれば上がるほど、次の1%を上げるためのコストは大きくなっていく。\n改善が進むほどに次の改善業務は困難になっていく。\n(蛇足だがある程度大きな組織でなければ高度で state-of-the-art な ML アルゴリズムは運用しない方がいいと考えている)\nでは既存ではない新しい適用箇所に ML を使えばいいのではとなるかもしれない。\nしかしやはりそれも難しい。\nビジネスインパクトが大きく、かつわかりやすい適用箇所にはおそらくすでに ML が適用されているからだ。\nその状態から更によい適用箇所を見つけるには深いドメイン知識が必要になったりする。\nという感じでいわゆるキラキラした「ML でビジネスをドライブ！」みたいなことは成熟フェーズでは難しいことが多いのではないか。\nしかしデータサイエンティストにやることがないわけではない。\n成熟フェーズで何ができるか ぱっと思いつくのは次のような仕事。\nデータドリブンな施策の立案・評価 これは事業フェーズ問わずあるべき ドメイン知識が必要 ML エンジニアリング パイプラインの改善や属人性をなくすお仕事 ML モデルの受動的なメンテナンス 精度が変化したときの調査 内部的・外部的要因によるデータの変化への対応 やっぱり ML モデルの精度改善 成熟フェーズということでビジネスもスケールしていれば 0.1% の精度改善でも売上的なインパクトは大きいかもしれない いわゆる狭義のデータサイエンスではなく、ドメイン知識であったりアナリストやエンジニア的な視点が絡んだ仕事が増えてくる。\nよくある「ML だけじゃなく◯◯もできると強いよね」みたいな話になってしまった。\nおわりに …という話が少し前に Twitter で知人との話題に上がった。","title":"成熟フェーズの事業におけるデータサイエンティスト"},{"content":"ストリーム処理のフレームワークが備える backpressure という機能がある。\nこのポストでは Apache Flink の backpressure について調べたことを記載する。\nBackpressure の目的 backpressure はストリーム処理システムにおける負荷管理の仕組みの一つ。\n一時的な入力データ量の増大に対応する。\nインターネットユーザの行動履歴やセンサーデータなどは常に一定量のデータが流れているわけではなく、単位時間あたりのデータ量は常に変動している。\n一時的にスパイクしてデータ量が増大するようなことも起こりうる。\n複数の operator からなる dataflow graph により構成されるストリーム処理システムにおいては、処理スピードのボトルネックとなる operator が存在する。\n一時的に入力データ量が増えてボトルネックの operator の処理速度を上回ってしまった場合に、データの取りこぼしが発生するのを防ぐのが backpressure の目的となる。\nBackpressure の仕組み Buffer-based ここでは以前のブログでも紹介した、ストリーム処理で必要とされる機能について書かれた Fragkoulis et al. 1 を引用して一般論としての backpressure について述べたい。\n上流／下流の operator をそれぞれ producer, consumer とする。\nproducer, consumer (それらの subtask と言ってもいいかも) がそれぞれ異なる物理マシンに deploy されているケースが Figure 12b となる。\n各 subtask は input と output の buffer を持っており、\nproducer は処理結果を output buffer に書き出す TCP 等の物理的な接続でデータを送信 consumer 側の output buffer にデータを格納 consumer がそれを読み込んで処理する というような流れになる。\nbuffer はマシンごとの buffer pool で管理されており、input/output で buffer が必要となった場合はこの buffer pool に buffer が要求される。\nここで赤い丸で示されている subtask の処理速度が入力データの速度よりも遅かったとする。\nconsumer 側の input buffer の待機列が長くなり、さらにこの状況が続くとやがて buffer pool の buffer を使い果たす。\nすると producer 側から新しいデータを送信することができなくなり、producer 側の output buffer を使い始める。\n同様に producer 側でも output buffer を追加できなくなると producer は処理を待たざるを得なくなる。\nこのようにボトルネックとなる operator から上流に向かって buffer が埋まっていくことになる。\nこれが backpressure だ。\ndataflow graph を構成する operator 全体、物理的にはそれらのマシンのメモリにより一時的なデータ量の増加を buffer するという形になる。\nFigure 2a は producer と consumer が同じマシンにある場合の例であり、この場合はネットワークを介さずに buffer 上でやりとりができる。\nボトルネックがあれば同様に buffer を使い切り、それが上流に向かって伝播していくことになる。\nCredit-based 上記のような buffer-based な流量制御の場合、複数の channel が同じ下流のマシンにデータを送信する場合、同じ TCP socket を使うことになる。\n下流のある一部の channel が遅延して backpressure が働くと (data skew) 上流のすべての channel が影響を受けるという問題がある。\nこれを解決するのが credit-based な流量制御であり、Figure 13 はそれを示したものである。\nデータ送信を試みる前に credit という形で consumer 側から producer 側に buffer 状況を送信する。\nある channel で consumer の buffer がなくなると credit=0 となり、producer 側でその channel に送信できなるなり backpressure が発生する。\n一方、並列する他の channel には backpressure はかからず、TCP socket は利用可能となっている。\nFlink の Backpressure 残念ながら Flink 公式のドキュメントには backpressure についてあまり詳しく説明されていない。\nモニタリングについて書かれているのみである。\nMonitoring Back Pressure backpressure が起こっているかどうかは web UI 上から確認できるとのことだ。\n一方で Flink のブログや Alibaba のブログ等では内部的な挙動が詳しく書かれている。\nAnalysis of Network Flow Control and Back Pressure: Flink Advanced Tutorials A Deep-Dive into Flink\u0026rsquo;s Network Stack 前述のように buffer-based な仕組みの上に credit-based な挙動が採用されていることがわかる。\nここで Flink の設定の中で backpressure に影響がありそうなものを見ておく。\ntaskmanager.network.memory.buffers-per-channel こちらは channel 単位の排他的な buffer の数を指定する。\nskew 発生時において、この値が大きすぎると遅延している channel 以外で buffer が遊ぶことになり、逆に小さすぎると遅延していない channel でも処理が滞りやすくなると考えられる。\ntaskmanager.network.memory.floating-buffers-per-gate すべての input channel で共有される floating buffer の数。\nこの部分で skew をある程度吸収しようとするのだろう。\ntaskmanager.network.memory.max-buffers-per-channel channel ごとの最大 buffer 数。\n最大 buffer 数を制限することにより skew 時に backpressure が起こりやすくなり、結果として checkpoint のアラインメントを速くする効果があるとのこと。\n最大 buffer 数の制限がゆるいとボトルネックの operator で長い待ち行列を待つ必要があり、checkpoint barrier が移動するのに時間がかかってしまうということだろうか。\nweb.backpressure.cleanup-interval web.backpressure.delay-between-samples web.backpressure.num-samples web.backpressure.refresh-interval 上の4つは web UI によるモニタリング関連の設定値であり、backpressure 関連の挙動に直接影響を与えるものではない。\n…と書いてみたものの通常はこれらの値をチューニングすることはあまりないのではという印象。\nまとめ Flink の backpressure がどのように働くかがだいたい概観できた。\nそもそもなぜ backpressure を調べたかというと今開発している Flink アプリケーションで checkpoint size が増大し続ける問題があって backpressure の影響を疑っていた。\n結局 backpressure は関係なさそうだな…\nFragkoulis, M., Carbone, P., Kalavri, V., \u0026amp; Katsifodimos, A. (2020). A Survey on the Evolution of Stream Processing Systems.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://soonraah.github.io/posts/backpressure-for-flink/","summary":"ストリーム処理のフレームワークが備える backpressure という機能がある。\nこのポストでは Apache Flink の backpressure について調べたことを記載する。\nBackpressure の目的 backpressure はストリーム処理システムにおける負荷管理の仕組みの一つ。\n一時的な入力データ量の増大に対応する。\nインターネットユーザの行動履歴やセンサーデータなどは常に一定量のデータが流れているわけではなく、単位時間あたりのデータ量は常に変動している。\n一時的にスパイクしてデータ量が増大するようなことも起こりうる。\n複数の operator からなる dataflow graph により構成されるストリーム処理システムにおいては、処理スピードのボトルネックとなる operator が存在する。\n一時的に入力データ量が増えてボトルネックの operator の処理速度を上回ってしまった場合に、データの取りこぼしが発生するのを防ぐのが backpressure の目的となる。\nBackpressure の仕組み Buffer-based ここでは以前のブログでも紹介した、ストリーム処理で必要とされる機能について書かれた Fragkoulis et al. 1 を引用して一般論としての backpressure について述べたい。\n上流／下流の operator をそれぞれ producer, consumer とする。\nproducer, consumer (それらの subtask と言ってもいいかも) がそれぞれ異なる物理マシンに deploy されているケースが Figure 12b となる。\n各 subtask は input と output の buffer を持っており、\nproducer は処理結果を output buffer に書き出す TCP 等の物理的な接続でデータを送信 consumer 側の output buffer にデータを格納 consumer がそれを読み込んで処理する というような流れになる。","title":"Apache Flink の Backpressure の仕組みについて調べた"},{"content":"はじめに 前回のポストではデータレイクとはどういうものかというのを調べた。\n今回はデータレイクの文脈でどのような OSS が注目されているのかを見ていきたい。\n以下は NTT データさんによる講演資料であり、その中で「近年登場してきた、リアルタイム分析に利用可能なOSSストレージレイヤソフト」というのが3つ挙げられている。\n大規模データ活用向けストレージレイヤソフトのこれまでとこれから（NTTデータ テクノロジーカンファレンス 2019 講演資料、2019/09/05） from NTT DATA Technology \u0026amp; Innovation Delta Lake Apache Hudi Apache Kudu これらはすべて論理的なストレージレイヤーを担う。\nこちらの講演資料に付け足すようなこともないかもしれないが、このポストではデータレイクという文脈から自分で調べて理解した内容をまとめるということを目的にする。\n当然 Hadoop, Hive, Spark 等もデータレイクの文脈において超重要だが、「データレイク」という言葉がよく聞かれるようになる前から普及していたのでこのポストでは触れないことにする。\nDelta Lake https://delta.io/\nDelta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.\nDelta Lake\nDelta Lake は Apache Spark の読み書きに ACID な transaction を提供するストレージレイヤーの OSS である。\nDatabricks が作り、2019年4月に v0.1.0 がリリースされたのが最初だ。\n使い方はめちゃ簡単で、dependency を設定した上で Spark で\ndataframe .write .format(\u0026#34;delta\u0026#34;) .save(\u0026#34;/data\u0026#34;) のように読み書きすればよい。\nデータレイク化が進むと多種多様なデータが一元的に管理され、それらデータに対して横断的なクエリを実行できるようになる。\n各データの更新タイミングも様々であり、そのような状況では ACID 特性の中でも特に Isolation (独立性) が問題となってくる。\nSpark を処理エンジンとして使う場合、データソース・読み方によっては isolation level が弱くなってしまうことがあるというのは過去のポストでも述べた。\nおそらくこのことが Delta Lake の開発の強い動機となっているのではないだろうか。\nDelta Lake は最も強い isolation level である \u0026ldquo;serializability\u0026rdquo; を提供する。\nACID transaction の他には schema に合わないデータを弾いたり過去のデータのスナップショットにアクセスしたりなどの機能がある。\nどれもデータレイクの治安を守る方向であり、データスワンプ化に抵抗したいようだ。\nこれらを実現しているのが transaction log という仕組みとのこと。\nDelta Lake は table に対する変更の transaction を atomic な commit という単位に分け、commit ごとに操作を JSON file に書き出していく。\nJSON file には 000000.json, 000001.json のように連番が振られており、2つの application が同時に table を更新するような場合は各 application が次の番号の JSON file を作れるかどうかで衝突を制御している。\nJSON file はずっと蓄積していくため再計算のコストが大きくなっていくが、その時点での table の完全な状態を parquet にした checkpoint file というものを時折出力するという、compaction のようなことも行われる。\n(Delta Lake は parquet 形式のデータ保存を前提としている)\n詳しくは Databricks の blog を参照。\nDiving Into Delta Lake: Unpacking The Transaction Log Apache Hudi https://hudi.apache.org/\nHudi brings stream processing to big data, providing fresh data while being an order of magnitude efficient over traditional batch processing.\nApache Hudi\nApache Hudi で何ができるかを一言で説明するのは難しい。\n簡単にまとめると HDFS や S3 等にある table にリアルタイムに近いデータ取り込みと、処理速度とデータの新鮮さのトレードオフに配慮した読み込みを提供する。\n2016年から Uber が開発をしており、2020年に Apache Software Fundation の top-level project となった。\nデータレイクではあらゆる種類のデータを一元管理するが、その中には当然リアルタイム性の高いデータも含むことになる。\nデータレイク中のリアルタイム性の高いデータについては次のような要求が出てくる。\ntable へのデータの取り込みはリアルタイムに近いスピードで細かくやりたい 分析クエリ等の table の読み込みにおけるクエリの速度は速くしたい (例えば列指向形式で保存されたデータの様に) 取り込まれたデータをすぐに読めるようにしたい Hudi はこれに応えるものとなっている。\nこれらを実現するのが Merge On Read のデータ構造だ。\nHudi の table には Copy On Write と Merge On Read の2種類がある。\nここでは Hudi の肝である後者について触れておきたい。\nApache Hudi Merge On Read TablePermalink\ntable に加えられた変更についての情報は timeline に追加される。\n時系列になった変更についてのメタデータのようなものだろうか。\nこの timeline によって snapshot isolation が保証される。\n最初にデータが追加されたときは parquet 等の列指向のフォーマットで保存される。\nその一方でその後のデータの追加・更新については Avro 等の行指向のフォーマットの delta log に記載される。\nここがミソであり、列指向のフォーマットでは1レコードずつなどの細かい追加・更新が高コストになるのでその部分を行指向の delta log にまかせている。\n追加・更新が増えてくると delta log がどんどん長くなってしまうので、あるタイミングで compaction を行って直近までの delta log の変更内容を反映した列指向ファイルを作成する。\nこれを読む方法は2通りある。\nSnapshot Queries では読み取り時に列指向ファイルと行指向の delta log をどちらも読んで merge して最新の結果を返す。\n(これが \u0026ldquo;Merge On Read\u0026rdquo; ということ)\n一方で Read Optimized Queries では直近で compaction が行われた時点での列指向ファイルだけを読み、その時点の結果を返す。\nつまりデータの新鮮さが重要な場合は Snapshot Queries, データの新鮮さよりもクエリの速度を優先したい場合は Read Optimized Queries が有利ということだ。\nこれらはトレードオフとなるので状況に応じて使いわけることになる。\n2通りと述べたが実際はもう1つ、 Incremental Queries というものもある。\nこれはある時点からの差分のみを読み出して処理するというものとなっている。\nevent time と processing time の差があるものを DFS 上に書き出すのに適している。\nちなみに Merge On Read ではないもう1つの table type である Copy On Write は、Merge On Read の構造から delta log を消したものとなっている。\nすなわち書き込み時に常に列指向ファイルの更新がおこり、新しく作り直される。(という意味で \u0026ldquo;Copy On Write\u0026rdquo;)\n書き込みのたびに常に compaction が発生していると言ってもよい。\n更新頻度が低いデータならこちらの table type を使うのが適しているだろう。\nHudi の table は Spark, Hive, Presto 等からクエリすることができる。\n公式ドキュメント的には Spark 推しの感がある。\nApache Kudu https://kudu.apache.org/\nApache Kudu is an open source distributed data storage engine that makes fast analytics on fast and changing data easy.\nApache Kudu はストリーム処理などの追加・更新の速いデータをすぐに分析できるようにすることを目的としている。\nCloudera 社の内部プロジェクトとして始まり、2016年から Apache Software Foundation の top-level project となった。\nKudu の目指すところは Hudi とよく似ているが、次の点で異なっている。\nKudu は OLTP つまり小さなデータアクセスを大量にさばくのにも向いている Kudu は Hudi の incremental queries のようなことはできない Kudu は HDFS や S3 のような cloud storage 上にデータを持つのでははく、Raft の合意で制御された独自のサーバー群を要する Kudu では各 table のデータが tablet という単位により構成される。\ntablet はいわゆる partition によく似た概念となっており、key の範囲による分割、ハッシュ値による分割、またはその組み合わせにより分割される。\n1つの tablet は複数の tablet server に replication されており、そのうちの1つが leader として振る舞い書き込みを受け付ける。\nleader と follower の関係は Raft 合意アルゴリズム により管理される。\n一方で master server では tablet のメタデータ等が管理されており、client はまず master と通信することになる。\nApache Kudu Architectural Overview\n読み書きに関する内部的な振る舞いについては Cloudera 社のブログ記事 (日本語訳) が参考になる。\nCLOUDERA Blog Apache Kudu Read \u0026amp; Write Paths\nclient 側からはおそらく見えないが、内部的には\nメモリ上の MemRowSet, DeltaMemStore 列指向の base data file 差分を表す delta file (UNDO/REDO records) の3段の構成になっている。 (それと WAL も)\ndelta file を使うのは Hudi 等と同じだが一度メモリ上で変更を受けるという段があるのが特徴的だ。\n挿入はある tablet のメモリ上の MemRowSet にまず追加される。\nまた任意の timestamp の snapshot を得るために、MemRowSet 上のデータへの更新・削除はの差分は REDO records へと保存される。\nMemRowSet がいっぱいになると最新の状態が列指向の base data file へ書き出され、更新・削除前の状態は UNDO records へと書き出される。\n読み取りのときは MemRowSet とディスク上の base data file + delta file をスキャンすることになる。\nしたがって delta file の数やサイズが大きくなると遅くなる。\nやはりここでも compaction が必要となってくる。\nこのように memory を使うため server が必要であり、データレイクでよく言われるコンピューティングとストレージの分離が完全にはできない。\nCloudera 的と言えるかもしれない。\n読み取りには2つのモードがある。\nデフォルトは READ_LATEST であり、名前のとおり snapshot をとってすぐにデータを読むとのこと。\nREAD_LATEST は比較的弱い read committed の isolation level を示す。\nこれはおそらく Raft や WAL を経て変更が可視になるまでに時間を要するためだ。\nread committed は実用では問題が起こることもある。(ex. Spark クエリの分離レベル)\nもう一つは READ_AT_SNAPSHOT であり、明示的 (推奨) または暗黙的に読み取る対象の timestamp を指定する。\n書き込みの operation が完了し、その timestamp までの変更が安全に読めるようになるまで待って結果を返すことになる。\nisolation level はおそらく最も強い serializable となっている。\nしたがって2つのモードはデータの新鮮さと isolation level (consistency も？) のトレードオフとなっている。\nまとめ Delta Lake, Apache Hudi, Apache Kudu の3つを見比べて見てとても面白いのは、課題感は少しずつ違っているのにどれも列指向ファイル + 差分ファイル (delta file) というアーキテクチャを中心に置いているということだ。\nDelta Lake では transaction を重視している一方で Hudi ではリアルタイムデータをすぐに分析することを目指し、かつ Kudu ではさらに OLTP もサポートする。\nおそらく導入は Delta Lake が最も簡単であり、Kudu に至っては server を用意する必要があるのでハードルが1段高い。\n同じアーキテクチャということもあり、例えば time travel の機能などは共通して提供されている。\nバランス的には Apache Hudi がよさそうだが、どれを使うべきかは work load 次第だろう。\nhourly のデータ更新に慣れすぎていて fast data - fast analysis の需要に気づけないといこともよくありそう。\n","permalink":"https://soonraah.github.io/posts/oss-for-data-lake/","summary":"はじめに 前回のポストではデータレイクとはどういうものかというのを調べた。\n今回はデータレイクの文脈でどのような OSS が注目されているのかを見ていきたい。\n以下は NTT データさんによる講演資料であり、その中で「近年登場してきた、リアルタイム分析に利用可能なOSSストレージレイヤソフト」というのが3つ挙げられている。\n大規模データ活用向けストレージレイヤソフトのこれまでとこれから（NTTデータ テクノロジーカンファレンス 2019 講演資料、2019/09/05） from NTT DATA Technology \u0026amp; Innovation Delta Lake Apache Hudi Apache Kudu これらはすべて論理的なストレージレイヤーを担う。\nこちらの講演資料に付け足すようなこともないかもしれないが、このポストではデータレイクという文脈から自分で調べて理解した内容をまとめるということを目的にする。\n当然 Hadoop, Hive, Spark 等もデータレイクの文脈において超重要だが、「データレイク」という言葉がよく聞かれるようになる前から普及していたのでこのポストでは触れないことにする。\nDelta Lake https://delta.io/\nDelta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.\nDelta Lake\nDelta Lake は Apache Spark の読み書きに ACID な transaction を提供するストレージレイヤーの OSS である。\nDatabricks が作り、2019年4月に v0.1.0 がリリースされたのが最初だ。","title":"データレイク関連の OSS - Delta Lake, Apache Hudi, Apache Kudu"},{"content":"最近よく聞かれるようになった「データレイク」という概念にあまりついていけていなかったため、いまさらながらざっと調べてみた。\nデータレイクとは Wikipedia によると最初にこの言葉を使ったのは Pentaho 社の CTO である James Dixon らしい。\nその時の彼のブログ (10年前…) を読むと、既にあったデータマートに対して\nOnly a subset of the attributes are examined, so only pre-determined questions can be answered. The data is aggregated so visibility into the lowest levels is lost\n\u0026ndash;Pentaho, Hadoop, and Data Lakes - James Dixon’s Blog というような問題意識からデータレイクというコンセプトを提案したようだ。\n最近？のデータレイクについてはベンダー等の記事が参考になる。\nデータレイクとは - AWS データレイクとは？ - talend データレイクとは？データレイクの落とし穴と効果 - Informatica 書籍だと『AWSではじめるデータレイク: クラウドによる統合型データリポジトリ構築入門』がいいだろうか。\nデータレイクの概要と AWS が考えている構築・運用がざっとわかる。\nAmazon で検索した限りだと現時点でタイトルに「データレイク」を含む和書はこれのみだった。\n上に挙げたような文献ではデータレイクはデータウェアハウスとの対比という形でよく語られている。\n共通している内容は概ね以下のとおり。\n加工前データや非構造化データを含むあらゆるデータを保存 データウェアハウスでは加工され構造化されたデータのみを含む データレイクでは加工前の半構造化、非構造化データも含む ex. ログ、画像、音声 Scheme on Read 書き込み時にデータの構造を決める (Scheme on Write) のではなく、使用時に決める なので詳細なスキーマ設計なしに様々なデータを置いていくことができる 一元的なデータ管理 データがサイロ化しないよう、組織全体のデータを一元的に管理 なのでデータへのアクセス権の管理が重要になる 多様な分析用途に対応 データウェアハウスはビジネスアナリストが決まったレポートを出すために使われる データレイクでは機械学習など更に高度な分析をデータサイエンティストが行う 機械学習の普及がデータレイクを強く後押ししているように思う。\n機械学習をやっていれば様々な特徴量を扱いたいというのは自然な欲求であり、データレイクはそれを実現する。\nまた、クラウドベンダーは以下のような点も強調している。\n従量課金のクラウドストレージによるメリット 運用開始前の時点でどんな生データがどれだけ来るか、見積もるのはとても難しい 従量課金のクラウドストレージ (Amazon S3, Google Cloud Storage, etc.) なら必要なときに必要なサイズだけ追加できる 安価なクラウドストレージの普及がデータレイクを後押し ストレージとコンピューティングの分離 処理側のリソースを処理内容に応じて確保できる 処理側のバージョンアップや変更が容易 (オンプレ Hadoop クラスタのバージョンアップの辛さを思い出してください) なるほどです。\n沼にはまらないために データレイクとの対比でデータスワンプ (沼) という言葉がある。\n以下の説明がわかりやすいだろうか。\nデータスワンプ（Data Swamp）とは、データの沼地（Swamp）という意味です。これの対比語としてデータレイク（Data Lake：データの湖）があります。沼には、いろんな魚が住んでいるかもしれませんが、水が濁っているため、どこにどんな魚がいるか全く見えません。また、全く見えないため「魚が住んでいないんじゃないか」とも思い、魚を捕るのも諦めてしまいがちです。その一方で、湖は、水が澄んでいるため、魚を見ることができ「おっ！魚がいるな。何とか捕まえてみよう」と思えます。\nこの沼と湖にいる魚を、データの例えとして使っているのが、データスワンプと、データレイクという言葉です。\n\u0026ndash; データスワンプ - Realize\nデータスワンプにならないようにということについては次の資料が参考になる。\nThe difference between a data swamp and a data lake? 5 signs Metadata Separates Data Lakes From Data Swamps From Data-Swamp to Data-Lake on AWS (Part 1) すごくざっくりまとめるとデータレイクの構築・運用に当たって次の点に配慮するとよい。\nメタデータ、カタログの整備 どんなに優れたデータレイクを構築したとしても、利用者がどこに何のデータがあるかわからないと意味がない 多種のデータ資産を一元的に管理するデータレイクにおいて目的のデータを見つけるために必要な機能 データガバナンス 誰がどのデータに対して何ができるのか 統制をきつくしすぎると自由度が減るため、これらのバランスを考慮しないといけない その他、常識的な運用 クラウドベンダーが推進するデータレイク クラウドベンダーがどのようなデータレイクを考えているかというのも見ておく。\nGCP Google Cloud データレイクとしての Cloud Storage\nデータレイクとしての Cloud Storage というドキュメントに GCP が考えるデータレイクの構成が記載されている。\n構成としては以下のとおり。\n保存: Cloud Storage 変換: Cloud Dataproc, Cloud Dataprep, Cloud Dataflow 分析: BigQuery, Cloud ML Engine, Cloud Datalab, etc. データ保存のストレージとして、みんな大好きな BigQuery ではなく Cloud Storage を想定しているのは非構造化データを扱うためだろう。\nいわゆるリレーショナル的なものより Cloud Storage や S3 のようなオブジェクトストレージの方がこの要件に適している。\nもちろん Cloud Storage に保存された生データを処理してから BigQuery のデータウェアハウスに再びつっこむのも悪くない。\n変換や分析には様々なサービスが使えるし、今後も幅は広がっていくと考えられる。\nクラウドにおけるストレージとコンピューティングの分離の恩恵と言えるだろう。\n現時点ではこのドキュメントにはカタログ化についての記述はないが、GCP には Data Catalog というサービスがあり Cloud Storage のメタデータを扱えるようだ。\nAWS AWS Sample AWS data lake platform\n前述の書籍『AWSではじめるデータレイク: クラウドによる統合型データリポジトリ構築入門』に加え、Building Big Data Storage Solutions (Data Lakes) for Maximum Flexibility も読んでおくとよい。\n保存: Amazon S3 (, AWS Glue Data Catalog) 変換: AWS Glue ETL, Amazon EMR, AWS Lambda, etc. 分析: Amazon Athena, Amazon Redshift Spectrum, Amazon ML, etc. やはり中心となるのは S3 だ。\nGCP も AWS もストレージのコストメリットを推している。\nGlue Data Catalog がメタデータを管理する。\nGCP 同様、変換・分析には様々なオプションがある。\nBuilding Big Data Storage Solutions (Data Lakes) for Maximum Flexibility には記載がないのだが、AWS Lake Formation というサービスもある。\n上に挙げたようなサービスの上に一枚被せて、データレイクとして運用しやすくするといったようなものらしい。\n特に権限管理的な意味合いが強いように思う。\n組織のデータを一元的に管理するデータレイクには様々な部署・役割の人からのアクセスがある。\nどのデータを誰が扱えるのか、複数のサービスを横断して IAM で権限を管理のはかなりきつそうな印象でそういうのを楽にしてくれるのだろう。\nまとめ データレイクの概観やベンダーが考えていることがわかった。\n普段業務で使っているということもあり、全体的に AWS 寄りになってしまったように思うのでその点をご留意していただきたい。\nこの後はデータレイク関連の OSS について調べておきたいところ。\n","permalink":"https://soonraah.github.io/posts/what-is-a-data-lake/","summary":"最近よく聞かれるようになった「データレイク」という概念にあまりついていけていなかったため、いまさらながらざっと調べてみた。\nデータレイクとは Wikipedia によると最初にこの言葉を使ったのは Pentaho 社の CTO である James Dixon らしい。\nその時の彼のブログ (10年前…) を読むと、既にあったデータマートに対して\nOnly a subset of the attributes are examined, so only pre-determined questions can be answered. The data is aggregated so visibility into the lowest levels is lost\n\u0026ndash;Pentaho, Hadoop, and Data Lakes - James Dixon’s Blog というような問題意識からデータレイクというコンセプトを提案したようだ。\n最近？のデータレイクについてはベンダー等の記事が参考になる。\nデータレイクとは - AWS データレイクとは？ - talend データレイクとは？データレイクの落とし穴と効果 - Informatica 書籍だと『AWSではじめるデータレイク: クラウドによる統合型データリポジトリ構築入門』がいいだろうか。\nデータレイクの概要と AWS が考えている構築・運用がざっとわかる。\nAmazon で検索した限りだと現時点でタイトルに「データレイク」を含む和書はこれのみだった。\n上に挙げたような文献ではデータレイクはデータウェアハウスとの対比という形でよく語られている。\n共通している内容は概ね以下のとおり。","title":"いまさらながらのデータレイク"},{"content":"ストリーム処理における CSV ファイルの読み込み Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。\nしかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。\nstar schema における dimension table 的な情報をストリームに結合したい場合 等が考えられる。\nこのポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。\nFlink は現時点の stable である v1.11 を想定。\nCSV ファイルを読む方法 DataStream API ベースの実装で CSV ファイルを読むには StreamExecutionEnvironment のメソッドである readFile() を使う。\noverload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。\nまず1つめは FileInputFormat\u0026lt;OUT\u0026gt; inputFormat であり、こちらは data stream の生成に用いる入力フォーマットを指定する。\nおそらく最も一般的なのが TextInputFormat だと思われる。\nもちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。\nPojoCsvInputFormat RowCsvInputFormat TupleCsvInputFormat なんとなく名前でわかると思うが、それぞれ readFile() の結果として返される DataStreamSource が内包する型が異なる。\nこれについては後述の実験にて確認する。\n次に FileProcessingMode watchType も見ておきたい。\nこの引数ではデータソースの監視についてのモードを指定する。\nモードは2つある。\nFileProcessingMode.PROCESS_CONTINUOUSLY 対象のファイルが更新され、その更新に追随する必要がある場合に利用 指定のインターバルでファイルの更新をチェック 更新があった場合はファイル全体を読む FileProcessingMode.PROCESS_ONCE 対象のファイルの更新がない、更新について考えない場合に利用 最初に一度だけファイルを読む おそらく多くの場合は前者が必要になるのではないだろうか。\n利用にあたっては更新があった場合にファイル全体が読まれるということに注意が必要だ。\n例えばファイル末尾にレコードを1件追加するような更新であったとしても、全レコードが再度ストリームに流されるということである。\n詳しくは ドキュメント を参照。\nこれはファイル全体で1つの atomic な単位だとみなされているものと思われる。\nレコード単位で処理していくストリーム処理にファイルというバルクな単位のデータを流そうとしているのでこうなってしまう。\nそう考えるとやはり static なファイルのデータは dimension table として情報を付加するような、ストリームの本川に合流する支川のような使い方が想定されているのだろう。\nちなみに CsvReader というものもあるが、こちらは DataSet API、つまりバッチ処理向けのようなので今回は扱わない。\n実験 実際にコードを書いて readFile() で CSV を読んでみる。\nここでは PojoCsvInputFormat と TupleCsvInputFormat を切り替えられるようにした。\nコード package com.example.entry import org.apache.flink.api.common.typeinfo.BasicTypeInfo import org.apache.flink.api.java.io.{PojoCsvInputFormat, TupleCsvInputFormat} import org.apache.flink.api.java.tuple.Tuple3 import org.apache.flink.api.java.typeutils.{PojoField, PojoTypeInfo, TupleTypeInfo} import org.apache.flink.core.fs.Path import org.apache.flink.streaming.api.functions.source.FileProcessingMode import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, createTypeInformation} import scala.collection.JavaConverters._ import scala.concurrent.duration._ /** * The experiment to read CSV file by Flink. * It reads CSV file as POJOs or tuples and just prints on console. */ object ReadCsvFileExperimentRunner { /** POJO */ case class Company(name: String, ticker: String, numEmployees: Int) /** Tuple */ type CompanyTuple = Tuple3[String, String, Int] def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val companiesFilePath = \u0026#34;data/companies.csv\u0026#34; val interval = 10.seconds args.headOption match { case None | Some(\u0026#34;pojo\u0026#34;) =\u0026gt; val inputFormat = createPojoCsvInputFormat(companiesFilePath) env .readFile(inputFormat, companiesFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, interval.toMillis) .map(_.toString) .print() case Some(\u0026#34;tuple\u0026#34;) =\u0026gt; val inputFormat = createTupleCsvInputFormat(companiesFilePath) env .readFile(inputFormat, companiesFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, interval.toMillis) .map(_.toString) .print() case _ =\u0026gt; throw new RuntimeException(s\u0026#34;Unsupported input format: ${args(0)}\u0026#34;) } env.execute() } private def createPojoCsvInputFormat(csvFilePath: String): PojoCsvInputFormat[Company] = { val clazz = classOf[Company] val pojoFields = Seq( new PojoField(clazz.getDeclaredField(\u0026#34;name\u0026#34;), BasicTypeInfo.STRING_TYPE_INFO), new PojoField(clazz.getDeclaredField(\u0026#34;ticker\u0026#34;), BasicTypeInfo.STRING_TYPE_INFO), new PojoField(clazz.getDeclaredField(\u0026#34;numEmployees\u0026#34;), BasicTypeInfo.INT_TYPE_INFO) ).asJava val pojoTypeInfo = new PojoTypeInfo[Company](clazz, pojoFields) val fieldNames = Array(\u0026#34;name\u0026#34;, \u0026#34;ticker\u0026#34;, \u0026#34;numEmployees\u0026#34;) val inputFormat = new PojoCsvInputFormat[Company](new Path(csvFilePath), pojoTypeInfo, fieldNames) inputFormat.setSkipFirstLineAsHeader(true) inputFormat } private def createTupleCsvInputFormat(csvFilePath: String): TupleCsvInputFormat[CompanyTuple] = { val types = Seq( BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.INT_TYPE_INFO ) val tupleTypeInfo = new TupleTypeInfo[CompanyTuple](classOf[CompanyTuple], types: _*) val inputFormat = new TupleCsvInputFormat[CompanyTuple](new Path(csvFilePath), tupleTypeInfo) inputFormat.setSkipFirstLineAsHeader(true) inputFormat } } ポイントは POJO 版も tuple 版も型情報を作ってやる必要があるということだ。\nそれぞれ PojoTypeInfo, TupleTypeInfo を用意してやる必要があり、これがやや癖があって面倒。\nあるフィールドを数値として読むことは可能だが、日付の parse のようなことはできないようである。\nというのを考えると TextInputFormat で読んで自分で parse するのと比べてあまりうれしくないような…\nデータ 実験用のデータとして会社情報を示す簡単な CSV ファイルを適当に作って data/companies.csv に配置。\nname,ticker,num_employees Alphabet Inc,GOOG,98771 Apple Inc,AAPL,147000 Facebook Inc,FB,49942 Amazon.com Inc,AMZN,798000 実行 まずは POJO 版を実行してみた。\nプログラムが起動するとすぐに以下が出力された。\n[info] running com.example.entry.ReadCsvFileExperimentRunner pojo 2\u0026gt; Company(Alphabet Inc,GOOG,98771) 1\u0026gt; Company(Facebook Inc,FB,49942) 2\u0026gt; Company(Apple Inc,AAPL,147000) 1\u0026gt; Company(Amazon.com Inc,AMZN,798000) Company インスタンスとして CSV ファイルの内容を取得できている。\nプログラムは止まっていないが CSV ファイルの内容を一通り吐き出したところで出力は止まった。\nここで CSV ファイルに次の1行を追加してみる。\nMicrosoft Corporation,MSFT,163000 すると出力は\n[info] running com.example.entry.ReadCsvFileExperimentRunner pojo 2\u0026gt; Company(Alphabet Inc,GOOG,98771) 1\u0026gt; Company(Facebook Inc,FB,49942) 2\u0026gt; Company(Apple Inc,AAPL,147000) 1\u0026gt; Company(Amazon.com Inc,AMZN,798000) 2\u0026gt; Company(Alphabet Inc,GOOG,98771) 1\u0026gt; Company(Amazon.com Inc,AMZN,798000) 2\u0026gt; Company(Apple Inc,AAPL,147000) 1\u0026gt; Company(Microsoft Corporation,MSFT,163000) 2\u0026gt; Company(Facebook Inc,FB,49942) となり、最初に出力された4行に加えて新たに5行追加された。\nCSV ファイルには1行追加しただけだが、既存の行も含む CSV ファイル全体が再度出力された。\nドキュメントに記載されているとおりの仕様となっている。\ntuple 版で実行すると出力は次のようになった。\n[info] running com.example.entry.ReadCsvFileExperimentRunner tuple 1\u0026gt; (Facebook Inc,FB,49942) 1\u0026gt; (Amazon.com Inc,AMZN,798000) 2\u0026gt; (Alphabet Inc,GOOG,98771) 2\u0026gt; (Apple Inc,AAPL,147000) tuple として読めているようだ。\nwatchType が同じなので CSV ファイルの更新についての挙動は同様だった。\nちなみに実行可能なプロジェクトは GitHub に置いている。\nsbt 'runMain com.example.entry.ReadCsvFileExperimentRunner pojo' または sbt 'runMain com.example.entry.ReadCsvFileExperimentRunner tuple' で実行できる。\n(Ctrl + C で終了)\nまとめ TextInputFormat で読んで自分で parse するのと比べ、*CsvInputFormat を使う方法はコーディングとしてはあまりメリットが感じられなかった。\nまた、ストリーム処理においてやはりファイルというデータソースは傍流なんだなという感じ。\nちなみに Table API で CSV を読むこともおそらく可能。\n気が向いたら書く。\n","permalink":"https://soonraah.github.io/posts/read-csv-by-flink-datastream-api/","summary":"ストリーム処理における CSV ファイルの読み込み Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。\nしかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。\nstar schema における dimension table 的な情報をストリームに結合したい場合 等が考えられる。\nこのポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。\nFlink は現時点の stable である v1.11 を想定。\nCSV ファイルを読む方法 DataStream API ベースの実装で CSV ファイルを読むには StreamExecutionEnvironment のメソッドである readFile() を使う。\noverload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。\nまず1つめは FileInputFormat\u0026lt;OUT\u0026gt; inputFormat であり、こちらは data stream の生成に用いる入力フォーマットを指定する。\nおそらく最も一般的なのが TextInputFormat だと思われる。\nもちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。\nPojoCsvInputFormat RowCsvInputFormat TupleCsvInputFormat なんとなく名前でわかると思うが、それぞれ readFile() の結果として返される DataStreamSource が内包する型が異なる。","title":"Apache Flink の DataStream API 利用時の CSV ファイル読み込み"},{"content":"ちょっと昔話 かつて参画したプロジェクトの話。\nそのプロジェクトでは他社から受注した受託開発として機械学習系のシステムを開発していた。\n当時としては新しいフレームワークを使い、かなり頑張ってなんとか納期内で完成させた。\nその中の1つの機能として A/B テストができるようにしていた。\nパラメータチューニングによりパフォーマンスを改善することを想定していた。\nしかし結局その機能は使われることがなかった。\nなぜか。\nA/B テストを実施するためのクライアントの追加の予算がつかなかったためである。\n受託なのでなおさらなのだが、売上にならなければ工数をかけるこはできない。\n工数を使ってパフォーマンス改善することはできなかった。\n手はあるのに。\n機械学習の精度は必ずしも利益に結びつかない この昔話で何が言いたいかというと、機械学習の精度改善は必ずしも利益に結びつかないということである。\nそのことを示しているとても素晴らしい資料がこちら。\n機械学習の精度と売上の関係 from Tokoroten Nakayama 前述の昔話の例はこの資料で言うところの③ロジスティック型 (=外注) となる。\nいったん売上が立った後、追加予算がつかなかったので精度改善では売上は増えなかったのだ。\n倫理感による精度改善 受託開発を主としている組織であれば工数にはシビアなので、売上の立たない工数をかけることはあまりないだろう。\n(よっぽどの炎上鎮火とかでなければ)\nしかし自社で製品やサービスを作って提供しているような組織の場合、利益にならない精度改善をしているのを時折見かける。\nなぜそのようなことが起こるかと言うと多くの場合はデータサイエンティスト／機械学習エンジニアとしての倫理感からなのではないだろうか。\n「◯◯予測という機能なのでできるだけ良い予測精度を示すべきだ」\n「ユーザには気づかれない部分だが精度が悪いので改善したい」\n倫理感や興味が先行してしまっているのだ。\nしかしその精度を上げた先に利益があるとは限らない。\n機械学習で職を得ている人間は自分の仕事を機械学習の精度を上げるゲームだとみなす傾向があるように思う。\n例えばインターネット広告の CTR 予測。\nこれは予測精度が高いほど利益は改善するし、広告主に価値も提供できる。\n精度改善に倫理と利益が伴っている、とても機械学習がハマる例だと思う。\n本来はこれらを兼ね備えているのが良い適用先であるはずだ。\nイシューは行き渡っているのか 利益に結びつかない、または間接的にしか結びつかないような精度改善をやることが許されるというのは組織に余裕があるということで悪いことではないのかもしれない。\nしかし単によいイシューの設定ができてないだけという可能性もある。\n自社で製品やサービスを作って提供しているような組織において、単純なロジスティック回帰でコアなところのビジネスを大きく加速させることができた時期を過ぎると機械学習で解くのに適したよい問題を恒常的に見つけ出すのは実は難しいのではないだろうかと最近考えるようになった。\nビジネスの領域拡大よりも既存領域への機械学習の適用の方が速いということは十分ありうる。\nもちろんチームの規模にもよる。\n機械学習チームの人的リソースの規模に対して機械学習で解くべきよいイシューを見つけ出せているのか、ということだ。\n少し前にちょっと話題になったこちらの件もイシューが大事だと言っている。\n全ての機械学習の論文は新しいアルゴリズムを提案しているのですか？ - Quora キャリアの行く末 事業会社においてビジネスの領域拡大よりも既存領域への機械学習の適用の方が速く、よいイシューを提供しにくいということがよく起こるのであれば、機械学習チームのリソースは余剰気味になりやすいということになる。\nこれが続くと今後機械学習しかやらない人材の市場価値は下がっていくのかもしれない。\nもしくは自社で製品やサービスを持っている組織ではなく、受託開発やコンサルが主戦場になっていくのかもしれない。\n何にせよ特定のプロダクトに commit したいのであれば機械学習エンジニアは機械学習以外のスキルも磨いていく必要があるように思う。\nおわりに 見える範囲にいる人が利益にならない精度改善をしているのを横目で見てこのようなことを考えていた。\n難しいけどできるだけ金を生んでいきたい。\n","permalink":"https://soonraah.github.io/posts/ml-accuracy-profit-ethic-issue/","summary":"ちょっと昔話 かつて参画したプロジェクトの話。\nそのプロジェクトでは他社から受注した受託開発として機械学習系のシステムを開発していた。\n当時としては新しいフレームワークを使い、かなり頑張ってなんとか納期内で完成させた。\nその中の1つの機能として A/B テストができるようにしていた。\nパラメータチューニングによりパフォーマンスを改善することを想定していた。\nしかし結局その機能は使われることがなかった。\nなぜか。\nA/B テストを実施するためのクライアントの追加の予算がつかなかったためである。\n受託なのでなおさらなのだが、売上にならなければ工数をかけるこはできない。\n工数を使ってパフォーマンス改善することはできなかった。\n手はあるのに。\n機械学習の精度は必ずしも利益に結びつかない この昔話で何が言いたいかというと、機械学習の精度改善は必ずしも利益に結びつかないということである。\nそのことを示しているとても素晴らしい資料がこちら。\n機械学習の精度と売上の関係 from Tokoroten Nakayama 前述の昔話の例はこの資料で言うところの③ロジスティック型 (=外注) となる。\nいったん売上が立った後、追加予算がつかなかったので精度改善では売上は増えなかったのだ。\n倫理感による精度改善 受託開発を主としている組織であれば工数にはシビアなので、売上の立たない工数をかけることはあまりないだろう。\n(よっぽどの炎上鎮火とかでなければ)\nしかし自社で製品やサービスを作って提供しているような組織の場合、利益にならない精度改善をしているのを時折見かける。\nなぜそのようなことが起こるかと言うと多くの場合はデータサイエンティスト／機械学習エンジニアとしての倫理感からなのではないだろうか。\n「◯◯予測という機能なのでできるだけ良い予測精度を示すべきだ」\n「ユーザには気づかれない部分だが精度が悪いので改善したい」\n倫理感や興味が先行してしまっているのだ。\nしかしその精度を上げた先に利益があるとは限らない。\n機械学習で職を得ている人間は自分の仕事を機械学習の精度を上げるゲームだとみなす傾向があるように思う。\n例えばインターネット広告の CTR 予測。\nこれは予測精度が高いほど利益は改善するし、広告主に価値も提供できる。\n精度改善に倫理と利益が伴っている、とても機械学習がハマる例だと思う。\n本来はこれらを兼ね備えているのが良い適用先であるはずだ。\nイシューは行き渡っているのか 利益に結びつかない、または間接的にしか結びつかないような精度改善をやることが許されるというのは組織に余裕があるということで悪いことではないのかもしれない。\nしかし単によいイシューの設定ができてないだけという可能性もある。\n自社で製品やサービスを作って提供しているような組織において、単純なロジスティック回帰でコアなところのビジネスを大きく加速させることができた時期を過ぎると機械学習で解くのに適したよい問題を恒常的に見つけ出すのは実は難しいのではないだろうかと最近考えるようになった。\nビジネスの領域拡大よりも既存領域への機械学習の適用の方が速いということは十分ありうる。\nもちろんチームの規模にもよる。\n機械学習チームの人的リソースの規模に対して機械学習で解くべきよいイシューを見つけ出せているのか、ということだ。\n少し前にちょっと話題になったこちらの件もイシューが大事だと言っている。\n全ての機械学習の論文は新しいアルゴリズムを提案しているのですか？ - Quora キャリアの行く末 事業会社においてビジネスの領域拡大よりも既存領域への機械学習の適用の方が速く、よいイシューを提供しにくいということがよく起こるのであれば、機械学習チームのリソースは余剰気味になりやすいということになる。\nこれが続くと今後機械学習しかやらない人材の市場価値は下がっていくのかもしれない。\nもしくは自社で製品やサービスを持っている組織ではなく、受託開発やコンサルが主戦場になっていくのかもしれない。\n何にせよ特定のプロダクトに commit したいのであれば機械学習エンジニアは機械学習以外のスキルも磨いていく必要があるように思う。\nおわりに 見える範囲にいる人が利益にならない精度改善をしているのを横目で見てこのようなことを考えていた。\n難しいけどできるだけ金を生んでいきたい。","title":"機械学習の精度と利益と倫理とイシューと"},{"content":"はじめに このポストではストリーム処理の survay 論文の話題に対して Apache Flink における例を挙げて紹介する。\n論文概要 Fragkoulis, M., Carbone, P., Kalavri, V., \u0026amp; Katsifodimos, A. (2020). A Survey on the Evolution of Stream Processing Systems.\n2020年の論文。\n過去30年ぐらいのストリーム処理のフレームワークを調査し、その発展を論じている。\nストリーム処理に特徴的に求められるいくつかの機能性 (functionality) についてその実現方法をいくつか挙げ、比較的古いフレームワークと最近のフレームワークでの対比を行っている。\nこのポストのスコープ このポストでは前述のストリーム処理システムに求められる機能性とそれがなぜ必要となるかについて簡単にまとめる。\n論文ではそこからさらにその実現方法がいくつか挙げられるが、ここでは個人的に興味がある Apache Flink ではどのように対処しているかを見ていく。\nちなみに論文中では Apache Flink はモダンなフレームワークの1つとしてちょいちょい引き合いに出されている。\nここでは Flink v1.11 をターゲットとする。\n以下では論文で挙げられている機能性に沿って記載していく。\nOut-of-order Data Management Out-of-order ストリーム処理システムにやってくるデータの順序は外的・内的要因により期待される順序になっていないことがある。\n外的要因としてよくあるのはネットワークの問題。\nデータソース (producer) からストリーム処理システムに届くまでのルーティング、負荷など諸々の条件により各レコードごとに転送時間は一定にはならない。\n各 operator の処理などストリーム処理システムの内的な要因で順序が乱されることもある。\nout-of-order は処理の遅延や正しくない結果の原因となることがある。\nout-of-order を管理するためにストリーム処理システムは処理の進捗を検出する必要がある。\n\u0026ldquo;進捗\u0026rdquo; とはある時間経過でレコードの処理がどれだけ進んだかというもので、レコードの順序を表す属性 A (ex. event time) により定量化される。\nある期間で処理された最古の A を進捗の尺度とみなすことができる。\nApache Flink の場合 Flink ではこの進捗を測るのに watermark という概念が使われている。\nEvent Time and Watermarks Apache Flink Event Time and Watermarks\n(図が見にくい場合はページ上部の太陽みたいなマークをクリックして light mode にしてください)\nこちらの図でストリーム中の破線で描かれているのが watermark であり、W(11) の wartermark は「timestamp が11以下の event はこの後もう来ないものとみなす」ということを下流の operator に伝えるものである。\nwatermark は metadata 的なものだが、通常の event と同じようにストリーム中を流れている (これを panctuation という)。\n下流の operator が window 処理をしていた場合、W(11) が届いた時点で timestamp が11までのところの window 処理を完結してさらに下流に output することができる。\nwatermark がいつ・どのような値で発生するかについては Flink application の開発者の実装次第ということになる。\nしかし現実的には Writing a Periodic WatermarkGenerator の例にある BoundedOutOfOrdernessGenerator のように、 WatermarkGenerator にやってきた event の event time を元に決めることが多いと思われる。\nState Management ストリーム処理における状態 \u0026ldquo;状態\u0026rdquo; とは継続的なストリーム処理の中で内部的な副作用をとらえたもの。\nアクティブな window、レコードのかたまり、aggregation の進捗など。\nユーザ定義のものも含まれる。\n状態については以下のようなトピックがある。\nProgrammability プログラミングモデルにおいて状態がどのように定義・管理されるか 定義と管理についてそれぞれシステムとユーザの場合がある Scalability and Persistency 最近のストリーム処理は scalable の時流を汲んでおり、scale out するときに状態をどのように扱うか 内外の記憶装置に状態を永続化するという方法がよく取られる Consistency transaction level の保証について Apache Flink の場合 ドキュメントの TOP ページ における Flink を表す一文\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams.\nにおいても \u0026ldquo;stateful\u0026rdquo; という言葉が使われているとおり、状態の扱いは Flink の設計思想の中でもかなり重要な部分となっている。\nFlink における状態の扱いについてはこちらを参照。\nStateful Stream Processing Programmability Flink では application 開発者が任意の状態を定義することができる。\nWorking with State 一方で状態の管理はフレームワーク側でやってくれるので、開発者は checkpoint や restore 等のことは特に配慮する必要はない。\n論文中ではこれを \u0026ldquo;User-Declared System-Managed State\u0026rdquo; と呼んでおり、最近のストリーム処理システムの傾向となっている。\nScalability Flink では keyBy() により key-level の状態を持つことができる。\nkey ごとに並列 task 内での partitioning し、分散することが可能ということである。\nPersistency 論文では永続化については scalability と絡めて述べられていたが、Flink のドキュメントでは fault tolerance の文脈で永続化について書かれている。\nFlink の fault tolerance の肝は stream replay と checkpointing である。\ncheckpointing とはストリームと operator の状態の一貫性のあるスナップショットをとることである。\nApache Flink Snapshotting Operator State\nこの checkpoint を作成する過程で各 operator の状態が state backend へと永続化される。\nstate backend では RocksDB の key/value store に各 checkpoint, 各 operator の状態が保存される。\n(RocksDB 以外にもメモリやファイルシステムなどもある)\nConsistency Flink では checkpoint のインターバルの期間の単位 (epoch という) で一貫性のある状態を永続化する。\n上図の barriers がその単位を決めている。\nChandy Lamport algorithm という分散スナップショットの手法がインスパイアされており、unaligned/aligned で各 operator の状態のスナップショットを取るようになっている。\nFault Tolerance \u0026amp; High Availability Fault Tolerance ストリーム処理システムにとって fault tolerance は2つの理由から重要である。\nストリーム処理システムは stateful な計算を終わりのないデータに対して行っている fault tolerance がなければ、障害があったときに最初から状態を計算しなおさなければならない 一方で、多くの場合過去に処理されたデータは既に失われている 最近のストリーム処理システムは分散アーキテクチャを採用している 物理マシンの数だけ問題が起こりやすくなる output commit problem についても考慮する必要がある。\nこれは出力が公開された位置から状態を復元できることが確かな場合のみ、システムは外界に出力を公開するというもの。\n言い換えると、障害からの復旧時などに同じ出力を2回してしまわない、出力を exactly-once にできるかというものである。\nHigh Availability 過去の研究においてストリーム処理システムの可用性は recovery time, performance overhead (throughput \u0026amp; latency), resource utilization により定量化されてきた。\nこの論文では\nA streaming system is available when it can provide output based on the processing of its current input.\nを可用性の定義として提案する。\n時間ごとの processing time と event time の差により定量化される。\nApache Flink の場合 Fault Tolerance 論文中では Flink は output commit problem については Kafka などの出力先の外部システムの責任とするスタンスだとしている。\nKafka には idempotent producer という機能があり、たぶんこれのことを言っている。\nまた一方で TwoPhaseCommitSinkFunction の2相コミットによって exectly-once semantics を提供するという方法も示されている。\nAn Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!) checkpointing における JobManager を2相コミットの coordinator とみなし、checkpoint barrier が最後の operator に到達するまでをコミット要求相、その後の JobManager からの checkpointing 完了通知をコミット相としている。\nコミット相において外部システムへの書き出しの transaction が完了する形となる。\nfault tolerance については State Management の項も参照。\nHigh Availability Flink のプロセスには JobManager と TaskManager があり、前者は cluster に1つだけ動く。\nしたがって JobManager が SPOF になり、可用性に影響しうる。\nhigh availability (高可用性) を実現するためには JobManager が SPOF となることを避けることができる。\nstandalone または YARN の cluster として deploy した場合は JobManager が SPOF となることを避けることができる。\n以下は standalone の例。\n1つの JobManager が leader として動いているが、それが crash すると standby のインスタンスが leader を引き継ぐ。\n(論文中では passive replication として紹介)\nApache Flink Standalone Cluster High Availability\nLoad Management, Elasticity \u0026amp; Reconfiguration Load Management ストリーム処理システムは、外部のデータソースがデータを送る流速を制御することができない。\n入力データの流速がシステムのキャパより大きいことによるパフォーマンス劣化を防ぐための対応が必要となる。\n次のような手法がある。\nload shedding 多すぎる入力データを落とす方法 back-pressure 入力データを落とせないときに buffering と組み合わせて使う dataflow graph 上に速度制限が波及していく elasticity 分散アーキテクチャと cloud にもとづく方法 いわゆる scale out Apache Flink の場合 Flink では back pressure および elasticity の組み合わせとなっている。\nback pressure は一時的な入力データの増加に対応する。\n各 operator (subtask?) は入出力の buffer を持っており、これにより operator 間の処理速度の違いをある程度吸収できる。\nしかし入力データが著しく多くなると\nボトルネックとなる operator の処理が滞る その operator の入力 buffer がいっぱいになる (ボトルネックではない) 上流の operator の出力 buffer がいっぱいになる 上流の operator の処理が滞る (以降繰り返し) のように、dataflow graph の上流へ上流へと遅延が波及する。\nelasticity の面では、JobManager や TaskManager の追加や削除ができるようになっている。\nAdding JobManager/TaskManager Instances to a Cluster TaskManager の追加や削除においては状態の再配分が行われる。\n再配分される状態は key group という単位で partitioning されており、consistent hash 的な方法で各 TaskManager 配下の operator へと配分される。\nちなみに AWS が提供する Flink の managed service である Amazon Kinesis Data Analytics for Apache Flink では CPU 使用率をモニタリングして自動的に scale out が行われるようになっている。\nApplication Scaling in Kinesis Data Analytics for Apache Flink まとめ バッチ処理ではあまりクリティカルにならないような問題でもストリーム処理では重大な影響を及ぼすことがある。\nストリーム処理に求められる機能性を実現するに当たり、Apache Flink では checkpoint の仕組みが中心的な役割を果たしているということが理解できた。\n","permalink":"https://soonraah.github.io/posts/functionality-of-streaming-system/","summary":"はじめに このポストではストリーム処理の survay 論文の話題に対して Apache Flink における例を挙げて紹介する。\n論文概要 Fragkoulis, M., Carbone, P., Kalavri, V., \u0026amp; Katsifodimos, A. (2020). A Survey on the Evolution of Stream Processing Systems.\n2020年の論文。\n過去30年ぐらいのストリーム処理のフレームワークを調査し、その発展を論じている。\nストリーム処理に特徴的に求められるいくつかの機能性 (functionality) についてその実現方法をいくつか挙げ、比較的古いフレームワークと最近のフレームワークでの対比を行っている。\nこのポストのスコープ このポストでは前述のストリーム処理システムに求められる機能性とそれがなぜ必要となるかについて簡単にまとめる。\n論文ではそこからさらにその実現方法がいくつか挙げられるが、ここでは個人的に興味がある Apache Flink ではどのように対処しているかを見ていく。\nちなみに論文中では Apache Flink はモダンなフレームワークの1つとしてちょいちょい引き合いに出されている。\nここでは Flink v1.11 をターゲットとする。\n以下では論文で挙げられている機能性に沿って記載していく。\nOut-of-order Data Management Out-of-order ストリーム処理システムにやってくるデータの順序は外的・内的要因により期待される順序になっていないことがある。\n外的要因としてよくあるのはネットワークの問題。\nデータソース (producer) からストリーム処理システムに届くまでのルーティング、負荷など諸々の条件により各レコードごとに転送時間は一定にはならない。\n各 operator の処理などストリーム処理システムの内的な要因で順序が乱されることもある。\nout-of-order は処理の遅延や正しくない結果の原因となることがある。\nout-of-order を管理するためにストリーム処理システムは処理の進捗を検出する必要がある。\n\u0026ldquo;進捗\u0026rdquo; とはある時間経過でレコードの処理がどれだけ進んだかというもので、レコードの順序を表す属性 A (ex. event time) により定量化される。","title":"ストリーム処理システムに求められる機能性、および Apache Flink におけるその対応"},{"content":"ほとんどバッチ処理しか書いたことのない者だがストリーム処理のシステムを開発することになった。\nそれにあたって独学で調べたことなどまとめておく。\nストリーム処理とは そもそも \u0026ldquo;ストリーム処理\u0026rdquo; とは何を指しているのか。\n以下の引用が簡潔に示している。\na type of data processing engine that is designed with infinite data sets in mind. Nothing more.\n\u0026ndash; Streaming 101: The world beyond batch\nこちらは \u0026ldquo;streaming system\u0026rdquo; について述べたものだが、つまり終わりのないデータを扱うのがストリーム処理ということである。\n例えば web サービスから生まれ続けるユーザ行動ログを逐次的に処理するというのがストリーム処理。\nweb サービスが終了しないかぎりはユーザ行動ログの生成には終わりがない。\nこれに対して \u0026ldquo;1日分のユーザ行動ログ\u0026rdquo; 等のように有限の量のデータを切り出して処理する場合、これはバッチ処理となる。\nストリーム処理とバッチ処理の違いは扱うデータが無限なのか有限なのかということだ。\nこの後触れていくが、この終わりのないデータを継続的に処理し続けるというところにバッチ処理にはない難しさがある。\nなぜストリーム処理なのか なぜストリーム処理なのか。\nひとえに逐次的な入力データに対する迅速なフィードバックが求められているからと言えるだろう。\n迅速なフィードバックがビジネス上のメリットとなることは自明だ。\nSNS の配信 カーシェアリングにおける配車や料金設定 クレジットカードや広告クリックなどの不正検知 もしこれらの application が例えば hourly のバッチ処理で実装されていたらどうだろうか。\nまあ待っていられない。\n一般的なストリーム処理の構成 モダンな…と言っていいのかわからないが、ストリーム処理を行うための一般的なシステムは次の3つの要素で構成される。\nproducer broker consumer producer は最初にレコードを生成する、ストリームデータの発生源となるものである。\n例えばログを生成する web application であったり、何らかのセンサーを持つ IoT 機器であったりがこれに該当する。\nproducer は絶え間なくログを生成し、それを broker へと送る。\nbroker は producer から送られたログを格納し、任意のタイミングで取り出せるようにするものである。\n誤解を恐れずに言うとメッセージキューに近いイメージだ。\nApache Kafka クラスタや Amazon Kinesis Data Streams 等がこれに該当する。\nconsumer は broker からログを取り出し、それに対し何かしらの処理を行うものだ。\ntime window 集計であったりログからの異常検知であったり、処理した結果として何かビジネス上意味があるものを得るのである。\nこれを行うフレームワークとしては Spark Streaming や Apache Flink 等がメジャーなのだろうか。\nproducer と consumer の間に broker を挟むメリットとしては次のようなことが挙げられる。\nproducer が M 個、consumer が N 個の場合に M * N の関係になるところを broker を挟めば M + N にできる producer, consumer に多数のシステムがあったとしても各自は broker との接続だけを考えればよい 任意のタイミングでデータを読み出せる producer または consumer に問題が発生してもデータロスが起こりにくくできる その分 broker には高い可用性が求められる Kafka はクラスタで冗長構成 Kinesis Data Streams は複数 AZ でレプリケーション 時間の概念 ストリーム処理では時間の概念がいくつかあり、集計などの処理をどの時間をベースにして実行するのか、意識する必要がある。\nevent time producer 側でログイベントが発生した時間 ingestion time broker にそのログイベントのレコードが挿入された時間 processing time consumer 側でレコードを処理した時間 processing time を使うのが一番簡単なのだが、おそらく分析系の処理であれば window 集計等では event time を使うことが多いのではないだろうか。\ningestion time はおそらく実際のプロダクトではあまり使われないのではと思われる。\n(ネットワークのパフォーマンスを見るぐらい？)\nWindowing ストリーム処理の中で sum, count などを伴う集計処理を行う場合、通常は時間方向の window で切って処理するということになるのではないだろうか。\nwindow で切らずに完全なデータセットがそろうまで待つことはできないし、データが来るたびに逐次的に全体の結果を更新するしていくというのも割に合わない。\nwindow の切り方もいくつかある。\ntumbling window 固定長でオーバーラップしない sliding window 固定長でオーバーラップを含む session window いわゆる web の session のように、ある種のイベントがある期間発生しないことにより window が区切られる これらについては Flink のドキュメントが図もあってわかりやすい。\n個人的な感想だが、この time window の集計がない単なる map 的なストリーム処理であれば traditional なアーキテクチャでも難しくはない。\nしかし time window 集計が必要となった場合は Spark Streaming 等のモダンなフレームワークが威力を発揮してくる。\nWatermark 時間で window を切るときは、前述のどの時間の定義を用いるかを考えなければいけない。\nprocessing time を用いる場合は簡単だが event time はやや難しい。\nconsumer 側では event のレコードがどれくらい遅れてやってくるかわからないためだ。\nネットワークその他の影響により、event のレコードが producer -\u0026gt; broker -\u0026gt; consumer という経路で consumer に届くまでの時間というのは一定にはならない。\nまた、古い event が新しい event より後に届くというように順番が前後することも起こりうる。\nここで \u0026ldquo;watermark\u0026rdquo; という考え方が必要になってくる。\nA watermark with a value of time X makes the statement: \u0026quot;all input data with event times less than X have been observed.\u0026quot;\n\u0026ndash; Streaming 102: The world beyond batch\nある processing time において「event time X より前のレコードはすべて到着したよ」というのが watermark である。\n別の言い方をすると watermark により event のレコードがどの程度遅延してもよいかが定義される。\nevent time X より前のレコードが真の意味ですべて到着した、というのは難しい。\n実際には heuristic にどの程度遅れていいかを決め、それより遅れた場合はある event time 期間における window 処理には含めないということになる。\nwatermark の決め方はフレームワーク次第だろうか。\n例えば Spark Structured Streaming の例だと図もあって比較的わかりやすい。\nSchema Evolution 何らかの業務システムや web システム等をある程度運用したことがある人ならわかると思うが、データの schema というのはナマモノだ。\n一度決めたら終わりというわけではなくプロダクトやビジネスの変化に応じて変化していく。\nカラムが増えたり、削除されたり、名前や型が変わったり…\nこのようにデータの構造が変化していくこと、またはそれを扱うことを \u0026ldquo;schema evolution\u0026rdquo; という。\nバッチ処理において schema の変更に追従することを考えるのはそれほど難しくない。\nhourly のバッチ処理であったとしても、バッチ処理とバッチ処理の間の時間で application を更新すればいいだけだ。\n(が、実際に行うのは困難が伴うことも多い)\nではストリーム処理ではどうだろうか。\nいわゆるストリーム処理においては処理と処理の間というものがなく、application がずっと稼働しっぱなしということになる。\nバッチ処理のような更新はできない。\nもっと言うと producer で生まれた新しい schema のレコードがいつ届くかもわからない。\nおそらくこの問題には2つの対応方法がある。\n1つめは consumer 側のシステムで前方互換性を保つという方法である。\nこの場合、新しいフィールドは必ず末尾に追加される等、producer 側での schema 更新についてある程度のルールが必要となるだろう。\nproducer 側で生成されるレコードの schema の変更が必ず事前にわかるというのであれば後方互換性でもいいが、多くの場合は難しい。\nところで前方互換と後方互換、どっちがどっちなのか覚えられません。\n2つめの方法として schema 情報をレコード自体に入れ込んでしまうという方法もある。\nApach Avro のような serialization の方法を取っているとレコード自体に schema の情報を付与することができる。\nおそらく最もエレガントにこれをやるのが Confluent の Schema Registry という機能だ。\nproducer から送出されるレコードには schema ID を付与する。\nschema の実体は Schema Registry という broker とは別の場所で管理されており、consumer 側では受け取ったレコードに付与されている schema ID と Schema Registry に登録されている shcema の実体を参照してレコードを deserialize することができる。\nDeploy ストリーム処理を行うシステムは終わりのないデータを処理するためのものであり、ずっと動き続けることが期待されている。\nしかし通常システムは一度立ち上げれば終わりということではなく、運用されている中で更新していく必要がある。\nずっと動かしながらどのように deploy, release するのか。\nこの問題は主に consumer 側のシステムで配慮が必要になると思われる。\n正直これについてはちゃんと調べられていないが、2点ほど述べておきたい。\nまず1点目、application を中断・更新・再開するのにどの程度の時間がかかるのかを知っておく必要があるということ。\nアーキテクチャやフレームワーク、処理の内容や checkpoint (後述) を使うか等によりこの時間は変わってくる。\n一例だが、AWS 環境において\nAWS Glue + Spark Structured Streaming Amazon Kinesis Data Analytics + Flink の比較をしたことがある。\n前者は再開に数分かかったのに対し、後者は1分未満で再開できた。\n再開までの時間が十分に短いと判断できるのであればそのまま deploy, release してしまっていいだろう。\n一方そうでない場合はどうすべきかという話が2点目。\n再開までの時間が長く、システム要件的に許容できないというのであれば、release 時は二重で動かすというような措置が必要かもしれない。\nおそらく Blue-Green Deployment のようなことを考えることになるだろう。\nCheckpoint 前述のとおり、ストリーム処理を行うシステムはずっと動き続けることが期待されている。\nしかし予定された application の更新や不測のエラー等、何らかの理由で一時的に中断されるということが実際の運用中には起こる。\n中断されたとき速やかに復帰する仕組みとして \u0026ldquo;checkpoint\u0026rdquo; というものがいくつかの consumer 側のフレームワークで提供されている。\n雑に説明すると、処理のある時点における進捗や内部状態などをディスク等に永続化し、そこから処理を再開できるようにするものである。\nRecovering from Failures with Checkpointing - Apache Spark Checkpointing - Apache Flink 上記は Spark Structured Streaming と Flink の例だ。\ncheckpoint には次のようなメリットがあり、運用上有用だと言える。\n内部の状態を保持しているため、速やかに復帰できる 中断した位置から再開できるので出力に穴が開かない 一方で落とし穴もある。\ncheckpoint では内部の状態が永続化されるわけだが、内部の状態というのは当然 application の実装が決めているものである。\napplication のコードを変更したとき、変更の内容によっては永続化された checkpoint と application が合わなくなることがあるのだ。\n未定義の挙動となることもあるので、checkpoint の運用には十分に配慮する必要がある。\nどのような変更なら checkpoint が安全に利用できるのかはフレームワークのドキュメントに記載があるので確認しておきたい。\nRDB の世界との折り合い みんな大好きな RDB の世界では table を操作してデータの処理を行う。\n基本的には table というものはある時点における完全なデータセットを表すものである。 (ex. isolation)\n他方、ストリーム処理はやってきたデータを逐次的に処理するものである (mini-batch の場合もあるが)。\n直感的にこの2つは相性が悪そうに見える。\nしかし Spark や Flink では table ベースの操作でストリーム処理を行うための API が提供されている。\nおそらく\nストリーム処理の周辺のデータソースとして RDB が存在する RDB 的な table 操作があまりにも浸透している というところが API が必要である理由なのだろう。\nストリームデータを table 的に扱うというのが、やや直感的な理解をしにくいものとなっている。\nフレームワークのドキュメントを確認しておきたい。\n例えば Spark Structured Streaming であれば処理の出力のための3つの output mode が示されている。\nAppend mode: 追加された行だけ出力 Complete mode: table 全体を出力 Update mode: 更新された行だけ出力 どれを選ぶかにより必要とする内部メモリの大きさも影響される。\nまとめ 思ったより長文になってしまった。\n結局ストリーム処理の難しさは以下の2点に尽きるだろう。\n複数の時間の概念 常時稼働のシステム 独学なので抜け漏れがあったり、話が新しくなかったりすることもあると思われる。\n参考 Streaming 101: The world beyond batch Apache Beam PMC によるストリーム処理の解説ポスト。必読 Streaming 102: The world beyond batch 上の続きであり watermark について触れている Analytics Lens - AWS Well-Architected Framework AWS の資料。ストリーム処理のシステムの全体感がつかめる Structured Streaming Programming Guide - Apache Spark Spark Structured Streaming のドキュメント。consumer の気持ちがわかる Flink DataStream API Programming Guide - Apache Flink Flink のドキュメントの方がより詳しい。DataStream API の解説を中心に読むとよい ","permalink":"https://soonraah.github.io/posts/study-streaming-system/","summary":"ほとんどバッチ処理しか書いたことのない者だがストリーム処理のシステムを開発することになった。\nそれにあたって独学で調べたことなどまとめておく。\nストリーム処理とは そもそも \u0026ldquo;ストリーム処理\u0026rdquo; とは何を指しているのか。\n以下の引用が簡潔に示している。\na type of data processing engine that is designed with infinite data sets in mind. Nothing more.\n\u0026ndash; Streaming 101: The world beyond batch\nこちらは \u0026ldquo;streaming system\u0026rdquo; について述べたものだが、つまり終わりのないデータを扱うのがストリーム処理ということである。\n例えば web サービスから生まれ続けるユーザ行動ログを逐次的に処理するというのがストリーム処理。\nweb サービスが終了しないかぎりはユーザ行動ログの生成には終わりがない。\nこれに対して \u0026ldquo;1日分のユーザ行動ログ\u0026rdquo; 等のように有限の量のデータを切り出して処理する場合、これはバッチ処理となる。\nストリーム処理とバッチ処理の違いは扱うデータが無限なのか有限なのかということだ。\nこの後触れていくが、この終わりのないデータを継続的に処理し続けるというところにバッチ処理にはない難しさがある。\nなぜストリーム処理なのか なぜストリーム処理なのか。\nひとえに逐次的な入力データに対する迅速なフィードバックが求められているからと言えるだろう。\n迅速なフィードバックがビジネス上のメリットとなることは自明だ。\nSNS の配信 カーシェアリングにおける配車や料金設定 クレジットカードや広告クリックなどの不正検知 もしこれらの application が例えば hourly のバッチ処理で実装されていたらどうだろうか。\nまあ待っていられない。\n一般的なストリーム処理の構成 モダンな…と言っていいのかわからないが、ストリーム処理を行うための一般的なシステムは次の3つの要素で構成される。\nproducer broker consumer producer は最初にレコードを生成する、ストリームデータの発生源となるものである。\n例えばログを生成する web application であったり、何らかのセンサーを持つ IoT 機器であったりがこれに該当する。","title":"バッチ処理おじさんがストリーム処理のシステムを開発するにあたって調べたこと"},{"content":"前提 ここでは web システムで使われている機械学習のモデルやアルゴリズムを改善するための online の A/B テストを考える。\n具体的に述べると web 広告における CTR 予測や EC サイトのレコメンデーション等が対象である。\nよくあるやつ。\nweb システムにおいて online の A/B テストは KPI 改善の根幹でありとても重要だ。\nそれが重くなるとつらい、という話。\nここで「重い」と言っているのは計算資源のことではなく、A/B テストを実施する担当者の運用コストについて。\nA/B テストの運用が重い場合のデメリット デメリット 1. KPI 改善が遅くなる デメリットと言えばこれが一番大きい。\n単純に A/B テストを1回まわすのに時間がかかってしまうし、それがゆえに online の A/B テストに入るまでの offline のテストが厚くなりここでも時間がかかってしまう。\nKPI 改善に時間がかかるというのはつまり売上や利益を大きくするのに時間がかかってしまうということである。\nデメリット 2. KPI 改善における offline テストの比重が大きくなる 前述のとおりだが online の A/B テストが重いとそこで失敗できなくなり、結果としてその前段の offline のテストを厚くするということになる。\noffline のテストが厚いことの何が問題だろうか。\nここで前提としている CTR 予測やレコメンデーションのようなタスクの場合、offline のデータは既存のモデルやアルゴリズムの影響を受けることになる。\n例えばレコメンデーションの場合を考えると、新しいモデルを offline で評価するための実験データの正例 (コンテンツの閲覧等) は既存モデルによって生み出される。\n既存モデルが「このコンテンツがいいよ」といってユーザに出したリスト、その中からコンテンツの閲覧が行われ正例となるからだ。\nこのような状況下での offline テストにおいては既存モデルと近い好みを持ったモデルのスコアが高くなる傾向がある。\nもしかしたら既存モデルの影響を受けないようなデータをあえて用意しているような場合もあるかもしれないが、レアケースだろう。\n既存モデルの影響を受けないということは恩恵を受けないということなのでトレードオフでもある。\nデメリット 3. 新モデル／アルゴリズムを却下しにくくなる A/B テストの運用が重いと何度も繰り返すことができない。\nなので一発の A/B テストで既存モデル／アルゴリズムを置き換える成果を出さなければならないという圧が強くかかってしまう。\n既存モデル／アルゴリズムが勝って新しいものが負けてしまったときの埋没費用が大きいからだ。\nそうなると次のような事が起こる。\n「新モデルをチューニングしてみよう！」 数ヶ月前のデータでチューニングされていない既存モデル vs. 直近のデータでチューニングされた新モデル 「新モデルが KPI で勝っている間に意思決定しよう！」 勝ったり負けたりする中で… つまりフェアなテストではなくなってしまう。\nなぜ A/B テスト運用が重くなるのか 理由 1. リリース作業のコスト A/B テストの運用が重くなる理由の1として、A/B テストを始めるときのオペレーション、つまり新しいモデル／アルゴリズムをリリースする際の作業コストが重いことが挙げられる。\nこれにはモデル／アルゴリズムが web のシステムに対してどのような形でデプロイされているかが影響する。\nmercari が次のような資料を公開しているので参考にしたい。\n機械学習システムの設計パターンを公開します。 例えば Synchronous pattern のように web サーバと推論をする場所が分離している場合に比べて Web single pattern のように一体化している場合は新しいモデル／アルゴリズムのリリースに繊細にならざるを得ないのではないだろうか。\nコードベースもそうだし、マシンリソースの管理も後者の方が難しい。\n特にリリース担当者がデータサイエンティスト的な人だった場合、リリースの心理的障壁が上がる。\n理由 2. はっきりしない指標 A/B テストするときはテストの指標として何らかの KPI を置く。\nこの KPI が複数ある場合がある。\n例えば web 広告における CTR 予測モデルであれば CTR と impression 等。\n現実のビジネスは複雑なので複数の KPI があるのは珍しくないと思われる。\n一方で KPI が複数になると評価が難しくなる。\n「KPI A は5上がったけど B は3下がった、この場合はどうなの？」ということになり \u0026ldquo;マネージャの肌感\u0026rdquo; みたいなものが必要になってしまう。\n当然機械的に判定することによる自動化なども行えない。\nA/B テストの経過における現状の良し悪しの判断にコストがかかってしまうことになる。\nまた判断基準が複雑なことによりおとり効果のようなことも発生しうるだろう。\nこれについての1つの解として OEC: Overall Evaluation Criterion という考え方がある。\nWhat does \u0026ldquo;Overall Evaluation Criterion\u0026rdquo; mean? 複数の KPI を重みをつけて組み合わせてただ1つの値として評価できるようにする、というものらしい。\n逆に考えると OEC を定義せずに複数の KPI で A/B テストをするということは KPI 間のバランスの合意なしで走ってしまっているということになる。\n理由 3. 煩雑な流量オペレーション これは実際に見たことだが、A/B テストの運用に際して組織の一部の人間しか理解していない煩雑なオペレーションを伴うことがある。\n最初は control group を 0.1% だけ流して、1日経って問題がなければ 1% に上げて、その次は5%で…などのような操作を求められるのである。\n担当者にすればまあまあのストレスだし、時間がかかってしまう。\nまとめ ソフトウェア開発の分野でも大きな変更をえいやーでリリースするのはキツイということで git-flow であったり GitHub Flow であったりが導入されてきたはず。\nA/B テストが重いということはそれの逆をいっている状態となる。つらい。\n","permalink":"https://soonraah.github.io/posts/heavy-ab-testing-operation/","summary":"前提 ここでは web システムで使われている機械学習のモデルやアルゴリズムを改善するための online の A/B テストを考える。\n具体的に述べると web 広告における CTR 予測や EC サイトのレコメンデーション等が対象である。\nよくあるやつ。\nweb システムにおいて online の A/B テストは KPI 改善の根幹でありとても重要だ。\nそれが重くなるとつらい、という話。\nここで「重い」と言っているのは計算資源のことではなく、A/B テストを実施する担当者の運用コストについて。\nA/B テストの運用が重い場合のデメリット デメリット 1. KPI 改善が遅くなる デメリットと言えばこれが一番大きい。\n単純に A/B テストを1回まわすのに時間がかかってしまうし、それがゆえに online の A/B テストに入るまでの offline のテストが厚くなりここでも時間がかかってしまう。\nKPI 改善に時間がかかるというのはつまり売上や利益を大きくするのに時間がかかってしまうということである。\nデメリット 2. KPI 改善における offline テストの比重が大きくなる 前述のとおりだが online の A/B テストが重いとそこで失敗できなくなり、結果としてその前段の offline のテストを厚くするということになる。\noffline のテストが厚いことの何が問題だろうか。\nここで前提としている CTR 予測やレコメンデーションのようなタスクの場合、offline のデータは既存のモデルやアルゴリズムの影響を受けることになる。\n例えばレコメンデーションの場合を考えると、新しいモデルを offline で評価するための実験データの正例 (コンテンツの閲覧等) は既存モデルによって生み出される。\n既存モデルが「このコンテンツがいいよ」といってユーザに出したリスト、その中からコンテンツの閲覧が行われ正例となるからだ。","title":"A/B テストの運用が重くてつらいという話"},{"content":"前回の記事 では Apache Flink における stream data と static data の join において、DataStream API における broadcast state pattern を使う方法を示した。\n今回の記事では Table API の temporal table function を用いた実験を行う。\nTable API Table API は名前のとおりで class Table を中心として SQL-like な DSL により処理を記述するという、DataStream API より high-level な API となっている。\nこれらの関係は Apaceh Spark の RDD と DataFrame (DataSet) の関係に似ている。\nSQL-like な API で記述された処理が実行時に最適化されて low-level API の処理に翻訳されるところも同じだ。\nRDB の table の概念を元にしているものと考えられるが、本質的に table の概念とストリーム処理はあまりマッチしないと思う。\ntable はある時点のデータセット全体を表すのに対し、ストリーム処理ではやってくるレコードを逐次的に処理したい。\nここを合わせているため、ストリーム処理における Table API による処理の挙動の理解には注意が必要だ。\nStreaming Concepts 以下のドキュメントを確認しておきたい。\nTemporal Table Function star schema における、変更されうる dimension table を stream data と結合する方法として、temporal table という仕組みが提供されている。\nドキュメントでは為替レートの例が示されている。\nstream でやってくる fact table 的なレコードに対して、為替のように時々刻々と変化する dimension table をそのレコードの時刻における snap shot としてぶつけるような形となる。\nレコードの時刻としては processing time または event time を扱うことができる。\nevent time の場合であっても watermark で遅延の許容を定義できるため dimension table のすべての履歴を状態として保持する必要はなく、processing time または event time の watermark に応じて過去の履歴は捨てることが可能となっている。\nTable API において temporal table を使うには temporal table function という形を取ることになる。\n実験 実験概要 やることは 前回の記事 とまったく同じで乱数で作った株価のデータを扱う。\n前回と違うのは DataStream API ではなく Table API で処理を記述したところである。\nコード 上記を実行するためのコードを以下に示す。\n実行可能なプロジェクトは GitHub に置いておいた。\nEntry Point toTable() により入力データの DataStream を Table に変換した後、処理を記述した。\nfunc が temporal table function に当たる。\n今回は processing time を基準として join しているが、実際のシステムでは event time を基準としたいことが多いのではないだろうか。\npackage example import org.apache.flink.api.java.io.TextInputFormat import org.apache.flink.api.scala._ import org.apache.flink.core.fs.Path import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration import org.apache.flink.streaming.api.TimeCharacteristic import org.apache.flink.streaming.api.functions.source.FileProcessingMode import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment} import org.apache.flink.table.api.bridge.scala.{StreamTableEnvironment, _} import org.apache.flink.table.api.{AnyWithOperations, EnvironmentSettings, FieldExpression, call} import org.apache.flink.test.util.MiniClusterWithClientResource object FlinkTableJoinTest { // See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html#testing-flink-jobs private val FlinkCluster = new MiniClusterWithClientResource( new MiniClusterResourceConfiguration .Builder() .setNumberSlotsPerTaskManager(2) .setNumberTaskManagers(1) .build ) def main(args: Array[String]): Unit = { FlinkCluster.before() // for batch programs use ExecutionEnvironment instead of StreamExecutionEnvironment val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime) env.setParallelism(2) // create settings val setting = EnvironmentSettings .newInstance() .useBlinkPlanner() .inStreamingMode() .build() // create a TableEnvironment val tableEnv = StreamTableEnvironment.create(env, setting) // create a Table instance for Company val companiesMasterFilePath = \u0026#34;data/companies.csv\u0026#34; val companies = readCompaniesMaster(companiesMasterFilePath, env) .toTable(tableEnv, $\u0026#34;ticker\u0026#34;.as(\u0026#34;c_ticker\u0026#34;), $\u0026#34;name\u0026#34;, $\u0026#34;c_proc_time\u0026#34;.proctime) // temporal table function val func = companies.createTemporalTableFunction($\u0026#34;c_proc_time\u0026#34;, $\u0026#34;c_ticker\u0026#34;) // create a Table instance for Stock val stocks = env .fromCollection(new UnboundedStocks) .toTable(tableEnv, $\u0026#34;ticker\u0026#34;.as(\u0026#34;s_ticker\u0026#34;), $\u0026#34;price\u0026#34;, $\u0026#34;s_proc_time\u0026#34;.proctime) // join with a temporal table function val results = stocks .joinLateral(call(func, $\u0026#34;s_proc_time\u0026#34;), $\u0026#34;s_ticker\u0026#34; === $\u0026#34;c_ticker\u0026#34;) .select($\u0026#34;s_ticker\u0026#34;, $\u0026#34;name\u0026#34;, $\u0026#34;price\u0026#34;) .toAppendStream[(String, String, Double)] .print env.execute() FlinkCluster.after() } private def readCompaniesMaster(companiesMasterFilePath: String, env: StreamExecutionEnvironment): DataStream[Company] = { env .readFile( new TextInputFormat(new Path(companiesMasterFilePath)), companiesMasterFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 10 * 1000 ) .map { line =\u0026gt; val items = line.split(\u0026#34;,\u0026#34;) Company(items(0), items(1)) } } } 実行結果 上記を実行すると以下のような stream と static が結合された結果レコードが流れ続ける。\n2\u0026gt; (AMZN,Amazon,110.05826176785374) 2\u0026gt; (AMZN,Amazon,237.82717323588966) 1\u0026gt; (FB,Facebook,147.96046700184428) 1\u0026gt; (GOOGL,Google,393.58555322242086) 2\u0026gt; (AMZN,Amazon,104.18843434881401) 前回と同様に data/companies.csv の中身を更新するとその結果が反映される。\n削除が反映されないのも同じだった。\nおそらく physical な処理としてはほぼ同じようになっていると思われる。\nまとめ 前回と同様の stream data と static data の join を、Table API + temporal table function で行えることを確認した。\ntemporal table function の概念さえ把握できれば Straem API のときに比べて簡潔に処理を記述できた。\n","permalink":"https://soonraah.github.io/posts/flink-join-by-temporal-table-function/","summary":"前回の記事 では Apache Flink における stream data と static data の join において、DataStream API における broadcast state pattern を使う方法を示した。\n今回の記事では Table API の temporal table function を用いた実験を行う。\nTable API Table API は名前のとおりで class Table を中心として SQL-like な DSL により処理を記述するという、DataStream API より high-level な API となっている。\nこれらの関係は Apaceh Spark の RDD と DataFrame (DataSet) の関係に似ている。\nSQL-like な API で記述された処理が実行時に最適化されて low-level API の処理に翻訳されるところも同じだ。\nRDB の table の概念を元にしているものと考えられるが、本質的に table の概念とストリーム処理はあまりマッチしないと思う。\ntable はある時点のデータセット全体を表すのに対し、ストリーム処理ではやってくるレコードを逐次的に処理したい。\nここを合わせているため、ストリーム処理における Table API による処理の挙動の理解には注意が必要だ。","title":"Apache Flink の Temporary Table Function を用いた stream data と static data の join"},{"content":"star schema における fact table と dimension table の join のようなことを Apache Flink におけるストリーム処理で行いたい。\nstream data と static data の join ということになる。\nただし dimension table 側も更新されるため、完全な static というわけではない。\nこのポストでは Flink v1.11 を前提とした。\njoin の方法 今回は DataStream API でこれを実現することを考える。\nFlink のドキュメントを読むと broadcast state pattern でできそうだ。\nThe Broadcast State Pattern やり方としては次のようになる。\nstatic data のファイルを FileProcessingMode.PROCESS_CONTINUOUSLY で読み込み DataStream 化 1 を broadcast() stream data の DataStream と 2 を connect() static data を PROCESS_CONTINUOUSLY で読むのは変更を得るため。\nPROCESS_ONCE で読んでしまうとストリーム処理の開始時に1回読むだけになり、dimension table の変更を得られない。\nこのあたりの仕様については Data Sources を参照。\nその後は broadcast state pattern にそのまま従う。\nしたがって BroadcastProcessFunction or KeyedBroadcastProcessFunction を実装する必要がある。\nその中で static data を取り込んで state として持ち、stream data 側で参照すればよい。\n2つのデータの各 term に対する関係性を以下に示す。\nin star schema stream or static broadcast or not dimension table static data broadcasted fact table stream data non-broadcasted 実験 実験概要 stream data として株価のデータを考える。\n適当に乱数で作った株価が \u0026ldquo;GOOGL\u0026rdquo; 等の ticker とともに流れてくる。\n一方、会社情報が記載された dimension table 的なファイルも用意する。\n流れ続ける株価データに対して ticker を key にして会社情報を紐付ける、ということを行う。\nコード 上記を実行するためのコードを以下に示す。\n実行可能なプロジェクトを GitHub に置いたので興味があればどうぞ。\nEntry Point main() の実装。\nMiniClusterWithClientResource は本来は単体テスト用だが、簡単に local で cluster を動かすためにここで使用している。\npackage example import org.apache.flink.api.common.state.MapStateDescriptor import org.apache.flink.api.java.io.TextInputFormat import org.apache.flink.core.fs.Path import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration import org.apache.flink.streaming.api.functions.source.FileProcessingMode import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment, createTypeInformation} import org.apache.flink.test.util.MiniClusterWithClientResource object FlinkJoinTest { // See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html#testing-flink-jobs private val FlinkCluster = new MiniClusterWithClientResource( new MiniClusterResourceConfiguration .Builder() .setNumberSlotsPerTaskManager(2) .setNumberTaskManagers(1) .build ) def main(args: Array[String]): Unit = { FlinkCluster.before() val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val companiesMasterFilePath = \u0026#34;data/companies.csv\u0026#34; val companies = readCompaniesMaster(companiesMasterFilePath, env) .broadcast(new MapStateDescriptor( \u0026#34;CompanyState\u0026#34;, classOf[String], // ticker classOf[String] // name )) // the broadcast state pattern // See https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html env .fromCollection(new UnboundedStocks) .connect(companies) .process(new StockBroadcastProcessFunction) .print() env.execute(\u0026#34;flink join test\u0026#34;) FlinkCluster.after() } private def readCompaniesMaster(companiesMasterFilePath: String, env: StreamExecutionEnvironment): DataStream[Company] = { env .readFile( new TextInputFormat(new Path(companiesMasterFilePath)), companiesMasterFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 10 * 1000 ) .map { line =\u0026gt; val items = line.split(\u0026#34;,\u0026#34;) Company(items(0), items(1)) } } } Records 各種レコードを表す case class。\nUnboundedStocks は一定のインターバルで Stock を無限に返す iterator であり、stream data 生成に利用する。\ncase class Company(ticker: String, name: String) case class Stock(ticker: String, price: Double) /** * Iterator to generate unbounded stock data */ class UnboundedStocks extends Iterator[Stock] with Serializable { override def hasNext: Boolean = true // unbounded override def next(): Stock = { Thread.sleep(1000) val tickers = Seq(\u0026#34;GOOGL\u0026#34;, \u0026#34;AAPL\u0026#34;, \u0026#34;FB\u0026#34;, \u0026#34;AMZN\u0026#34;) val ticker = tickers(Random.nextInt(tickers.size)) // one of GAFA val price = 100 + Random.nextDouble() * 300 // random price Stock(ticker, price) } } BroadcastProcessFunction 肝である BroadcastProcessFunction の実装。\npackage example import org.apache.flink.api.common.state.MapStateDescriptor import org.apache.flink.streaming.api.functions.co.BroadcastProcessFunction import org.apache.flink.util.Collector class StockBroadcastProcessFunction extends BroadcastProcessFunction[Stock, Company, (String, String, Double)] { private val StateDescriptor = new MapStateDescriptor( \u0026#34;CompanyState\u0026#34;, classOf[String], // ticker classOf[String] // name ) override def processElement(value: Stock, ctx: BroadcastProcessFunction[Stock, Company, (String, String, Double)]#ReadOnlyContext, out: Collector[(String, String, Double)]): Unit = { val companyName = ctx.getBroadcastState(StateDescriptor).get(value.ticker) out.collect((value.ticker, Option(companyName).getOrElse(\u0026#34;-\u0026#34;), value.price)) } override def processBroadcastElement(value: Company, ctx: BroadcastProcessFunction[Stock, Company, (String, String, Double)]#Context, out: Collector[(String, String, Double)]): Unit = { ctx.getBroadcastState(StateDescriptor).put(value.ticker, value.name) } } 実行結果 上記を実行すると以下のような stream と static が結合された結果レコードが流れ続ける。\n2\u0026gt; (FB,Facebook,158.76057838239333) 1\u0026gt; (GOOGL,Google,288.4271251901199) 2\u0026gt; (AAPL,Apple,191.00515338617706) 1\u0026gt; (FB,Facebook,121.98205452369652) 2\u0026gt; (FB,Facebook,140.05023554456997) この状態で会社情報が記載されている data/companies.csv を更新することを考える。\n例えば \u0026ldquo;GOOGL\u0026rdquo; の社名を \u0026ldquo;Google\u0026rdquo; から \u0026ldquo;Alphabet\u0026rdquo; に変更して保存してみた。\nするとしばらくしてその修正が反映された結果が流れてくるようになる。\n1\u0026gt; (GOOGL,Alphabet,288.1008081843843) 2\u0026gt; (AMZN,Amazon,137.11135563851838) 1\u0026gt; (GOOGL,Alphabet,121.78368168964735) 2\u0026gt; (FB,Facebook,236.53483047124948) 1\u0026gt; (FB,Facebook,220.44300865769645) static data の更新が反映されることが確認できた。\n今回は10秒に1回のインターバルで元ファイルを確認するようにファイルを読んでいるため、変更してからそれが反映されるまで最大10秒程度かかる。\n懸念点 join はできたが次のような懸念点がある。\nレコードの削除に対応していない state を上書きしているだけなので companies.csv から削除されたレコードは感知できない ファイルの更新時に処理が重くなる可能性がある companies.csv の更新タイミングでその中の全レコードを処理してしまう checkpoint が大きくなる state が broadcast されているため、task ごとに重複した state が保存されてしまう See Important Considerations まとめ このように broadcast state pattern によって stream data と static data との join 処理を行うことができた。\nただし、まだちゃんと調べていないが DataStream API ではなく Table API を使えばもう少しカジュアルな感じで近いことができるかもしれない。\n気が向いたらそちらも試してみる。\n追記 Table API を使った場合についての記事を追加しました。\nApache Flink の Temporary Table Function を用いた stream data と static data の join ","permalink":"https://soonraah.github.io/posts/flink-join-by-broadcast-state-pattern/","summary":"star schema における fact table と dimension table の join のようなことを Apache Flink におけるストリーム処理で行いたい。\nstream data と static data の join ということになる。\nただし dimension table 側も更新されるため、完全な static というわけではない。\nこのポストでは Flink v1.11 を前提とした。\njoin の方法 今回は DataStream API でこれを実現することを考える。\nFlink のドキュメントを読むと broadcast state pattern でできそうだ。\nThe Broadcast State Pattern やり方としては次のようになる。\nstatic data のファイルを FileProcessingMode.PROCESS_CONTINUOUSLY で読み込み DataStream 化 1 を broadcast() stream data の DataStream と 2 を connect() static data を PROCESS_CONTINUOUSLY で読むのは変更を得るため。","title":"Apache Flink の Broadcast State Pattern を用いた stream data と static data の join"},{"content":"GitHub Flow ベースの開発においてはあまり大きな pull request を作ってほしくないという話。\nなんというか今更わざわざ言わなくてもいいんだけど…\n仕事で何度か大きな pull request が投げられているのを見てしまったので、それはあまりよくないよというのを自分でも指摘しやすくするためにまとめておく。\nReference 最初に参考資料を挙げておく。\n100 Duck-Sized Pull Requests 「巨大プルリク1件vs細かいプルリク100件」問題を考える（翻訳） Optimal pull request size これらを読めば特に私から言うこともないのだが…\nこれらに書いてあるとおりだが補足しておく。\nPull Request を小分けにしたときのメリット module を適切に切り出すモチベーションが得られる 次のような話がある。\n共通化という考え方はアンチパターンを生み出すだけ説 これを理由に module を分けるべきところが分けられず、いろんなことができてしまうベタッと大きな module が生まれるのを見た。\nもちろんよく考えられていない共通化は駄目だが、上記ポストでは一方で\nただし共通化という名の下におこなわれるのは「同じロジックを持つコードをまとめる」行為であって、抽象化のようにそのコード単位の意味を捉える作業はその範疇にない。抽象化というのはロジックを意味単位ごとにひとくくりにしていく行為で、これがどういうことなのかは次以降で述べていく。\n\u0026ndash; 共通化という考え方はアンチパターンを生み出すだけ説 - タオルケット体操\nと述べており抽象化、つまりコードの意味を考慮した上で適切な単位でまとめておくことは否定していない。\npull request を小さく分けるという行為のためには module や機能を適度なまとまりで切り分けること、つまり抽象化を考えていくことが必要となる。\nしたがって何でもできてしまう大きな module が生まれるのを防ぐ方向に働く。\n(もちろんここで駄目な共通化がなされてしまうこともあるだろう)\nリリースまでの期間が短く済む 前述の参考資料においても pull request が大きいと\nDevelopers would put off reviews until they had time/energy and the development process would come to a halt.\n\u0026ndash; 100 Duck-Sized Pull Requests\ndevelopment continues on the master branch, it often results in merge conflicts, rebases, and other fun.\n\u0026ndash; 100 Duck-Sized Pull Requests\nとなると述べられている。\n仮にすぐにレビューされ、かつ conflict なども発生しなかっとしても大きな1つの pull request をレビューする場合とそれを3つに分けた場合の開発プロセスの進行を比較すると\n大きな1つの pull request 全体の開発 全体のレビュー 3つに分割された pull request part-1 の開発 part-1 のレビュー \u0026amp; part-2 の開発 part-2 のレビュー \u0026amp; part-3 の開発 part-3 のレビュー のようになり、開発とレビューを並列で進められる部分があるので全体としての開発期間を短くすることができる。\nもちろんこれはうまく噛み合ったときの例だが。\n早めにフィードバックが得られるのも大きい。\nPull Request を小分けにしたときのデメリット 小分けにすることでレビュワーが全体感をつかみにくくなるというのはあるかもしれない。\npart-1 と part-2 で同じレビュワーになるとも限らない。\nこれに対しては開発に着手する前にまず全体のざっくりとした設計についてレビューを受けることで対応できる。\nこのステップを入れることで「とりあえず手を動かそう」となりにくくなる。\n手を動かすと仕事してる感が出るのでとりあえず手を動かしたくなりがちだが、ちょっと待てよと。\nまず何を作るか、全体の工程を考えましょうと。\nWorking under these constraints causes developers to break problems down into incremental deliverables. It helps avoid the temptation of jumping into development without a clear plan.\n\u0026ndash; 100 Duck-Sized Pull Requests\nまとめ pull request を分けてくれ、頼む。\n","permalink":"https://soonraah.github.io/posts/no-more-huge-pull-request/","summary":"GitHub Flow ベースの開発においてはあまり大きな pull request を作ってほしくないという話。\nなんというか今更わざわざ言わなくてもいいんだけど…\n仕事で何度か大きな pull request が投げられているのを見てしまったので、それはあまりよくないよというのを自分でも指摘しやすくするためにまとめておく。\nReference 最初に参考資料を挙げておく。\n100 Duck-Sized Pull Requests 「巨大プルリク1件vs細かいプルリク100件」問題を考える（翻訳） Optimal pull request size これらを読めば特に私から言うこともないのだが…\nこれらに書いてあるとおりだが補足しておく。\nPull Request を小分けにしたときのメリット module を適切に切り出すモチベーションが得られる 次のような話がある。\n共通化という考え方はアンチパターンを生み出すだけ説 これを理由に module を分けるべきところが分けられず、いろんなことができてしまうベタッと大きな module が生まれるのを見た。\nもちろんよく考えられていない共通化は駄目だが、上記ポストでは一方で\nただし共通化という名の下におこなわれるのは「同じロジックを持つコードをまとめる」行為であって、抽象化のようにそのコード単位の意味を捉える作業はその範疇にない。抽象化というのはロジックを意味単位ごとにひとくくりにしていく行為で、これがどういうことなのかは次以降で述べていく。\n\u0026ndash; 共通化という考え方はアンチパターンを生み出すだけ説 - タオルケット体操\nと述べており抽象化、つまりコードの意味を考慮した上で適切な単位でまとめておくことは否定していない。\npull request を小さく分けるという行為のためには module や機能を適度なまとまりで切り分けること、つまり抽象化を考えていくことが必要となる。\nしたがって何でもできてしまう大きな module が生まれるのを防ぐ方向に働く。\n(もちろんここで駄目な共通化がなされてしまうこともあるだろう)\nリリースまでの期間が短く済む 前述の参考資料においても pull request が大きいと\nDevelopers would put off reviews until they had time/energy and the development process would come to a halt.","title":"あまり大きな Pull Request を作ってほしくない"},{"content":"2020-07-31 にオンライン開催された Spark Meetup Tokyo #3 Online に参加した。\n感想などメモしておく。\n全体感 トピックとしては主に\nSpark 3.0 Spark + AI Summit 2020 Spark 周辺要素 といったところだろうか。\n最近のコミュニティの動向や関心を日本語で聞くことができてよかった。\n運営 \u0026amp; スピーカーの皆様、ありがとうございます。\n発表 発表資料は公開されたら追加していく。\nSPARK+AI Summit 2020 イベント概要 スピーカー: @tokyodataguy さん\nSummit は金融業界の参加者が増えているらしい Spark で最も使われている言語は Python とのことだったが、Databricks の notebook サービスの話だった プロダクションコードではまた違うのだろう Spark 3.0 の update をざっくりと Spark 周辺要素の話をざっくりと Koalas Delta Lake Redash MLflow Introducing Koalas 1.0 Introducing Koalas 1.0 (and 1.1) from Takuya UESHIN スピーカー: @ueshin さん\nWhat\u0026rsquo;s Koalas open source の pure Python library pandas の API で Spark を動かせるようにする 小さいデータも大きなデータも同じように扱えるように 最近 v1.1 をリリース (個人的には pandas の API があまり好きではなく…) SPARK+AI Summit 2020 のセッションハイライト Spark + AI Summit 2020セッションのハイライト（Spark Meetup Tokyo #3 Online発表資料） from NTT DATA Technology \u0026amp; Innovation スピーカー: @masaru_dobashi さん\nSummit のセッションから case study 的なセッションを2つピックアップ USCIS の例 Lessons Learned from Modernizing USCIS Data Analytics Platform 古き良き Data Warehouse から Data Lake への移行 injection には Kafka も利用 諸々気にせずに気軽にデータをストレージに置きたい Alibaba の例 Spark Structured Streming の上に SQL-like なものを載せた？ ストリームの mini-batch 処理の合間で compaction を行う (つらそう) スライド中の「パイプラインの途中でモダンな技術に流し込めるかどうか？」という言葉が印象的だった モダンなものに移行するとき現実的に重要 Spark v3.0の紹介 前半 Introduction new features in Spark 3.0 from Kazuaki Ishizaki スピーカー: @kiszk さん\nSQL の性能に関わる7大機能の話 Query planの新しい表示方法 Join hintsの強化 Adaptive query execution Dynamic partitioning pruning nested column pruning \u0026amp; pushdown の強化 Aggregation のコード生成の改良 ScalaとJavaの新バージョンのサポート 実行計画が見やすくなるのは本当にうれしい 人がチューニングしていた部分が Adaptive Query Plan や Dynamic Partition Pruning で自動化されるのもうれしい 後半 Apache Spark 3.0新機能紹介 - 拡張機能やWebUI関連のアップデート（Spark Meetup Tokyo #3 Online） from NTT DATA Technology \u0026amp; Innovation スピーカー: @raspberry1123 さん\nAccelarator Aware Scheduling Project Hydrogen プラグイン機能 executor や driver の機能を user が拡張できる executor については Spark 2.4 からあったそうだが知らなかった… Structured Straming web UI ストリーム処理にはこういった chart が必要だと思う SparkにPRを投げてみた Sparkにプルリク投げてみた from Noritaka Sekiyama スピーカー: @moomindani さん\nスピーカーは AWS で Glue の開発をしている方 Glue は Spark に強く依存しているため、機能追加にモチベーションがあったとのこと 私も投げてみたい！ LT: td-spark internals: AirframeでSparkの機能を拡張するテクニック td-spark internals: Extending Spark with Airframe - Spark Meetup Tokyo #3 2020 from Taro L. Saito スピーカー: @taroleo さん\nAirframe とは Scala の application 開発用ツール群のようなもの？ Spark の機能を拡張 LT: Spark 3.1 Feature Expectation LT: Spark 3.1 Feature Expectation from Takeshi Yamamuro スピーカー: @maropu さん\n新機能 Support Filter Pushdown JSON JSON の読み取りが早くなる Better Handling for Node Shutdown node 離脱時にその node が保持する shuffle や cache の情報が失われていた 計画された node の離脱に対して shuffle や cache ブロックを別 node に委譲 まとめ どんどん便利になっていくなという印象。\n手でパフォーマンスチューニングする要素はどんどん減っていくのだろう。\nDelta Lake が盛り上がっているように感じた。\nデータレイクについて勉強しないと…\n","permalink":"https://soonraah.github.io/posts/spark-meetup-tokyo-3/","summary":"2020-07-31 にオンライン開催された Spark Meetup Tokyo #3 Online に参加した。\n感想などメモしておく。\n全体感 トピックとしては主に\nSpark 3.0 Spark + AI Summit 2020 Spark 周辺要素 といったところだろうか。\n最近のコミュニティの動向や関心を日本語で聞くことができてよかった。\n運営 \u0026amp; スピーカーの皆様、ありがとうございます。\n発表 発表資料は公開されたら追加していく。\nSPARK+AI Summit 2020 イベント概要 スピーカー: @tokyodataguy さん\nSummit は金融業界の参加者が増えているらしい Spark で最も使われている言語は Python とのことだったが、Databricks の notebook サービスの話だった プロダクションコードではまた違うのだろう Spark 3.0 の update をざっくりと Spark 周辺要素の話をざっくりと Koalas Delta Lake Redash MLflow Introducing Koalas 1.0 Introducing Koalas 1.0 (and 1.1) from Takuya UESHIN スピーカー: @ueshin さん","title":"勉強会メモ: Spark Meetup Tokyo #3 Online"},{"content":"Spark バッチ処理の問題を調べていたら分離レベルという概念にたどりついた。\n分離レベルについて調べたので、Spark の問題の内容と絡めて記しておく。\n考えてみれば当たり前でたいした話ではない。\n分離レベルとは トランザクションの挙動についての暗黙の理解 アドホックな分析クエリやプロダクションコード中のクエリを書くとき、その単一のクエリのトランザクションにおいて「同時に実行されている別のクエリの commit 前の状態や commit 結果に影響され、このクエリの結果がおかしくなるかもしれない」ということは通常考えない。\nトランザクションはデータベースのある時点の状態に対して正しく処理される、というほぼ無意識の理解をおそらくほとんどの開発者が持っている。\n多くの場合この理解は間違っていない。\nそれはなぜかというと DB 等のデータ処理フレームワークがある強さの分離レベルを提供しているからである。\nいろいろな分離レベル ACID 特性のうちの1つ、分離性 (Isolation) の程度を表すのが分離レベル。\nトランザクション中に行われる操作の過程が他の操作から隠蔽されることを指し、日本語では分離性、独立性または隔離性ともいう。より形式的には、独立性とはトランザクション履歴が直列化されていることと言える。この性質と性能はトレードオフの関係にあるため、一般的にはこの性質の一部を緩和して実装される場合が多い。 \u0026ndash; Wikipedia ACID (コンピュータ科学)\n分離レベルには名前のついたものがいくつかあり、分離性の保証の強さが異なる。\n具体的にはトランザクションの並行性の問題への対応力が異なる。\n名著「データ指向アプリケーションデザイン」の第7章で分離レベルについて詳しく述べられているので、以下ではそちらからの引用。\n分離レベルを弱い順に並べる。\nread uncommitted\nこのレベルではダーティライトは生じませんが、ダーティリードは妨げられません。\nread committed\nデータベースからの読み取りを行った際に見えるデータは、コミットされたもののみであること（ダーティリードは生じない）。 データベースへの書き込みを行う場合、上書きするのはコミットされたデータのみであること（ダーティライトは生じない）。 snapshot isolation\nスナップショット分離の考え方は、それぞれのトランザクションがデータベースの一貫性のあるスナップショットから読み取りを行うというものです。すなわち、トランザクションが読み取るデータは、すべてそのトランザクションの開始時点のデータベースにコミット済みのものだけということです。\nserializability\nこの分離レベルはトランザクションが並行して実行されていても、最終的な答えはそれぞれが1つずつ順番に、並行ではなく実行された場合と同じになることを保証します。\n日本語で「分離レベル」を検索すると snapshot isolation の代わりに repeatable read が出てくる事が多い。\nしかし repeatable read の名前は実装によって意味が違っていたりして扱いが難しいらしい。\n分離レベルと race condition の関係 以下に各分離レベルとトランザクションの並行性の問題 (race condition) の関係を示す。\n各 race condition の説明については割愛するが、複数のトランザクションが並行して実行されることにより起こりうる期待されていない挙動だと思えばよい。\n○はその分離レベルにおいてその race condition が発生しないことを示す。\n△は条件によっては発生する。\ndirty read dirty write read skew (nonrepeatable read) lost update write skew phantom read read uncommitted ○ - - - - - read committed ○ ○ - - - - snapshot isolation ○ ○ ○ △ - △ serializability ○ ○ ○ ○ ○ ○ 下に行くほど強い分離レベルとなっている。\n分離レベルが強くなるほど race condition が発生しにくくなるが、一方で lock 等によりパフォーマンスが下がっていくというトレードオフがある。\n各種データベースの分離レベル ここでは MySQL と Hive においてどの分離レベルが提供されているかを見てみる。\nMySQL の場合 MySQL の分離レベルについては以下のドキュメントで述べられている。\n15.7.2.1 Transaction Isolation Levels MySQL (というか InnoDB?) では次の4つの分離レベルを設定することができる。\nREAD UNCOMMITTED READ COMMITTED REPEATABLE READ (default) SERIALIZABLE デフォルトの分離レベルは REPEATABLE READ だが、これは前述の snapshot isolation に相当するらしい。\n分離レベルは、例えば set transaction 構文により次のようにして指定できる。\nset transaction isolation level SERIALIZABLE; この場合は現在のセッション内で実行される次のトランザクションについて適用される。\nすべてのセッションやすべてのトランザクション等の指定もできる。\n詳しくは以下。\n13.3.7 SET TRANSACTION Statement Hive の場合 Hive についてはドキュメントに次のような記載がある。\nAt this time only snapshot level isolation is supported. When a given query starts it will be provided with a consistent snapshot of the data.\n\u0026ndash; Hive Transactions\nHive は snapshot isolation のみ提供しているとのこと。\nThe default DummyTxnManager emulates behavior of old Hive versions: has no transactions and uses hive.lock.manager property to create lock manager for tables, partitions and databases.\n\u0026ndash; Hive Transactions\nlock は小さくとも partition の単位になるのだろうか。\nであるとすると予想通りだが MySQL よりもいかつい挙動になっている。\nこのように多くの DB では snapshot isolation の分離レベルが基本となっている。\nSpark クエリの分離レベル では Spark のクエリはどうだろうか。\nここからようやく本題となる。\nread committed 相当 Spark において DataFrame を用いたデータ処理を記述するとき、それは1つの SQL クエリを書くのと近い感覚になる。\nそもそも DataFrame は SQL-like な使い心地を目的として作られた API だから当然だ。\nDataFrame で記述された処理は実行時に RDD として翻訳されるが、分離レベルを考えるにあたって RDD の特性がキーとなってくる。\nBy default, each transformed RDD may be recomputed each time you run an action on it.\n\u0026ndash; RDD Operations\nあまり良い説明を見つけられなかったが、1回の action を伴う処理においても同じ RDD が複数回参照されるとき、その RDD までの計算は通常やり直されることになる。\nしたがって例えば self join のようなことをするとき、同じデータソースを2回読みに行くということが起こってしまう。\nHDFS や S3 上のファイル、JDBC 経由の外部 DB など Spark は様々なデータソースを扱うことができるが、通常 Spark がその2回の読み込みに対して lock をかけたりすることはできない。\nつまり non-repeatable read や phantom read を防ぐことができない。\nread committed という弱い分離レベルに相当するということになってしまう。\n分離レベルという言葉はトランザクションという概念に対して使われるものであり、DataFrame のクエリをトランザクションと呼んでいいのかはわからない。\nなので分離レベルという言葉をここで使うのが適切でないかもしれないということは述べておく。\n検証 MySQL からデータを読み取り Spark で処理することを考える。\nまず local の MySQL で次のような table を用意する。\nmysql\u0026gt; describe employees; +---------------+---------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------------+---------+------+-----+---------+-------+ | id | int(11) | NO | PRI | NULL | | | salary | int(11) | YES | | NULL | | | department_id | int(11) | YES | | NULL | | +---------------+---------+------+-----+---------+-------+ 3 rows in set (0.03 sec) 部署 (department) ごとの給料 (salary) の平均から各従業員の給料がどれくらい離れているかの差分を見たいものとする。\nSpark のコードは次のようになる。\nSpark のバージョンはこれを書いている時点での最新 3.0.0 とした。\npackage com.example import org.apache.spark.sql.functions.avg import org.apache.spark.sql.SparkSession object IsolationLevelExperiment { def main(args: Array[String]): Unit = { // Prepare SparkSession val spark = SparkSession .builder() .appName(\u0026#34;Isolation Level Experiment\u0026#34;) .master(\u0026#34;local[*]\u0026#34;) .getOrCreate() import spark.implicits._ // Read from MySQL val dfEmployee = spark .read .format(\u0026#34;jdbc\u0026#34;) .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:mysql://localhost\u0026#34;) .option(\u0026#34;dbtable\u0026#34;, \u0026#34;db_name.employees\u0026#34;) .option(\u0026#34;user\u0026#34;, \u0026#34;user_name\u0026#34;) .option(\u0026#34;password\u0026#34;, \u0026#34;********\u0026#34;) .option(\u0026#34;driver\u0026#34;, \u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;) .load .cache // Get average salary val dfAvg = dfEmployee .groupBy($\u0026#34;department_id\u0026#34;) .agg(avg($\u0026#34;salary\u0026#34;).as(\u0026#34;avg_salary\u0026#34;)) // Calculate diff val dfResult = dfEmployee .as(\u0026#34;e\u0026#34;) .join( dfAvg.as(\u0026#34;a\u0026#34;), $\u0026#34;e.department_id\u0026#34; === $\u0026#34;a.department_id\u0026#34;, \u0026#34;left_outer\u0026#34; ) .select( $\u0026#34;e.id\u0026#34;, $\u0026#34;e.department_id\u0026#34;, ($\u0026#34;e.salary\u0026#34; - $\u0026#34;a.avg_salary\u0026#34;).as(\u0026#34;salary_diff\u0026#34;) ) // Output results dfResult.show spark.stop() } } このコードを実行する前に MySQL の general query log を ON にする。\nmysql\u0026gt; set global general_log = \u0026#39;ON\u0026#39;; Query OK, 0 rows affected (0.02 sec) mysql\u0026gt; show global variables like \u0026#39;general_log%\u0026#39;; +------------------+--------------------------------------+ | Variable_name | Value | +------------------+--------------------------------------+ | general_log | ON | | general_log_file | /usr/local/var/mysql/MacBook-Pro.log | +------------------+--------------------------------------+ 2 rows in set (0.01 sec) これによって MySQL に対して発行されたクエリがログとして記録されるようになる。\n直感的に snapshot isolation になっているのであれば MySQL に対する select 文は1回だけ発行されるはずである。\nしかし前述のとおり RDD や DataFrame の処理は途中の状態を通常保存せず、同じ RDD や DataFrame を参照していたとしても再計算される。\n上記コードの例だと dfEmployee が2回参照されている。\nコードを実行すると general query log には次のように、データ取得のための select 文が2つ記録されていた。\nそれぞれ join() の左右の table のデータソースを示している。\n8 Query SELECT `id`,`salary`,`department_id` FROM test_fout_dsp.employees 7 Query SELECT `salary`,`department_id` FROM test_fout_dsp.employees WHERE (`department_id` IS NOT NULL) 2つの select 文はそれぞれ別のクエリ、トランザクションとして発行されている。\nしたがって前者の select 文が実行された後、後者の select 文が実行される前に別のトランザクションにより employees が更新されたり挿入・削除されたりすると non-repeatable read や phantom read が発生してしまうのである。\n今回はデータソースへのアクセスを確認するためにデータソースとして MySQL を使ったが、同じことはファイルや他の DB など別のデータソースで起こりうる。\n回避策 プロダクト運用上、non-repeatable read や phantom read が発生しうる状況というのは多くの場合で厄介である。\n一時的なデータソースの状態に依存して問題が発生するため、バグの原因追求がとても困難だからだ。\n見た目上の分離レベルを強くし、これらを避けるには2つの方法が考えられる。\nimmutable なデータにのみアクセスする 単純な話で non-repeatable read や phantom read が発生するようなデータソースを参照しなければよい。\n例えばユーザの行動ログのような蓄積されていくデータが hour 単位で partitioning されているような場合、基本的に過去の partition に対しては変更や挿入・削除は行われない。\nこのような partition にアクセスする分には前述のような厄介な問題は起こらない。\ncache する データソースから読み取った結果の DataFrame に対して cache() または persist() をするとよい。\nSpark SQL can cache tables using an in-memory columnar format by calling spark.catalog.cacheTable(\u0026quot;tableName\u0026quot;) or dataFrame.cache().\n\u0026ndash; Caching Data In Memory\n前述のコードで dfEmployee に対して .cache() をした場合、MySQL へのデータ取得のための select 文の発行は1回になった。\n大きなデータソースを cache() するときだけメモリや HDD 容量に気をつけておきたい。\nまとめ DataFrame はいわゆる RDBMS を操作しているように見えてしまうところが難しいところだろうか。\n「データ指向アプリケーションデザイン」に至極真っ当なことが書いてあったので、その言葉を借りて締めておく。\n何も考えずにツールを信じて依存するのではなく、並行性の問題にはどういったものがあるのか、そしてそれらを回避するにはどうしたら良いのかを、しっかり理解しなければなりません。そうすれば、信頼性があり、正しく動作するアプリケーションを手の届くツールを使って構築できるようになります。\n\u0026ndash; Martin Kleppmann データ指向アプリケーションデザイン\n","permalink":"https://soonraah.github.io/posts/weak_isolation_level_of_dataframe/","summary":"Spark バッチ処理の問題を調べていたら分離レベルという概念にたどりついた。\n分離レベルについて調べたので、Spark の問題の内容と絡めて記しておく。\n考えてみれば当たり前でたいした話ではない。\n分離レベルとは トランザクションの挙動についての暗黙の理解 アドホックな分析クエリやプロダクションコード中のクエリを書くとき、その単一のクエリのトランザクションにおいて「同時に実行されている別のクエリの commit 前の状態や commit 結果に影響され、このクエリの結果がおかしくなるかもしれない」ということは通常考えない。\nトランザクションはデータベースのある時点の状態に対して正しく処理される、というほぼ無意識の理解をおそらくほとんどの開発者が持っている。\n多くの場合この理解は間違っていない。\nそれはなぜかというと DB 等のデータ処理フレームワークがある強さの分離レベルを提供しているからである。\nいろいろな分離レベル ACID 特性のうちの1つ、分離性 (Isolation) の程度を表すのが分離レベル。\nトランザクション中に行われる操作の過程が他の操作から隠蔽されることを指し、日本語では分離性、独立性または隔離性ともいう。より形式的には、独立性とはトランザクション履歴が直列化されていることと言える。この性質と性能はトレードオフの関係にあるため、一般的にはこの性質の一部を緩和して実装される場合が多い。 \u0026ndash; Wikipedia ACID (コンピュータ科学)\n分離レベルには名前のついたものがいくつかあり、分離性の保証の強さが異なる。\n具体的にはトランザクションの並行性の問題への対応力が異なる。\n名著「データ指向アプリケーションデザイン」の第7章で分離レベルについて詳しく述べられているので、以下ではそちらからの引用。\n分離レベルを弱い順に並べる。\nread uncommitted\nこのレベルではダーティライトは生じませんが、ダーティリードは妨げられません。\nread committed\nデータベースからの読み取りを行った際に見えるデータは、コミットされたもののみであること（ダーティリードは生じない）。 データベースへの書き込みを行う場合、上書きするのはコミットされたデータのみであること（ダーティライトは生じない）。 snapshot isolation\nスナップショット分離の考え方は、それぞれのトランザクションがデータベースの一貫性のあるスナップショットから読み取りを行うというものです。すなわち、トランザクションが読み取るデータは、すべてそのトランザクションの開始時点のデータベースにコミット済みのものだけということです。\nserializability\nこの分離レベルはトランザクションが並行して実行されていても、最終的な答えはそれぞれが1つずつ順番に、並行ではなく実行された場合と同じになることを保証します。\n日本語で「分離レベル」を検索すると snapshot isolation の代わりに repeatable read が出てくる事が多い。\nしかし repeatable read の名前は実装によって意味が違っていたりして扱いが難しいらしい。\n分離レベルと race condition の関係 以下に各分離レベルとトランザクションの並行性の問題 (race condition) の関係を示す。\n各 race condition の説明については割愛するが、複数のトランザクションが並行して実行されることにより起こりうる期待されていない挙動だと思えばよい。\n○はその分離レベルにおいてその race condition が発生しないことを示す。","title":"Spark DataFrame クエリの弱い分離レベル"},{"content":"東京の IT 企業で働く中年エンジニアが勉強したことや考えていることなどを書きます。\nこのブログは Hugo で作成しました。\nTheme には PaperMod を利用しています。\n","permalink":"https://soonraah.github.io/about/","summary":"東京の IT 企業で働く中年エンジニアが勉強したことや考えていることなどを書きます。\nこのブログは Hugo で作成しました。\nTheme には PaperMod を利用しています。","title":"About"},{"content":"はじめに Apache Spark 3.0.0 がリリースされました。\nSpark Release 3.0.0 release note を見て個人的に気になったところなど簡単に調べました。\n書いてみると Databricks の記事へのリンクばっかになってしまった…\n全体感 こちらの記事を読めば全体感は OK.\nIntroducing Apache Spark 3.0 公式の release note には\nPython is now the most widely used language on Spark.\nとあってそうなん？ってなったけど、こちらの記事だと\nPython is now the most widely used language on Spark and, consequently, was a key focus area of Spark 3.0 development. 68% of notebook commands on Databricks are in Python.\nと書いてありどうやら Databricks の notebook の話らしく、だったらまあそうかなという感じ。\nプロダクトコードへの実装というよりは、アドホック分析や検証用途の話なんでしょう。\n[Project Hydrogen] Accelerator-aware Scheduler SPARK-24615 Spark 上で deep learning できるようにすることを目指す Project Hydrogen、その3つの大きな目標のうちの一つ。\nBig Data LDN 2018: PROJECT HYDROGEN: UNIFYING AI WITH APACHE SPARK YARN や Kubernetes では GPU や FPGA を扱えるようになっているので Spark でも扱えるようにしたいというモチベーション。\nSpark のドキュメント によると\nFor example, the user wants to request 2 GPUs for each executor. The user can just specify spark.executor.resource.gpu.amount=2 and Spark will handle requesting yarn.io/gpu resource type from YARN.\nのようにして executor に GPU リソースを要求できるみたいです。\nAdaptive Query Execution SPARK-31412 平たく言うと実行時に得られる統計情報を使って plan を最適化すると、静的に生成された plan より効率化できるよねという話。\nspark.sql.adaptive.enabled=true にすることで有効になります。\n処理の途中で中間生成物が materialize されるタイミングで、その時点の統計情報を使って残りの処理を最適化する、というのを繰り返します。\nSpark 3.0.0 では以下3つの AQE が実装されました。\nCoalescing Post Shuffle Partitions Converting sort-merge join to broadcast join Optimizing Skew Join Spark 2 以前だとこのあたりは実行しつつチューニングするような運用になりがちでした。\n特に skew の解消は salt を追加したりなど面倒だったりします。\nこれらが自動で最適化されるというのは運用上うれしいところ。\n急なデータ傾向の変化に対しても自動で最適化して対応できるという面があります。\nAQE に関してもやはり Databricks の解説記事がわかりやすいです。\n図もいい感じ。\nAdaptive Query Execution: Speeding Up Spark SQL at Runtime Dynamic Partition Pruning SPARK-11150 こちらも AQE 同様にクエリのパフォーマンスを改善する目的で導入されたもの。\n改善幅は AQE より大きいようです。\nやはり実行時の情報を使って partition pruning を行い、不要な partition の参照を減らすという方法。\n主に star schema における join 時のように、静的には partition pruning が行えない場合を想定しています。\n比較的小さいことが多いと思われる dimension table 側を broadcast する broadcast join において、broadcast された情報を fact table の partition pruning に利用するというやり口。\nDynamic Partition Pruning in Apache Spark Structured Streaming UI SPARK-29543 \u0026ldquo;Structured Streaming\u0026rdquo; というタブが UI に追加された件。\nSpark のドキュメントに例があります。\nStructured Streaming Tab\nSpark 2.4 系では Structured Streaming を動かしていてもせいぜい job や stage が増えていくという味気ないものしか見えませんでした。\nSpark 3.0.0 で実際に動かしてみたけど欲しかったやつ！という感じ。\nストリーム処理では入力データ量の変化の可視化がマストだと思ってます。\nCatalog plugin API SPARK-31121 SPIP: Spark API for Table Metadata これまでは CTAS (Create Table As Select) の操作はあったが、外部のデータソースに対して DDL 的な操作をする API が足りていませんでした。\nCTAS も挙動に実装依存の曖昧さがありました。\nそこで create, alter, load, drop 等のテーブル操作をできるようにしたという話。\nドキュメントの DDL Statements のあたりを読め何ができるかわかります。\n以前のバージョンでも一部のデータソースについてはできた模様 (ex. Hive)。\n今の自分の業務では Spark から DDL を扱うようなことはしないのでそれほど恩恵は感じられません。\nnotebook からアドホックな Spark のバッチを動かすというような使い方をしていればうれしいかもしれません。\nAdd an API that allows a user to define and observe arbitrary metrics on batch and streaming queries SPARK-29345 クエリの途中の段階で何らかの metrics を仕込んでおいて callback 的にその metrics にアクセスできる仕組み。\nDataset#observe() の API ドキュメント を読むのが一番早いです。\nこの例ではストリーム処理を扱っているが、バッチ処理の例を自分で書いて試してみました。\n// Register listener spark .listenerManager .register(new QueryExecutionListener { override def onSuccess(funcName: String, qe: QueryExecution, durationNs: Long): Unit = { val num = qe.observedMetrics .get(\u0026#34;my_metrics\u0026#34;) .map(_.getAs[Long](\u0026#34;num\u0026#34;)) .getOrElse(-100.0) println(s\u0026#34;num of data: $num\u0026#34;) } override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {} }) // Make DataFrame val df = Seq .range(0, 1000) .map((_, Seq(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;)(Random.nextInt(3)), math.random())) .toDF(\u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;, \u0026#34;value\u0026#34;) // Observe and process val dfResult = df .observe(\u0026#34;my_metrics\u0026#34;, count($\u0026#34;*\u0026#34;).as(\u0026#34;num\u0026#34;)) .groupBy($\u0026#34;type\u0026#34;) .agg(avg($\u0026#34;value\u0026#34;).as(\u0026#34;avg_value\u0026#34;)) // Run dfResult.show これを動かしたときの出力は次のようになりました。\n+----+------------------+ |type| avg_value| +----+------------------+ | c|0.5129435063033314| | b|0.4693004460694317| | a|0.4912087482418599| +----+------------------+ num of data: 1000 observe() はその出力の DataFrame に対して schema やデータの中身を変更することはありません。\nmetrics を仕込むのみ。\nlogical plan を出力してみると observe() を入れることにより途中に CollectMetrics という plan が挿入されていました。\nソースを見ると accumulator を使っている模様。\nなので observe() の集計のみで一度 job が動くわけではなく、クエリが2回走るという感じではありません。\n全体の処理の中でひっそりと accumulator で脇で集計しておくといった趣でしょうか。\nこれは結構有用だと思います。\n例えば何らかの集計をやるにしてもその最初や途中で、例えば入力データは何件あったか？みたいなことをログに出しておきたいことがあります。\nというか accumulator で頑張ってそういうものを作ったことがある…\nこれがフレームワーク側でサポートされるのはうれしいです。\nまとめ 2つのダイナミックな最適化に期待大。\n気が向いたら追加でまた調べるかもしれません。\n","permalink":"https://soonraah.github.io/posts/study-spark-3-0-0/","summary":"はじめに Apache Spark 3.0.0 がリリースされました。\nSpark Release 3.0.0 release note を見て個人的に気になったところなど簡単に調べました。\n書いてみると Databricks の記事へのリンクばっかになってしまった…\n全体感 こちらの記事を読めば全体感は OK.\nIntroducing Apache Spark 3.0 公式の release note には\nPython is now the most widely used language on Spark.\nとあってそうなん？ってなったけど、こちらの記事だと\nPython is now the most widely used language on Spark and, consequently, was a key focus area of Spark 3.0 development. 68% of notebook commands on Databricks are in Python.\nと書いてありどうやら Databricks の notebook の話らしく、だったらまあそうかなという感じ。","title":"Apache Spark 3.0.0 について調べた"}]