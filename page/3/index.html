<!doctype html><html lang=ja dir=auto><head><meta name=generator content="Hugo 0.138.0"><script async src="https://www.googletagmanager.com/gtag/js?id=G-NSEGH2YT17"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NSEGH2YT17")</script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Froglog</title>
<meta name=description content="blog"><meta name=author content="soonraah"><link rel=canonical href=https://soonraah.github.io/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://soonraah.github.io/favicon2.ico><link rel=icon type=image/png sizes=16x16 href=https://soonraah.github.io/image/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://soonraah.github.io/image/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://soonraah.github.io/static/image/favicon/apple-touch-icon.png><link rel=mask-icon href=https://soonraah.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://soonraah.github.io/index.xml><link rel=alternate type=application/json href=https://soonraah.github.io/index.json><link rel=alternate hreflang=ja href=https://soonraah.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://soonraah.github.io/"><meta property="og:site_name" content="Froglog"><meta property="og:title" content="Froglog"><meta property="og:description" content="blog"><meta property="og:locale" content="ja"><meta property="og:type" content="website"><meta property="og:image" content="https://soonraah.github.io/image/brand/soonraah_full.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://soonraah.github.io/image/brand/soonraah_full.png"><meta name=twitter:title content="Froglog"><meta name=twitter:description content="blog"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Froglog","url":"https://soonraah.github.io/","description":"blog","logo":"https://soonraah.github.io/favicon2.ico","sameAs":["https://twitter.com/soonraah","https://github.com/soonraah"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://soonraah.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://soonraah.github.io/image/brand/favicon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://soonraah.github.io/about/ title=About><span>About</span></a></li><li><a href=https://soonraah.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://soonraah.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://soonraah.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://soonraah.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/photo/xavier-von-erlach-AU2goNvfyWU-unsplash.jpg alt="Maturing cheese at the Alpage des Lachiores, Val d'Hérens."></figure><header class=entry-header><h2 class=entry-hint-parent>成熟フェーズの事業におけるデータサイエンティスト</h2></header><div class=entry-content><p>ポエムです。
事業フェーズごとのデータサイエンティストの役割 まずはこちらの発表。
事業立ち上げにデータサイエンティストは必要なのか？ | CA BASE NEXT とても納得できる内容だった。
一部抜き出して要約すると
事業の立ち上げフェーズ データがまだなかったり、整備されていない状態 データサイエンスによる改善がしにくい 事業のグロースフェーズ 大規模なデータが使える状態 データサイエンスによる改善がやりやすい とのこと。異論はない。
では事業が立ち上がり、グロースが落ち着いたその後の成熟フェーズではどうなのだろうかという話。
成熟フェーズにおける改善の難しさ 端的に言うと成熟フェーズでは ML によるさらなる改善は困難になってくると思う。
ここで言う成熟フェーズにおいてはプロダクトの進化とともに機械学習もそれなりに適用されてきたものとする。
成熟フェーズということで既存の ML モデル、特にビジネスインパクトが大きい箇所はこれまでいろいろな改善が重ねられてきている。
そのモデルの精度をさらに上げるとなると、より高度なアルゴリズム、より複雑なデータ等を扱う必要がある。
しかし技術的によっぽど大きなブレークスルーがない限りは精度の改善幅はグロースフェーズよりもかなり小さいものとなるだろう。
精度が上がれば上がるほど、次の1%を上げるためのコストは大きくなっていく。
改善が進むほどに次の改善業務は困難になっていく。
(蛇足だがある程度大きな組織でなければ高度で state-of-the-art な ML アルゴリズムは運用しない方がいいと考えている)
では既存ではない新しい適用箇所に ML を使えばいいのではとなるかもしれない。
しかしやはりそれも難しい。
ビジネスインパクトが大きく、かつわかりやすい適用箇所にはおそらくすでに ML が適用されているからだ。
その状態から更によい適用箇所を見つけるには深いドメイン知識が必要になったりする。
という感じでいわゆるキラキラした「ML でビジネスをドライブ！」みたいなことは成熟フェーズでは難しいことが多いのではないか。
しかしデータサイエンティストにやることがないわけではない。
成熟フェーズで何ができるか ぱっと思いつくのは次のような仕事。
データドリブンな施策の立案・評価 これは事業フェーズ問わずあるべき ドメイン知識が必要 ML エンジニアリング パイプラインの改善や属人性をなくすお仕事 ML モデルの受動的なメンテナンス 精度が変化したときの調査 内部的・外部的要因によるデータの変化への対応 やっぱり ML モデルの精度改善 成熟フェーズということでビジネスもスケールしていれば 0.1% の精度改善でも売上的なインパクトは大きいかもしれない いわゆる狭義のデータサイエンスではなく、ドメイン知識であったりアナリストやエンジニア的な視点が絡んだ仕事が増えてくる。
よくある「ML だけじゃなく◯◯もできると強いよね」みたいな話になってしまった。
おわりに …という話が少し前に Twitter で知人との話題に上がった。
若者が歴史的にいろんな人が改善に取り組んできた ML モデルの改善にアサインされている、というのが近いところで観測されたのでたいへんそうだなあと思いつつこの件を思い出したので書いてみた。
...</p></div><footer class=entry-footer><span title='2021-07-12 22:00:00 +0900 JST'>7月 12, 2021</span>&nbsp;·&nbsp;soonraah</footer><a class=entry-link aria-label="post link to 成熟フェーズの事業におけるデータサイエンティスト" href=https://soonraah.github.io/posts/ds-in-maturation-phase/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/photo/simone-pellegrini-L3QG_OBluT0-unsplash.jpg alt="Back of Hercules in main square in Florence, Italy."></figure><header class=entry-header><h2 class=entry-hint-parent>Apache Flink の Backpressure の仕組みについて調べた</h2></header><div class=entry-content><p>ストリーム処理のフレームワークが備える backpressure という機能がある。
このポストでは Apache Flink の backpressure について調べたことを記載する。
Backpressure の目的 backpressure はストリーム処理システムにおける負荷管理の仕組みの一つ。
一時的な入力データ量の増大に対応する。
インターネットユーザの行動履歴やセンサーデータなどは常に一定量のデータが流れているわけではなく、単位時間あたりのデータ量は常に変動している。
一時的にスパイクしてデータ量が増大するようなことも起こりうる。
複数の operator からなる dataflow graph により構成されるストリーム処理システムにおいては、処理スピードのボトルネックとなる operator が存在する。
一時的に入力データ量が増えてボトルネックの operator の処理速度を上回ってしまった場合に、データの取りこぼしが発生するのを防ぐのが backpressure の目的となる。
Backpressure の仕組み Buffer-based ここでは以前のブログでも紹介した、ストリーム処理で必要とされる機能について書かれた Fragkoulis et al. 1 を引用して一般論としての backpressure について述べたい。
上流／下流の operator をそれぞれ producer, consumer とする。
producer, consumer (それらの subtask と言ってもいいかも) がそれぞれ異なる物理マシンに deploy されているケースが Figure 12b となる。
各 subtask は input と output の buffer を持っており、
producer は処理結果を output buffer に書き出す TCP 等の物理的な接続でデータを送信 consumer 側の output buffer にデータを格納 consumer がそれを読み込んで処理する というような流れになる。
buffer はマシンごとの buffer pool で管理されており、input/output で buffer が必要となった場合はこの buffer pool に buffer が要求される。
...</p></div><footer class=entry-footer><span title='2021-02-28 21:00:00 +0900 JST'>2月 28, 2021</span>&nbsp;·&nbsp;soonraah</footer><a class=entry-link aria-label="post link to Apache Flink の Backpressure の仕組みについて調べた" href=https://soonraah.github.io/posts/backpressure-for-flink/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/photo/claudia-chiavazza-N9vsB6OEeKM-unsplash.jpg alt=Lake></figure><header class=entry-header><h2 class=entry-hint-parent>データレイク関連の OSS - Delta Lake, Apache Hudi, Apache Kudu</h2></header><div class=entry-content><p>はじめに 前回のポストではデータレイクとはどういうものかというのを調べた。
今回はデータレイクの文脈でどのような OSS が注目されているのかを見ていきたい。
以下は NTT データさんによる講演資料であり、その中で「近年登場してきた、リアルタイム分析に利用可能なOSSストレージレイヤソフト」というのが3つ挙げられている。
大規模データ活用向けストレージレイヤソフトのこれまでとこれから（NTTデータ テクノロジーカンファレンス 2019 講演資料、2019/09/05） from NTT DATA Technology & Innovation Delta Lake Apache Hudi Apache Kudu これらはすべて論理的なストレージレイヤーを担う。
こちらの講演資料に付け足すようなこともないかもしれないが、このポストではデータレイクという文脈から自分で調べて理解した内容をまとめるということを目的にする。
当然 Hadoop, Hive, Spark 等もデータレイクの文脈において超重要だが、「データレイク」という言葉がよく聞かれるようになる前から普及していたのでこのポストでは触れないことにする。
Delta Lake https://delta.io/
Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.
Delta Lake
Delta Lake は Apache Spark の読み書きに ACID な transaction を提供するストレージレイヤーの OSS である。
Databricks が作り、2019年4月に v0.1.0 がリリースされたのが最初だ。
使い方はめちゃ簡単で、dependency を設定した上で Spark で
...</p></div><footer class=entry-footer><span title='2021-01-26 08:00:00 +0900 JST'>1月 26, 2021</span>&nbsp;·&nbsp;soonraah</footer><a class=entry-link aria-label="post link to データレイク関連の OSS - Delta Lake, Apache Hudi, Apache Kudu" href=https://soonraah.github.io/posts/oss-for-data-lake/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/photo/stephen-walker-mhqoyciC5I0-unsplash.jpg alt=Lake></figure><header class=entry-header><h2 class=entry-hint-parent>いまさらながらのデータレイク</h2></header><div class=entry-content><p>最近よく聞かれるようになった「データレイク」という概念にあまりついていけていなかったため、いまさらながらざっと調べてみた。
データレイクとは Wikipedia によると最初にこの言葉を使ったのは Pentaho 社の CTO である James Dixon らしい。
その時の彼のブログ (10年前…) を読むと、既にあったデータマートに対して
Only a subset of the attributes are examined, so only pre-determined questions can be answered. The data is aggregated so visibility into the lowest levels is lost
–Pentaho, Hadoop, and Data Lakes - James Dixon’s Blog というような問題意識からデータレイクというコンセプトを提案したようだ。
最近？のデータレイクについてはベンダー等の記事が参考になる。
データレイクとは - AWS データレイクとは？ - talend データレイクとは？データレイクの落とし穴と効果 - Informatica 書籍だと『AWSではじめるデータレイク: クラウドによる統合型データリポジトリ構築入門』がいいだろうか。
データレイクの概要と AWS が考えている構築・運用がざっとわかる。
Amazon で検索した限りだと現時点でタイトルに「データレイク」を含む和書はこれのみだった。
...</p></div><footer class=entry-footer><span title='2020-12-31 20:00:00 +0900 JST'>12月 31, 2020</span>&nbsp;·&nbsp;soonraah</footer><a class=entry-link aria-label="post link to いまさらながらのデータレイク" href=https://soonraah.github.io/posts/what-is-a-data-lake/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/photo/mika-baumeister-Wpnoqo2plFA-unsplash.jpg alt=CSV></figure><header class=entry-header><h2 class=entry-hint-parent>Apache Flink の DataStream API 利用時の CSV ファイル読み込み</h2></header><div class=entry-content><p>ストリーム処理における CSV ファイルの読み込み Apache Flink は unbounded なストリームデータを処理するためのフレームワークだ。
しかし現実的な application を開発する場合、ストリームデータに加えて static なファイルや DB 等を読み込みたいこともある。
star schema における dimension table 的な情報をストリームに結合したい場合 等が考えられる。
このポストでは Flink で DataStream API ベースでの実装において CSV ファイルを読むことを考える。
Flink は現時点の stable である v1.11 を想定。
CSV ファイルを読む方法 DataStream API ベースの実装で CSV ファイルを読むには StreamExecutionEnvironment のメソッドである readFile() を使う。
overload された同名のメソッドがいくつか存在するが、次の2つの引数が特に重要だろう。
まず1つめは FileInputFormat&lt;OUT> inputFormat であり、こちらは data stream の生成に用いる入力フォーマットを指定する。
おそらく最も一般的なのが TextInputFormat だと思われる。
もちろん単なる text として CSV ファイルを読み込み、後続の処理で各レコードを parse することも可能だが CSV 用の入力フォーマットがいくつか用意されているようだ。
PojoCsvInputFormat RowCsvInputFormat TupleCsvInputFormat なんとなく名前でわかると思うが、それぞれ readFile() の結果として返される DataStreamSource が内包する型が異なる。
これについては後述の実験にて確認する。
...</p></div><footer class=entry-footer><span title='2020-12-01 00:30:00 +0900 JST'>12月 1, 2020</span>&nbsp;·&nbsp;soonraah</footer><a class=entry-link aria-label="post link to Apache Flink の DataStream API 利用時の CSV ファイル読み込み" href=https://soonraah.github.io/posts/read-csv-by-flink-datastream-api/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/photo/sharon-mccutcheon-8lnbXtxFGZw-unsplash.jpg alt=Profit></figure><header class=entry-header><h2 class=entry-hint-parent>機械学習の精度と利益と倫理とイシューと</h2></header><div class=entry-content><p>ちょっと昔話 かつて参画したプロジェクトの話。
そのプロジェクトでは他社から受注した受託開発として機械学習系のシステムを開発していた。
当時としては新しいフレームワークを使い、かなり頑張ってなんとか納期内で完成させた。
その中の1つの機能として A/B テストができるようにしていた。
パラメータチューニングによりパフォーマンスを改善することを想定していた。
しかし結局その機能は使われることがなかった。
なぜか。
A/B テストを実施するためのクライアントの追加の予算がつかなかったためである。
受託なのでなおさらなのだが、売上にならなければ工数をかけるこはできない。
工数を使ってパフォーマンス改善することはできなかった。
手はあるのに。
機械学習の精度は必ずしも利益に結びつかない この昔話で何が言いたいかというと、機械学習の精度改善は必ずしも利益に結びつかないということである。
そのことを示しているとても素晴らしい資料がこちら。
機械学習の精度と売上の関係 from Tokoroten Nakayama 前述の昔話の例はこの資料で言うところの③ロジスティック型 (=外注) となる。
いったん売上が立った後、追加予算がつかなかったので精度改善では売上は増えなかったのだ。
倫理感による精度改善 受託開発を主としている組織であれば工数にはシビアなので、売上の立たない工数をかけることはあまりないだろう。
(よっぽどの炎上鎮火とかでなければ)
しかし自社で製品やサービスを作って提供しているような組織の場合、利益にならない精度改善をしているのを時折見かける。
なぜそのようなことが起こるかと言うと多くの場合はデータサイエンティスト／機械学習エンジニアとしての倫理感からなのではないだろうか。
「◯◯予測という機能なのでできるだけ良い予測精度を示すべきだ」
「ユーザには気づかれない部分だが精度が悪いので改善したい」
倫理感や興味が先行してしまっているのだ。
しかしその精度を上げた先に利益があるとは限らない。
機械学習で職を得ている人間は自分の仕事を機械学習の精度を上げるゲームだとみなす傾向があるように思う。
例えばインターネット広告の CTR 予測。
これは予測精度が高いほど利益は改善するし、広告主に価値も提供できる。
精度改善に倫理と利益が伴っている、とても機械学習がハマる例だと思う。
本来はこれらを兼ね備えているのが良い適用先であるはずだ。
イシューは行き渡っているのか 利益に結びつかない、または間接的にしか結びつかないような精度改善をやることが許されるというのは組織に余裕があるということで悪いことではないのかもしれない。
しかし単によいイシューの設定ができてないだけという可能性もある。
自社で製品やサービスを作って提供しているような組織において、単純なロジスティック回帰でコアなところのビジネスを大きく加速させることができた時期を過ぎると機械学習で解くのに適したよい問題を恒常的に見つけ出すのは実は難しいのではないだろうかと最近考えるようになった。
ビジネスの領域拡大よりも既存領域への機械学習の適用の方が速いということは十分ありうる。
もちろんチームの規模にもよる。
機械学習チームの人的リソースの規模に対して機械学習で解くべきよいイシューを見つけ出せているのか、ということだ。
少し前にちょっと話題になったこちらの件もイシューが大事だと言っている。
全ての機械学習の論文は新しいアルゴリズムを提案しているのですか？ - Quora キャリアの行く末 事業会社においてビジネスの領域拡大よりも既存領域への機械学習の適用の方が速く、よいイシューを提供しにくいということがよく起こるのであれば、機械学習チームのリソースは余剰気味になりやすいということになる。
これが続くと今後機械学習しかやらない人材の市場価値は下がっていくのかもしれない。
もしくは自社で製品やサービスを持っている組織ではなく、受託開発やコンサルが主戦場になっていくのかもしれない。
何にせよ特定のプロダクトに commit したいのであれば機械学習エンジニアは機械学習以外のスキルも磨いていく必要があるように思う。
おわりに 見える範囲にいる人が利益にならない精度改善をしているのを横目で見てこのようなことを考えていた。
難しいけどできるだけ金を生んでいきたい。</p></div><footer class=entry-footer><span title='2020-11-12 09:30:00 +0900 JST'>11月 12, 2020</span>&nbsp;·&nbsp;soonraah</footer><a class=entry-link aria-label="post link to 機械学習の精度と利益と倫理とイシューと" href=https://soonraah.github.io/posts/ml-accuracy-profit-ethic-issue/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/photo/nathan-anderson-8X1-pcDF8l0-unsplash.jpg alt=Stream></figure><header class=entry-header><h2 class=entry-hint-parent>ストリーム処理システムに求められる機能性、および Apache Flink におけるその対応</h2></header><div class=entry-content><p>はじめに このポストではストリーム処理の survay 論文の話題に対して Apache Flink における例を挙げて紹介する。
論文概要 Fragkoulis, M., Carbone, P., Kalavri, V., & Katsifodimos, A. (2020). A Survey on the Evolution of Stream Processing Systems.
2020年の論文。
過去30年ぐらいのストリーム処理のフレームワークを調査し、その発展を論じている。
ストリーム処理に特徴的に求められるいくつかの機能性 (functionality) についてその実現方法をいくつか挙げ、比較的古いフレームワークと最近のフレームワークでの対比を行っている。
このポストのスコープ このポストでは前述のストリーム処理システムに求められる機能性とそれがなぜ必要となるかについて簡単にまとめる。
論文ではそこからさらにその実現方法がいくつか挙げられるが、ここでは個人的に興味がある Apache Flink ではどのように対処しているかを見ていく。
ちなみに論文中では Apache Flink はモダンなフレームワークの1つとしてちょいちょい引き合いに出されている。
ここでは Flink v1.11 をターゲットとする。
以下では論文で挙げられている機能性に沿って記載していく。
Out-of-order Data Management Out-of-order ストリーム処理システムにやってくるデータの順序は外的・内的要因により期待される順序になっていないことがある。
外的要因としてよくあるのはネットワークの問題。
データソース (producer) からストリーム処理システムに届くまでのルーティング、負荷など諸々の条件により各レコードごとに転送時間は一定にはならない。
各 operator の処理などストリーム処理システムの内的な要因で順序が乱されることもある。
out-of-order は処理の遅延や正しくない結果の原因となることがある。
out-of-order を管理するためにストリーム処理システムは処理の進捗を検出する必要がある。
“進捗” とはある時間経過でレコードの処理がどれだけ進んだかというもので、レコードの順序を表す属性 A (ex. event time) により定量化される。
ある期間で処理された最古の A を進捗の尺度とみなすことができる。
...</p></div><footer class=entry-footer><span title='2020-11-07 16:00:00 +0900 JST'>11月 7, 2020</span>&nbsp;·&nbsp;soonraah</footer><a class=entry-link aria-label="post link to ストリーム処理システムに求められる機能性、および Apache Flink におけるその対応" href=https://soonraah.github.io/posts/functionality-of-streaming-system/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/photo/jon-flobrant-rB7-LCa_diU-unsplash.jpg alt=Stream></figure><header class=entry-header><h2 class=entry-hint-parent>バッチ処理おじさんがストリーム処理のシステムを開発するにあたって調べたこと</h2></header><div class=entry-content><p>ほとんどバッチ処理しか書いたことのない者だがストリーム処理のシステムを開発することになった。
それにあたって独学で調べたことなどまとめておく。
ストリーム処理とは そもそも “ストリーム処理” とは何を指しているのか。
以下の引用が簡潔に示している。
a type of data processing engine that is designed with infinite data sets in mind. Nothing more.
– Streaming 101: The world beyond batch
こちらは “streaming system” について述べたものだが、つまり終わりのないデータを扱うのがストリーム処理ということである。
例えば web サービスから生まれ続けるユーザ行動ログを逐次的に処理するというのがストリーム処理。
web サービスが終了しないかぎりはユーザ行動ログの生成には終わりがない。
これに対して “1日分のユーザ行動ログ” 等のように有限の量のデータを切り出して処理する場合、これはバッチ処理となる。
ストリーム処理とバッチ処理の違いは扱うデータが無限なのか有限なのかということだ。
この後触れていくが、この終わりのないデータを継続的に処理し続けるというところにバッチ処理にはない難しさがある。
なぜストリーム処理なのか なぜストリーム処理なのか。
ひとえに逐次的な入力データに対する迅速なフィードバックが求められているからと言えるだろう。
迅速なフィードバックがビジネス上のメリットとなることは自明だ。
SNS の配信 カーシェアリングにおける配車や料金設定 クレジットカードや広告クリックなどの不正検知 もしこれらの application が例えば hourly のバッチ処理で実装されていたらどうだろうか。
まあ待っていられない。
...</p></div><footer class=entry-footer><span title='2020-09-06 16:30:00 +0900 JST'>9月 6, 2020</span>&nbsp;·&nbsp;soonraah</footer><a class=entry-link aria-label="post link to バッチ処理おじさんがストリーム処理のシステムを開発するにあたって調べたこと" href=https://soonraah.github.io/posts/study-streaming-system/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/photo/jason-dent-JVD3XPqjLaQ-unsplash.jpg alt="A/B Testing"></figure><header class=entry-header><h2 class=entry-hint-parent>A/B テストの運用が重くてつらいという話</h2></header><div class=entry-content><p>前提 ここでは web システムで使われている機械学習のモデルやアルゴリズムを改善するための online の A/B テストを考える。
具体的に述べると web 広告における CTR 予測や EC サイトのレコメンデーション等が対象である。
よくあるやつ。
web システムにおいて online の A/B テストは KPI 改善の根幹でありとても重要だ。
それが重くなるとつらい、という話。
ここで「重い」と言っているのは計算資源のことではなく、A/B テストを実施する担当者の運用コストについて。
A/B テストの運用が重い場合のデメリット デメリット 1. KPI 改善が遅くなる デメリットと言えばこれが一番大きい。
単純に A/B テストを1回まわすのに時間がかかってしまうし、それがゆえに online の A/B テストに入るまでの offline のテストが厚くなりここでも時間がかかってしまう。
KPI 改善に時間がかかるというのはつまり売上や利益を大きくするのに時間がかかってしまうということである。
デメリット 2. KPI 改善における offline テストの比重が大きくなる 前述のとおりだが online の A/B テストが重いとそこで失敗できなくなり、結果としてその前段の offline のテストを厚くするということになる。
offline のテストが厚いことの何が問題だろうか。
ここで前提としている CTR 予測やレコメンデーションのようなタスクの場合、offline のデータは既存のモデルやアルゴリズムの影響を受けることになる。
例えばレコメンデーションの場合を考えると、新しいモデルを offline で評価するための実験データの正例 (コンテンツの閲覧等) は既存モデルによって生み出される。
既存モデルが「このコンテンツがいいよ」といってユーザに出したリスト、その中からコンテンツの閲覧が行われ正例となるからだ。
このような状況下での offline テストにおいては既存モデルと近い好みを持ったモデルのスコアが高くなる傾向がある。
...</p></div><footer class=entry-footer><span title='2020-08-23 12:30:00 +0900 JST'>8月 23, 2020</span>&nbsp;·&nbsp;soonraah</footer><a class=entry-link aria-label="post link to A/B テストの運用が重くてつらいという話" href=https://soonraah.github.io/posts/heavy-ab-testing-operation/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://soonraah.github.io/image/logo/flink_squirrel_color_logo.png alt="Apache Flink"></figure><header class=entry-header><h2 class=entry-hint-parent>Apache Flink の Temporary Table Function を用いた stream data と static data の join</h2></header><div class=entry-content><p>前回の記事 では Apache Flink における stream data と static data の join において、DataStream API における broadcast state pattern を使う方法を示した。
今回の記事では Table API の temporal table function を用いた実験を行う。
Table API Table API は名前のとおりで class Table を中心として SQL-like な DSL により処理を記述するという、DataStream API より high-level な API となっている。
これらの関係は Apaceh Spark の RDD と DataFrame (DataSet) の関係に似ている。
SQL-like な API で記述された処理が実行時に最適化されて low-level API の処理に翻訳されるところも同じだ。
RDB の table の概念を元にしているものと考えられるが、本質的に table の概念とストリーム処理はあまりマッチしないと思う。
table はある時点のデータセット全体を表すのに対し、ストリーム処理ではやってくるレコードを逐次的に処理したい。
ここを合わせているため、ストリーム処理における Table API による処理の挙動の理解には注意が必要だ。
Streaming Concepts 以下のドキュメントを確認しておきたい。
...</p></div><footer class=entry-footer><span title='2020-08-16 00:30:00 +0900 JST'>8月 16, 2020</span>&nbsp;·&nbsp;soonraah</footer><a class=entry-link aria-label="post link to Apache Flink の Temporary Table Function を用いた stream data と static data の join" href=https://soonraah.github.io/posts/flink-join-by-temporal-table-function/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://soonraah.github.io/page/2/>«&nbsp;前へ&nbsp;
</a><a class=next href=https://soonraah.github.io/page/4/>次へ&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://soonraah.github.io/>Froglog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>